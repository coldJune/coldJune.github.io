<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[架构设计师备考]]></title>
    <url>%2F2021%2F07%2F22%2F%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%B8%88%E5%A4%87%E8%80%83%2F</url>
    <content type="text"><![CDATA[软考备考 计算机组成与体系结构Flynn分类法 体系结构类型 结构 关键特性 代表 单指令流单数据流(SISD) 控制部分:一个处理器:一个主存模块:一个 单处理器系统 单指令流多数据流(SIMD) 控制部分:一个处理器:多个主存模块:多个 各处理器以异步的形式执行同一条指令 并行处理机阵列处理机超级向量处理机 多指令流单数据流(MISD) 控制部分:多个处理器:一个主存模块:多个 被证明不可能，至少是不实际 目前没有，有文献称流水线计算机为此类 多指令多数据流(MIMD) 控制部分:多个处理器:多个主存模块:多个 能够实现作业、任务、指令等各级全面并行 多处理机系统多计算机 阵列处理机：适合数组类的运算，比如二维数组 CISC与RISC 指令系统类型 指令 寻址方式 实现方式 其它 CISC(复杂) 数量多，使用频率差别大，可变长格式 支持多种 微程序控制技术(微码) 研制周期长 RISC(精简) 数量少，使用频率接近，订长格式，大部分为单周期指令，操作寄存器，只有Load/Store操作内存 支持方式少 增加了通用寄存器；硬布线逻辑控制为主；适合采用流水线 优化编译，有效支持高级语言 流水线存储系统 Cache 提高CPU数据输入输出的速率，突破冯·诺伊曼瓶颈，即CPU与存储系统间数据传送带宽的限制 在计算机存储系统体系中，Cache是访问速度最快的层次(因寄存器在CPU中，大多数时候未将寄存器放在存储体系中讨论，最快的有寄存器选寄存器，没有选Cache) 使用Cache改善性能的依据是程序的局部性原理 如果以$h$代表对Cache的访问命中率，$t_1$表示Cache的周期时间，$t_2$表示主存储器周期时间，以读操作为例，使用“Cache+主存储器”的系统的平均周期为$t_3$，则: t_3=h\times t_1 +(1-h)\times t_2其中，$(1-h)$又称为失效率(未命中率) 局部性原理 时间局部性 空间局部性 工作集理论：工作集是进程运行时被频繁访问的页面集合 主存分类 随机存取存储器:掉电信息丢失 DRAM(Dynamic RAM, 动态RAM) -SDRAM SRAM(Static RAM, 静态RAM) 只读存储器:掉电信息不丢失 MROM(Mask ROM，掩模式ROM) PROM(Programmable ROM，一次可编程ROM) EPROM(Erasable PROM，可擦除的PROM) 闪速存储器(flash memory，闪存) 编址 例题内存地址从AC000H到C7FFFH，共有_K个地址单元，如果该内存地址按字(16bit)编址，由28片存储器芯片构成。已知构成此内存的芯片每片由16K个存储单元，则该芯片每个存储单元存储_位1 磁盘结构与参数 存取时间=寻道时间+等待时间(平均定位时间+转动延迟)寻道时间指磁头移动到磁道所需的时间；等待时间为等待读写的扇区转到磁头下方所用的时间 例题2 校验码并行处理系统配置与性能评价性能指标 字长和数据通路宽度 主存容量和存取速度 运算速度 主频与CPU时钟周期 CPI与IPC MIPS与MFLOPS MIPS = 指令条数/(执行时间\times 10^6)=主频/CPI=主频\times IPCMFLOPS = 浮点操作次数/(执行时间\times 10^6) 吞吐量与吞吐率 响应时间与完成时间 兼容性阿姆达尔解决方案 对系统中某组件采用某种更快的执行方式，所获得的系统性能的改变程度，取决于该组件被使用的频率，或所占总执行时间的比例。加速比计算公式如下： R=\frac{T_p}{T_i}=\frac{1}{(1-F_e)+F_e/S_e}其中，$T_p$表示不使用改进组件时完成整个任务的时间，$T_i$表示使用改进组件时完成整个任务的时间。加速比主要取决于两个因素： 在原有的系统上，能被改进的部分在总执行时间中所占的比例。这个值称为改进比例，记为$F_e$，它总小于1 通过改进的执行方式所取得的性能提高，即如果整个系统使用了改进的执行方式，那么系统的执行速度会有多少提高，这个值等于在原来条件下系统执行时间与使用改进组件后系统的执行时间之比，记为$S_e$，它总大于1 例题在计算机系统中，某一功能的处理时间为整个系统运行时间的50%，若使该功能的处理速度加快10倍，根据Amdahl定律，这样做可以使整个系统的性能提高_倍。若要使整个系统的性能提高1.5倍，则该功能的处理速度应加快__倍。3 性能评价方法 时钟频率法 CPU、片面性极大，早期方式 指令执行速度法 单位时间执行指令数，片面性大(只衡量加法指令) 等效指令速度法(吉普森混合法) 每条指令使用概率乘以每条指令执行时间，相当于取平均值，依然局限在运算能力 数据处理速率法(PDR) 会考虑存储交互性 综合理论性能法(CTP) 考虑每一个处理部件、计算单元的有效计算率，根据字长调整，比较平衡地取得每个计算单位理论的性能值 基准程序法 Dhrystone基准程序 Linpack基准程序 Whetstone基准程序 SPEC基准程序(SPECmark、SPECint、SPECfp、SPECrate) TPC基准程序 TPC-A：OLTP环境下的数据库和硬件性能 TPC-B：不包括网络的纯事务处理，模拟企业计算环境 TPC-C：联机订货系统 TPC-D、TPC-H、TPC-R：决策支持系统 TPC-E：大型企业信息服务系统 TPC-W：通过Internet进行市场服务和销售的商业行为 性能检测方法 软件监控：使用软件对系统性能数据进行采集分析，此方法会消耗较多的系统资源 硬件监控：使用专用硬件设备对系统性能进行采集分析，适用于高负载的计算机系统 操作系统基本原理进程管理 状态 前趋图 考虑哪些有先后，哪些可并行。前趋图描述一系列活动的先后约束关系。 同步与互斥互斥：在同一时刻只允许一个进程使用一个资源同步：速度有差异，在一定情况下等待 PV 操作 临界资源：诸进程间需要互斥方式对其进行共享的资源，如打印机、磁带机等临界区：每个进程中访问临界资源的那段代码称为临界区信号量：一种特殊的变量(控制临界资源是否能够访问，为1其实就是锁) 单缓冲区生产者、消费者问题PV原语描述，S1初始值为1，S2初始值为01234567891011生产者: 生产一个产品; P(S1); 送产品到缓冲区; V(S2);消费者: P(S2); 从缓冲区取产品; V(S1); 消费产品; PV操作与前趋图结合 死锁问题 一个进程等待一件不可能发生的事，则进程死锁。如果一个或多个进程产生死锁，就会造成系统死锁四大条件：环路等待、互斥、保持和等待、不剥夺死锁的预防：打破四大条件死锁的避免：有序资源分配法、银行家算法 银行家算法 当一个进程对资源的最大需求量不超过系统的资源数时可以接纳该进程进程可以分期请求资源，但请求的总数不能超过最大需求量当系统现有的资源不能满足进程尚需资源数时，对进程的请求可以推迟分配，但总能使进程在有限的时间里得到资源 例题 例题14 例题25 例题36 例题47系统有3个进程，A、B、C。这3个进程都需要5个系统资源。则系统至少有多少个资源不可能发生死锁。 例题58 存储管理页式存储 优点：利用率高，碎片小，分配及管理简单 缺点：增加了系统开销(查表)；可能产生抖动现象 段式存储 段页式存储 优点：空间浪费小，存储共享容易，存储保护容易，能动态链接 缺点：由于管理软件的增加，复杂性和开销也随之增加，需要的硬件以及占用的内容也有所增加，使得执行速度大大下降 快表快表是一块小容量的相联存储器(Associative Memory)，由高速缓存器组成，速度快，并且可以从硬件上保证按内容并行查找，一般用来存放当前访问最频繁的少数活动页面的页号。 页面置换算法 最优(Optimal,OPT)算法 随机(RAND)算法 先进先出(FIFO)算法：有可能产生抖动。例如432143543215序列，用3个页面，比4个缺页要少 最近最少使用(LRU)算法：不会抖动例题 例题19 例题210 例题311 文件管理索引文件结构 文件系统和树型目录结构 文件属性:只读文件属性(R)、存档属性(A)、系统文件(S)、隐藏文件(H) 文件名组成:驱动器号、路径、主文件名、扩展名 绝对路径:是从盘符开始的路径 相对路径:是从当前路径开始的路径若当前路径为D1，要求F2路径，则绝对路径为：/D1/W2/F2，相对路径：W2/F2 空闲存储空间的管理 空闲区表法(空闲文件目录) 空闲链表法 位示图法 成组连接法例题 例题112 例题213 设备管理数据传输控制方式 程序控制方式（程序查询方式）：最低级的，CPU介入最多—查询，外设处于被动 程序中断方式：外设可以主动触发，效率更高 DMA方式(直接存取控制方式)：由专门的DMA控制器控制外设与内存的数据交换 通道 输入输出处理机 微内核操作系统 实质 优点 缺点 单体内核 将图形、设备驱动及文件系统等功能全部在内核中实现，运行在内核状态和同一地址空间 减少进程间通信和状态切换的系统开销，获得较高的运行效率 内核庞大，占用资源较多且不易裁剪系统的稳定性和安全性不好 微内核 只实现基本功能，将图形系统、文件系统、设备驱动及通信功能放在内核之外 内核精炼，便于裁剪和移植系统服务程序运行在用户地址空间，系统的可靠性、稳定性和安全性较高可用于分布式系统 用户状态和内核状态需要频繁切换，从而导致系统效率不如单体内核 数据库系统三级模式-两级映射 数据库设计过程 E-R模型 集成的方法 多个局部E-R图一次集成 逐步集成，用累加的方式一次集成两个局部E-R 集成产生的冲突及解决方法 属性冲突：包括属性域冲突和属性取值冲突 命名冲突：包括同名异义和异名同义 结构冲突：包括同一对象在不同应用中具有不同的抽象，以及同一实体在不同局部E-R图中所包含的属性个数和属性排列次序不完全相同 E-R模型转关系模式 一个实体转换为一个关系模式 1:1联系：最少可以转成2个关系模式，一个实体对应一个关系模式，将联系放入任何一个实体都可 1:n联系：最少可以转成2个关系模式，一个实体对应一个关系模式，可以把联系记录在n的一端 m:n联系： 最少可以转成3个关系模式，一个实体对应一个关系模式，联系需要一个单独的关系模式记录 例题 例题114 关系代数 并、交、差、笛卡尔积、投影、选择、联结 交并差 笛卡尔积、投影、选择 联结 规范化理论 价值与用途：非规范化的关系模式，可能存在的问题包括：数据冗余，更新异常、插入异常、删除异常 函数依赖 设R(U)是属性U上的一个关系模式，X和Y是U的子集，r为R的任一关系，如果对于r中的任意两个元组u，v，只要有$u[X]=v[X]$，就有$u[Y]=v[Y]$,则称X函数决定Y，或称Y函数依赖于X，记为$X\rightarrow Y$ 键 求解候选键 将关系模式的函数依赖关系用“有向图”的方式表示 找入度为0的属性，并以该属性集合为起点，尝试遍历有向图，若能正常遍历图中所有结点，则该属性集即为关系模式的候选键 若入度为0的属性不能遍历图中所有结点，则需要尝试性地将一些中间结点(即有入度，也有出度的结点)并入入度为0的属性集中，直至该集合能遍历所有结点，集合为候选键 例题 例题115 例题216 例题317 范式 第一范式(1NF)：在关系模式R中，当且仅当所有域只包含原子值，即每个分量都是不可再分的数据项，则陈R是第一范式 第二范式(2NF)：当且仅当R是第一范式，且第一个非主属性完全依赖主键(不存在部分依赖)时，则称R是第二范式 第三范式(3NF)：当且仅当R是1NF，且E中没有非主属性传递依赖的码时，则称R是第三范式 BC范式(BCNF)：设R是一个关系模式，F是它的依赖集，R属于BCNF当且仅当其F中每个依赖的决定因素必定包含R的某个候选码 例题 例题118 模式分解 保持函数依赖分解 设数据库模式$\sigma = \{R1,R2,\dots,Rk\}$是关系模式R的一个分解，F是R上的函数依赖集，$\sigma$中每个模式Ri上的FD集是Fi。如果$\{F1,F2,\dots,Fk\}$与F是等价的(即相互逻辑蕴涵)，那么称分解$\sigma$保持FD。 无损分解 有损即不能还原，无损即可以还原 无损联接分解 指将一个关系模式分解成若干个关系模式后，通过自然联接和投影等运算仍能还原到原来的关系模式。 定理：如果R的分解为$\sigma={R_1,R_2}$，F为R所满足的函数依赖集合，分解$\sigma$具有无损联接性的充分必要条件是： R_1\cap R_2 \rightarrow (R_1-R_2)或 R_1 \cap R_2 \rightarrow (R_2-R_1)其中，$R_1\cap R_2 $表示模式的交，为$R_1$与$R_2$中公共属性组成，$R_1-R_2$或$R_2-R_1$表示模式的差集，$R_1-R_2$表示$R_1$中去除$R_1$和$R_2$的公共属性所组成。当模式R分解成两个关系模式$R_1$和$R_2$时，如果$R_1$与$R_2$的公共属性能函数决定$R_1$中或$R_2$中的其它属性，这样的分解就具有无损联接性。(只适用于一分为二的分解) 例题 例题119 例题220 并发控制 并发存在的问题 封锁协议 一级封锁协议。事务T在修改数据R之前必须先对其加X锁，直到事务结束才释放。可防止丢失修改 二级封锁协议。一级封锁协议加上事务T在读取数据R之前先对其加S锁，读完后即可释放S锁。可防止丢失修改，还可防止读“脏”数据 三级封锁协议。一级封锁协议加上事务T在读取数据R之前先对其加S锁，直到事务结束才释放。可防止丢失修改，防止读“脏”数据与防止数据重复读 两阶段协议。可串行化，可能发生死锁 数据库完整性约束 实体完整性约束：给数据表定义主键，约束的是主键的值不能为空和重复 参照完整性约束：外键完整性约束，允许为空 用户自定义完整性约束：可以设置属性值的要求触发器：应对复杂的完整性约束，通过脚本约束一些复杂要求。上诉三种只能应付简单的要求。 数据库安全 措施 说明 用户标识和鉴定 最外层的安全保护措施，可以使用用户账户、口令及随机数检验等方式 存取控制 对用户进行授权，包括操作类型（如查找、插入、删除、修改等动作）和数据对象（主要是数据范围）的权限 密码存储和传输 对远程终端信息用密码传输 视图的保护 对视图进行授权 审计 使用一个专用文件或数据库，自动将用户对数据库的所有操作记录下来 数据备份 冷备份：也称静态备份，是将数据库正常关闭，在停止状态下，将数据库的文件全部备份(复制)下来 热备份：也称为动态备份，是利用备份软件，在数据库正常运行的状态下，将数据库中的数据文件备份出来 备份方式 优点 缺点 冷备份 非常快速的备份方法(只需复制文件)；容易归档(简单复制即可)；容易恢复到某个时间点上(只需将文件再复制回去)；能与归档方法相结合，做数据库“最佳状态”的恢复；低度维护，高度安全 单独使用时，只能提供到某一时间点上的恢复；在实施备份的全过程中，数据库必须要作备份而不能做其它工作；若磁盘空间有限只能复制到磁带等其它外部存储设备上，速度会很慢；不能按表或按用户恢复 热备份 可在表空间或数据库文件级备份，备份时间短；备份时数据库仍可使用；可达到秒级恢复(恢复到某一时间点上)；可对几乎所有数据库实体做恢复；恢复是快速的 不能出错，否则后果严重；若热备份不成功所得结果不可用于时间点的恢复；困难于维护，所以特别小心，不允许”以失败告终” 完全备份：备份所有数据 差量备份：仅备份上一次完全备份之后变化的数据 增量备份：备份上一次备份之后变化的数据 转储 静态海量转储：在系统中无运行事务时进行，每次转储全部数据库 静态增量转储：在系统中无运行事务时进行，每次只转储上一次转储后更新过的数据 动态海量转储：转储期间允许对数据库进行存取或修改，每次转储全部数据库 动态增量转储：转储期间允许对数据库进行存取和修改，每次只转储上一次转储后更新过的数据。 日志文件：事务日志是针对数据库改变所做的记录，它可以记录针对数据库的任何操作，并将记录结果保存在独立的文件中 数据库故障与恢复 故障关系 故障原因 解决方法 事务本身的可预期故障 本身逻辑 在程序中预先设置RollBack语句 事务本身的不可预期故障 算术溢出、违反存储保护 由DBMS的恢复子系统通过日志，撤销事务对数据库的修改，回退到事务初始状态 系统故障 系统停止运转 通常使用检查点法 介质故障 外存被破坏 一般使用日志重做业务 分布式数据库 分布透明性 分片透明性 水平分片 垂直分片 混合分片 位置透明性 局部数据模型透明性 分布式数据库管理系统-组成 LDBMS GDBMS 全局数据字典 通信管理(CM) 分布式数据库管理系统-结构 全局控制集中的DDBMS 全局控制分散的DDBMS 全局控制部分分散的DDBMS 数据库优化 数据仓库与数据挖掘 面向主题 集成的 相对稳定的(非易失的) 反映历史变化 数据挖掘方法分类 方法 决策树 神经网络 遗传算法 关联规则挖掘算法 分类 关联分析：挖掘出隐藏在数据间的相互关系 序列模式分析：侧重点是分析数据间的前后关系(因果关系) 分类分析：为每一个记录赋予一个标记再按标记分类 聚类分析：分类分析法的逆过程 联邦数据库 联邦数据库系统(FDBS)是一个彼此协作却又相互独立的成员数据库(CDBS)的集合，它将成员数据库系统按不同程度进行集成，对该系统整体提供控制和协同操作的软件叫做联邦数据库管理系统(FDBMS) 联邦数据库特征 分布性 异构性 自治性 透明性 联邦数据库分类 紧耦合 松耦合 NoSQL 关系数据库模式 NoSQL模式 并发支持 支持并发、效率低 并发性能高 存储与查询 关系表方式存储、SQL查询 海量数据存储、查询效率高 扩展方式 向上扩展，升级服务器 向外扩展，扩展集群 索引方式 B树、哈希等 键值索引 应用领域 面向通用领域 特定应用领域 缺点(存疑，目前已经算是比较成熟) 成熟度不够，大量关键特性有待实现 开源数据库产品的支持力度有限 数据挖掘与商务智能支持不足，现有的产品无法直接使用NoSQL数据库 NoSQL数据库专家较少，大部分都处于学习阶段 SQL + NoSQL = MoreSQL/NewSQL Redis、MongoDB、Flare… 反规划化 由于规范化会使表不断地拆分，从而导致数据表过多。这样虽然减少了数据冗余，提高了增、删、改的速度，但会增加查询的工作量。系统需要进行多次连接，才能进行查询操作，使得系统效率大大下降。 技术手段 增加派生性冗余列 增加冗余列 重新组表 分割表 大数据基本概念 数据量(Volume)、速度(Velocity)、多样性(Variety)、值(Value) 比较纬度 传统数据 大数据 数据量 GB或TB级 PB级或以上 数据分析需求 现有数据的分析和检测 深度分析(关联分析、回归分析) 硬件平台 高端服务器 集群平台 大数据处理系统应具有的重要特性 高度可扩展性 高性能 高度容错 支持异构环境 较短的分析延迟 易用且开放的接口 较低成本 向下兼容性 计算机网络OSI/RM七层模型 例题 例题121 网络技术标准与协议 TCP/IP协议：Internet，可扩展，可靠，应用最广，牺牲速度和效率 IPX/SPX协议：NOVELL，路由，大型企业网 NETBEUI协议：IBM，非路由，快速 TCP 三次握手 可靠传输 DHCP 客户机/服务器模型 租约默认为8天 当租约过半时，客户机需要向DHCP服务器申请续租 当租约超过87.5%时，如果仍然没有和当初提供IP的DHCP服务器联系上，则开始联系其他的DHCP服务器 固定分配、动态分配和自动分配 169.254.X.X（Windows）和0.0.0.0（Unix）说明未和DHCP服务器联系上 DNS协议 递归查询：服务器必须回答目标IP与域名的映射关系。主机向本地域名服务器的查询采用递归查询 迭代查询：服务器收到一次迭代查询回复一次结果，这个结果不一定是目标IP与域名的映射关系，也可以是其它DNS服务器的地址。本地域名服务器向根域名服务器的查询通常采用迭代查询 例题 例题122 网络规划与设计 网络规划原则 实用性原则 开放性原则 先进性原则 网络设计任务 确定网络总体目标 确定总体设计原则 通信子网设计 资源子网设计 设备选型 网络操作系统与服务器资源设备 网络安全设计 网络设计原则 可用性：指网络或网络设备可用于执行预期任务时间所占总量的百分比 可靠性：网络设备或计算机持续执行预定功能的可能性 可恢复性：指网络从故障中恢复的难易程度和时间 适应性：指在用户改变应用要求时网络的应变能力 可伸缩性：指网络技术或设备随着用户需求的增长而扩充的能力 网络实施原则 可靠性原则 安全性原则 高效性原则 可扩展性原则 网络实施步骤 工程实施计划 网络设备到货验收 设备安装 系统测试 系统试运行 用户培训 系统转换 逻辑设计 利用需求分析和现有网络体系分析的结果来设计逻辑网络结构，最后得到一份逻辑网络设计文档 逻辑网络设计图 IP地址方案 安全方案 具体的软硬件、广域网连接设备和基本服务 招聘和培训网络员工的具体说明 对软硬件、服务、员工和培训的费用初步估计 物理网络设计 物理网络设计是对逻辑网络设计的物理实现，通过对设备的具体物理分布、运行环境等确定，确保网络的物理连接服务逻辑连接的要求 网络物理结构图和布线方案 设备和部件的详细列表清单 软硬件和安装费用的估算 安装日程表，详细说明服务的时间以及期限 安装后的测试计划 用户的培训计划 分层设计 接入层：向本地网段提供用户接入 汇聚层：网络访问策略控制、数据包处理、过滤、寻址 核心层：数据交换 无线网 分类 无线局域网(WLAN,802.11,WI-FI) 无线城域网(WMAN,802.16,WiMax) 无线广域网(WWAN,3G/4G) 无线个人网(WPAN,802.15,Bluetooth) 优势 移动性 灵活性 成本低 容易扩充 无线局域网接入方式 有接入点模式 无接入点模式(对等模式) 网络接入技术 有限接入 公用交换电话网络(PSTN)-(POS机，传真) 数字数据网(DDN) 综合业务数字网(ISDN) 非对称数字用户线路(ADSL)-介质是电话线，上下行不等 同轴光纤技术(HFC)-有线电视网络 无线接入 IEEE 802.11(WiFi) IEEE 802.15(蓝牙Bluetooth) 红外(IrDA) WAPI 3G WCDMA CDMA2000 TD-SCDMA 4G LET-Advanced TDD(时分):由TD-SCDMA发展而来 FDD(频分):由WCDMA发展而来 WirelessMAN- Advanced(802.16m)(WiMAX) 网络存储技术分类 直连式存储(DAS)：服务器直接和存储相连，可以接磁盘阵列，最原始的一种方式，适用于数据量不大，业务不复杂的场景 网络附加存储(NAS)：存储附加于网络之上，即插即用的存储，是一台存储服务器。网络在某些时间节点很繁忙，是由于NAS未将存储网络和业务网络分开导致 存储区域网络(SAN)：将业务网络和存储网络完全分开，存储网络使用光纤网络，速率远高于IP网络，是首次提出业务网络和存储网络分开的技术，性能卓越，缺点只有成本高 IPSAN(iSCSI)：SAN的廉价方案，性能比SAN低 Raid Raid0(条块化):性能最高，并行处理，无冗余，损坏无法恢复 Raid1(镜像结构):可用性，可恢复性好，仅有50%利用率 Raid0磁盘利用率为100&amp;，访问速度最快。Raid1磁盘利用率为50%，具备纠错功能。现在企业采用Raid0+1 Raid0+1(Raid10):Raid0与Raid1长处的结合，高效也可靠，利用率也只有50% Raid3(奇偶校验并行传送): N+1模式，有固定的校验盘，坏一个盘可恢复 Raid5(分布式奇偶校验的独立磁盘): N+1模式，无固定的校验盘，坏一个盘可恢复(相比Raid3，优势在于降低整个系统的磁盘出错损坏的概率) Raid5磁盘利用率为(n-1)/n，具有容错功能 Raid6(两种存储的奇偶校验):N+2模式，无固定的校验盘，坏两个盘可恢复 IPv6 IPv6是设计用于替代现行版本IP协议(IPv4)的下一代IP协议 特点 IPv6地址长度为128位，地址空间增大了$2^{96}$倍 灵活的IP报文头部格式。使用一系列固定格式的扩展头部取代了IPv4种可变长度的选项字段。IPv6中选项部分的出现方式也有所变化，是路由器可以简单路过选项而不做任何处理，加快了报文处理速度 IPv6简化了报文头部格式，字段只有8个，加快报文转发，提高了吞吐量 提高安全性。身份认证和隐私权是IPv6的关键特性 支持更多的服务类型 允许协议继续演变，增加新的功能，使之适应未来技术的发展 地址分类 单播地址(Unicast):用于单个接口的标识符 任播地址(Anycast):泛播地址。一组接口的标识符，IPv4广播地址 组播地址(Multicast):IPv6的组播在功能上与IPv4中的组播类似 物联网 物联网(The Internet of Things)是实现物物相连的互联网络，其内涵包含两个方面：第一，物联网的核心和基础仍然是互联网，是在互联网基础上延伸和扩展的网络；第二，其用户端延伸和扩展到了任何物体与物体之间，使其进行信息交换和通信 感知层：识别物体、采集信息。如：二维码、RFID、摄像头、传感器 网络层：传递信息和处理信息。通信网与互联网的融合网络、网络管理中心、信息中心和智能处理中心等 应用层：解决信息处理和人机交互的问题 RFID 射频识别技术(Radio Frequency Identification,RFID)，又称电子标签，是一种通信技术，可通过无线电讯号识别特定目标并读写相关数据，而无需识别系统与特定目标之间建立机械或光学接触。该技术是物联网的一项核心技术，很多物联网应用都离不开它。 RFID的基本组成部分通常包括：标签、阅读器、天线 二维码 二维码是用某种特定的几何图形按一定规律在平面(二维方向上)分布的黑白相间的图形记录数据符号信息的。在代码编制上巧妙地利用构成计算机内部逻辑基础的0、1比特流的概念，使用若干个与二进制相对应的几何形体来表示文字数值信息，通过图像输入设备或光电扫描设备自动识读以实现信息自动处理 二维码常用的码制有：Data Matrix,Maxi Code,Aztec,QR Code,Vericode,PDF417,U1tacode,Code 49,Code 16k PDF417 若采用扩展的字母数字压缩格式，可容纳1850个字符 若采用二进制/ASCII格式，可容纳1108个字节 若采用数字压缩格式，可容纳2710个数字 传感网 传感网是由随机分布的，集成有传感器(传感器有很多种类型，包括：温度、湿度、速度、气敏等)、数据处理单元和通信单元单元的微小节点，通过自组织的方式构成无线网络 M2M M2M是将数据从一台终端传送到另一台终端，也就是机器与机器(Machine to Machine)的对话。但从广义上M2M可代表机器对机器(Machine to Machine)、人对机器(Man to Machine)、机器对人(Machine to Man)、移动网络对机器(Mobile to Machine)之间的连接和通信，它涵盖了所有实现在人、机器、系统之间建立通信连接的技术和手段 云计算 云计算是一种基于互联网的计算方式，通过这种方式，共享的软硬件资源和信息可以按需提供给计算机和其它设备。云其实是网络、互联网的一种比喻说法。云计算的核心思想，是将大量用网络连接的计算资源统一管理和调度，构成一个计算资源池向用户按需服务。提供资源的网络被称为”云”。 狭义云计算指IT基础设施的交付和使用模式，指通过网络以按需、易扩展的方式获取所需资源 广义云计算指服务的交付和使用模式，指通过网络以按需、易扩展的方式获得所需服务。这种服务可以是IT和软件、互联网相关、也可是其他服务 特点 集合了大量计算机，规模达到成千上万 多种软硬件技术相结合 对客户端设备的要求低 规模化效应 类型 软件即服务(SaaS) 平台即服务(PaaS) 基础设施即服务(IaaS) 应用 存储服务 搜索 科学计算 安全应用 软件即服务 企业信息化战略与实施信息与信息化的概念 信息 维纳(Norber Wiener)：信息就是信息，既不是物质也不是能量 香农(Claude E.Shannon)：信息就是不确定性的减少 哲学界：信息是事物普遍联系的方式 其它：信息是事先不知道的报导 信息化 信息化就是计算机、通信和网络技术的现代化 信息化就是从物质生产占主导地位的社会向信息产业占主导地位社会转变的发展过程 信息化就是从工业社会向信息社会演进的过程 信息系统的概念 系统指多个元素有机地结合在一起，执行特定的功能以达到特定目标的集合体 信息系统是输入数据，通过加工处理，产生信息的系统，可以是人工的，手动的 信息系统类型 数据环境分类 数据文件 应用数据库：比数据文件的好处不用管具体操作，只需告诉要做什么；便于共享 主题数据库 面向业务主题 信息共享 一次一处输入系统 由基本表组成 信息检索系统 应用层次分类 战略级(企业最高管理层) 战术级(企业中层经理及其管理部门) 操作级(服务型企业的业务部门) 事务级(企业的管理业务人员) 信息系统战略规划-方法 以数据处理为核心围绕职能部门需求 企业系统规划法(BSP):通过大量调研了解企业战略目标，通过战略目标拆分，得到一些过程和需要处理的数据，然后进行表格转换。用到了多种矩阵，比如UC矩阵，找出数据和过程相关性 关键成功因素法(CSF):有目的地找企业的关键成功因素 战略集合转化法(SST):企业战略是指导信息系统战略的一个前提，把企业战略相关的集合转化成信息战略的集合三种方法结合使用称为BCS方法 以企业内部MIS为核心围绕企业整体需求 战略数据规划法(SDP):以BSP为基础，规划企业全局数据，提出主题数据库的概念 信息工程法(IE):以BSP为基础，融合SDP。其不仅仅是一门方法，还是一门工程学科，把信息系统开发过程进行了工程化 战略栅格法(SG):使用2X2矩阵，从战略影响方面标出企业现有和未来的信息系统组合的特征。对企业生存前景的影响做分析 综合考虑企业内外环境，以集成为核心，围绕企业战略需求 价值链分析法(VCA) 战略一致性模型(SAM) 政府信息化与电子政务 企业信息化与电子商务企业资源计划(ERP) 财会管理 会计核算 总账 应收账 应付账 现金 固定资产 多币制 财务管理 财务计划 控制 分析 预测 物流管理 分销管理 库存控制 采购管理 生产控制管理 主生产计划 物料需求计划 能力需求计划 车间控制 制造标准 人力资源管理 人力资源规划 招聘管理 工资核算 工时管理 差旅费核算 客户关系管理(CRM) 供应链管理(SCM) 设计原则 自顶向下和自底向上结合(所有信息化体系的共性，即全局规划加具体实现) 简洁性原则 互补性原则 协调性原则 动态性原则 创新性原则 战略性原则 商业智能(BI) 电子数据交换 EDI系统三要素：EDI软件和硬件、通信网络、数据标准化 EDI的特点 EDI的使用对象是不同的组织之间，EDI传输的企业间的报文，是企业间信息交流的一种方式 EDI所传送的资料是一般业务资料，如发票、订单等，而不是指一般性的通知 EDI传输的报文是格式化的，是符合国际标准的，这是计算机能够自动处理报文的基本前提 EDI使用的数据通信网络一般是增值网、专用网 数据传输由收送双方的计算机系统直接传送、交换资料，不需要人工介入操作 EDI与传真或电子邮件的区别是：传真与电子邮件，需要人工阅读判断处理才能进入计算机系统。人工将资料重复输入计算机系统中，既浪费人力资源，也容易发生错误，而EDI不需要再将有关资料人工重复输入系统 EDI在外贸领域仍在广泛应用 企业应用集成(常考) 过度性质的方案，未在数据上真正整合 表示集成(界面集成) 数据集成 控制集成(应用集成，API集成) 业务流程集成(过程集成)：会考虑业务流程优化的问题，不局限于企业内部系统 存储体和交换数据的角度划分 消息集成：适用于数据量小，但要求频繁地、立即地、异步地数据交换场合 共享数据库：实用性强、可以频繁交互、数据的交换属于同步方式 文件传输：适用于数据量大、交换频度小，即时性要求低的情况 电子商务 四流 信息流(核型) 资金流(辅助) 物流(辅助) 商流 类型 企业对消费者(B2C) 企业对企业(B2B) 消费者对消费者(C2C) 线上对线下(O2O) 信息系统开发方法 结构化法 用户至上 严格区分工作节点，每阶段有任务和成果 强调系统开发过程的整体性和全局性 系统开发过程工程化，文档资料标准化 自顶向下，逐步分解(求精) 原型法 适用于需求不明确的开发 包括抛弃式原型和演化式原型 面向对象方法 更好的复用性 关键在于建立一个全面、合理、统一的模型 分析、设计、实现三个阶段，界限不明确 面向服务方法 SO方法有三个主要的抽象级别：操作、服务、业务流程 SOAD分为三个层次：基础设计层(底层服务构件)、应用结构层(服务之间的接口和服务级协定)和业务组织层(业务流程建模和服务流程编排) 服务建模：分为服务发现、服务规约和服务实现三个阶段 软件工程软件开发模型瀑布模型(SDLC) 结构化方法模型典型代表，用于结构化开发 适合需求明确，二次开发的场景，或按其他方法把需求做明确再按瀑布模型开发 缺点：需求阶段难以把控 增量模型与螺旋模型 增量模型 螺旋模型 由多个模型组合，原型、瀑布模型、演化模型等(题目描述说针对需求不明确的情况只能选原型，遵循最匹配原则) 引入风险分析 构建组装模型(CBSD) 把软件开发中的各个模块做成构建，由构建组装形成软件 极大提高软件开发复用性，减小软件开发总时长，降低软件开发成本，提高软件可靠性 V模型、喷泉模型和RAD V模型 与瀑布模型接近，测试有更重要的地位 测试计划提前，有助于尽早发现问题 强调及早进行测试，测试贯穿开发整个过程中 喷泉模型 面向对象模型 RAD 由SDLC和CBSD模型组合而成 能快速构建应用系统 其他经典模型 原型和瀑布模型是互补的模型，原型强调在项目初期构造简易系统(界面式或初步系统)，针对需求不明确的情况，只应用于开发中需求分析的阶段 演化模型是指从最初的原型通过多步演化调整，最终变成软件产品 螺旋模型有原型、演化模型和瀑布模型的特征 增量模型由原型的思想和瀑布模型的思想构成，先完成一部分再完成一部分，最后完成整个产品(风险小很对) 统一过程(UP/RUP) 敏捷开发方法适合小型项目，强调小步快跑 逆向工程 由已经形成的产品反推设计和需求 需求工程OOA OOA相关概念：对象、类(实体类、边界类、控制类)、抽象、封装、继承与泛化、多态、接口、消息、组件、模式和复用 UML 图 分类 结构图/静态图：类图、对象图、包图、组合结构图、构件图、部署图、制品图 行为图/动态图：用例图(用例)、顺序图/序列图、通信图/协作图、定时图、状态图、活动图、交互概览图用例图(用例)分类有歧义，选择时先看其他选项，实在无选择再选这个，大多数时候归为动态，少部分时间归为静态 功能部署图描述的是软件的构建应该部署在哪个硬件节点上，其他静态图描述的均为关系 用例图(用例)：表达的是系统和外部的交互关系 顺序图/序列图：强调按时间关系 通信图/协作图：与顺序图相同，区别在于没有强调按时间顺序 状态图：表达状态的变迁转移的情况 活动图：和流程图结构一致 需求分类和需求获取 分类 业务需求 用户需求 系统需求 功能需求 性能需求(非功能需求)：响应时间、安全性、可靠性 设计约束：和功能与性能无关的影响 质量展开模型(QFD) 基本需求：用户明确提出需要完成的 期望需求：用户未明确提出但理所应当觉得应该具有的功能 兴奋需求：用户未提出也没觉得应该做但做了的 获取方法 收集资料：了解企业现状 联合需求计划(开会)：已经获取初步需求，各方人员参会讨论;消耗资源多 用户访谈：找关键角色探讨对需求的想法；流程包括准备访谈、确定访谈对象(3个或3个以下一次)、准备问题(开放式/封闭式)、限制访谈时间(90min之内，长了拆分)、寻找异常和错误情况、做好相应记录、总结访谈 结构化形式：逻辑层次严谨，依据充分；事先准备问题，有针对性提问；局限性更大 非结构化形式：开始只有粗略想法，根据访谈进度灵活调整； 优点：灵活度高，应用范围广，了解细节深入 缺点：用户忙难安排时间、信息量大难记录 书面调查：用户访谈的补充形式，问卷下发用户； 缺点：只能看到结果，不能了解选择时的疑问；用户不认真；问题不容易深入；问卷调查设计有难度；问卷发下去收不全(解释重要性) 情节串联板：可以以讲故事思路将各个环节串联起来 现场观摩 参加业务实践 阅读历史文档：获取数据层次时有效 抽样调查：数据量大，解决时间，比较有性价比 需求分析 数据流图(分层数据流图DFD) 数据流：箭头指示的为数据流 加工：处理数据的部件 数据存储：存储部件(文件) 外部实体：位置在系统之外顶层图和0层图之间应该保持平衡，即数据流交换应该一致，输入输出要平衡 状态转换图(STD) 描述业务流程，描述行为 ER图 数据字典 解释相关信息 数据元素 数据结构 数据流 数据存储 加工逻辑 结构化语言 判定树 判定表 外部实体 系统设计处理流程设计业务流程重组 BPR是对企业流程进行根本性的再思考和彻底性的再设计，从而获得可以用诸如成本、质量、服务和速度等方面的业绩来衡量的显著性成就 业务流程管理 BPM是一种以规范化的构造端到端的卓越业务流程为中心，以持续的提高组织业务绩效为目的的系统化方法 PDCA闭环的管理过程 明确业务流程所欲获取的成果 开发或计划系统的方法，实现以上成果 系统地部署方法，确保全面实施 根据对业务的检查和分析以及持续的学习活动，评估和审查所执行的方法，并进一步提出计划和实施改进措施 流程管理包含三个层面：规范流程、优化流程和再造流程 BPM与BRP管理四项最根本的不同就在于流程管理并不要求对所有的流程进行再造。构造卓越的业务流程并不是流程再造，而是根据现有流程的具体情况，对流程进行规范化的设计 人机界面设计 置于用户控制之下 以不强迫用户进入不必要的或不希望的动作的方式来定义交互方式 提供灵活的交互 允许用户交互可以被中断和撤销 当技能级别增加时可以使交互流水化并允许定制交互 使用户隔离内部技术细节 设计应允许用户和出现在屏幕上的对象直接交互 减少用户的记忆负担 较少对短期记忆的要求 建立有意义的缺省 定义直觉性的捷径 界面的视觉布局应该基于真实世界的隐喻 以不断进展的方式提示信息 保持界面的一致性 允许用户将当前任务放入有意义的语境 在应用系列内保持一致性 如果去得交互模型已建立起了用户期望，除非有迫不得已的理由，不要改变它 结构化设计 分为概要设计、详细设计 基本原则 自顶向下、逐步求精 信息隐藏 模块独立(高内聚、低耦合、复杂度) 保持模块的大小适中 尽可能减少调用的深度 多扇入、少扇出 单入口、单出口 模块的作用域应该在模块之内 功能应该时可预测的 内聚与耦合 内聚 内聚程度从高到低 内聚类型 描述 功能内聚 完成一个单一功能，各个部分协同工作，缺一不可 顺序内聚 处理元素相关，而且必须顺序执行 通信内聚 所有处理元素集中在一个数据结构的区域上 过程内聚 处理元素相关，而且必须按特定的次序执行 瞬时内聚(时间内聚) 所包含的任务必须在同一时间间隔内执行 逻辑内聚 完成逻辑上相关的一组任务 偶然内聚(巧合内聚) 完成一组没有关系或松散关系的任务 耦合 耦合程度由低到高 耦合类型 描述 非直接耦合 两个模块直接没有直接关系，它们之间的联系完全时通过主模块的控制和调用来实现的 数据耦合 一组模块借助参数表传递简单数据 标记耦合 一组模块通过参数表传递记录信息(数据结构) 控制耦合 模块之间传递的信息中包含用于控制模块内部逻辑的信息 外部耦合 一组模块都访问同一全局简单变量，而不是通过参数表传递该全局变量的信息 公共耦合 多个模块都访问同一个公共数据环境 内容耦合 一个模块直接访问另一个模块的内部数据；一个模块不通过正常入口转到另一个模块的内部；两个模块有一部分程序代码重叠；一个模块有多个入口 系统结构/模块结构变换型系统结构b重点掌握 面向对象设计-设计原则 单一职责原则：设计目的单一的类 开放-封闭原则：对扩展开放，对修改封闭 里氏(Liskov)替换原则：子类可以替换父类 依赖倒置原则：要依赖于抽象，而不是具体实现；针对接口编程，不要针对实现编程 接口隔离原则：使用多个专门的接口比使用单一的总接口要好 组合重用原则：要尽量使用组合，而不是继承关系达到重用的目的 迪米特(Demeter)原则(最少只是法则)：一个对象应当对其它对象由尽可能少的了解 设计模式概念 架构模式：软件设计中的高层决策，例如C/S结构就属于架构模式，架构模式反映了开发软件系统过程中所作的基本设计决策(B/S、SOA) 设计模式：主要关注软件系统的设计，与具体的实现语言无关(在进行构件设计时) 惯用法：最底层的模式，关注软件系统的设计与实现，实现时通过某种特定的程序设计语言来描述构件与构件之间的关系。每种编程语言都有它自己特定的模式，即语言的惯用法。例如引用-计数就是C++语言中的一种惯用法 分类 创建型模式：工厂方法(factory method)模式、抽象工厂(abstract factory)模式、原型(prototype)模式、单例(singleton)模式、构建器(builder)模式 结构型模式：适配器(adapter)模式、桥接(bridge)模式、组合(composite)模式、装饰(decorator)模式、外观(facade)模式、享元(flyweight)模式、代理(proxy)模式 行为型模式：职责链(chain of responsibility)模式、命令(command)模式、解释器(interpreter)模式、迭代器(iterator)模式、中介者(mediator)模式、备忘录(memento)模式、观察者(observer)模式、状态(state)模式、策略(strategy)模式、模板方法(template method)模式、访问者(visitor)模式 创建型模式 设计模式名称 简要说明 Abstract Factory抽象工厂模式 提供一个接口，可以创建一系列相关或相互依赖的对象，而无需指定它们具体的类 Builder构建器模式 将一个复杂类的表示与其构造相分离，使得相同的构建过程能够得出不同的表示 Factory Method工厂方法模式 定义一个创建对象的接口，但由子类决定需要实例化哪一个类，工厂方法使得子类实例化的过程推迟 Prototype原型模式 用原型实例指定创建对象的类型，并且通过拷贝这个原型来创建新的对象 Singleton单例模式 保证一个类只有一个实例，并提供一个访问它的全局访问点 结构型模式 设计模式名称 简要说明 速记关键字 Adapter适配器模式 将一个类的接口转换成用户希望得到的另一种接口。它使原本不相容的接口得以协同工作 转换接口 Bridge桥接模式 将类的抽象部分和它的实现部分分离开来，使它们可以独立地变化 继承树拆分 Composite组合模式 将对象组合成树型结构以表示“整体-部分”的层次结构，使得用户对单个对象和组合对象的使用具有一致性 树形目录结构 Decorator装饰模式 动态地给一个对象添加一些额外的职责。它提供了用子类扩展功能的一个灵活的替代，比派生一个子类更加灵活 附加职责 Facade外观模式 定义一个高层接口，为子系统中的一组接口提供一个一致的外观，从而简化该子系统的使用 对外统一接口 Flyweight享元模式 提供支持大量细粒度对象共享的方法 Proxy代理模式 为其他对象提供一种代理以控制这个对象的访问 行为型模式 设计模式名称 简要说明 速记关键字 Chain of Responsibility职责链模式 通过给多个对象处理请求的机会，减少请求的发送者与接收者之间的耦合。将接收对象链接起来，在链中传递请求，直到有一个对象处理这个请求 传递职责 Command命令模式 将一个请求封装为一个对象，从而可用不同的请求对客户进行参数化，将请求排队或记录请求日志，支持可撤销操作 日志记录，可撤销 Interpreter解释器模式 给定一种语言，定义它的文法表示，并定义一个解释器，该解释器用来根据文法表示来解释语言中的句子 Iterator迭代器模式 提供一种方法来顺序访问一个聚合对象中的各个元素而不需要暴露该对象的内部表示 Mediator中介者模式 用一个中介对象来封装一系列的对象交互，它使各对象不需要显示地相互调用，从而达到低耦合，还可以独立地改变对象间的交互 不直接引用 Memento备忘录模式 在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态，从而可以在以后将对象恢复到原先保存的状态 Observer观察者模式 定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并自动更新 State状态模式 允许一个对象在其内部状态改变时改变它的行为 状态变成类 Strategy策略模式 定义一系列算法，把它们一个个封装起来，并且使它们之间可互相替换，从而让算法可以独立于使用它的用户而变化 多方案切换 Template Method模板方法模式 定义一个操作中的算法骨架，而将一些步骤延迟到子类中，使得子类可以不改变一个算法的接口即可重新定义算法的某些特定步骤 Vistor访问者模式 表示一个作用于对象结构中的各元素的操作，使得在不改变各元素的类的前提下定义作用于这些元素的新操作 软件测试原则和类型 原则 尽早、不断地进行测试 程序员避免测试自己设计的程序 既要选择有效、合理的数据、也要选择无效、不合理的数据 修改后进行回归测试 尚未发现的错误数量与该程序已发现的错误数成正比 分类 动态测试 黑盒测试法 白盒测试发 灰盒测试法 静态测试 桌前检查 代码走查 代码审查 测试用例设计 黑盒测试 等价类划分：把所有数据进行归类，同一类的选一个进行测试 边界值分析：端点、略小于下区间的值、略大于大区间的值 错误推测 因果图：由果分析因 白盒测试 基本路径测试 循环覆盖测试 逻辑覆盖测试：语句覆盖(层次最低)、判定覆盖(真假分支各走一遍)、条件覆盖(拆分判定)、条件判定覆盖、修正的条件判断覆盖、条件组合覆盖、点覆盖、边覆盖、路径覆盖(层次最高) 测试阶段 单元测试：模块级，局部功能，模块相关接口 集成测试：各个模块联合测试，测试模块之间衔接、接口（增量式组装工作量更大但更全面） 确认测试 系统测试：偏重于压力性能等(负载测试(高并发)、强度测试(资源突然下降)、容量测试) 面向对象的测试 算法层(单元测试)：包括等价类划分测试、组合功能测试（基于判定表的测试）、递归函数测试和多态消息测试 类层(模块测试)：包括不变式边界测试、模态类测试和非模态类测试 模板层/类树层(集成测试)：包括多态服务测试和展平测试 系统层(系统测试) 测试管理 测试团队管理 测试计划管理 错误(缺陷)跟踪管理 错误植入法：$a_1:b_1=a_2:b_2$，人为植入错误，从结果查看哪些使植入的，按比例估算错误数，以此考核团队成员是否有效发现错误。(估算错误数) 两组并行测试：$DN=(a\times b)/c$，a代表a团队测出的错误数，b代表b团队测出的错误数，c代表俩团队共同测出的错误数。(估算错误数) 错误曝光率(DRE)：$DRE=e/(e+d)$，e表示测试阶段发现的问题，d表示用户使用时发现的问题，两者之和为缺陷的总数。(客观、准确，缺点是只有交付之后才能计算) 测试件管理 软件调试 软件调试方法 蛮力法：主要是想是“通过计算机找错”，低效，耗时 回溯法：从出错处人工沿控制流程往回追踪，直至发现出错的根源，复杂程度由于回溯路径多，难以实施 原因排除法：主要思想是演绎和归纳，用二分法实现 软件调试和测试的区别 测试的目的是找出存在的错误，而调试的目的是定位错误并修改程序以修正错误 调试时测试之后的活动，测试和调试在目标、方法和思路上都有所不同 测试从一个已知的条件开始，使用预先定义的过程，有预知的结果；调试从一个未知的条件开始，结束的过程不可预计 测试过程可以事先设计，进度可以事先确定；调试不能描述过程或持续时间 系统运行与维护 软件维护时生命周期的一个完整部分。可以将软件维护定义为需要提供软件支持的全部活动，这些活动包括在交付前完成的活动，以及交付后完成的活动。交付前完成的活动包括交付后运行的计划和维护计划等；交付后的活动包括软件修改、培训、帮助资料等 可维护性 易分析性：源代码理解容易 易改变性：修改代码比较容易(耦合度低) 稳定性 易测试性 维护类型 改正性维护(25%)/正确性维护：用户发现缺陷则修正缺陷 适应性维护(20%)：环境改变(操作系统、数据环境)，软件需要适应的情况均属于此范畴 完善性维护(50%)：在系统运行过程中扩充功能，改善性能 预防性维护(5%)：现在不维护将来可能导致问题 软件过程改进(CMMI) 阶段式：组织能力成熟度 成熟度等级 过程域 已管理级(项目级) 需求管理有、项目计划、配置管理、项目监督与控制、供应商合同管理、度量和分析、过程和产品质量保证 已定义级(组织级、文档化、标准化、知识由隐性变为显性) 需求开发、技术解决方案、产品集成、验证、确认、组织级过程焦点、组织级过程定义、组织级培训、集成项目管理、风险管理、集成化的团队、决策分析和解决方案、组织级集成环境 定量管理级(量化) 组织级过程性能、定量项目管理 优化级 组织级改革与实施、因果分析和解决方案 连续式：软件过程能力 连续式分组 过程域 过程管理 组织级过程焦点、组织级过程定义、组织级培训、组织级过程性能、组织级改革与实施 项目管理 项目计划、项目监督与控制、供应商合同管理、集成项目管理、风险管理、集成化的团队、定量项目管理 工程 需求管理、需求开发、技术解决方案、产品集成、验证、确认 支持 配置管理、度量和分析、过程和产品质量保证、决策分析和解决方案、组织级集成环境、因果分析和解决方案 项目管理范围管理 范围定义 产品范围：需求调研获取到的信息 工作范围：完成项目需要开展的一系列工作 创建WBS：工作包的分解，界定项目范围 时间管理 活动定义：把活动找出来 活动排序：采取拓扑排序的方式 活动资源估算：人员、时间、物料、服务器、设备等 活动历时估算：活动需要多少时间 三点估算法：$\frac{best+4\times mid+ worst}{6}$ 自下而上的估算：必须先完成WBS分解 制订进度计划： 进度控制：单代号网络图 FS: A完成B才能开始 FF：A完成B才能完成，谁先开始没有约定 SS：A开始B才能开始 SF：A开始B才能结束，例如新老系统切换，新系统运行才能下线老系统 整推 ES：最早开始时间(上一个活动的最大EF) EF：最早结束时间(ES+持续时间) 逆推 LS：最晚开始时间(LF-持续时间) LF：最晚结束时间(下一个节点最小的LS)$总时差=ES-LS=EF-LF$总时差等于0的节点连起来就为关键路径 关键路径越少越好，否则管理难度增大 双代号网络图 边表示活动，节点表示事件；虚线画的活动不会给活动名或者给一个活动名而不是给历时(虚活动，虚构出来的活动，表达依赖关系不能直接去掉但不占用资源) 求解关键路径关键路径为1-2-3-4-6-7-9 (A、E、H、I、K) 关键路径法 关键路径法是在制定进度计划时使用的一种进度网络分析技术。关键路线法沿用项目进度网络路线进行正向与反向分析，从而计算出所有计划活动理论上的最早开始与完成日期、最迟开始与完成日期，不考虑任何资源限制 总时差(松弛时间)：在不延误总工期的前提下，该活动的机动时间。活动的总时差等于该活动最迟完成时间与最早完成时间之差，或该活动最迟开始时间之差 自由时差：在不影响紧后活动的最早开始时间前提下，该活动的机动时间 对于有紧后活动的活动，其自由时差等于所有紧后活动最早开始时间减本活动最早完成时间所得之差的最小值 对于没有紧后活动的活动，也就是以网络计划终点节点为完成节点的活动，其自由时差等于计划工期与本活动最早完成时间之差 对于网络计划中以终点节点为完成节点的活动，其自由时差与总时差相等。此外，由于活动的自由时差时其总时差的构成部分，所以，当活动的总时差为零时，其自由时差必然为零，可不必进行专门计算 自由时差例题23 甘特图 细横线：代表计划值；粗横线：代表实际完成值 优点：甘特图直观、简单、容易制作，便于理解，能很清晰地标识出直到每一项任务的起始与结束时间，一般适用比较简单的小型项目，可用于WBS的任何层次、进度控制、资源优化、编制资源和费用计划 缺点：不能系统地表达一个项目所包含的各项工作之间的复杂关系，难以进行定量的计算和分析，以及计划的优化等 成本管理 成本估算 自顶向下的估算 自底向上的估算 成本预算 直接成本与间接成本：直接成本指由项目组花掉的钱(人力成本等，项目经理能控制的)；间接成本指公摊性质的成本(项目经理无法控制的)，以每天多少成本衡量 管理储备：项目经理无权动用，要企业高层审批 零基准预算 成本控制 挣值分析 挣值管理 计算工作量的预算成本(PV)：$PV=计划工作量 \times 预算定额$(顺序3) 已完成工作量的实际成本(AC)：项目实际花掉的钱，考题一般会直接给出(顺序1) 已完成工作量的预算成本(EV)：$EV=已完成工作量 \times 预算定额$(顺序2) 完工预算(BAC)：$BAC = 完工时的PV总和$ 进度偏差：$SV=EV-PV$，为负代表进度滞后 成本偏差：$CV=EV-AC$，为负代表成本超支 进度绩效指数：$SPI=EV/PV$，小于1表示不好，等于1表示与计划持平，大于1表示超预期 成本绩效指数：$CPI=EV/AC$，小于1表示不好，等于1表示与计划持平，大于1表示超预期 剩余工作的成本(ETC)：按计划走(非典型偏差)：$ETC=BAC-EV$不按计划走(典型偏差)：$ETC=\frac{BAC-EV}{CPI}$ 完工预估(EAC)：$EAC=AC+ETC$ 例题124 希赛教育在线测试项目涉及对10个函数代码的编写（假设每个函数代码的编写工作量相等），项目由2个程序员进行结对编程，计划在10天内完成，总体预算时1000元，每个函数的平均成本时100元，项目进行到第5天，实际消耗费用时400元，完成了3个函数的编写。 质量管理 质量模型 外部和内部质量 质量管理过程(PDCA循环) 质量保证和质量控制 质量保证一般是每隔一定时间(例如，每个阶段末)进行的，主要通过系统的质量审计和过程分析来保证项目的质量(强调过程) 质量控制是实时监控项目的具体结果，以判断它们是否符合相关质量标准，制定有效方案，以消除产生质量问题的原因(强调结果) 一定时间内质量控制的结果也是质量保证的质量审计对象。质量保证的成果又可以指导下一阶段的质量工作，包括质量控制和质量改进 质量工具 核对表：即checklist，清单 帕累托分析(排列图)：统计错误出现的频次，按从高到底排列，先解决小部分高频次的问题 因果分析(鱼骨图)：鱼头是要解决的问题，鱼骨是朔源，由果反推因 项目管理三角形 配置管理 IEEE对配置项的定义为硬件、软件或二者兼有的集合，为配置管理制定的 在配置管理过程中作为一个单独的实体对待，可作为配置项管理的有：外部交付的软件产品和数据、指定的内部软件工作产品和数据、指定的用于创建或支持软件产品的支持工具、供方/供应商提供的软件和客户提供的设备/软件 典型配置项：项目计划书、需求文档、设计文档、源代码、可执行代码、测试用例、运行软件所需的各种数据(它们经评审和检查通过后进入软件配置管理(SCM)) 配置项的主要属性：名称、标识符、文件状态、版本、作者和日期等。所有配置项都被保存在配置库里，确保不会混淆、丢失、配置项及其历史记录反映了软件的演化过程 配置库 开发库(动态库、程序员库、工作库；动态系统、开发者系统、开发系统、工作空间)：开发阶段(不断地调整修改阶段用的)，管辖内容程度最低，修改基本没有约束 受控库(主库、系统库；主系统、受控系统)：完成测试(测试通过)后放入，管理系统的基线，不允许随意修改，修改需要走变更控制流程 产品库(备份库、静态库、软件仓库；静态系统)：完全不能修改 检查点：只在规定的时间间隔内对项目进行检查，比较实际与计划之间的差异，并根据差异进行调整 里程碑：完成阶段性工作的标志，不同类型的项目里程碑不同 基线：指一个(或一组)配置项在项目生命周期的不同时间点上通过正式评审而进入正式受控的一种状态，基线是一些重要的里程碑。但相关交付成果要通过正式评审，并作为后续工作的基准和出发点。基线一旦建立后其变化需要受控制 变更控制 版本控制 草稿版：0.YZ 正式版：X.Y 修改版：X.YZ 风险管理特性 关心未来、关心变化、关心选择 风险存在的客观性和普遍性 某一具体风险发生的偶然性和大量风险发生的必然性 风险的可变性 风险的多样性和多层次性 基本属性：随机性和相对性 风险承受能力 分类 项目风险 潜在的预算、进度、人员和组织(预算本身不足)、资源、用户和需求问题 项目复杂性、规模和结构的不确定性 技术风险 潜在的设计、实现、接口、测试和维护方面的问题 规格说明的多义性、技术上的不确定性、技术陈旧、最新技术(不成熟) 商业风险 市场风险：系统虽然很优秀但不是市场真正所想要的 策略风险：系统不再符合企业的信息系统战略 销售风险：开发了销售部门不清楚如何推销的系统 管理风险：由于重点转移或人员变动而失去上级支持(预算足够，但是上层不给) 预算风险：开发过程没有得到预算或人员的保证(预算足够，但是上层不给) 风险曝光度 风险曝光度(Risk Exposure):计算方法是风险出现的概率乘以风险可能造成的损失 例 假设正在开发的软件项目可能存在一个未被发现的错误，而这个错误出现的概率是0.5%，给公司造成的损失将是1000000元，那么这个错误的风险曝光度应为$1000000 \times 0.5\%=5000元$ 项目管理工具 能做什么(项目管理相关的工作辅助)：任务调度、成本估算、资源分配、预算跟踪、人时统计、配置控制、确定关键路径、松弛时间、超前时间和滞后时间，生成一定格式的报表和报告 不能做什么(开发技术相关的辅助工作)：不能指导软件设计人员按软件生存周期各个阶段的适用技术进行设计工作 软件架构设计软件架构风格 架构设计的一个核心问题是能否达到架构级的软件复用架构风格反映了领域中众多系统所共有的结构和语义特性，并指导如何将各个构件有效地组织成一个完整的系统架构风格定义了用于描述系统的术语表和一组指导构建系统的规则 数据流风格：批处理序列、管道-过滤器 调用/返回风格：主程序/子程序、、面向对象、层次结构 独立构件风格：进程通信、事件驱动系统(隐式调用) 虚拟机风格：解释器、基于规则的系统 仓库风格：数据库系统、超文本系统、黑板系统 数据流风格 批处理序列：构件为一系列固定顺序的计算单元，构件之间只通过数据传递交互。每个处理步骤是一个独立的程序，每一步必须在其前一步结束后才能开始，数据必须是完整的，以整体的方式传递。整个处理过程没有用户交互 管道-过滤器：每个构件都有一组输入和输出，构件读输入的数据流，经过内部处理，然后产生输出数据流，支持流式处理，不必等一批全部处理完在走下一个。（这个过程通常是通过对输入数据流的变换或计算来完成的，包括通过计算或增加信息以丰富数据、通过浓缩或删除以精简数据、通过改变记录方式以转化数据和递增地转化数据等。）这里的构件称为过滤器，连接件就是数据流传输的管道，将一个过滤器的输出传到另一个过滤器的输入 调用/返回风格 主程序/子程序：单线程控制，把问题划分为若干个处理步骤，构件即为主程序和子程序，子程序通常可合成为模块。过程调用作为交互机制，即充当连接件的角色。调用关系具有层次性，其语义逻辑表现为主程序的正确性取决于它调用的子程序的正确性。(结构化开发中常见) 面向对象：显式调用。构件是对象，对象是抽象数据类型的实例。在抽象数据类型中，数据的表示和它们的相应操作被封装起来，对象的行为体现在其接受和请求的动作。连接件即是对象间交互的方式，对象是通过函数和过程的调用来交互的 层次结构：构建组织成一个层次结构，连接件通过决定层间如何交互的协议来定义。每层为上一层提供服务，使用下一层的服务，只能见到与自己邻接的层。通过层次结构，可以将大的问题分解为若干个渐进的小问题逐步解决，可以隐藏问题的复杂度，修改某一层，最多影响相邻的两层(通常只能影响上层)(缺点：层次多则效率低、层次划分没有定论) 独立构件风格 进程通信：独立构件。构件是独立的过程，连接件是消息传递。构件通常是命名过程，消息传递的方式可以是点对点，异步或同步方式，以及远程过程(方法)调用等 事件驱动系统：隐式调用。构件不直接调用一个过程，而是触发或广播一个或多个事件。构件中的过程在一个或多个事件中注册，当某个事件被触发时，系统自动调用在这个事件中注册的所有过程。一个事件的触发就导致了另一个模块中的过程调用。这种风格中的构件是匿名的过程，它们之间交互的连接件往往是以过程之间的隐式调用来实现的。主要优点是为软件复用提供了强大的支持，为构件的维护和演化带来了方便；其缺点是构件放弃了对系统计算的控制 虚拟机风格 解释器：解释器通常包括一个完成解释工作的解释引擎、一个包含将被解释的代码的存储区、一个记录解释引擎当前工作状态的数据结构，以及一个记录源代码被解释执行的进度的数据结构。具有解释器风格的软件中含有一个虚拟机，可以仿真硬件的执行过程和一些关键应用，其缺点是执行效率比较低 基于规则的系统：基于规则的系统包括规则集、规则解释器、规则/数据选择器和工作内存，一般用在人工智能领域和DSS中 仓库风格 以数据为核心 数据库系统：数据共享。构件主要有两大类，一类是中央共享数据源，保存当前系统的数据状态；另一类是多个独立处理单元，处理单元对数据元素进行操作 黑板系统：包括知识源、黑板和控制三部分。知识源包括若干独立计算的不同单元，提供解决问题的知识。知识源响应黑板的变化，也只修改黑板；黑板是一个全局数据库，包含问题域解空间的全部状态，是知识源相互作用的唯一媒介；知识源响应是通过黑板状态的变化来控制的。黑板系统通常应用在对于解决问题没有确定性算法的软件中(信号处理、问题规划和编译器优化等) 超文本系统：构件以网状链接方式相互连接，用户可以在构件之间进行按照人类的联想思维方式任意跳转到相关构件。超文本是一种非线性的网状信息组织方式。它以结点为基本单位，链作为结点之间的联想式关联。超文本系统通常应用在互联网领域 C/S架构 两层C/S架构 开发成本较高、客户端程序设计复杂、信息内容和形式单一、用户界面风格不一、软件移植困难、软件维护和升级困难、新技术不能轻易应用 三层C/S架构 各层在逻辑上保持相对独立，整个系统的逻辑结构更为清晰，能提高系统和软件的可维护性和可扩展性允许灵活有效地选用相应的平台和硬件系统，具有良好的可升级性和开放性各层可以并行开发，各层也可以选择各自最适合的开发语言功能层有效地隔离表示层与数据层，为严格的安全管理奠定了坚实的基础；整个系统的管理层次也更加合理和可控制 三层B/S架构 B/S架构缺乏对动态页面的支持能力，没有集成有效的数据库处理功能 B/S架构的安全性难以控制 采用B/S架构的应用系统，在数据查询等响应速度上，要远远低于C/S架构 B/S架构的数据提交一般以页面为单位，数据的动态交互性不强，不利于OLTP应用 混合架构风格 内外有别模型：内部C/S发挥性能优势，外部B/S架构 查改有别模型 富互联网应用(RIA) RIA结合了C/S架构反应速度快、交互性强的优点、以及B/S架构传播范围广及容易传播的特性 RIA简化并改进了B/S架构的用户交互 数据能够被缓存在客户端，从而可以实现一个比基于HTML的响应速度更快且数据往返于服务器的次数更少的用户界面 AJAX 基于HTML和CSS标准的表示 使用DOM进行动态显示和交互 使用XML和XLST进行数据交换及相关操作 使用XMLHttpRequest与服务器进行异步通信 使用JavaScript绑定一切 mushup RSS：一种用于对网站内容进行描述和同步的格式，是目前最广泛的Web资源发布方式 REST：从资源的角度看待整个网络，各处的资源由URI确定，客户端的应用通过URI获取资源的表示 SOAP：一种基于XML的数据格式定义，用来进行Web服务调用过程中的参数调用和返回 ATOM：一种基于XML的文档格式和基于HTTP的协议，用来聚合网络内容 基于服务的架构(SOA) 服务是一种为了满足某项业务需求的操作、规则等的逻辑组合，它包含一系列有序活动的交互，为实现用户目标提供支持 特色：松散耦合、粗粒度(服务&gt;构件&gt;对象)、标准化接口 与传统构件的区别 服务构件粗粒度，传统构件细粒度居多 服务构件的接口是标准的，主要是WSDL接口，传统构件常以具体API形式出现 服务构建的实现与语言无关，传统构件绑定某种特定的语言 服务构件可以通过构件容器提供QoS服务，传统构件完全由程序代码直接控制 实现方式 Web Service 功能 协议 发现服务 UDDI、DISCO 描述服务 WSDL、XML Schema 消息格式层 SOAP、REST 编码格式层 XML 传输协议层 HTTP、TCP/IP、SMTP等 ESB 提供位置透明性的消息路由和寻址服务 提供服务注册和命名的管理功能 支持多种消息传递范型 支持多种可以广泛使用的传输协议 支持多种数据格式及其相互转换 提供日志和监控功能 软件架构评估质量属性 性能 性能(performance)是指系统的响应能力，即要经过多长事件才能对某个事件做出响应，或者在某段时间内系统所能处理的事件的个数。经常用单位事件内所处理事务的数量或系统完成某个事务处理所需的时间来对性能进行定量的表示。性能测试经常要使用基准测试程序（用以测量性能指标的特定事务集或工作量环境） 可靠性 可靠性(reliability)是软件系统在应用或系统错误面前，在意外或错误使用的情况下维持软件系统的功能特性的基本能力。可靠性通常用平均失效等待时间(Mean Time To Failure，简称MTTF)和平均失效间隔时间(Mean Time Between Failure，简称MTBF)来衡量。在失效率为常数和修复时间很短的情况下，MTTF和MTBF几乎相等 可用性 可用性(avaliability)是系统能够正常运行的时间比例。经常用两次故障之间的时间长度或在出现故障时系统能够恢复正常的速度来表示。 安全性 安全性(security)时滞系统在向合法用户提供服务的同时能够阻止非授权用户使用的企图或拒绝服务的能力。安全性时根据系统可能收到的安全威胁的类型来分类的。安全性又可划分为机密性、完整性、不可否认性及可控性等特性 可修改性 可修改性(modifiability)是指能够快速地以较高的性能价格比对系统进行变更的能力。通常以某些具体的变更为基准，通过考察这些变更的代价衡量可修改性(设计阶段就需要考虑) 功能性 功能性(functionality)是系统所能完成所期望的工作的能力。一项任务的完成需要系统中许多或大多数构件的相互协作 可变性 可变性(changeability)是指系统结构经扩充或变更而成为新体系结构的能力。这种新体系结构应该符合预先定义的规则，在某些具体方面不同于所有的体系结构。当要将某个体系结构作为一系列相关产品(例如，软件产品线)的基础时，可变性是很重要的 互操作性 作为系统组成部分的软件不是独立存在的，经常与其它系统或自身环境相互作用。为了支持互操作性(interoperation)，软件体系结构必须为外部可视的功能特性和数据结构提供精心设计的软件入口，程序和用其他编程语言编写的软件系统的交互作用就是互操作性的问题，这种互操作性也影响应用的软件体系结构 概念表述 敏感点：变化值很小但对结果影响很大的点(敏感度很高的值，往往是一个) 权衡点：影响到多个质量属性的特性，多个都是属于敏感点(多个质量属性的折中) 风险点：潜在的存在问题的隐患 非风险点 评估方法 基于调查问卷(检查表)的方式(最客观的方式) 基于度量的方式 基于场景的方式 基于场景的方式 确定应用领域的功能和软件架构的结构之间的映射设计用于体现待评估质量属性的场景分析软件架构对场景的支持程度 架构权衡分析法(ATAM) 软件架构分析法(SAAM) 成本效益分析法(CBAM) 软件产品线 过程驱动、特定领域、技术支持、以架构为中心 过程模型-双生命周期模型 过程模型-SEI模型 过程模型-三生命周期模型 组织结构 设立独立的核心资源小组：核心的、行业共性的开发工作(绝大多数部分) 不设立独立的核心资源小组：核心开发和应用开发在一个小组，但仍然会区分核心部分和应用部分，两者分别对待 动态的组织结构 建立方式 将现有产品演化为产品线 用软件产品线替代现有产品集 全新软件产品线的演化 全新软件产品线的开发 演化方式 革命方式 基于现有产品 基于现有产品架构设计产品线的架构，经演化现有构件，开发产品线构件 核心资源的开发基于现有产品集的需求和可预测的、将来的需求的超集 全新产品线 产品线核心资源随产品新成员的需求而演化 开发满足所有预期产品线成员的需求的核心资源 中间件技术 中间件是一种独立的系统软件或服务程序，可以帮助分布式应用软件在不同的技术之间共享资源 负责客户机与服务器之间的连接和通信，以及客户机与应用层之间的高效率通信机制 提供应用层不同服务之间的互操作机制，以及应用层与数据库之间的连接和控制机制 提供多层架构的应用开发和运行的平台，以及应用开发框架，支持模块化的应用开发 屏蔽硬件、操作系统、网络和数据库的差异 提供应用的负载均衡和高可用性、安全机制与管理功能，以及交易管理机制，保证交易的一致性 提供一组通用的服务去执行不同的功能，避免重复的工作和使应用之间可以协作 主要中间件 远程过程调用 对象请求代理 远程方法调用 面向消息的中间件 事务处理监控器 Corba(公共对象请求代理体系结构) 典型应用架构J2EE-分布式多层应用程序 业务层 会话Bean：短暂会话 实体Bean：持久化数据 消息驱动Bean：会话Bean+JMS .NET 通用语言运行环境：支持多种语言，各语言被转换为通用语言运行规范，在通用语言运行环境上运行(对应java的虚拟机) .NET与J2EE JVM和CLR：同样思想下的产物 对多层分布式应用的支持：没有多大差异 安全性 应用程序的部署 可移植性：.NET的可移植性较差(依附于微软自己的操作系统平台)，J2EE可移植性较好 外部支持：.NET支持语言多一下 MVC设计模式 主动MVC 被动MVC MVP设计模式 MVP是MVC的变种 MVP实现了V与M之间的解耦(V不直接使用M，修改V不会影响M) MVP更好的支持单元测试(业务逻辑在P中，可以脱离V来测试这些逻辑；可以将一个P用于多个V，而不需要改变P的逻辑) MVP中V要处理界面事件，业务逻辑在P中，MVC中界面事件由C处理 信息安全分析与设计信息系统安全属性 保密性：最小授权原则、防暴露、信息加密、物理加密 完整性：安全协议、校验码、密码校验、数字签名、公证 可用性：综合保障（IP过滤、业务流控制、路由选择控制、审计跟踪） 不可抵赖性：数字签名 对称加密技术 DES：替换+移位、56位密钥、64位数据块、速度快、密钥易生成 3DES(三重DES)：两个56位的密钥K1、K2（加密：K1加密-&gt;K2解密-&gt;K1加密；解密：k1解密-&gt;K2加密-&gt;K1解密） AES：高级加密标准Rijndael加密法，是美国联邦政府采用的一种区块加密标准。这个标准用来替代原先的DES。堆区要求是“至少与3DES一样安全” RC-5：RSA数据安全公司的很多产品都使用了RC-5 IDEA算法：128位密钥、64位数据块、比DES的加密性好、对计算机功能要求相对低，PGP 优点：加密速度快、效率高；缺点：加密强度不高、密钥分发困难 非对称加密技术 RSA：512位（或1024位）密钥、计算量极大、破解难 Elgamal：其基础是Diffie-Hellman密钥交换算法 ECC：椭圆曲线算法 其它非对称算法包括：背包算法、Rabin、D-H 优点：破解难；缺点：加密速度慢，不适合加密大数据量 信息摘要 单向散列函数（单向Hash函数）、固定长度的散列值 常用的信息摘要算法有MD5、SHA等，市场上广泛使用的MD5、SHA算法的散列值分别位128位和160位，由于SHA通常采用的密钥长度较长，因此安全性高于MD5 数字签名 防抵赖的技术、没有保密的职能只有识别身份的作用 数字信封与PGP 数字信封 发送方将原文用对称加密传输，而将对称密钥用接收方公钥加密发送给对方 接收方收到电子信封，用自己的私钥解密信封，取出对称密钥解密得到原文 PGP协议 PGP可用于电子邮件、也可以用于文件存储。采用了杂合算法，包括IDEA、RSA、MD5、ZIP数据压缩算法 PGP承认两种不同的证书格式：PGP证书和X.509证书 PGP证书包含PGP版本号、证书持有者的公钥、证书持有者的信息、证书拥有者的数字签名、证书的有效期、密钥首选的对称加密算法 X.509证书包含证书版本、证书的序列号、签名算法标识、证书有效期、证书发行商名字、证书主体名、主体公钥信息、发布者的数字签名 练习题-设计邮件加密系统要求邮件以加密方式传输，邮件最大附件内容可达500MB，发送者不可抵赖，若邮件被第三方截获，第三方无法篡改25 PKI公钥体系 防欺骗 运行流程 PKI公钥体系分层 CA、RA、证书受理点、密钥管理中心-KMC 信息系统安全保障层次 身份认证 用户名+口令 数字证书 生物特征识别 访问控制 自主访问控制(DAC)：权限赋予过程有自主权利，给相应的主体赋予能够访问哪些客体的权限(存在安全风险) 访问控制列表(ACL)：从客体出发，看哪些主体能访问客体(给资源指定能访问的主体列表) 强制访问控制(MAC)：对主体和客体分级 基于角色的访问控制模型(RBAC) 基于任务的访问控制(TBAC) 安全审计与安全系统设计原则 安全审计的作用 震慑、警告 发现计算机的滥用情况 提供有效的追纠证据 帮助发现系统的入侵和漏洞 帮助发现系统性能上的不足 设计原则 木桶原则：首先加强短板 整体性原则 安全性评价与平衡原则 标准化和一致性原则 技术与管理相结合原则 统筹规划，分步实施原则 等级性原则 动态发展原则 易操作性原则 网络安全-各个网络层次的安全保障 SET：面向电子商务 SSL：做选择时不要先对SSL所属的层次做定义，先对其他做判断 网络威胁与攻击 威胁名称 描述 重放攻击(ARP)/ARP欺骗攻击 所截获的某次合法的通信数据拷贝，出于非法的目的而被重新发送 拒绝服务(DOS) 对信息或其它资源的合法访问被无条件地阻止 窃听 用各种可能的合法或非法的手段窃取系统中的信息资源和敏感信息。例如对通信线路中传输的信号进行搭线监听，或者利用通信设备在工作过程中产生的电磁泄漏截取游用信息等 业务流分析 通过对系统进行长期监听，利用统计分析方法对诸如通信频度、通信的信息流向、通信总量的变化等参数进行研究，从而发现有价值的通信和规律 信息泄露 信息被泄露或透露给某个非授权的实体 破坏信息的完整性 数据被非授权地进行增删、修改或破坏而受到损失 非授权访问 某一资源被非授权的人、或以非授权的方式使用 假冒 通过欺骗通信系统(或用户)达到非法用户冒充合法用户，或者特权小的用户冒充特权大的用户的目的。黑客大多时采用假冒进行攻击 旁路控制 攻击者利用系统的安全缺陷或安全性上的脆弱之处获得非授权的权利或特权。例如，攻击者通过各种攻击手段发现原本应保密，但是却又暴露出来的一些系统“特性”。利用这些“特性”，攻击者可以绕过防线守卫者侵入系统的内部 授权侵犯 被授权以某一目的的使用某一系统或资源的个人，却将此权限用于其它非授权的目的，也称为“内部攻击” 特洛伊木马 软件中含有一个察觉不出的或者无害的程序段，当它被执行时，会破坏用户的安全 陷阱门 在某个系统或某个部件中设置了“机关”，使得当提供特定的输入数据时允许违反安全策略 抵赖 这是一种来自用户的攻击，比如：否认自己曾经发布过的某条信息，伪造一份对方来行等 DoS(拒绝服务)与DDoS 破坏系统的可用性 防火墙 防外不防内 网络级：层次低，效率高(只检测IP头可以伪造) 状态检测：检测TCP/IP连接信息 应用级：层次高，效率低(开箱检查) 屏蔽子网：在外网和内网之间建立屏蔽子网区/隔离区/DMZ非军事区，既不属于内部网络也不属于外部网络。主要放对外提供服务的服务器 入侵检测 入侵检测流程 入侵检测特点 计算机病毒与木马 病毒：编制或者在计算机程序中插入的破坏计算机功能或破坏数据，影响计算机使用并且能够自我复制的一组计算机指令或者程序代码。病毒具有：传染性、隐藏性、潜伏性、破坏性、针对性、衍生性、寄生性、未知性 木马：计算机木马是一种后门程序，常被黑客用作控制远程计算机的工具 病毒分类 系统引导型病毒 文件外壳型病毒 目录型病毒：破坏目录文件 蠕虫病毒（Worm）：熊猫烧香、罗密欧与朱丽叶、恶魔、尼达姆、冲击波(感染可执行文件) 木马（Trojan）：QQ消息尾巴木马 宏病毒（Macro）：美丽莎（Melissa）、台湾1号(感染Office体系的) 几种代表性病毒 CIH病毒：史上唯一破坏硬件的病毒 红色代码：蠕虫病毒+木马 系统可靠性分析和设计系统故障模型 部件失效、物理干扰、操作或设计不当引起的错误状态 表现形式：永久性、间歇性、瞬时性 逻辑级的故障模型：短路故障、开路故障、桥接故障(物理层次) 数据结构级的故障：独立差错、算数差错、单向差错 软件故障和软件差错：非法转移、误转移、死循环、空间溢出、数据执行、无理数据 系统级的故障模型 可靠性指标 平均无故障事件(MTTF)：$MTTF = \frac{1}{\lambda}$，$\lambda$为失效率 平均故障修复时间(MTTR)：$MTTR = \frac{1}{\mu}$，$\mu$为修复率 平均故障间隔事件(MTBF)：$MTBF=MTTR+MTTF$ 系统可用性：$\frac{MTTF}{MTTR+MTTF}\times 100\%$ 在实际应用中，一般MTTR很小，所以通常认为$MTBF \approx MTTF$ 可靠性和可用性 系统可靠性是系统在规定的时间内及规定的环境条件下，完成规定功能的能力，也就是系统无故障运行的概率 系统可用性是指在某个给定的时间点上系统能够按照需求执行的效率 提高可靠性需要强调减少系统中断(故障)的次数，提高可用性需要强调减少从灾难中恢复的事件 例假设同一型号的1000台计算机，在规定的条件下工作1000小时，其中有10台出现故障。这种计算机千小时的可靠度R为$\frac{(1000-10)}{1000}=0.99$失效率为$\frac{10}{1000 \times 1000}=1\times 10^{-5}$$MTTF=\frac{1}{1\times 10^{-5}}=10^{5}小时$ 可靠性分析 串联系统 并联系统 失效率可以用$1-R$求得即1减去可靠度 N模冗余系统 混合系统 系统容错 避错技术：测试、验证等 容错技术 结构冗余（硬件冗余、软件冗余） 静态冗余（屏蔽冗余、被动冗余、模冗余系统）：通过表决和比较来屏蔽系统的错误，没有过多检测和反复的过程 动态冗余（主动冗余：备份系统、集群系统）：通过检测定位问题然后进行恢复 故障检测 故障定位 故障恢复 混合冗余： 信息冗余（校验码） 事件冗余（重复多次进行相同得计算） 冗余附加（为实现上述冗余技术所需的资源和技术） 冗余系统 故障检测 故障屏蔽 故障限制 复执 故障诊断 系统重配置 系统恢复 前向恢复：使当前的计算继续下去，把系统恢复成连贯的正确状态，弥补当前状态的不连贯情况 后向恢复：系统恢复到前一个正确状态，继续执行 系统重新启动 修复 系统重组合 后向恢复简单地把变量恢复到检查点的取值；前向恢复将对一些变量的状态进行修改和处理，且这个恢复过程将由程序设计者设计前向恢复适用于可预见的易定义的的错误；后向恢复可屏蔽不可预见的错误 软件容错 N版本程序设计 与通常软件开发过程不同的是，N版本程序设计增加了三个新的阶段：相异成份规范评审、相异性确认、背对背测试N版本程序的同步、N版本程序之间的通信、表决算法（全等表决、非精确表决、Cosmetie表决）、一致比较问题、数据相异性 恢复块方法 设计时应保证实现主块和后备块之间的独立性，避免相关错误的产生，使主块和备份块之间的共性错误降到最低程度必须保证验证测试程序的正确性 N版本和恢复块比较 恢复块方法 N版本程序设计 硬件运行环境 单机 多机 错误检测方法 验证测试程序 表决 恢复策略 后向恢复 前向恢复 实时性 差 好 防卫式程序设计 对于程序中存在的错误和不一致性，通过在程序中包含错误检查代码和错误恢复代码，使得一旦错误发生，程序能撤销错误状态，恢复到一个已知的正确状态中去实现策略：错误检测、破坏估计、错误恢复 双机容错 双机热备模式（主系统、备用系统） 双机互备模式（同时提供不同的服务，心不跳则接管） 双机双工模式（同时提供相同的服务，集群的一种） 集群技术 可伸缩性、高可用性、可管理性、高性价比、高透明性 高性能计算集群 负载均衡集群 基于特定软件的负载均衡 基于DNS的负载均衡 基于NAT的负载均衡 反向代理负载均衡 混合型负载均衡 高可用性集群 负载均衡算法 静态算法：轮转算法、加权轮转算法、最小连接数算法、加权最小连接数算法、源地址哈希散列算法、目标地址哈希散列算法、随机算法 动态算法：加权百分比算法 1.112、4 ↩2.C、B ↩3.1.8、3 ↩4.A、C ↩5.C、A、A ↩6.D、A、C ↩7.13，计算公式：a*(n-1)+1 ↩8.B ↩9.D、B(访问位为1的不能淘汰，只能淘汰为0的) ↩10.12、5、9 ↩11.B、C(指令一次性读入，只产生一次缺页中断，数据跨页产生两次) ↩12.C、D ↩13.D($\frac{4195+1}{32}$)、B(第132字第0位置为：4192，第1位置：4193，第2位置：4194，第3位置：4195)，字从1开始算，位置从0开始算 ↩14.C ↩15.A ↩16.ABCD ↩17.B ↩18.C、D、A ↩19.是 ↩20.是、否 ↩21.B ↩22.A ↩23.程序员A1:2;程序员A2:0；程序员B:0；单元测试A:1；单元测试B:0； 集成测试:0 ↩24.AC:400，EV:300，PV：500，SV=-200，CV=-100，SPI=0.6，CPI=0.75，ETC=700(按计划)/933.33(不按计划走)，EAC=1100(按计划)/1333.33(不按计划) ↩25.加密解密技术、对称加密技术、数字信封、数字摘要、数字签名 ↩]]></content>
      <categories>
        <category>软考</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[面试记录]]></title>
    <url>%2F2020%2F08%2F16%2F%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[concurrentHashMap并发实现方式 分段锁、读写效率、 synchronized关键字 监视对象、无中断、锁对象(Class也是对象) zookeeper选举 Paxos、ZAB(Zookeeper相关只是可以参考《从Paxos到Zookeeper 分布式一致性原理与实践》) redis锁 setNx方法 一致性hash 环、hash、虚拟节点 this引用逃逸 未初始化完成、构造器启动线程（参考《Java并发编程》） violate关键字 轻量级、非原子性、同步、读写屏障、禁止指令重排 单例模式 DCL双重检验、静态内部类、枚举、 cpu负载100%可能的原因（面试官说比如高并发对共享hasmap操作造成循环链表） java并发工具集 线程池(参数)、阻塞队列(QUEUE)、原子操作(Atomic*)、Future等 不停机升级服务 切流量、循环起 zookeeper实现机制 Watcher(Zookeeper相关只是可以参考《从Paxos到Zookeeper 分布式一致性原理与实践》) tcp滑动窗口 缓冲区、ACK信号(参考《计算机网路》) socket编程 阻塞、NIO、IO多路复用 拦截器和过滤器区别 拦截器 过滤器 实现方式 反射 函数回调 依赖 不依赖Servlet容器 依赖Servelet容器 作用范围 action请求 所有请求 action上下文、值栈对象 能访问 不能访问 action的生命周期 可以多次调用 一次 业务侵入 拦截器可以获取IOC容器中的各个bean，可调用业务逻辑 不能 spring事务 传播机制、声明式、隔离级别、只读、超时、回滚 主键索引和非主键索引区别 非主键索引的叶子节点存放的是主键的值，主键索引的叶子节点存放的是整行数据，其中非主键索引也被称为二级索引，而主键索引也被称为聚簇索 如何检测链表中的环 双指针、快慢 大文件如何查找IP hash，分割文件]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式]]></title>
    <url>%2F2020%2F06%2F21%2F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[面向对象 特性：封装、继承、抽象、多态 面向对象编程：以类和对象作为组织代码的基本单元，将四大特性作为代码设计和实现的基石 面向对象编程语言：支持类和对象的语法机制，能方便地实现面向对象编程四大特性的编程语言 封装(Encapsulation) 信息隐藏或数据访问保护 需要语言语法提供访问权限控制 提高类中属性可控性 提高类的易用性 抽象(Abstraction): 隐藏方法的具体实现 并不一定要为实现类定义接口类（万物皆可抽象） 继承(Inheritance): 代码复用 过度使用使可读性可维护性变差 多态(Polymorphism): 继承加方法重写实现 接口实现 提高代码的可扩展性和复用性 面向对象和面向过程 面向对象 面向过程 以类和对象作为组织代码的基本单元 以过程（方法、函数、操作等）作为组织代码的基本单元，是一种流程化的编程风格 方法和其数据结构绑定 方法和其数据结构分开 更能应对大规模复杂程序开发，使用类先建模后实现（模块化） 大规模复杂程序开发呈现网状结构，梳理逻辑困难 更易复用、扩展、维护 不易复用、扩展和维护（要实现代价更高） 更高级、人性化和智能（人的思维方式） 不够高级、人性化和智能（计算机思维方式，顺序编程） 似对象实过程 滥用set/get方法(set使封装性丧失/get返回对象使上层能够修改对象导致数据不一致) 滥用全局变量和全局方法（实在要用可以细化分类，尽量哪儿使用哪儿定义/职责单一） 定义数据和方法分离的类 基于贫血模型的开发模式 抽象类和接口 抽象类 接口 不允许实例化(new报错)，只能被继承 不能是实例化，只能实现 可以包含属性和方法，方法可以实现也可以不实现(抽象方法) 不能包含属性，只能声明方法不能包含具体实现 子类继承抽象类必须实现抽象方法 类实现接口时必须实现接口声明的所有方法 一种特殊的类 (is-a) 表示具有某些功能(has-a)/协议(contract) 自下而上，现有子类代码重复然后抽象为父类 自上而下，先考虑接口再考虑具体实现 抽象类 代码复用 能使用多态特性(父类无法实例化，子类强制要求实现抽象方法) 无抽象类语法的问题： 影响父类代码可读性 忘记重新父类的方法（抽象类中为抽象方法的部分） 父类能被实例化，增加类被误用的风险 接口 提高复用性 解耦(侧重) 模拟实现抽象类和接口 不能实例化(显式定义构造函数且并让其拥有访问控制，比如java修饰为protected） 成员变量(不定义） 子类必须实现方法(abstract/virtual修饰，不支持的语言如python直接在父类方法抛出异常) 基于接口（抽象）而非实现编程(program to an interface,not to an implementation)： 抽象层面的接口指的一组协议或约定 代码层面上接口可理解为编程语法的接口类或抽象类 将接口和实现分离，暴露稳定的接口，封装不稳定的实现（实现更改时调用方不用更改，提高可扩展性） 如何遵循： 函数命名不能暴露任何实现细节 封装具体的实现细节 为实现类定义抽象的接口 功能如只有一种实现方式，未来也不会改动（没必要实现接口） 多用组合少用继承不推荐继承的原因 继承层次过深过复杂影响代码可维护性 父类修改影响所有子类 组合： 职责单一，可复用（通过组合、接口、委托） 需要更细粒度的拆分/更多的类和接口 如何选择 继承：类之间继承结构稳定、层次浅，关系不复杂，无法改变类中方法只能继承 组合：系统不稳定、层次深，关系复杂 贫血模型、充血模型和领域驱动设计(DDD)：贫血模型 只包含数据不包含业务逻辑的类（如Entity） 将数据与操作分离，破坏了OOP的封装性 重Service轻BO（BO只包含数据） 面向SQL开发 充血模型 数据和对应的业务逻辑封装在一个类里 轻Service重Domain（Domain包括数据和业务逻辑） 领域模型相当于可复用的业务中间件 DDD 指导如何解耦业务系统，划分业务模块，定义业务领域模型及其交互 贫血盛行原因 业务系统简单，基于CRUD都能完成 充血模型设计更有难度 思维固化 充血模型场景 复杂系统（包括各种利息计算模型和还款模型的金融系统） 贫血模型 充血模型 业务逻辑全在service 部分业务逻辑移到domain controller和repository层两者基本相同 保留service不移除是为了和repository层交互 设计原则单一职责原则 一个类或模块只负责完成一个职责或功能 实现代码高内聚低耦合，提高代码复用性、可读性和可维护性 拆分过细会降低内聚性 判断是否职责是否单一 类中代码行数、函数或属性过多（200行/10个） 类依赖的其他类过多，或者依赖类的其他类过多 私有方法过多 类中的大量方法集中操作类中的某几个属性 开闭原则 软件实体（类模块、类、方法等）应该对扩展开放，对修改关闭 以最小修改代码的代价来完成新功能的开发 同样的改动，在粗粒度下被认为是修改，细粒度下被认定为扩展 方法 具备扩展意识、抽象意识、封装意识 事先留好扩展点 多态、依赖注入、基于接口而非实现编程，以及大部分的设计模式（比如，装饰、策略、模板、职责链、状态） 里氏替换原则 子类对象能够替换父类对象出现的任何地方，并且保证原来程序的逻辑行为不变及正确性不被破坏 按照协议设计：子类设计时要遵守父类的行为约定（输入、输出、异常的约定甚至注释中罗列的任何特殊说明） 接口隔离原则 客户端（接口的调用者或使用者）不应该强迫依赖它不需要的接口 接口含义 一组API接口集合：某个为服务或类库的接口，部分接口只被部分调用者使用就应该将这部分接口给力出来单独提供给对应的调用者 单个API接口或函数：函数的设计要功能单一，不将多个不同的功能逻辑在一个函数中实现 OOP中的接口概念 接口隔离 单一职责 更侧重于接口设计 针对模块、类、接口 提供了判断接口是否单一的标准：通过调用者如何使用接口 拆分粒度可大可小、可粗可细，没有明确的标准 依赖反转原则/依赖倒置原则 高层（调用者）模块不依赖低层（被调用者）模块，高层模块和低层模块应该通过抽象来互相依赖；抽象不依赖具体实现细节，具体实现细节依赖抽象 控制反转：控制指对程序执行流程的控制，反转指的是使用框架前程序员自己控制程序执行；使用框架后整个执行流程通过框架控制 依赖注入：不通过 new 的方式在类内部创建依赖类的对象，而是将依赖的类对象在外部创建好之后，通过构造函数、函数参数等方式传递（或注入）给类来使用 KISS原则 尽量保持简单 本身便复杂的问题用复杂的方法解决不算违背KISS原则 方法 不要使用复杂的技术或过于高级的特性 善于使用已经有的工具类 不要过度优化 YAGNI原则 不要设计当前用不到的功能 DRP原则 不要重复你自己 重复情况 实现逻辑重复：实现相同但语义不同不视为违反DRP；对于重复的代码可以抽象为更细粒度的函数来解决 功能语义重复：功能语义重复就算实现不同，也认为违反了DRP 代码执行重复 代码复用性 代码复用 DRP原则 表示代码可被复用的特性或能力 表示一种行为：开发新功能时复用已经存在的代码 不要写重复的代码 不重复不代表可复用 从代码开发者的角度 从代码使用者的角度 提高复用性 第一次不考虑复用性，第二次用到再来重构 减少代码耦合 满足单一职责原则：粒度越细越容易被复用 模块化：将功能独立的代码封装成模块 业务与非业务逻辑分离：越是业务无关的代码越容易复用 通用代码下沉：越底层的代码越通用，只允许上层调用下层和同层，不允许下层调用上层 继承、多态、抽象、封装：越抽象越不依赖具体实现；隐藏可变细节，暴露不变接口 应用模版等设计模式 泛型编程 迪米特法则（最小知识原则） 每个模块只应该了解那些与它关系密切的模块 高内聚：相近的功能应该放到同一个类中，不相近的功能不放到同一个类 低耦合：类与类之间的依赖关系简单清晰 规范与重构概念 重构是一种对软件内部结构的改善，目的是在不改变软件的可见行为的情况下，使其更易理解，修改成本更低 持续重构 重构对象 大规模高层次（大型重构） 小规模低层次（小型重构） 对象：系统、模块、代码结构、类与类之间的关系 对象：类、函数、变量等代码级 手段：分层、模块化、解耦、抽象可复用组件 手段：规范命名、规范注释、消除超大类或函数、提取重复代码 提前做好完善的重构计划，每个阶段完成一部分在开始下一阶段，保证代码一直处于可运行逻辑正确的状态 因范围小，随时都可以 单元测试 集成测试 单元测试 研发工程师自己编写，测试代码正确性 整个系统或某个功能模块，是端到端的测试 类或者函数是否按照预期逻辑执行，代码层级 粒度更小 作用 单元测试能有效发现代码中的bug 单元测试能发现代码设计上的问题 单元测试是对集成测试的有力补充 写单元测试的过程本身就是重构的过程 阅读单元测试能快速熟悉代码 单元测试是TDD可落地执行的改进方案 方法 编写单元测试尽管繁琐但并不耗时，大多可以copy-paste 可以稍微放低对单元测试代码质量的要求 覆盖率作为衡量单元测试的唯一标准是不合理的 单元测试不要依赖被测试代码的具体实现逻辑 单元测试框架无法测试，多半是因为代码的可测试性不好 可测试性 针对代码编写单元测试的难易程度 方法 使用依赖注入而不是new 多用组合少用继承 mock解决外部以来的服务问题（面向接口而非实现编程） 测试不友好的代码 代码中包含未决行为逻辑 滥用可变全局变量 滥用静态方法 使用复杂的继承关系 高度耦合的代码 解耦 解耦保证代码松耦合高内聚，是控制代码复杂度的有效手段 判断方法 间接：看代码修改是否牵一发而动全身 直接：画出模块与模块、类与类之间的依赖关系图，根据依赖关系图的复杂性判断是否需要解耦 方法 封装和抽象 中间层 引入一个中间层，包裹老的接口，提供新的接口定义 新开发的代码依赖中间层提供的新接口 将依赖老接口的代码改为调用新接口 确保所有的代码都调用新接口之后，删除掉老的接口 模块化 其它设计思想和原则 单一职责原则 基于接口而非实现编程 依赖注入 多用组合少用继承 迪米特法则 编码规范命名 长度：以能准确达意为目标，众人熟知的用缩写，函数内临时变量可以短，类名等作用域大的用长命名 利用上下文简化命名 命名要可读可搜索：不用生僻难发音的单词，统一规约方便搜索 命名接口和抽象类：接口加I前缀或者不加而其实现类加impl后缀；抽象类加Abstract前缀或不加；项目里保证统一 注释 内容 做什么/为什么/怎么做：简单的类名不能包涵太多信息，需要注释详尽描述；注释起到总结性作用、文档的作用；总结性注释让代码结构更清晰 多少 太多意味着代码可读性不高，需要大量注释，且后续维护困难；类和函数要写注释，要尽量详尽、全面；函数内部注释较小，靠好的命名约束，提高可读性 代码风格 类和函数多大合适 函数大小不超过显示屏的垂直高度（50行） 类行数过多：当一个类的代码阅读困难；实现某个功能不知道该用哪个函数；只用到一个小功能需要引入整个类 一行代码多长合适 一行代码最长不能超过IDE显示的宽度 善用空行分割单元块 成员变量与函数之间 静态成员变量和普通成员变量、函数之间 成员变量之间 四格缩进还是两格缩进 跟业内推荐的风格统一，跟著名开源项目统一 推荐两格缩进：节省空间；四格缩进代码嵌套层次过深，累计缩进过多，容易导致一行折成两行 不用tab键缩进，不同ide下tab键的显示宽度不同 打括号是否要另起一行 将括号放到语句同一行：节省代码行数 跟随业内标准和开源项目 类中成员的排列顺序 成员变量排在函数前面 成员变量和函数之间按照先静态（静态成员变量或静态函数）后普通（非静态成员变量或非静态函数）的方式排列 成员变量和函数之间按照作用域范围从大到小的顺序排列，先写public成员变量或函数，然后写protected，最后是private 把有调用关系的函数放一块儿，一个public调用一个private则两者放一块儿 编程技巧 把代码分割成更小的单元块 避免函数参数过多 考虑函数职责是否单一，是否能通过拆分为多个函数的方式减少参数 将函数的参数封装成对象 勿用函数参数来控制逻辑：存在控制标志可以考虑拆分为不同函数，保持职责单一 函数设计要职责单一 移除过深的嵌套层次 去掉多余的if或else语句 使用编程语言提供的continue、break、return关键字提前退出嵌套 调整执行顺序减少嵌套 将部分嵌套逻辑封装成函数调用 使用多态来替代if-else、switch-case条件判断的方法 学会使用解释性变量 常量取代魔法数字 使用解释性变量来解释复杂表达式 发现代码质量问题常规checklist 目录设置是否合理 模块划分是否清晰 代码结构是否满足“高内聚、松耦合” 是否遵循经典的设计原则和设计思想（SOLID、DRY、KISS、YAGNI、LOD 等） 设计模式是否应用得当、是否有过度设计 代码是否容易扩展，如果要添加新功能，是否容易实现 代码是否可以复用，是否可以复用已有的项目代码或类库，是否有重复造轮子 代码是否容易测试，单元测试是否全面覆盖了各种正常和异常的情况 代码是否易读，是否符合编码规范（比如命名和注释是否恰当、代码风格是否一致等） 业务需求checklist 代码是否实现了预期的业务需求 逻辑是否正确，是否处理了各种异常情况 日志打印是否得当，是否方便 debug 排查问题 接口是否易用，是否支持幂等、事务等 代码是否存在并发问题，是否线程安全 性能是否有优化空间，比如，SQL、算法是否可以优化 是否有安全漏洞，比如输入输出校验是否全面 函数出错返回什么 返回错误码（有异常处理机制尽量不用错误码） 返回null值（查询类表示数据不存在可用） 返回空对象（空字符串和空集合） 抛出异常对象 处理异常 直接吞掉 原封不动re-throw 包装成新的异常re-throw 原则 如果 func1() 抛出的异常是可以恢复，且 func2() 的调用方并不关心此异常，可以在 func2() 内将 func1() 抛出的异常吞掉 如果 func1() 抛出的异常对 func2() 的调用方来说，也是可以理解的、关心的 ，并且在业务概念上有一定的相关性，可以选择直接将 func1 抛出的异常 re-throw； 如果 func1() 抛出的异常太底层，对 func2() 的调用方来说，缺乏背景去理解、且业务概念上无关，可以将它重新包装成调用方可以理解的新异常，然后 re-throw 设计模式与范式：创建型单例模式 一个类只允许创建一个对象（或者叫实例） 为什么使用单例模式 处理资源访问冲突 表示全局唯一类 如何实现一个单例 构造函数需要private访问权限，这样才能避免外部通过new创建实例 考虑对象创建时的线程安全问题 考虑是否支持延迟加载 考虑getInstance()性能是否足够高（是否加锁） 分类 饿汉式：在类加载的时候，instance 静态实例就已经创建并初始化好了；不支持延迟加载 懒汉式：使用时再初始化，但因为加锁导致并行度低；支持延迟加载 双重检测：即支持延迟加载又支持高并行 静态内部类：运用静态内部类的特性，类似饿汉式但是做到了延迟加载，instance的唯一性和线程安全性都由JVM保证，所以这种方式即保证线程安全又支持懒加载 枚举：通过java枚举类型本身的特性保证线程安全和实例的唯一性 单例模式存在的问题 对OOP特性的支持不友好 单例会隐藏类之间的依赖关系 单例对代码的扩展性不友好 单例对代码的可测试性不友好 单例不支持有参数的构造函数 解决方案 创建完实例后再调用一个init()函数传递参数 将参数放到getInstance()方法中（存在调用两次传递不同参数的问题，可以在传递两次参数不同时给予提示） 将参数放到另一个全局变量中（最推荐的方式） 替代解决方案 可以使用静态方法而不是单例保证全局唯一（但存在比单例更多的问题，不支持延迟加载，不灵活 工厂模式保证类对象全局唯一性 IOC容器保证类对象全局唯一性 其他知识点 单例类对象中的唯一性的作用范围是进程唯一的 线程唯一的单例可以通过hashmap实现，key存储线程id，value存储对象；Java提供的ThreadLocal并发工具类可以实现线程单例 集群间单例可以通过将对象存储在外存，通过序列化反序列化获取对象，通过加锁避免其他进程获取对象 多例模式可以通过Map存储对象类型和对象之间的对应关系来控制对象个数 对于Java而言，单例的作用域并非进程而是类加载器（因为双亲委托模式） 工厂模式分类 简单工厂 工厂方法 抽象工厂 场景 代码中存在 if-else 分支判断，动态地根据不同的类型创建不同的对象。针对这种情况，考虑使用工厂模式，将if-else 创建对象的代码抽离出来，放到工厂类中（简单工厂模式） 单个对象本身的创建过程比较复杂，比如做各种初始化操作。在这种情况下，考虑使用工厂模式，将对象的创建过程封装到工厂类中（工厂方法模式） 作用 封装变化：创建逻辑有可能变化，封装成工厂类之后，创建逻辑的变更对调用者透明 代码复用：创建代码抽离到独立的工厂类之后可以复用 隔离复杂性：封装复杂的创建逻辑，调用者无需了解如何创建对象 控制复杂度：将创建代码抽离出来，让原本的函数或类职责更单一，代码更简洁 DI容器设计 配置解析：容器读取配置文件，根据配置文件提供的信息来创建对象 对象创建：利用反射机制动态加载创建对象 对象生命周期管理：如返回单例对象还是调用一次创建一个、是否支持懒加载、配置对象init和destroy方法 建造者模式使用场景 属性过多，避免构造方法参数列表过长 属性之间有一定的依赖关系或者约束条件 希望创建不可变对象，对象一旦创建后不能改变，所以不能暴露set方法设置属性值 实现方式 私有化构造函数 定义静态内部类Builder，暴露set方法设置属性 调用Builder的build方法创建对象 和工厂模式的区别 工厂模式：创建不同的同一类型对象，由给定的参数来创建哪种类型的对象 建造者模式：创建一种类型的复杂对象，通过可设置的参数定制化创建对象 原型模式 基于原型来创建对象 应用场景 对象中的数据需要进行复杂的计算才能获得 需要从RPC、网络、数据库、文件系统等慢速I/O中读取 实现方式 深拷贝：即复制索引也复制数据，得到的是完全独立的对象 浅拷贝：只复制索引本身，不复制数据，得到的对象和原始对象共享一份数据（除非操作非常耗时，否则不推荐浅拷贝）在 Java 语言中，Object 类的 clone() 方法执行的就是浅拷贝。它只会拷贝对象中的基本数据类型的数据（比如，int、long），以及引用对象（SearchWord）的内存地址，不会递归地拷贝引用对象本身 深拷贝实现 递归拷贝对象、对象的引用对象以及引用对象的引用对象，直到要拷贝的对象只包含基本数据类型数据，没有引用对象为止 先将对象序列化再反序列化为新的对象 应用方式 先通过浅拷贝复制对象，然后对需要更新的部分采用深拷贝，即利用了浅拷贝节省时间、空间的优点，又兼顾了深拷贝所具备的数据不共享性，不会因为更改而影响老数据的使用 设计模式与范式：结构型代理模式 在不改变原始类（或叫被代理类）代码的情况下，通过引入代理类来给原始类附加功能 静态代理 代理类和原始类实现相同的接口，原始类只负责业务功能，代理类负责在业务代码执行前后附加其他逻辑代码，并通过委托的方式调用原始类来执行业务代码（需要原始类和代理类有相同的接口） 如果原始类与代理类没有相同的接口，并且原始类的代码并非自己维护，对于这种外部类扩展采用继承的方式，通过代理类继承原始类，然后扩展附加功能 存在的问题 代理类中需要将原始类所有方法实现一遍，并且每个方法都附加相似的代码逻辑 如果要添加的附加功能的类不止一个，需要针对每个类创建一个代理类 动态代理 不事先为每个原始类编写代理类，而是在运行的时候，动态地创建原始类对应的代理类12345678910111213141516171819202122232425262728293031323334353637public class MetricsCollectorProxy &#123; private MetricsCollector metricsCollector; public MetricsCollectorProxy() &#123; this.metricsCollector = new MetricsCollector(); &#125; public Object createProxy(Object proxiedObject) &#123; Class&lt;?&gt;[] interfaces = proxiedObject.getClass().getInterfaces(); DynamicProxyHandler handler = new DynamicProxyHandler(proxiedObject); return Proxy.newProxyInstance(proxiedObject.getClass().getClassLoader(), interfaces, handler); &#125; private class DynamicProxyHandler implements InvocationHandler &#123; private Object proxiedObject; public DynamicProxyHandler(Object proxiedObject) &#123; this.proxiedObject = proxiedObject; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; long startTimestamp = System.currentTimeMillis(); Object result = method.invoke(proxiedObject, args); long endTimeStamp = System.currentTimeMillis(); long responseTime = endTimeStamp - startTimestamp; String apiName = proxiedObject.getClass().getName() + ":" + method.getName(); RequestInfo requestInfo = new RequestInfo(apiName, responseTime, startTimestamp); metricsCollector.recordRequest(requestInfo); return result; &#125; &#125;&#125;//MetricsCollectorProxy使用举例MetricsCollectorProxy proxy = new MetricsCollectorProxy();IUserController userController = (IUserController) proxy.createProxy(new UserController()); 使用场景 业务系统的非功能性需求开发：监控、统计、鉴权、限流、事务、幂等、日志 在RPC、缓存中使用 桥接模式 将抽象和实现解耦，让它们可以独立变化 一个类存在两个（或多个）变化的维度，可以通过组合的方式，让两个（或多个）纬度可以独立扩展 使用场景 需要分离抽象和实现的业务场景（参见jdbc Driver源码） 装饰器模式解决问题 解决继承关系过于复杂的问题，通过组合来替代继承 作用 给原始类增强功能 特点 可以对原始类嵌套使用多个装饰器（装饰器类需要跟原始类继承相同的抽象类或接口） 适配器模式 将不兼容的接口转换为可兼容的接口，让原本由于接口不兼容而不能一起工作的类可以一起工作 实现方式 类适配器：使用继承实现 对象适配器：使用组合实现 选择依据 如果待转化的接口方法不多，两种均可 如果待转化的接口方法很多，而且大多数和待适配的接口相同，推荐使用类适配器，因为适配器能复用父类的方法，比起对象适配器代码量更少 如果待转化的接口方法很多，而大多数与待适配的接口不同，推荐使用对象适配器，因为组合结构相对于继承更灵活 使用场景 封装有缺陷的接口设计 统一多个类的接口设计 替换以来的外部系统 兼容老版本接口 适配不同格式的数据 代理、桥接、装饰器、适配器 4 种设计模式的区别 代理模式：代理模式在不改变原始类接口的条件下，为原始类定义一个代理类，主要目的是控制访问，而非加强功能，这是它跟装饰器模式最大的不同。 桥接模式：桥接模式的目的是将接口部分和实现部分分离，从而让它们可以较为容易、也相对独立地加以改变。 装饰器模式：装饰者模式在不改变原始类接口的情况下，对原始类功能进行增强，并且支持多个装饰器的嵌套使用。 适配器模式：适配器模式是一种事后的补救策略。适配器提供跟原始类不同的接口，而代理模式、装饰器模式提供的都是跟原始类相同的接口 门面模式 门面模式为子系统提供一组统一的接口，定义一组高层接口让子系统更易用 组织门面接口 门面接口不多，可以和非门面接口放在一起不做特殊标记 门面接口很多，在已有的接口上再重新抽象一层专门放置门面接口，从类、包的命名上跟原来的接口层做区分 门面接口特别多，可以将门面接口放到一个新的子系统中 应用场景 通过将多个接口调用替换为一个门面接口调用来解决因需要调用多个接口造成的性能问题 通过提供一组更加简单易用、更高层的接口来解决易用性问题 通过门面模式包装需要保证事务性的多个接口来解决分布式事务问题 组合模式 将一组对象组织（Compose）成树形结构，以表示一种“部分 - 整体”的层次结构。组合让客户端可以统一单个对象和组合对象的处理逻辑 前提 业务场景必须能表示成树形结构 享元模式 当一个系统中存在大量重复对象的时候，如果这些重复的对象是不可变对象，就可以利用享元模式将对象设计成享元，在内存中只保留一份实例，供多处代码引用 代码结构 通过工厂模式，在工厂类中通过一个map来缓存已经创建过的享元对象来达到复用的目的 享元模式 vs 单例、缓存、对象池 单例：在单例模式中，一个类只能创建一个对象，而在享元模式中，一个类可以创建多个对象，每个对象被多处代码引用共享 缓存：平时所讲的缓存，主要是为了提高访问效率，而非复用 池化技术：池化技术中的“复用”可以理解为“重复使用”，主要目的是节省时间；享元模式中的“复用”可以理解为“共享使用”，在整个生命周期中，都是被所有使用者共享的，主要目的是节省空间 享元在Integer中的使用 在 Java Integer 的实现中，-128 到 127 之间的整型对象会被事先创建好，缓存在 IntegerCache 类中。当使用自动装箱或者 valueOf() 来创建这个数值区间的整型对象时，会复用 IntegerCache 类事先创建好的对象。IntegerCache 类就是享元工厂类，事先创建好的整型对象就是享元对象 享元在String中的使用 String 类的享元模式的设计，跟 Integer 类稍微有些不同。Integer 类中要共享的对象，是在类加载的时候，就集中一次性创建好的。对于字符串来说，是在某个字符串常量第一次被用到的时候，存储到常量池中，当之后再用到的时候，直接引用常量池中已经存在 设计模式与规范：行为型观察者模式 观察者模式也称为发布订阅模式，在对象之间定义一个一对多的依赖，当一个对象状态改变的时候，所有依赖的对象都会自动收到通知 实现方式 同步阻塞：观察者和被观察者代码在同一个线程内执行，被观察者一直阻塞，直到所有的观察者代码都执行完成之后，才执行后续的代码 异步非阻塞：在每个 执行函数中，创建一个新的线程执行代码；基于 EventBus 来实现 进程内的实现方式：以上两种均为进程内实现方式 跨进程的实现方式：基于消息队列的方式 模版模式 模板方法模式在一个方法中定义一个算法骨架，并将某些步骤推迟到子类中实现。模板方法模式可以让子类在不改变算法整体结构的情况下，重新定义算法中的某些步骤 作用 复用 Java InputStream Java AbstractList 扩展：基于这个作用，模板模式常用在框架的开发中，让框架用户可以在不修改框架源码的情况下，定制化框架的功能 Java Servlet JUnit TestCase 回调原理 A 类事先注册某个函数 F 到 B 类，A 类在调用 B 类的 P 函数的时候，B 类反过来调用 A 类注册给它的 F 函数。这里的 F 函数就是“回调函数”。A 调用 B，B 反过来又调用 A，这种调用机制就叫作“回调”。 分类 同步回调：函数返回之前执行回调函数（像模板模式） 异步回调：函数返回之后执行回调函数（像观察者模式） 应用 JdbcTemplate：JdbcTemplate 通过回调的机制，将不变的执行流程抽离出来，放到模板方法 execute() 中，将可变的部分设计成回调 StatementCallback，由用户来定制。query() 函数是对 execute() 函数的二次封装，让接口用起来更加方便。 setClickListener：往 setOnClickListener() 函数中注册好回调函数之后，并不需要等待回调函数执行 addShutdownHook：Tomcat和JVM的shutdown hook；JVM 提供了 Runtime.addShutdownHook(Thread hook) 方法，可以注册一个 JVM 关闭的 Hook。当应用程序关闭的时候，JVM会自动调用Hook代码 回调 模板模式 应用场景几乎一致 应用场景几乎一致 基于组合关系来实现，把一个对象传递给另一个对象，是一种对象之间的关系 基于继承关系来实现，子类重写父类的抽象方法，是一种类之间的关系 像 Java 这种只支持单继承的语言，基于模板模式编写的子类，已经继承了一个父类，不再具有继承的能力 可以使用匿名类来创建回调对象，可以不用事先定义类 针对不同的实现都要定义不同的子类 回调就更加灵活，只需要往用到的模板方法中注入回调对象即可 如果某个类中定义了多个模板方法，每个方法都有对应的抽象方法，那即便我们只用到其中的一个模板方法，子类也必须实现所有的抽象方法 策略模式 定义一族算法类，将每个算法分别封装起来，让它们可以互相替换。策略模式可以使算法的变化独立于使用它们的客户端 策略定义 包含一个策略接口和一组实现这个接口的策略类。所有的策略类都实现相同的接口，客户端代码基于接口而非实现编程，可以灵活地替换不同的策略 策略创建 使用策略的时候通过类型来判断创建哪个策略使用 为了封装创建细节，把根据类型创建策略的逻辑抽离出来放到工厂类中 无状态可以缓存策略，不用每次创建 有状态需要每次创建 策略使用 运行时动态确定使用哪种策略，即在程序运行期间根据配置、用户输入、计算结果等不确定因素动态决定使用哪种策略 解决if-esle过多的问题 不每次创建的策略实际是使用查表法，根据类型查表替代根据类型分支判断 每次创建的策略实际是将原本的if-else分支判断转移到工厂之中 作用 解耦策略的定义、创建和使用，控制代码的复杂度，让每个部分都不至于过于复杂、代码量过多 对于复杂代码来说，还能让其满足开闭原则，添加新策略的时候，最小化、集中化代码改动，减少引入 bug 的风险 职责链模式 将请求的发送和接收解耦，让多个接收对象都有机会处理这个请求。将这些接收对象串成一条链，并沿着这条链传递这个请求，直到链上的某个接收对象能够处理它为止 实现方式 用链表存储处理器 用数组存储处理器 应用场景 敏感词过滤 过滤器（Servlet Filter） 拦截器（Spring Interceptor） 为什么 可以应对代码复杂性 让代码满足开闭原则，提高代码扩展性 配置链更灵活，可以选择配置哪几个处理器而不是全部 Servlet Filter tomcat ApplicationFilterChain：使用递归实现，可以做到双向过滤 Spring Interceptor 因为请求和响应分开拦截，所以未使用递归 AOP、Servlet Filter、Spring Interceptor选择 根据需求粒度进行选择 Servlet Filter：与框架无关，对所有请求响应有效，能处理原始请求，但无法获取请求控制器的信息 Spring Interceptor：与框架耦合，可以通过Spring提供配置，相比而言更灵活 AOP：业务内拦截，更细粒度 状态模式状态机实现方式 分支逻辑法：参照状态转移图，将每一个状态转移，原模原样地直译成代码。编写的代码会包含大量的 if-else 或 switch-case 分支判断逻辑，甚至是嵌套的分支判断逻辑 查表法：状态机用二维表来表示，在二维表中，第一维表示当前状态，第二维表示事件，值表示当前状态经过事件之后，转移到的新状态及其执行的动作 状态模式：通过将事件触发的状态转移和动作执行，拆分到不同的状态类中，来避免分支判断逻辑 迭代器模式迭代器模式将集合对象的遍历操作从集合类中拆分出来，放到迭代器类中，让两者的职责更加单一 实现 迭代器中需要定义 hasNext()、currentItem()、next() 三个最基本的方法 待遍历的容器对象通过依赖注入传递到迭代器类中 容器通过 iterator() 方法来创建迭代器 使用迭代器的原因 迭代器模式封装集合内部的复杂数据结构，开发者不需要了解如何遍历，直接使用容器提供的迭代器即可 迭代器模式将集合对象的遍历操作从集合类中拆分出来，放到迭代器类中，让两者的职责更加单一 迭代器模式让添加新的遍历算法更加容易，更符合开闭原则。除此之外，因为迭代器都实现自相同的接口，在开发中，基于接口而非实现编程，替换迭代器也变得更加容易 修改集合导致未决行为 遍历时不允许增删元素（无法得知结束点，实现困难） 增删元素后让遍历报错（设置modCount和expectedModCount，修改之后更新modCount，迭代时每次检查两个值，不相等则报错） 安全删除集合元素（Java） 增加一个lastRet成员变量来记录游标指向的前一个元素，通过迭代器去删除这个元素的时候，更新迭代器中的游标和 lastRet 值，来保证不会因为删除元素而导致某个元素遍历不到 实现支持快照功能的迭代器 在迭代器类中定义一个成员变量 snapshot 来存储快照。每当创建迭代器的时候，都拷贝一份容器中的元素到快照中，后续的遍历操作都基于这个迭代器自己持有的快照来进行（由于拷贝，会增加内存消耗） 为每个元素保存两个时间戳，一个是添加时间戳 addTimestamp，一个是删除时间戳 delTimestamp。当元素被加入到集合中的时候，将 addTimestamp 设置为当前时间，将 delTimestamp 设置成最大长整型值（Long.MAX_VALUE）。当元素被删除时，将 delTimestamp 更新为当前时间，表示已经被删除，只做标记删除（三个数组，一个元素数组，两个时间戳数组，为了支持随机下表访问，再增加一个元素数组，一个真实删除，一个作为快照数组逻辑删除） 访问者模式 允许一个或多个操作应用到一组对象上，解耦操作和对象本身 设计意图 解耦操作和对象本身 保持类职责单一 满足开闭原则 应对代码的复杂性 设计难点 难点在代码实现。原因是函数重载在大部分面向对象编程语言中是静态绑定的。调用类的哪个重载函数，是在编译期间，由参数的声明类型决定的，而非运行时，根据参数的实际类型决定的 应用场景 访问者模式针对的是一组类型不同的对象（PdfFile、PPTFile、WordFile）。尽管这组对象的类型是不同的，但是它们继承相同的父类（ResourceFile）或者实现相同的接口。在不同的应用场景下，我们需要对这组对象进行一系列不相关的业务操作（抽取文本、压缩等），但为了避免不断添加功能导致类（PdfFile、PPTFile、WordFile）不断膨胀，职责越来越不单一，以及避免频繁地添加功能导致的频繁代码修改，我们使用访问者模式，将对象与操作解耦，将这些业务操作抽离出来，定义在独立细分的访问者类（Extractor、Compressor）中1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192public abstract class ResourceFile &#123; protected String filePath; public ResourceFile(String filePath) &#123; this.filePath = filePath; &#125; abstract public void accept(Visitor vistor);&#125;public class PdfFile extends ResourceFile &#123; public PdfFile(String filePath) &#123; super(filePath); &#125; @Override public void accept(Visitor visitor) &#123; visitor.visit(this); &#125; //...&#125;//...PPTFile、WordFile跟PdfFile类似，这里就省略了...public interface Visitor &#123; void visit(PdfFile pdfFile); void visit(PPTFile pdfFile); void visit(WordFile pdfFile);&#125;public class Extractor implements Visitor &#123; @Override public void visit(PPTFile pptFile) &#123; //... System.out.println("Extract PPT."); &#125; @Override public void visit(PdfFile pdfFile) &#123; //... System.out.println("Extract PDF."); &#125; @Override public void visit(WordFile wordFile) &#123; //... System.out.println("Extract WORD."); &#125;&#125;public class Compressor implements Visitor &#123; @Override public void visit(PPTFile pptFile) &#123; //... System.out.println("Compress PPT."); &#125; @Override public void visit(PdfFile pdfFile) &#123; //... System.out.println("Compress PDF."); &#125; @Override public void visit(WordFile wordFile) &#123; //... System.out.println("Compress WORD."); &#125;&#125;public class ToolApplication &#123; public static void main(String[] args) &#123; Extractor extractor = new Extractor(); List&lt;ResourceFile&gt; resourceFiles = listAllResourceFiles(args[0]); for (ResourceFile resourceFile : resourceFiles) &#123; resourceFile.accept(extractor); &#125; Compressor compressor = new Compressor(); for(ResourceFile resourceFile : resourceFiles) &#123; resourceFile.accept(compressor); &#125; &#125; private static List&lt;ResourceFile&gt; listAllResourceFiles(String resourceDirectory) &#123; List&lt;ResourceFile&gt; resourceFiles = new ArrayList&lt;&gt;(); //...根据后缀(pdf/ppt/word)由工厂方法创建不同的类对象(PdfFile/PPTFile/WordFile) resourceFiles.add(new PdfFile("a.pdf")); resourceFiles.add(new WordFile("b.word")); resourceFiles.add(new PPTFile("c.ppt")); return resourceFiles; &#125;&#125; 单分派（Single Dispatch） 执行哪个对象的哪个方法，只跟“对象”的运行时类型有关 执行哪个对象的方法，根据对象的运行时类型来决定 执行对象的哪个方法，根据方法参数的编译时类型来决定 双分派（Double Dispatch） 执行哪个对象的哪个方法，跟“对象”和“方法参数”两者的运行时类型有关 执行哪个对象的方法，根据对象的运行时类型来决定 执行对象的哪个方法，根据方法参数的运行时类型来决定。 备忘录模式 在不违背封装原则的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态，以便之后恢复对象为先前的状态 要求 存储副本以便后期恢复 在不违背封装原则的前提下，进行对象的备份和恢复 应用场景 防丢失 撤销 恢复 与备份的区别 备忘录模式：侧重于代码的设计和实现 备份：侧重架构设计和产品设计 大对象备份问题 只备份必要的恢复信息，结合最新的数据来恢复 全量备份和增量备份相结合 低频全量备份，高频增量备份，两者结合来做恢复 命令模式 命令模式将请求（命令）封装为一个对象，这样可以使用不同的请求参数化其他对象（将不同请求依赖注入到其他对象），并且能够支持请求（命令）的排队执行、记录日志、撤销等（附加控制）功能 应用场景 用来控制命令的执行，比如，异步、延迟、排队执行命令、撤销重做命令、存储命令、给命令记录日志等等 与策略模式的区别 策略模式：不同的策略具有相同的目的、不同的实现、互相之间可以替换 命令模式：不同的命令具有不同的目的，对应不同的处理逻辑，并且互相之间不可替换 解释器模式 解释器模式为某个语言定义它的语法（或者叫文法）表示，并定义一个解释器用来处理这个语法 实现 思想：将语法解析的工作拆分到各个小类中，以此来避免大而全的解析类 做法：语法规则拆分成一些小的独立的单元，然后对每个单元进行解析，最终合并为对整个语法规则的解析 中介模式 中介模式定义了一个单独的（中介）对象，来封装一组对象之间的交互。将这组对象之间的交互委派给与中介对象交互，来避免对象之间的直接交互 与观察者模式的区别 观察者模式：参与者之间的交互比较有条理，一般都是单向的，一个参与者只有一个身份，要么是观察者，要么是被观察者 中介模式：参与者之间的交互关系错综复杂，既可以是消息的发送者、也可以同时是消息的接收者]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高并发设计]]></title>
    <url>%2F2020%2F04%2F04%2F%E9%AB%98%E5%B9%B6%E5%8F%91%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[基础通用设计方法 横向扩展(Scale out) Scale up(纵向扩展） Scale out（横向扩展） 通过购买性能更好的硬件提升并发处理能力 将多个低性能的机器组成一个分布式集群来共同抵御高并发流量 简单 引入一定的复杂度 系统设计初期 系统并发超过单机限制 缓存-&gt;提升系统的访问性能 CPU执行指令和内存寻址ns（纳秒）级别，千兆网卡读取数据微秒级别 磁盘因为物理结构存在寻道时间，是计算机中慢的一环 异步 同步 异步 调用方要阻塞等待被调用方逻辑执行完成并返回 不用等待被调用方就可执行其他逻辑 放入消息队列并返回用户正在处理 高并发设计是循序渐进的，不能毕其功于一役 分层设计mvc/三层架构 表现层：展示数据和接受用户指令 逻辑层：复杂业务逻辑 数据访问层：处理和存储之间的交互 分层的好处 简化系统设计，不同的人专注做一件事 高复用 更容易做横向扩展（高并发的基础） 做系统分层 终端显示层：各端模版渲染并执行显示 开放接口层：Service层封装为开放接口并做网关安全控制和流量控制 Web层：对访问控制转发/基本参数校验/不复用业务逻辑简单处理 Service层：业务逻辑层 Manager层：通用业务逻辑处理（通用能力下沉/封装对第三方接口调用，如支付等） DAO层：数据访问层 外部接口或第三方平台：其他部分或公司提供的外部接口 分层的不足 增加代码复杂度 通过网络交互带来性能损失 设计思想 单一职责原则：每个类只有单一的功能（每一层职责单一，层与层边界清晰） 迪米特法则：对其他对象尽可能少的了解，不能跨层访问 开闭原则：对扩展开放，对修改封闭 高并发设计的目标：高性能、高可用、可扩展 高并发：运用系统设计手段让系统能处理更多用户请求流量 性能优化原则 问题导向：不要盲目提早优化 二八原则：即20%的精力优化80%的问题，抓住主要矛盾 数据支撑：优化使响应时间减少量和吞吐提升量 持续性：达到目标为止 性能度量指标 吞吐量和响应时间呈倒数关系，响应时间200ms是一个分界点，之前感受不到延迟；1s也是一个分界点，超过有明显等待的感觉 响应时间平均值：敏感度较低，瞬时改变可能无法反应，只能作为一个参考 响应时间最大值：过于敏感 分位数：排除偶发极慢请求影响，分位数越高对慢请求的影响越敏感 性能优化 提升系统处理核心：增加系统并行处理能力（思路简单，吞吐量=并发进程数/响应时间） 减少单次任务响应时间 减少单次任务响应时间的方式 CPU密集型：更高效的算法或减少运算次数，通过Profile工具找到消耗CPU消耗时间最多的模块（Linux的perf/eBPF） IO密集型：数据库问题看锁表和索引等/网络IO问题看网络参数能否优化，是否有大量超时重传和丢包；采用工具或对每一步骤做分时监控发现问题 高可用（High Availability，HA）：系统具备较高的无故障运行能力可用性度量 MTBF(Mean Time Between Failure)：平均故障间隔，即系统平均正常运行时间，越长越稳定 MTTR(Mean Time To Repair)：故障的平均恢复时间，即平均故障时间，越小对用户影响越小 Availability=MTBF/(MTBF+MTTR)：通过几个九描述系统可用性 高可用设计思路系统设计 Design for failure 故障转移（failover） 完全对等的节点：随机访问另一个节点即可 不对等的节点（主备）：需要选主操作 超时控制：收集系统之间的调用日志，统计比如99%响应时间，据此设置超时时间 降级：为了保证核心业务的稳定而牺牲非核心业务 限流：对并发请求进行限速 系统运维 灰度发布：系统变更不一次性推到线上，而是按比例逐步推进 故障演练：对系统进行一些破坏性手段，观察局部故障时系统的表现从而发现潜在问题（混沌工程） 扩展性提升扩展性的复杂性 处理核心增长和并行处理能力增长并非线性关系 数据库、缓存、依赖的第三方、负载均衡、交换机带宽会成为系统扩展的瓶颈 高可扩展的设计思路拆分 把复杂的系统拆分成独立具有单一职责的模块 存储 存储拆分考虑业务纬度 当业务拆分也达到单机极限时考虑按照数据特征做水平拆分（数据迁移成本极高，尽量一次性增加足够节点） 数据库拆分之后尽量不使用事务（超过一个数据库时涉及两阶段提交，协调各数据库成本极高） 业务层 业务维度：相同业务拆分为单独业务池，每个业务依赖于自己的数据库资源（减少扩容复杂性） 重要性维度：将业务分为核心池和非核心池（优先保证核心池功能，降级非核心池功能以保证系统稳定性） 请求来源维度：根据接入客户端类型不同划分业务池 存储池化技术 以空间换时间 原因：建立数据库连接耗时 TCP三次握手 服务端校验密码 用连接池预先建立数据库连接 最小连接数（10左右） 最大连接数（20～30左右） 检测连接是否可用 启用一个线程定期检测连接池中的连接是否可用（select 1）（推荐） 获取链接之后先校验连接是否可用（引入多余开销，测试使用） 用线程池预先创建线程：JDK1.5引入ThreadPoolExcutor核心参数 coreThreadCount maxThreadCount 适用场景与注意事项 适用于CPU密集型的任务（创建相当于CPU核心数的线程） 需要监控线程池的等待队列 不能使用无界队列，否则导致任务不会被抛弃从而大量任务占据大量内存，引发full gc导致宕机 数据库主从读写分离 主从复制：从库过多导致主库资源消耗（一般挂3～5个从库即可） 数据冗余解决延迟问题（一次性发送所有数据而不是关键数据） 使用缓存（适合新增数据的场景） 查询主库（不推荐） 要关注主从延迟带来的问题（正常时间为毫秒级） 访问数据库的方式 TDDL(Taobao Distributed Data Layer)：以代码形式内嵌运行在应用程序内部/配置多个数据源（简单易用，无多余的部署成本） 单独部署的代理层方案：由代理层管理数据源，应用只需访问代理层（所有Sql都要跨两次网络 互联网优先考虑性能而不是强一致性 分库分表：4核8G云服务器MySQL5.7做benchmark可以支撑500TPS和10000QPS 垂直拆分 按照业务类型拆分，专库专用（无法解决单表过大的问题） 水平拆分 将单一数据表按照规则拆分到多个数据库 按照某一个字段的hash值进行拆分（实体表，如用户表） 按照某一个字段的区间进行拆分（列表数据，如某一个人的订单）/需提前建好 引入的问题解决 分区键：建立ID和查询字段的映射表（只有少数字段，也可以分库分表） 数据库特性难以实现：join、计数等操作（可以将计数记录在redis等缓存中） 分库分表后全局id唯一性保证：主键选择 业务字段（不适用） 使用生成的唯一ID 搭建发号器服务生成UUID（不适合）优点 性能好 不依赖于三方系统 缺点 无序性 不具备业务含义 占用空间大（32个16进制数） SnowFlake算法： ID单调递增能支持排序和提高写入性能 简单易实现 全局唯一性 包含一定的业务含义 实现方式 嵌入到业务代码，分布在业务服务器中 优点：不需要跨网络调用 缺点：需要更多的机器id位来支持更多业务服务器（引入Zookeeper等分布一致性组件确保每次机器重启获得唯一的机器id） 独立部署发号器服务 优点： 减少机器码位数，留更多位给最后的自增信息位 因为机器位少或者是没有，可以将机器位写入配置文件而不用引入三方组件* 缺点：多一次网络调用 使用发号器的缺点 依赖系统时间戳：发现系统时钟不准停止发号直至准确 请求发号器QPS不高，比如每毫秒一个会导致最后一位都为1，从而使分库分表分配不均 解决方法 时间戳记录到秒 生成器的起始序号做随机 数据库和NoSQL互补使用NoSQL的原因 弥补了传统数据库性能的不足 数据库变更方便，不需要更改原来的数据结构 适合互联网项目大数据量场景 使用NoSQL的作用 提升写入性能：随机写转换为顺序写，大多数使用基于LSM树的存储引擎，牺牲一定读性能来换取写性能的提升（HBase、Cassandra、LevelDB） 场景补充：为搜索建立倒排索引 提升扩展性：天生支持分布式，支持数据冗余和数据分片的特性 缓存 缓存基础案例 Linux内存管理中的TLB组件 http缓存 缓存和缓冲区 缓存 缓冲区 空间换时间的优化手段 临时存储数据的区域，类似于消息队列，数据最终要输入到其他地方 分类 静态缓存：缓存静态资源 分布式缓存：缓存动态请求 热点本地缓存：极端热点数据查询 不足 适合读多写少，最好数据带有热点属性 给系统带来一定复杂度 有数据不一致风险 通常使用内存，但内存有限，成本高 带来运维成本 读写策略Cache Aside（旁路缓存）策略 应同时变更数据库和缓存，否则会带来数据不一致 直接更新缓存会带来丢失更新的问题 同样存在数据不一致的可能，但由于写数据库比缓存慢，所以概率极小 写入频繁会导致缓存频繁清理，影响缓存命中率 更新数据时更新缓存，更新缓存前加一个分布式锁 更新数据时更新缓存，给缓存一个较短的过期时间 读策略 从缓存读取数据 缓存命中则直接返回 缓存不命中则从数据库查询 查询到数据后将数据写入缓存并返回给用户 写策略 更新数据库记录 删除缓存记录 Read/Write Through（读穿/写穿策略） 用户只与缓存打交道 write through 查询要写入的数据在缓存是否存在 存在则更新缓存，由缓存组件同步更新到数据库 缓存不存在则发生Write Miss Write Allocate（按写分配）：写入缓存相应位置并由缓存组件同步更新到数据库 No-Write Allocate（不按写分配）：不写入缓存直接更新数据库（少一次缓存写入，常用） Read Through 查询缓存数据是否存在 存在直接返回 不存在由缓存组件同步数据 Write Back（写回）策略 计算机体系中的设计，不能运用到常用的数据库和缓存场景 写策略 写入数据时只写入缓存，把缓存块标记为脏 脏块再次被使用时将其中的数据写入后端 读策略 缓存命中则返回 缓存不命中则寻找一个可用的缓存块 缓存块为脏则先将之前的数据写入后端，并从后端加载数据到缓存块 不是脏块则由缓存组件将后端数据加载到缓存，并设置为非脏并返回数据 缓存的高可用客户端方案 客户端配置多个缓存节点，通过缓存写入和读取算法策略实现分布式，提高可用性 写入数据时要进行数据分片 读数据利用多组缓存做容错提升可用性 如何分片 hash分片：对缓存的key做hash算法，然后对总的缓存节点个数取余（节点数目变化造成缓存不可用，降低命中率，最好有另外一层缓存兜底） 一致性hash分片：将hash值的空间组织成一个圆环，将节点做hash后放在圆环上，对key做同样的hash然后在圆环上找到顺时针方向碰到的第一个节点（解决节点数目变化带来的问题）（缓存节点分布不均会导致部分节点压力过大）（脏数据问题） 脏数据：节点掉线后又恢复（设置缓存过期时间解决） 主从机制（能解决大多数场景） 每一组master配置一组slave 优先从slave读数据 读取不到则穿透到master并将数据回种到slave slave宕机有master兜底 多副本 在master/slave之间增加一个副本层 先选择一个副本组进行查询 失败则查询master/slave，然后回种到副本组 场景 极端流量场景，slave网卡成为瓶颈 副本组要比master/slave小，只存储更热的数据 中间代理层方案 在应用代码和缓存节点之间增加代理层，客户端所有写入读取请求都通过代理层，代理层内置高可用策略提高缓存系统可用性 通过通用协议实现多语言复用 服务端方案Redis Sentinel方案(&gt;2.4) 主节点挂了之后自动将从节点提升为主节点 配置master地址，监控master状态 master一段时间无反应则认为挂了，选取一个从节点提升为主节点 把所有其他从节点设置为新主的从节点 缓存穿透解决 缓存中查不到数据而不得不从后端查询 核心缓存命中率要保持99%以上，非核心缓存的命中率要保持90%以上 少量穿透无害，大量穿透会压垮后端 原因 互联网数据量大而缓存有限，无法存储所有数据 28原则，经常访问的是20%的数据，剩下80%不常访问 解决方案 回种空值：数据库查询到为空值时会发生异常时向缓存回种一个空值，并设置一个较短的过期时间（如果大量空值请求会使缓存中空值占用大量空间而使正常数据穿透，需评估缓存容量） 布隆过滤器：使用hash算法将元素映射到数组，查询元素使用相同的hash算法（因为hash碰撞会把不是集合中的元素判断为集合中，不支持删除元素）（其false positive的特性尤其适合缓存穿透：判断不在则一定不在）（使用多个hash，删除可以通过打标实现） 狗桩效应：极热数据失效导致大量穿透 启动后台线程从数据库加载数据到缓存，未加载之前直接返回不穿透 通过Memcached或Redis设置分布式锁，未获取锁的不允许穿透 CDN静态资源加速 就近访问 CDN：将静态资源分发到位于多个地理位置机房的服务器上 如何让请求到达CDN节点 通过DNS解析映射节点域名和实际ip DNS解析性能问题（向上多次查询） 启动时对要解析的域名做预解析，把解析结果存放在本地LRU缓存中 如何找到离用户最近的CDN节点GSLB（Global Server Load Balance，全局负载均衡）：对于部署在不同地域的服务器之间做负载均衡 是一种负载均衡服务器，让流量平均分配是下面管理的服务器负载更均衡 需要保证流量流经的服务与流量源头在边缘上比较接近 数据迁移目标 在线迁移 保证数据完整性 迁移过程中能回滚 方案双写 将新库配置为源库的从库 改造业务代码，要同时写入新库和旧库，写入新库失败的日志单独记录，方便恢复数据 数据校验（全量过多可抽样检验） 读流量切换到新库（全量切换可能有未知影响，可灰度发布） 存在任何问题可切换到旧库 确认无问题后将双写改为只写新库 考虑 在迁移之前写好数据校验工具或脚本，做充分测试 自建机房到云需考虑网络延迟和带宽 迁移过程随时可回滚，风险最低；时间周期长，有改造成本 级联同步（适合自建机房到云） 简单易施，无改造成本；切换写需暂停业务 新库配置为旧库从库，用作数据同步 新增备库为新库从库，用作数据备份 同时写入三个库待数据一致则切换读流量到新库 暂停业务写入，将写流量切换到新库（需选低峰期） 缓存预热 缓存迁移的重点是保持缓存热度 使用副本组预热缓存 在云端布置一个副本组 云上的副本组足够热之后（命中率&gt;90%）将云上机房的缓存服务器指向该组（云上请求发生穿透会存在跨专线访问数据库的问题） 改造云上副本组 云上部署多组副本组，自建机房写入请求优先写入自建机房缓存节点，异步写入云上节点 指定部分流量走云上（比如10%），使穿透可控 云上缓存命中率达90%后在云上部署应用服务器，云上的应用服务器走云上节点 消息队列秒杀场景读请求过高： 热点数据使用缓存策略 能静态化的尽量静态化 限流策略（对同一个请求做丢弃处理） 写请求过高消息队列 平衡低速系统和高速系统处理任务的时间差 削峰填谷 异步处理 系统模块之间松耦合 消息投递消息丢失 生产过程中丢失：网络抖动导致消息生产超时（重传2～3次）（有重复的风险） 消息队列中丢失：消息队列意外未刷盘（消息队列部署集群方式保证多个副本） 有一定容忍度：不建议集群 消费过程中丢失：消息消费时由于业务异常导致未执行完成（接受和处理完消息后再更新消息进度） 消息只被消费一次 在生产消费中增加冥等性保证：每条消息生成唯一id，消费时比对id 事务机制：避免过程中掉电宕机等异常（成本高、一般不考虑） 使用乐观锁：在数据中增加一个版本号，更新时带上版本号查询并更新版本号，则相同的版本号的重复数据将不会更新 消息延迟监控消息延迟 使用消息队列自带的工具监控（JMX和kafka-consumer-groups.sh） 生成监控消息的方式监控：循环写入一个内容为消息生成时间的特殊消息，比对消息消费时间和生成时间的差值 两种结合使用效果更佳 减少延迟消费端 优化消费代码提升性能 增加消费者数量 方法 kafka创建多个分区：Kafka一个分区只能有一个消费者，所以增加消费者无用 消费者使用多线程并行异步处理，可以一次性多拉取几条消息（注意线程空转的问题，可在拉取不到消息时使线程暂停10～100ms） 消息队列 选择高性能存储方式：比如使用本地磁盘的pagecache 配合零拷贝技术：sendfile减少数据被拷贝次数 分布式服务系统架构一体化架构（项目初期）优势 开发简单直接，项目和代码集中管理 只需维护一个工程，维护人力成本低 排查问题只需排查一个应用，目标性强 缺点 数据库连接成为瓶颈 增加研发成本，抑制了研发效率提升 代码过多构建耗时 微服务化 按照业务做横向拆分，解决数据库层面的扩展性 将业务无关功能下沉为单独服务 构造微服务服务拆分原则 单一服务高内聚低耦合 拆分粒度：先粗略拆分再逐渐细化 服务接口定义需有扩展性 避免影响产品日常迭代 方式 优先剥离独立的边界服务 优先剥离被依赖的服务 微服务化的问题 服务调用接口从跨进程到跨网络，需要引入服务注册中心 多个服务有错综复杂的依赖关系：被依赖的服务出问题导致整个依赖链故障（熔断、降级、限流、超时控制） 链路上故障难以排查（引入分布式追踪工具、更细致的服务端监控报表） RPC框架提升网络传输性能高性能IO模型 同步阻塞I/O 同步非阻塞I/O 同步多路I/O复用（推荐） 信号驱动I/O 异步I/O 网络参数调优：比如强交互场景socket开启tcp_nodelay，使包发送不用等待/设置接受缓冲区和发送缓冲区大小 选择合适的序列化方法考虑因素 时间性能 空间性能：过大的二进制串占据传输带宽影响效率 跨语言跨平台 扩展性 序列化备选方案 JSON：简单易用、可读（性能要求不高，数据占用带宽不大的情况下选用） Thrift：Facebook开源的高性能序列化协议，时间空间上都有较高性能（有配套RPC框架，性能要求高并需要一体化解决方案选用） Protobuf：谷歌开源的序列化协议（性能要求高、存储的数据占用空间大选用） Thrift和Protobuf 引入IDL(Interface Description language)，通过特定编译器转换成各语言对应的代码实现跨语言（IDL的存在带来使用上的不便） 注册中心 不重启客户端变更服务节点 实现优雅关闭：先停流量再停服务 服务状态管理 主动探测：注册中心每隔一段时间RPC服务暴露的端口是否可用 心跳模式：RPC服务每隔一段时间向注册中心发送心跳 避免服务过度摘除：达到一定阈值停止摘除并触发告警 通知风暴：控制注册中心管理集群的规模/扩容注册中心节点 分布式TRACE一体化架构慢请求排查 使用requestID跟踪调用链 使用静态代理的方式实现切面编程打印调用日志（静态代理AspectJ比动态代理Spring AOP性能好一些） 增加日志采样率而不是全量打印 把日志打印到消息队列而不是本地文件（解决需要登录到服务器查看日志的问题） 分布式 traceId（requestId）+spanId（记录rpc调用）（spanid和tranceId在线程上下文获取，然后生成本次rpc调用的spanId，再将tranceId和本次调用spanId序列化发送给服务方） 技巧 开源组件选择较低采样率，观察系统性能后再调整到合适数值（Zipkin/Jaeger） 自研避坑可提供开关控制打印日志时间 负载均衡负载均衡服务器分类代理类负载均衡服务 以单独的服务方式部署，所有请求都经过负载均衡服务器，在负载均衡服务中选择一个合适的服务节点，由负载均衡服务调用这个服务节点来实现流量分发 需承担全部流量，性能要求高 方式 LVS：在OSI模型的第四层传输层工作，又称四层负载 Nginx：在OSI模型的第七层应用层工作，又称七层负载 建议 在入口处部署LVS将流量分发到多个Nginx服务器上，再由Nginx服务器分发到应用服务器上 QPS未超过10万不建议引入LVS 适用于普通Web服务，不适用于微服务（服务节点存储在注册中心，LVS很难拿到，微服务之间使用RPC而不是Http，Nginx很难满足） LVS Nginx 做请求包转发，客户端和后端直接建立连接，后续不再经过LVS服务器，性能高 性能相比而言会差一些，但也能承担每秒几万次的请求 由于在第四层，所以不能针对URL做更细致的请求分发 配置上更加灵活，还能感知后端服务是否出现问题 客户端负载均衡服务 将负载均衡的服务内嵌在RPC客户端中 和客户端部署在同一个进程 结合注册中心使用，注册中心提供服务节点的完整列表 客户端拿到列表后使用负载均衡服务的策略选取一个合适的节点发送请求 负载均衡策略静态策略 选择服务节点时不参考后端服务的实际运行情况 轮询（RoundRobin，RR）：记录上次请求后端服务的地址或序号，在请求时按照服务列表的顺序，请求下一个后端服务节点（平均分配流量到所有负载节点，未考虑服务配置，无法发挥性能） 带权重的轮询：给节点加上权重值，解决轮询的问题 Nginx：ip_hash和url_hash LVS：请求源地址和目标地址hash Dubbo：一致性hash和随机选取 优先考虑轮询和带权重轮询（Nginx优先使用轮询） 动态策略 依据后端服务的一些负载特性来决定选取哪一个节点（优先使用） 选取负载最小、资源最空闲的服务来调用 Dubbo的LeastActive策略：优先选择活跃连接数最少的服务 Spring Cloud 中Ribbon 提供的WeightedResponseTimeRule：是使用响应时间给每个服务节点计算一个权重，然后依据这个权重，来给调用方分配服务节点 检测节点可用性 淘宝开源的 Nginx 模块nginx_upstream_check_module：定期探测后端服务的一个指定接口，根据返回状态码判断是否存活，不存活次数达到阈值则从负载均衡服务器中摘除这个服务 节点检测能帮助实现优雅关闭 优雅关闭流程 服务刚启动时初始化默认的Http状态码为500，Nginx认为服务不可用，则能等待服务依赖的资源加载完成，避免服务初始启动的波动 完成初始化后状态码改为200，Nginx两次探测后就能标示为可用 服务关闭时先把状态码改为500，等待Nginx探测为不可用后，流量将不会发往该节点 等待服务处理完所有请求后重启服务 API网关 api网关是一种架构模式，它将一些服务共有的功能整合在一起独立部署成单独的一层，来解决一些服务治理的问题 分类入口网关 部署在负载均衡服务器和应用服务器之间 提供客户端一个统一接入地址，将用户请求动态路由到不同的业务服务，并且做一些必要的协议转换 植入一些服务治理的策略（熔断、降级、流量控制、分流等） 客户端的认证和鉴权 做黑白名单 请求日志记录 出口网关 应用服务器和第三方系统之间 对调用外部的API做统一的认证、授权以及控制访问 实现 考虑性能（首要任务，关键在于I/O模型） 扩展性：将每一个操作定义成filter，使用责任链模式将filter串起，责任链可以动态组织filter，实现解耦，方便动态增删 使用线程池并行执行请求：防止线程阻塞，需针对不同的服务做线程隔离或保护 线程池使用方式 后端服务多，针对不同服务采用不同线程池 线程池内部针对不同服务甚至不同接口做线程保护（比如设置每个服务最多使用线程数） 开源组件 Kong：运行在Nginx的Lua程序 Zuul：Spring全家桶成员 Tyk：Go实现的轻量网关 引入网关 将API网关从Web层独立出来：将协议转换、限流、黑白名单等事情挪到 API 网关中来处理，形成独立的入口网关层 服务接口数据聚合 服务接口数据聚合 再独立一组网关做服务聚合、超时控制（前一种为流量网关，此种为业务网关） 抽取独立的服务层，专门做接口聚合的操作（接口数据聚合是业务操作，推荐这个方案） 多机房部署 在不同的IDC机房中部署多套服务，这些服务共享同一套业务数据并都可以承接来自用户的流量 难点 延迟：同地双机房1～3ms/国内异地双机房50ms/国际双机房100～200ms 逐步迭代多机房部署方案 数据读取和服务调用尽量保持在同一个机房内 同城双活：机房级别容灾，无法做到城市级别容灾（复杂度低，延迟低） 异地多活：城市级别容灾（可用性高） 异地多活数据存储 基于存储系统的主从复制 基于消息队列的方式 服务治理概念 数据平面：负责数据传输， 控制平面：控制服务治理策略的植入 出于性能考虑会将服务治理策略植入到数据平面，控制平面负责服务治理策略数据的下发 方式 Istio：将组件分为数据平面和控制平面，数据平面即为sidecar，控制平面负责服务治理策略执行（每次请求都要通过控制平面，性能存在瓶颈） sidecar sidecar iptables：使用Linux内核防火墙软件的管理工具，Istio默认使用（对业务完全透明，高并发下有性能损耗） 轻量级客户端：在RPC客户端配置sidecar的部署端口，通过轻量级客户端将调用服务的请求发送给sidecar（建议） Cilium：从socket层面转发，避免iptables的损耗 开源框架 Istio Linkerd SOFAMesh 维护系统监控监控指标 延迟：请求的响应时间（接口、数据库、缓存） 通信量：即吞吐量，单位时间内请求量的大小 错误：当前发生的系统错误数（4、5和代码业务出错） 饱和度：服务或资源到达上限的程度（CPU使用率、内存使用率等） 采集 Agent：在数据源的服务器上部署自研或者开源的 Agent 来收集收据，发送给监控系统 埋点：面向切面实现或在资源客户端中直接计算调用资源或者服务的耗时、调用量、慢请求数，并且发送给监控服务器（先对埋点做汇总，避免过高请求量到达监控） 日志：Tomcat和Nginx日志，通过日志采集工具发送到监控服务器（Apache Flume、Fluentd和Filebeat） 处理和存储消息队列（两个） 接收数据后存到Elasticsearch，然后用Kibana展示，用作原始数据查询 流式中间件对数据作处理（storm、spark） 处理类型 解析数据格式（url、请求量、响应时间） 聚合运算（响应时间分位值，非200请求等） 将数据存储到时序数据库（InfluxDB、OpenTSDB、Graphite） 通过Grafana 来连接时序数据库，将监控数据绘制成报表 报表 访问趋势报表：接入的是 Web 服务器，和应用服务器的访问日志，展示了服务整体的访问量、响应时间情况、错误数量、带宽等信息。主要反映的是服务的整体运行情况，帮助你来发现问题 性能报表：对接的是资源和依赖服务的埋点数据，展示了被埋点资源的访问量和响应时间情况。它反映了资源的整体运行情况 资源报表：对接的是使用 Agent 采集的资源的运行情况数据。当某一个资源出现了问题，就可以进一步从这个报表中，发现资源究竟出现了什么问题，是连接数异常增高还是缓存命中率下降 应用性能管理 APM：对应用各个层面做全方位的监测，期望及时发现可能存在的问题，并加以解决，从而提升系统的性能和可用性 搭建 数据采集：在客户端植入SDK，SDK采集数据通过固定接口定时发送服务端 处理和存储：加密传输数据，存储到消息中间件，做聚合运算，绘制报表 监控的数据： 客户端网络问题（首要）：埋点的方式 价值 数据为用户上报真实数据，能准确真实实时反应用户操作体验 是性能优化的指向标，做优化行为时能够反馈用户性能数据，引导业务正向优化接口性能、可用性等指标 能监控CDN链路质量，弥补CDN监控依赖CDN厂商，但CDN厂商无法感知客户端到CDN链路的问题 能上报一些异常数据：登录失败、下单失败等 压力测试 压力测试指的是在高并发大流量下进行测试 应该周期性进行 应该包含整个调用链 流量发起应尽量接近客户端，远离服务 搭建要点 流量的隔离 风险的控制 模块流量构造和产生模块 拷贝入口流量存储到数据流量工厂（流量拷贝工具GoRelay） 对流量染色，标记为压测数据 压测数据隔离模块 针对读取数据的请求，不能压测的组件或服务采用Mock 写入数据到影子库（同一个实例的不同Schema），缓存则增加特定前缀 系统健康度检查和压测流量干预模块 逐步增大流量，观察瓶颈 开发流量监控组件，设置性能阈值（cpu使用率、慢请求比例等） 配置管理方式配置文件 把配置文件存储的目录标准化为特定的目录 使用版本管理工具管理配置项 相同配置项放在一个目录供多个机房共用 将不同的配置放在以机房名为名称的不同目录 先读取特定的配置再读取公共配置 配置中心（推荐Apollo）变更推送 轮询查询：应用程序想配置中心注册一个监听器，定期查询配置是否改变（多个服务器拉取配置，配置中心带宽可能成为瓶颈，通过计算配置的MD5判断是否改变来解决）（实时性要求不高，建议采用） 长连推送：配置中心服务端保存每个连接关注的配置列表，比轮询复杂但更实时 配置中心高可用 让配置中心旁路化：配置中心宕机，或者配置中心依赖的存储宕机，仍然能够保证应用程序可以启动 增加缓存：配置中心客户端上增加两级缓存，第一级为内存的缓存，第二级为文件的缓存 缓存方式 内存缓存：配置中心客户端获取后同步写入，降低客户端和配置中心的交互频率，提升性能 文件缓存：异步写入，做为容灾，配置中心发生故障，应用程序优先使用文件中的配置，虽无法获取变更，但仍可以起起来，算是降级 降级熔断 怕的不是宕是慢，慢会引发雪崩效应 熔断：发起服务调用的时候，如果返回错误或者超时的次数超过一定阈值，则后续的请求不再发向远程服务而是暂时返回错误（是一种有限状态机，在三个状态之间转换） 降级：站在整体系统负载的角度上，放弃部分非核心功能或者服务，保证整体的可用性的方法，是一种有损的系统容错方式 降级分类 限流降级 开关降级：在代码中预先埋设一些开关，可通过配置中心控制（需区分哪些是核心业务哪些不是） 降级策略 读取数据：直接返回降级数据（缓存数据，非核心则返回固定数据） 轮询查询：降低获取数据频率 写数据：同步写变成异步写只有演练的开关才是有用的开关 限流 通过限制到达系统的并发请求数量，保证系统能够正常响应部分用户请求，对于超过限制的流量，通过拒绝服务的方式保证整体系统的可用性 部署在服务的入口层，比如Api网关 微服务架构中在RPC客户端引入限流策略，保证单个服务的可用性 阈值放在配置中心动态配置 纬度 系统每分钟处理的请求数 单个接口每分钟处理的请求数 单个IP/用户ID/设备ID/在一段时间内发送的请求数 服务于第三方应用的平台限制appKey的访问接口速度 限流算法固定窗口和滑动窗口 无法限制短时间的集中流量 固定窗口 滑动窗口 记录时间范围内的请求量，超过则触发限流策略返回请求失败 将时间窗口划分为多个小窗口 实现简单，但无法限制短时间内的集中流量 解决了临界时间点上突发流量无法控制的问题，但因为增加了时间片存储，空间复杂度有所升高 漏桶和令牌桶算法 推荐使用令牌桶 漏桶算法 令牌桶算法 在流量产生端和接受端增加一个漏桶，流量暂存在里面，出口按照固定速率将流量漏出到接受端，当流量超过漏桶承受极限时多余的流量触发限流策略 设置放入令牌的时间间隔和令牌桶的大小，每个请求先获取令牌，没用令牌则等待或直接拒绝服务 随机产生的流量被整形成平滑的流量 随机产生的流量被整形成平滑的流量 使用消息队列存储，队列流量溢出则拒绝 单机使用变量，分布式可以使用redis（减少交互可以每一次获取一批令牌） 会增加请求响应时间]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>高并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVMInAction]]></title>
    <url>%2F2020%2F03%2F16%2FJVMInAction%2F</url>
    <content type="text"><![CDATA[由于总结到最后发现脑图变得无比庞大，所以这里就不再贴出图片了。可以到这里查看xmind的源文件，里面各个章节有到相应源码的连接也可以到这里查看导出的pdf也可以到这里查看导出的图片]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java并发编程脑图]]></title>
    <url>%2F2020%2F02%2F12%2Fjava%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E8%84%91%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[相关源码]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring脑图]]></title>
    <url>%2F2020%2F02%2F08%2FSpring%E8%84%91%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[相关源码]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习总结]]></title>
    <url>%2F2019%2F08%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设用P来评估计算机程序在某任务类T上的性能，若一个程序通过利用经验E在T中任务上获得了性能改善，则我们就说关于T和P，该程序对E进行了学习。Mitcell,1997 模型评估与选择误差 误差 误差是学习器的实际预测输出与样本真实输出之间的差异，其中在训练集上的误差称为训练误差或者经验误差，在新样本上的误差称为泛化误差。 过拟合与欠拟合 过拟合是指学习器把训练样本学习得“太好”，可能将训练样本的一些特点当做所有潜在样本都具有的一般性质而导致泛化性能下降。欠拟合则相反，它表示对训练样本的一般性质都未曾学到。欠拟合可以克服而过拟合只能缓解。 模型评估 留出法 留出法直接将训练集划分为两个不相交的子集，一个作为训练集，一个作为测试集。为了保证被划分后的数据拥有和原始数据同样的分布，避免因数据划分而引入额外的偏差影响最终的结果，可以采用分层采样(stratified sampling)来保留类别比例；因一个数据集可能存在多种划分方式，如果单次使用留出法可能导致结果不够稳定可靠，一般可采用随机划分、重复进行实验后取平均值作为留出法的评估结果；数据划分将导致最后的模型不是整个数据集的训练结果，而只是一部分数据训练出来的，这将降低评估结果的保真性(fidelity)，由于没有完美的解决方案，一般是将大约$\frac{2}{3}$~$\frac{4}{5}$的样本用作训练，剩余用于测试(测试集至少应含30个样例) 交叉验证法 交叉验证法将数据分为$k$个大小相似的互斥子集，每个子集尽可能保持数据分布的一致性(分层采样)，每次用$k-1$个子集的并集作为训练集，剩下的那个作为测试集，如此进行$k$次则可通过$k$组训练/测试集得到$k$个测试结果，最后取均值。交叉验证法又称为k折交叉验证(k-fold cross validation)，它评估结果的稳定性和保真性很大程度上取决于$k$，这里$k$的取值常用10；与留出法类似，一个数据集可能存在多种划分，为减小划分引入的差别，k折验证法需要使用不同的划分进行$p$次，最终的结果是这p次k折交叉验证结果的均值。 留一法 留一法(Leave-One-Out, LOO)是交叉验证法的特例，它将大小为$m$的数据集划分成$m$个子集，即每个子集只包含一个样本，这样就不会受随机划分的影响，同时也让用训练集训练的模型和期望评估的用整个数据集训练的模型相似(两个数据集样本数差一)，使结果更为准确。但是当数据量变大时，留一法需要训练$m$个模型，这个计算开销是巨大而不能忍受的。 自助法 所谓自助法(booststrapping)就是通过自助采样(booststrap sampling)1从原始数据集$D$中有放回地随机选取$m$个样本组成训练用数据集$D’$，因为是有放回的采样，所以$D$中的一部分样本可能在$D’$中多次出现，而另一部分样本则不会出现，样本在$m$次采样中始终不出现的概率为$(1-\frac{1}{1})^m$，对$m$取极限可得$\lim_{m \rightarrow \inf}{(1- \frac{1}{m})^m} = \frac{1}{e} \approx 0.368$。这说明通过自助采样之后有$36.8\%$的数据未参与训练，因此我们可以使用这部分数据作为测试数据集，这样获得的测试结果叫做包外估计(out-of-bag estimate)。虽然自助法在数据集较小，难以有效划分有效训练集/测试集时很有效，并且由于它能参数多个不同的训练集，在集成学习中也能发挥巨大的作用，但由于它产生的数据集改变了原始数据集的分布，引入了估计偏差，因此在数据量足够多时还是使用留一法或交叉验证法会更好一些。 性能度量回归&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;回归任务中最常用的性能度量是均方误差，即对各个样本预测值$f(\boldsymbol{x}_i)$与对应真实值$y_i$的差值的平方进行求和再取平均数：E(x;D)=\frac{1}{m}\sum^1_m(f(\boldsymbol{x}_i-y_i))^2 分类错误率与精度&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;错误率是分类错误的样本数占样本总数的比例，精度是分类正确的样本数占样本总数的比例，两者相加为$1$。 错误率2E(f;D)=\frac{1}{m}\sum_m^1\boldsymbol{I}(f(\boldsymbol{x}_i \ne y_i)) 精度acc(f;D)=\frac{1}{m}\sum_m^1\boldsymbol{I}(f(\boldsymbol{x}_i = y_i))查准率(准确率)、查全率(召回率)和F1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;查准率表示分类结果中真正为正的样本(真正例)在分类为正的样本中所占的比例，查全率表示分类结果中真正为正的样本在总样本中所占的比例。对于这两个度量标准，可以通过混淆矩阵进行直观的展现，&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中真正例(true positive)表示预测为真实际也为真，假反例(false negative)表示实际为真预测为加，假正例(false negative)表示预测为真实际为假，真反例(true negative)表示实际为假预测也为假，这四种情形对应的样例数之和为总的样本数。而查准率和查全率用可以用这几种情形进行表示 查准率(准确率)P = \frac{TP}{TP+FP} 查全率(召回率)F = \frac{TP}{TP+FN} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;查准率和查全率是一对相互矛盾的度量，查准率高则查全率低，反之亦然。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;P-R曲线又名PR图，其横轴为查全率，纵轴为查准率，P-R曲线往往是非平滑非单调的。如果一个学习器的P-R曲线被另一个学习器的曲线完全包住，则说明后者的性能优于前者；如果两者有交叉，则只能在具体的查准率和查全率下进行比较。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;平衡点(Break-Event Point,BEP)是一个综合考虑查准率和查全率的度量，它的取值为“查准率=查全率”时的值，一般而言，BEP越大学习器越优。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;F1度量同样时综合考虑查准率和查全率的度量，它比BEP复杂一些。F1是查准率和查全率的调和平均$\frac{1}{F1}=\frac{1}{2}\cdot (\frac{1}{P}+\frac{1}{R})$，它比算数平均$\frac{P+R}{2}$和几何平均$\sqrt{P\times R}$更重视较小值。 F1F1=\frac{2\times P\times R}{P+R}=\frac{2\times TP}{样例总数+TP-TN} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当应用对查准率和查全率的重视程度不同时，就要是F1的一般形式$F_\beta$来表达出对查准率/查全率的偏好，$\beta&gt;0$度量了查全率和查准率的相对重要性，$\beta=1$将退化成F1度量；$\beta&gt;1$时查全率的影响更大；$\beta&lt;1$时查准率的影响更大。 $F_\beta$F_\beta=\frac{(1+\beta)\times P\times R}{(\beta^2\times P)+R} &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在多分类任务中，需要考虑的混淆矩阵将不止一个，可能需要在$n$个混淆矩阵上综合考虑查全率和查准率，这时候有两种不同的度量。一种是先在各个混淆矩阵上计算出各自的查准率和查全率$(P_1,R_1),(P_2,R_2),\cdots,(P_n,R_n)$，然后求平均值，这样得到的是宏查准率(macro-P)，宏查全率(macro-R)，宏F1(macro-F1);另一种是先计算混淆矩阵对应元素的平均值,$\overline{TP}(TP),\overline{FP}(FP),\overline{TN}(TN),\overline{FN}(FN)$，在基于这些平均值计算出微查准率(micro-P)，微查全率(micro-R)，微F1(micro-F1)。 宏查准率，宏查全率，宏F1P_{macro}=\frac{1}{n}\sum_{i=1}^nP_iR_{macro}=\frac{1}{n}\sum_{i=1}^nR_iF1_{macro}=\frac{2\times P_{macro}\times R_{macro}}{P_{macro}+R_{macro}} 微查准率，微查全率，微F1P_{micro}=\frac{\overline{TP}}{\overline{TP}+\overline{FP}}R_{micro}=\frac{\overline{TP}}{\overline{TP}+\overline{FN}}F1_{micro}=\frac{2\times P_{micro} \times R_{micro}}{P_{micro}+R_{micro}} ROC与AUC&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;阈值是在分类过程中确定样本属于哪一类的标准值，大于阈值则划分为正类，否则为反类。如果将预测结果进行排序，最可能为正例的排在前面(概率大的)，最不可能的排在后面(概率小的)，则阈值便成为了区分正反例的截断点。如果更看重查准率，则可选取排序中靠前的位置进行截断(阈值大)，如果更看中查全率，则可选取排序中靠后的位置进行截断(阈值小)。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;受试者工作特征(Receiver Operating Characteristic, ROC)是通过考察排序本身的好坏和截断点的不同来研究学习器的泛化性能。ROC曲线的纵轴是真正例率(True Positive Rate, TPR)，横轴为假正例率(False Positive Rate, FPR)，对预测结果进行排序，然后按顺序逐个把样本当成正例进行预测，每次计算出这两个重要的值，然后以它们作为坐标作图。给定$m^+$个正例和$m^-$个反例，具体步骤如下： 根据学习器预测结果对样例进行排序 将分类阈值设为最大(所有样例均为反例，真正例率和假正例率均为0)，在坐标$(0,0)$处标记一个点 将分类阈值依次设置为每个样例的预测值(即依次将每个样例标记为正例)。假设前一个标记点为$(x,y)$,则如果当前样例为正例，则标记为$(x,y+\frac{1}{m^+})$，若为反例则标记为$(x+\frac{1}{m^-},y)$ 用线段将相邻的点连接 真正例率TPR = \frac{TP}{TP+FN} 假正例率FPR = \frac{FP}{FP+TN} ROC &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;与PR图类似，如果一个学习器的ROC曲线完全被另一个包住，则后者的性能优于前者；如果两个学习器的ROC曲线发生交叉则难以判定。AUC适用于这种交叉情况下的性能比较。AUC(Area Under ROC Curve)是ROC曲线下的面积，假定ROC曲线是由坐标${(x_1,y_1),(x_2,y_2),\cdots,(x_m,y_m)}$的点连接形成，则AUC的计算公式为： AUC=\frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_i)\cdot(y_i+y_{i+1})&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AUC考虑的是样本预测的排序质量，因此它与排序误差有紧密的关系。$\boldsymbol{l_{rank}}$对应的是ROC曲线之上的面积，是对排序损失的的定义，若一个正例在ROC曲线上对应标记点的坐标为$(x,y)$，则$x$恰是排序在其前的反例所占的比例，即假正例率，因此AUC=1-l_{rank} $l_{rank}$l_{rank}=\frac{1}{m^+m^-}\sum_{x^+ \in D+}\sum_{x^-\in D^-}(\boldsymbol{I}(f(x^+)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[House Prices: Advanced Regression Techniques(2)]]></title>
    <url>%2F2019%2F06%2F27%2FHouse-Prices-Advanced-Regression-Techniques-2%2F</url>
    <content type="text"><![CDATA[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;书接上文，在前文中我们已经了解到机器学习需要经历的基本过程，其中占据大部分篇幅的是数据分析和数据处理的部分，模型的训练反而占比不大。这其中的原因除了因为特征工程在机器学习的整个过程中应有如此大的比重之外，还因为之前训练的模型都是一些简单模型，并不涉及到大量参数的调试。而这里使用的集成学习将会涉及到不少的参数需要调节。 集成学习概述&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;既然已经单独将这部分内容提了出来，那么在进入正式的调参之前，我们先简要看看集成学习是个什么东西吧。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;集成学习通过构建并结合多个学习器来完成学习任务，其先产生一组“个体学习器”，再用某种策略将它们结合起来。集成的方式又分为同质集成和异质集成。同质集成只包含相同类型的个体学习器，其个体学习器也称为“基学习器”；异质集成中的个体学习器是不同类型的，其被称为“组件学习器”。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;根据个体学习器的生成方式集成学习又可以分为两大类，一种是串行生成的序列化方法，其个体学习器之间存在强依赖关系，代表为Boosting；另一种是同时生成的并行化方法，其个体学习器之间不存在强依赖关系，代表有Bagging和Random Forest。Boosting是一族可将弱学习器提升为强学习器的算法，它先从初始训练集训练出一个基学习器，再根据其表现调整训练样本的分布，即重新分配每个样本的权重，使表现不好的样本在下次训练时得到更多的关注，直至达到预定的训练次数，最后将所有的基学习器进行加权结合；Boosting每一次都是使用的全量数据，而Bagging却并不是，它采用有放回的采样的方式来生成训练集，每个基学习器使用不同的训练集来进行训练，有放回的采样使得同一个数据集能够被多次使用从而训练出不同的模型，最后可以通过投票(分类)和平均(回归)来结合各个基学习器的结果；Random Forest(RF)是在Bagging的基础上进一步在决策树的训练过程中引入随机属性选择,传统的决策树选择划分属性时是在当前节点的属性集合中选择一个最优属性，而在RF中是先从该结点的属性集合中随机选择一个包含k个属性的子集，再从子集中选择最优属性。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;集成学习还有很多细节上的东西，包括Boosting和Bagging的训练过程，基学习器预测结果的结合方式等等，在这里就不再进行一一陈述了。下面让我们进入主题——集成模型的训练吧。 模型选择&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我先创建了一个公用方法，其目的是为了画出网格搜索($GridSearch$)过程中的平均准确率和准确率的变异系数1。这两个分数是比较简单的用于衡量一个回归模型好坏的指标，这里将测试和训练进行了对比展示，从而对模型的泛化能力进行评估，对参数选择做出决断。12345678910111213141516171819202122232425262728293031323334353637def plot_acc_4_grid(grid_cv, param): fig = plt.figure(figsize=(10, 10)) mean_acc = fig.add_subplot(2,1,1) std_acc = fig.add_subplot(2,1,2) # 训练参数个数 params_num = len(grid_cv.cv_results_[&apos;params&apos;]) x_ticks = np.arange(params_num) # 把每一次的参数作为横坐标label score_label = [list(grid_cv.cv_results_[&apos;params&apos;][i].values())[0] for i in range(params_num)] # 平均精确度 mean_train_score = grid_cv.cv_results_[&apos;mean_train_score&apos;] mean_test_score = grid_cv.cv_results_[&apos;mean_test_score&apos;] # 方差 std_train_score = grid_cv.cv_results_[&apos;std_train_score&apos;] std_test_score = grid_cv.cv_results_[&apos;std_test_score&apos;] mean_acc.plot(mean_train_score, &apos;r-o&apos;, label=&apos;mean_train_score&apos;) mean_acc.plot(mean_test_score , &apos;b-o&apos;, label=&apos;mean_test_score&apos;) mean_acc.set_title(&apos;mean_acc@&apos;+param, fontsize=18) mean_acc.set_xticks(x_ticks) mean_acc.set_xticklabels(score_label) mean_acc.set_xlabel(param, fontsize=18) mean_acc.set_ylabel(&apos;mean_acc&apos;, fontsize=18) mean_acc.legend(loc=&apos;best&apos;, fontsize=18) mean_acc.grid() std_acc.plot(std_train_score,&apos;r-*&apos;, label=&apos;std_train_score&apos;) std_acc.plot(std_test_score, &apos;b-*&apos;, label=&apos;std_test_score&apos;) std_acc.set_title(&apos;std_acc@&apos;+param, fontsize=18) std_acc.set_xticks(x_ticks) std_acc.set_xticklabels(score_label) std_acc.set_xlabel(param, fontsize=18) std_acc.set_ylabel(&apos;std_acc&apos;, fontsize=18) std_acc.legend(loc=&apos;best&apos;, fontsize=18) std_acc.grid() plt.subplots_adjust(hspace=0.5) 随机森林&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们先来尝试一下上面提到的随机森林，这里主要关注的是影响性能的几个参数，罗列如下： .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; text-align: left; word-wrap:break-word; word-break:break-all; white-space:normal; max-width:650px; font-family:SimSun; } .dataframe thead th { text-align: center; } 参数 详情 n_estimators 子模型的数量&nbsp;&nbsp;&nbsp;&nbsp;• integer(n_estimators≥1) &nbsp;&nbsp;&nbsp;&nbsp;* 默认值为10 max_depth 树的最大深度 &nbsp;&nbsp;&nbsp;&nbsp;• integer(max_depth≥1) &nbsp;&nbsp;&nbsp;&nbsp;• None(树会生长到所有叶子节点都分到一个类或者某节点所代表的样本数据已小于min_samples_split) &nbsp;&nbsp;&nbsp;&nbsp;* 默认值为None max_features 在寻找最佳划分时考虑的最大特征数 &nbsp;&nbsp;&nbsp;&nbsp;• integer(n_features≥max_features≥1) &nbsp;&nbsp;&nbsp;&nbsp;• float(占所有特征的百分比) &nbsp;&nbsp;&nbsp;&nbsp;• "auto"(n_features，即所有特征) &nbsp;&nbsp;&nbsp;&nbsp;• "sqrt"(max_features=sqrt(n_features) &nbsp;&nbsp;&nbsp;&nbsp;• "log2"(max_features=log2(n_features)) &nbsp;&nbsp;&nbsp;&nbsp;• None(n_features，即所有特征) &nbsp;&nbsp;&nbsp;&nbsp;* 默认值为"auto" min_samples_split 内部节点分裂所需的最小样本数 &nbsp;&nbsp;&nbsp;&nbsp;• integer(min_samples_split≥2) &nbsp;&nbsp;&nbsp;&nbsp;• float(ceil(min_samples_split * n_samples)，即占所有样本的百分比向下取整) &nbsp;&nbsp;&nbsp;&nbsp;* 默认值为2 max_leaf_nodes 最大叶节点数 &nbsp;&nbsp;&nbsp;&nbsp;• integer(max_leaf_nodes≥1) &nbsp;&nbsp;&nbsp;&nbsp;• None(不限制叶节点个数) &nbsp;&nbsp;&nbsp;&nbsp;* 默认值为None min_weight_fraction_leaf 叶节点最小样本权重总值 &nbsp;&nbsp;&nbsp;&nbsp;• float(权重总值) &nbsp;&nbsp;&nbsp;&nbsp;* 默认值为0 min_samples_leaf 叶节点最小样本数 &nbsp;&nbsp;&nbsp;&nbsp;• integer(min_samples_leaf≥1) &nbsp;&nbsp;&nbsp;&nbsp;• float(ceil(min_samples_leaf * n_samples)，即占所有样本的百分比向下取整) &nbsp;&nbsp;&nbsp;&nbsp;* 默认值为1 bootstrap 是否使用bootstrap对样本进行采样 &nbsp;&nbsp;&nbsp;&nbsp;• False(所有子模型的样本一致，子模型强相关) &nbsp;&nbsp;&nbsp;&nbsp;• True(每个子模型的样本从总样本中有放回采样) &nbsp;&nbsp;&nbsp;&nbsp;* 默认值为True criterion 判断节点是否分裂的使用的计算方法 &nbsp;&nbsp;&nbsp;&nbsp;• "mse"(均方误差) &nbsp;&nbsp;&nbsp;&nbsp;• "mae"(平均绝对误差) &nbsp;&nbsp;&nbsp;&nbsp;* 默认值为"mse" &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中的n_estimators的值一般来说是越大性能越好，泛化能力越强，是单调递增的，但随着子模型的数量增加，训练算法所消耗的资源和时间将会急剧增加，而其性能的提升也会到达瓶颈。其它数值型参数对性能的影响都呈现出有增有减的，而枚举型的例如criterion则需要视情况而定了，需要在实际应用时灵活调整。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们已经对需要调节的参数有了一个直观的认识，知道每个参数代表的含义。但是我们怎么来调节他们呢？在一开始的时候，我选择了一种非常笨重的方式——直接将所有参数塞进GridSearchCV，这导致训练花费了大量的时间。举个例子，不如设想有3个参数需要调节，每个参数取10个待定值，最后需要尝试的组合高达1000个之多，而这里的参数有9个，如果是更复杂的神经网络，那基本上就是望山跑死马的事了。我后知后觉得意识到了网格查找的局限性，于是我马上尝试书里提到的随机方法RandomizedSearchCV，并一度以为这样就能完美解决问题，但显然是我过于乐观——最后训练出的模型基本上都是过拟合的，而且参数可控性极低。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了解决这些问题，最后尝试了一种基于贪心策略的坐标下降法，即每一次只调节一个参数，然后选择最优的参数固定下来继续训练下一个参数，这可以大大减少训练所需的资源和时间——将上面的1000减少到30，只要能保证训练的模型是凸的，就能取得不错的效果。下面让我们来一探究竟吧。 n_estimators &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先进行调节的是n_estimators这个参数，正如前文所述，这是一个使性能单调递增的参数，我们首先在粗粒度对它训练，观察训练的整体趋势。12345678from sklearn.ensemble import RandomForestRegressorfrom sklearn.model_selection import GridSearchCVrf_param = &#123; &apos;n_estimators&apos;: np.arange(1, 1000, 100),&#125;rf_grid_cv = GridSearchCV(RandomForestRegressor(n_estimators=50), param_grid=rf_param, cv=3, verbose=True, n_jobs=-1)rf_grid_cv.fit(x_train, y_train) Fitting 3 folds for each of 10 candidates, totalling 30 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 30 out of 30 | elapsed: 1.8min finished GridSearchCV(cv=3, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), fit_params=None, iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;n_estimators&#39;: array([ 1, 101, 201, 301, 401, 501, 601, 701, 801, 901])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=True) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下面是这10个模型的最终得分情况，我们可以发现训练得分和验证得分有比较大的差距，但是两者的提升都是同步的，不曾出现此消彼长的情况；在子模型数为1~101时两个得分的提高最为明显，之后增加子模型数量对平均分数并未有任何贡献，可见子模型数量带来的性能提升是有瓶颈的。虽然如此，通过观察变异系数可以发现验证集的得分的离散程度还有一定幅度的减小，这意味子模型数量的增加虽然无异于提高分数，但是其使模型更加稳定。1plot_acc_4_grid(rf_grid_cv, &apos;n_estimators&apos;) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;大刀阔斧地调参会忽视掉许多细节，比如是否两个取值点刚好错过了可能存在于两者之间的波峰或者波谷，结果是否存在波动等。基于此我们接下来根据上面得到的返回结果确定细粒度调节的区间。123456rf_param = &#123; &apos;n_estimators&apos;: np.arange(100, 200, 10),&#125;rf_grid_cv = GridSearchCV(RandomForestRegressor(n_estimators=50), param_grid=rf_param, cv=3, verbose=True, n_jobs=-1)rf_grid_cv.fit(x_train, y_train) Fitting 3 folds for each of 10 candidates, totalling 30 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 30 out of 30 | elapsed: 35.4s finished GridSearchCV(cv=3, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), fit_params=None, iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;n_estimators&#39;: array([100, 110, 120, 130, 140, 150, 160, 170, 180, 190])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=True) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;不出所料，这里的变异系数存在明显的波动，而最终得分却没什么提升，说明n_estimators是一个适合在粗粒度上进行调节的参数。最后我们选取验证变异系数最小的取值160作为模型中n_estimators参数的最终取值。1plot_acc_4_grid(rf_grid_cv, &apos;n_estimators&apos;) max_depth &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;max_depth控制着每棵树的深度，随着树的深度越深，子模型的偏差降低而方差升高，当方差升高到一定程度的时候将会使泛化性能下降，也就是出现过拟合现象。如果是一颗完全二叉树，其叶节点的数目为$2^{n-1}$，则这里使用的训练集的数量只需要一颗深度为10的树基本就能将所有实例分布在不同的叶节点上。当然树的结构显然不会如此理想，所以现将深度的查找范围扩大到100进行训练。 123456rf_param = &#123; &apos;max_depth&apos;: np.arange(1, 100),&#125;rf_grid_cv = GridSearchCV(RandomForestRegressor(n_estimators=160), param_grid=rf_param, cv=3, verbose=True, n_jobs=-1)rf_grid_cv.fit(x_train, y_train) Fitting 3 folds for each of 99 candidates, totalling 297 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 31.3s [Parallel(n_jobs=-1)]: Done 192 tasks | elapsed: 3.5min [Parallel(n_jobs=-1)]: Done 297 out of 297 | elapsed: 5.5min finished GridSearchCV(cv=3, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=160, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), fit_params=None, iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;max_depth&#39;: array([ 1, 2, ..., 98, 99])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=True) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;让我们来看看这些模型的整体表现如何，通过第一张图可以清晰的看到随着深度的增加，模型在变得越来越好，但是我们并不能因此而沾沾自喜，和在训练n_estimators时一样，我们还要考察它的变异系数。前期随着模型得分越来越高，变异系数也在稳步下降，但是在max_depth增长到5的时候，得分的增幅明显放缓，到9之后更是基本保持不变了，而反观验证得分的变异系数，它也基本在同样的节点发生了逆转，在max_depth为6时达到波谷，而后便开始逐步上升，在12之后出现明显的波动，这说明得分开始趋于不稳定，调节这一部分取值并不会有多大好处。1plot_acc_4_grid(rf_grid_cv, &apos;max_depth&apos;) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;到这里我们已经能够基本得到我们想要的取值了，但在确定这个参数之前，为了更准确地估计，我把参数调节的范围缩小到了1~20，希望这能带来更清楚的认识。123456rf_param = &#123; &apos;max_depth&apos;: np.arange(1, 20),&#125;rf_grid_cv = GridSearchCV(RandomForestRegressor(n_estimators=160), param_grid=rf_param, cv=3, verbose=True, n_jobs=-1)rf_grid_cv.fit(x_train, y_train) Fitting 3 folds for each of 19 candidates, totalling 57 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 28.9s [Parallel(n_jobs=-1)]: Done 57 out of 57 | elapsed: 44.3s finished GridSearchCV(cv=3, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=160, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), fit_params=None, iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;max_depth&#39;: array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=True) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在我们可以看到上面产生极具变化的那一部分的局部放大版了，这时候可以清除地看到验证得分的变异系数发生转变是在max_depth为5的时候，在为12的时候开始出现波动。至于取什么值，我们依然遵循前面的原则，分数尽可能高，而变异系数尽可能低，当然并不是说取对应的分数高和编译系数最低的，因为可能存在变异系数在低谷时得分并不高的情况。最后经过考虑之后暂时选择了max_depth为5。1plot_acc_4_grid(rf_grid_cv, &apos;max_depth&apos;) max_features &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;max_features决定了算法在分裂节点时需要考虑的最大特征数，其可以通过内置的枚举值通过计算取值也可以通过直接设置数值取值。因为这里的特征数量并不是很多，加上$OneHot$向量后一共有345个，所以我采用了设置数值的方式，这样既能够清晰地看到不同取值对模型的影响，又方便确定具体数值。123456rf_param = &#123; &apos;max_features&apos;: np.arange(1, 345, 10)&#125;rf_grid_cv = GridSearchCV(RandomForestRegressor(n_estimators=160, max_depth=5), param_grid=rf_param, cv=3, verbose=True, n_jobs=-1)rf_grid_cv.fit(x_train, y_train) Fitting 3 folds for each of 35 candidates, totalling 105 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 6.3s [Parallel(n_jobs=-1)]: Done 105 out of 105 | elapsed: 26.2s finished GridSearchCV(cv=3, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=5, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=160, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), fit_params=None, iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;max_features&#39;: array([ 1, 11, 21, 31, 41, 51, 61, 71, 81, 91, 101, 111, 121, 131, 141, 151, 161, 171, 181, 191, 201, 211, 221, 231, 241, 251, 261, 271, 281, 291, 301, 311, 321, 331, 341])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=True) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我想这幅图应该是目前为止最直观最简单的一副图了，其趋势非常明显——得分不断上升，变异系数不断下降。由此可知，只要max_features设置为最大值就可以了。甚至还可以在特征工程中人工增加一些特征来提升模型复杂度，充分发挥max_features带来的性能提升。1plot_acc_4_grid(rf_grid_cv, &apos;max_features&apos;) min_samples_split &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;min_samples_split同样是一个影响模型偏差/方差的参数，它限定了一个节点分裂所需的最小样本数，其值越大模型越简单，偏差越大方差越小，而调节这个参数就是为了在这之间做一个权衡。min_samples_split参数最小取值为2，基于和训练max_depth时一样的原因，这里将取值限定在2~100。 123456rf_param = &#123; &apos;min_samples_split&apos;: np.arange(2,100, 10),&#125;rf_grid_cv = GridSearchCV(RandomForestRegressor(n_estimators=160, max_depth=5, max_features=&apos;auto&apos;), param_grid=rf_param, cv=3, verbose=True, n_jobs=-1)rf_grid_cv.fit(x_train, y_train) Fitting 3 folds for each of 10 candidates, totalling 30 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 30 out of 30 | elapsed: 14.2s finished GridSearchCV(cv=3, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=5, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=160, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), fit_params=None, iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;min_samples_split&#39;: array([ 2, 12, 22, 32, 42, 52, 62, 72, 82, 92])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=True) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这幅图有一个有趣的地方，虽然得分在不断地下降，但是变异系数存在一个低谷，如果训练到此为止，似乎有理由去选择这个值，因为其有不差的评分和相对较低的变异系数。但事实真的如此吗？1plot_acc_4_grid(rf_grid_cv, &apos;min_samples_split&apos;) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;为了验证上面的假设，我们将取值范围集中在低谷附近，同时将它的步长从10降低到1，让我们来看看训练效果如何。123456rf_param = &#123; &apos;min_samples_split&apos;: np.arange(2,22),&#125;rf_grid_cv = GridSearchCV(RandomForestRegressor(n_estimators=160, max_depth=5, max_features=&apos;auto&apos;), param_grid=rf_param, cv=3, verbose=True, n_jobs=-1)rf_grid_cv.fit(x_train, y_train) Fitting 3 folds for each of 20 candidates, totalling 60 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 21.7s [Parallel(n_jobs=-1)]: Done 60 out of 60 | elapsed: 28.4s finished GridSearchCV(cv=3, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=5, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=160, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), fit_params=None, iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;min_samples_split&#39;: array([ 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=True) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;没错，上面解释过的现象又再次出现了，这里的波谷并不是全局的波谷，可以明显地发现其振荡的现象，但其总体趋势是在升高。因此可以得出结论，min_samples_split在这里并不适合调节，只需要将其设置为默认值就行了。1plot_acc_4_grid(rf_grid_cv, &apos;min_samples_split&apos;) max_leaf_nodes &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;max_leaf_nodes规定了最大叶节点数，与min_samples_split相反， max_leaf_nodes越大模型越复杂，方差越高。甚至可以不限制它的数量，任由其生长，基于此，我们这里选用一个较大的范围去观察它的整体趋势，然后再如同前面一样在细粒度上去进行调节。1234567rf_param = &#123; &apos;max_leaf_nodes&apos;: np.arange(2, 1000, 10)&#125;rf_grid_cv = GridSearchCV(RandomForestRegressor(n_estimators=160, max_depth=5, max_features=&apos;auto&apos;, min_samples_split=2), param_grid=rf_param, cv=3, verbose=True, n_jobs=-1)rf_grid_cv.fit(x_train, y_train) Fitting 3 folds for each of 100 candidates, totalling 300 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 25.9s [Parallel(n_jobs=-1)]: Done 192 tasks | elapsed: 2.0min [Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 3.0min finished GridSearchCV(cv=3, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=5, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=160, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), fit_params=None, iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;max_leaf_nodes&#39;: array([ 2, 12, ..., 982, 992])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=True) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;和前面大多数参数的表现一样，这里也是在取值较小时似乎就已经达到了比较不错的效果，后面的取值对结果也没有提升，反而徒增资源的消耗。1plot_acc_4_grid(rf_grid_cv, &apos;max_leaf_nodes&apos;) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;依葫芦画瓢地，我们缩小参数返回进行细粒度的调参，可以发现其在取值为22时表现已趋于稳定。1234567rf_param = &#123; &apos;max_leaf_nodes&apos;: np.arange(2, 200, 10)&#125;rf_grid_cv = GridSearchCV(RandomForestRegressor(n_estimators=160, max_depth=5, max_features=&apos;auto&apos;, min_samples_split=2), param_grid=rf_param, cv=3, verbose=True, n_jobs=-1)rf_grid_cv.fit(x_train, y_train) Fitting 3 folds for each of 20 candidates, totalling 60 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 26.0s [Parallel(n_jobs=-1)]: Done 60 out of 60 | elapsed: 35.6s finished GridSearchCV(cv=3, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=5, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=160, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), fit_params=None, iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;max_leaf_nodes&#39;: array([ 2, 12, 22, 32, 42, 52, 62, 72, 82, 92, 102, 112, 122, 132, 142, 152, 162, 172, 182, 192])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=True) 1plot_acc_4_grid(rf_grid_cv, &apos;max_leaf_nodes&apos;) min_weight_fraction_leaf &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;叶节点最小权重总值限制了叶子节点所有样本权重的最小值，如果小于这个值，则会和其他叶子节点一起被剪枝，提高模型偏差，降低方差。如果样本的分布存在偏斜或者有较多的缺失值可以考虑引入权重。由于之前已经在特征工程中处理了相应的问题，所以这里的调参对提升模型不会有什么作用，但是并不妨碍我们一窥究竟。1234567rf_param = &#123; &apos;min_weight_fraction_leaf&apos;: np.linspace(0, 0.5, 10)&#125;rf_grid_cv = GridSearchCV(RandomForestRegressor(n_estimators=160, max_depth=5, max_features=&apos;auto&apos;, min_samples_split=2, max_leaf_nodes=22 ), param_grid=rf_param, cv=3, verbose=True, n_jobs=-1)rf_grid_cv.fit(x_train, y_train) Fitting 3 folds for each of 10 candidates, totalling 30 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 30 out of 30 | elapsed: 7.9s finished GridSearchCV(cv=3, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=5, max_features=&#39;auto&#39;, max_leaf_nodes=22, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=160, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), fit_params=None, iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;min_weight_fraction_leaf&#39;: array([0. , 0.05556, 0.11111, 0.16667, 0.22222, 0.27778, 0.33333, 0.38889, 0.44444, 0.5 ])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=True) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我想这是这篇文章到此为止第二个如此直观的图了，那么我便不在做过多的解释，直接确定取值了。1plot_acc_4_grid(rf_grid_cv, &apos;min_weight_fraction_leaf&apos;) min_samples_leaf &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;min_samples_leaf是我们训练的最后一个关于子模型结构的参数了，它表示叶节点的最小样本树。如果返回前面去看我们已经训练过的参数，会发现一个和它非常相似的参数，就是min_samples_split，这两个参树可以说是直接限定定了叶节点样本个数的范围。下面让我们仿照min_samples_split的训练过程对min_samples_leaf的取值进行设定。1234567rf_param = &#123; &apos;min_samples_leaf&apos;: np.arange(1, 100, 10)&#125;rf_grid_cv = GridSearchCV(RandomForestRegressor(n_estimators=160, max_depth=5, max_features=&apos;auto&apos;, min_samples_split=2, max_leaf_nodes=22, min_weight_fraction_leaf=0 ), param_grid=rf_param, cv=3, verbose=True, n_jobs=-1)rf_grid_cv.fit(x_train, y_train) Fitting 3 folds for each of 10 candidates, totalling 30 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 30 out of 30 | elapsed: 13.5s finished GridSearchCV(cv=3, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=5, max_features=&#39;auto&#39;, max_leaf_nodes=22, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0, n_estimators=160, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), fit_params=None, iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;min_samples_leaf&#39;: array([ 1, 11, 21, 31, 41, 51, 61, 71, 81, 91])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=True) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如图所示，虽然在较粗粒度的层面上进行调参，但是其总体趋势确实非常明显，所以这里便不再多做赘述，直接将值取为1。1plot_acc_4_grid(rf_grid_cv, &apos;min_samples_leaf&apos;) bootstrap &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;到目前为止，我们已经通过以上的步骤训练完了直接影响子模型结构的参数(除了n_estimators)，现在我们稍微站高一点，尝试一下对booststrap这个参数取不同的值，看看最后的效果如何。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bootstrap决定了是否对样本进行抽样，也就是说它对训练使用什么哪些样本起着至关重要的作用。一般而言，使用子采样会降低子模型之间的关联度，降低最终模型的方差，这也是bagging的做法。12345678rf_param = &#123; &apos;bootstrap&apos;: [True, False]&#125;rf_grid_cv = GridSearchCV(RandomForestRegressor(n_estimators=160, max_depth=5, max_features=&apos;auto&apos;, min_samples_split=2, max_leaf_nodes=22, min_weight_fraction_leaf=0, min_samples_leaf=1), param_grid=rf_param, cv=3, verbose=True, n_jobs=-1)rf_grid_cv.fit(x_train, y_train) Fitting 3 folds for each of 2 candidates, totalling 6 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 6 out of 6 | elapsed: 4.4s remaining: 0.0s [Parallel(n_jobs=-1)]: Done 6 out of 6 | elapsed: 4.4s finished GridSearchCV(cv=3, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=5, max_features=&#39;auto&#39;, max_leaf_nodes=22, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0, n_estimators=160, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), fit_params=None, iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;bootstrap&#39;: [True, False]}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=True) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;经过验证，在这里展现出的结果也确实如此。那么我很有什么理由不使用默认值呢。1plot_acc_4_grid(rf_grid_cv, &apos;bootstrap&apos;) criterion &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在scikit-learn的0.18版中，mae作为新的计算方法被添加进来，以前都是使用mse来做为判断是否分裂节点的计算方法。既然如此我们也来尝试一下吧。12345678rf_param = &#123; &apos;criterion&apos;: [&apos;mse&apos;, &apos;mae&apos;]&#125;rf_grid_cv = GridSearchCV(RandomForestRegressor(n_estimators=160, max_depth=5, max_features=&apos;auto&apos;, min_samples_split=2, max_leaf_nodes=22, min_weight_fraction_leaf=0, min_samples_leaf=1, bootstrap=True), param_grid=rf_param, cv=3, verbose=True, n_jobs=-1)rf_grid_cv.fit(x_train, y_train) Fitting 3 folds for each of 2 candidates, totalling 6 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 6 out of 6 | elapsed: 59.7s remaining: 0.0s [Parallel(n_jobs=-1)]: Done 6 out of 6 | elapsed: 59.7s finished GridSearchCV(cv=3, error_score=&#39;raise-deprecating&#39;, estimator=RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=5, max_features=&#39;auto&#39;, max_leaf_nodes=22, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0, n_estimators=160, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False), fit_params=None, iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;criterion&#39;: [&#39;mse&#39;, &#39;mae&#39;]}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=True) 1plot_acc_4_grid(rf_grid_cv, &apos;criterion&apos;) RandomForestRegressor &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;经过繁琐的步骤，我们终于可以着手训练一个完整的随机森林模型了。将上面的参数一一对应，直接使用训练数据划分验证测试。123rf = RandomForestRegressor(n_estimators=160, max_depth=5, max_features=&apos;auto&apos;, min_samples_split=2, max_leaf_nodes=22, min_weight_fraction_leaf=0, min_samples_leaf=1, bootstrap=True) 1234rf_pred = cross_val_predict(rf, x_train, y_train, verbose=True, n_jobs=-1, cv=3)mse = mean_squared_error(y_train, rf_pred)np.sqrt(mse) [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 3 out of 3 | elapsed: 1.7s finished 0.16115609724602975 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;就这样，我们最后得到了一个自己亲手调试的集成模型。对这最后的成果我们可以做一个简要的分析。首先相比之前我多次尝试训练的随机森林，它有着一个极大的改变，那就是它的训练曲线不再是一条接近1的水平线了，good job！！！这说明通过调参已经有效的缓解了严重的过拟合现象；其次，我们也可以发现一些问题，模型似乎仍然存在一定程度的过拟合(两条线靠得并不太近)，同时模型的准确度似乎有着明显的下降。1plot_learning_curve(rf, x_train, y_train) [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [learning_curve] Training set sizes: [ 131 426 721 1016 1312] [Parallel(n_jobs=-1)]: Done 50 out of 50 | elapsed: 20.1s finished &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;虽然最后的模型看起来并不是很完美的解决方案，但这至少可以作为一个里程碑，它证明了贪心策略的可行性的同时也产出了一个完整的集成模型。 GBDT&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;梯度提升树是一种Boosting方法，每个子模型拟合的是上一个子模型的预测结果与真实结果的残差，即先训练一个弱分类器，然后用这个弱分类器去预测数据集，得到的预测结果和真实的结果取差，然后将得到的残差作为数据集新的预测目标，下一个分类器再去拟合这个残差，如此反复，最后将所有的弱分类器加权求和得到最终分类器，所以说梯度提升树是一种基于加法模型的算法。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;GBDT通常采用高偏差低方差的基函数，一般是CART Tree(分类回归树)。因为是基于树的集成模型，那么它同样涉及到树的生成问题，例如深度、叶子节点个数、分隔所需最小样本树等等。基于此，对这部分参数的训练可以直接仿照Random Forest的训练过程，所以便不再占用大量的篇幅去描述。 subsample &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;相比于随机森林，GBDT多训练了一个叫做subsample的参数，这个参数的中文名叫做子采样率，它表示每一次训练弱分类器所使用的样本比例，如果$\lt 1.0$表示使用随机梯度提升，能降低方差提高偏差，有效防止过拟合的发生。下面让我们看看它的训练效果。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们将值限定在一个极小数与1之间，然后取了100个值。12345678gbdt_param = &#123; &apos;subsample&apos;: np.linspace(1e-7, 1, 100),&#125;gbdt_grid_cv = GridSearchCV(GradientBoostingRegressor(n_estimators=23, max_depth=8, max_features=&apos;auto&apos;, min_samples_split=2, max_leaf_nodes=12, min_weight_fraction_leaf=0, min_samples_leaf=1), param_grid=gbdt_param, verbose=True, cv=3, n_jobs=-1)gbdt_grid_cv.fit(x_train, y_train) Fitting 3 folds for each of 100 candidates, totalling 300 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 90 tasks | elapsed: 8.1s [Parallel(n_jobs=-1)]: Done 240 tasks | elapsed: 29.4s [Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 39.7s finished GridSearchCV(cv=3, error_score=&#39;raise-deprecating&#39;, estimator=GradientBoostingRegressor(alpha=0.9, criterion=&#39;friedman_mse&#39;, init=None, learning_rate=0.1, loss=&#39;ls&#39;, max_depth=8, max_features=&#39;auto&#39;, max_leaf_nodes=12, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_sampl... subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False), fit_params=None, iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;subsample&#39;: array([1.00000e-07, 1.01011e-02, ..., 9.89899e-01, 1.00000e+00])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=True) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们可以发现随着subsample的取值越大，得分总体呈上升趋势并趋于平稳，变异系数整体呈下降趋势，然后开始震荡。1plot_acc_4_grid(gbdt_grid_cv, &apos;subsample&apos;) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;同理，让我们把目光集中在急速下降的地方，放大它的内部表现。12345678gbdt_param = &#123; &apos;subsample&apos;: np.linspace(1e-7,2.22222e-01, 100),&#125;gbdt_grid_cv = GridSearchCV(GradientBoostingRegressor(n_estimators=23, max_depth=8, max_features=&apos;auto&apos;, min_samples_split=2, max_leaf_nodes=12, min_weight_fraction_leaf=0, min_samples_leaf=1), param_grid=gbdt_param, verbose=True, cv=3, n_jobs=-1)gbdt_grid_cv.fit(x_train, y_train) Fitting 3 folds for each of 100 candidates, totalling 300 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 136 tasks | elapsed: 9.1s [Parallel(n_jobs=-1)]: Done 286 tasks | elapsed: 24.6s [Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 25.8s finished GridSearchCV(cv=3, error_score=&#39;raise-deprecating&#39;, estimator=GradientBoostingRegressor(alpha=0.9, criterion=&#39;friedman_mse&#39;, init=None, learning_rate=0.1, loss=&#39;ls&#39;, max_depth=8, max_features=&#39;auto&#39;, max_leaf_nodes=12, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_sampl... subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False), fit_params=None, iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;subsample&#39;: array([1.00000e-07, 2.24477e-03, ..., 2.19977e-01, 2.22222e-01])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=True) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;和上面的趋势几乎如出一辙。这同样说明该参数不太适合在过小的粒度上进行调节。1plot_acc_4_grid(gbdt_grid_cv, &apos;subsample&apos;) GBDT&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;没错，我们又得到了一个集成模型，现在让我们再看看这个模型的表现。 123gbdt = GradientBoostingRegressor(n_estimators=23, max_depth=8, max_features=&apos;auto&apos;, min_samples_split=2, max_leaf_nodes=12, min_weight_fraction_leaf=0, min_samples_leaf=1, subsample=1) 123gbdt_pred = cross_val_predict(gbdt, x_train, y_train)mse = mean_squared_error(y_train, gbdt_pred)np.sqrt(mse) 0.1558966244053635 1plot_learning_curve(gbdt, x_train, y_train) [learning_curve] Training set sizes: [ 131 426 721 1016 1312] [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 50 out of 50 | elapsed: 8.1s finished &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;相比于随机森林而言，它的均方误差变小了,同时准确率也有所提高。 GBDT历史版本&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;同时，这里将之前使用RandomizedSearchCV的版本罗列了出来，可以发现两者的巨大差异，虽然该版本的均方误差更低，但是其明显的过拟合现象就说明这不是一个好的模型了。1234567891011121314151617181920212223from sklearn.ensemble import GradientBoostingRegressorfrom sklearn.model_selection import RandomizedSearchCV# gbdt_param = &#123;# &apos;n_estimators&apos;: np.arange(1000, 5000, 100),# &apos;max_depth&apos;: np.arange(2, 10),# &apos;subsample&apos;: np.linspace(0.1, 1, 20),# &apos;max_features&apos;:[&apos;auto&apos;, &apos;sqrt&apos;, &apos;log2&apos;]# &#125;# gbdt_grid_cv = RandomizedSearchCV(GradientBoostingRegressor(n_estimators=100), param_distributions=gbdt_param, n_jobs=-1,# verbose=True, random_state=42, cv=3)# gbdt_grid_cv.fit(x_train, y_train)gbdt = GradientBoostingRegressor(alpha=0.9, criterion=&apos;friedman_mse&apos;, init=None, learning_rate=0.1, loss=&apos;ls&apos;, max_depth=4, max_features=&apos;auto&apos;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=3900, n_iter_no_change=None, presort=&apos;auto&apos;, random_state=None, subsample=0.5736842105263158, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False)gbdt.fit(x_train, y_train) GradientBoostingRegressor(alpha=0.9, criterion=&#39;friedman_mse&#39;, init=None, learning_rate=0.1, loss=&#39;ls&#39;, max_depth=4, max_features=&#39;auto&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=3900, n_iter_no_change=None, presort=&#39;auto&#39;, random_state=None, subsample=0.5736842105263158, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False) 1# gbdt_grid_cv.best_estimator_ 1# gbdt_grid_cv.best_score_ 1234567from sklearn.model_selection import cross_val_predictfrom sklearn.metrics import mean_squared_error# gbdt = gbdt_grid_cv.best_estimator_gbdt_pred = cross_val_predict(gbdt, x_train, y_train, verbose=True, n_jobs=-1, cv=5)mse = mean_squared_error(y_train, gbdt_pred)np.sqrt(mse) [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 5 out of 5 | elapsed: 58.5s finished 0.12245307114524771 1plot_learning_curve(gbdt, x_train, y_train) [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [learning_curve] Training set sizes: [ 131 426 721 1016 1312] [Parallel(n_jobs=-1)]: Done 50 out of 50 | elapsed: 4.5min finished XGBoost&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在，我们已经使用相同的方式训练了两个集成模型。另外，我还尝试了一些其他模型，比如这里的XGBoost。如果我们还按照上面的行文方式，那么就会变成记流水账了。既然训练方法和过程都已经熟悉，那么这里便直接给出训练后的结果，转而简单介绍一下XGBoost的原理。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;XGBoost是在GBDT的基础上改进而来的一种Boosting算法，虽然是Boosting，但是其可以通过使用特征上的并行计算提升训练效率。它在训练之前会对数据进行排序，然后保存为block结构以便在后序地迭代中使用。同样因为该结构的存在，在节点分裂需要计算每个特征值的增益的时候，就可以多线程地进行。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;XGBoost相比与GBDT还在代价函数中还加入了正则化方法用于控制模型的复杂度，正则化通过降低模型的方差达到防止过拟合的目的。XGBoost不仅仅只是使用CART（booster=&#39;gbtree&#39;）作为基分类器,还同时引入了线性分类器（booster=&#39;gblinear&#39;）,当使用线性分类器时就如同带有l1或l2正则的逻辑斯蒂回归（分类）和线性回归（回归）。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;除此之外，XGBoost还支持列抽样、自动学习缺失值的分裂方向等。XGBoost是一个非常优秀的模型，我这里只是大概做了一个梳理，想要完全掌握这个算法我实在是还差得很远。具体的内容可以查看官方文档，也可以去翻看相关论文进行更深度的学习。1234567xg = XGBRegressor(objective=&apos;reg:squarederror&apos;, booster=&apos;gbtree&apos;, n_estimators=470, max_depth=2, reg_lambda=0, reg_alpha=0.08163, colsample_bylevel=0.06122, colsample_bynode=0, colsample_bytree=1, min_child_weight=1)xg_pred = cross_val_predict(xg, x_train, y_train, verbose=True, n_jobs=-1, cv=5)mse = mean_squared_error(y_train, xg_pred)np.sqrt(mse) [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 5 out of 5 | elapsed: 12.1s finished 0.1472574980184566 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最后我们来看看XGBoost模型的表现情况，可以发现它比之前的GBDT在结果上有不小的提升，两条曲线非常接近，也没有严重的过拟合问题，可以说是一个不错的结果了。1plot_learning_curve(xg, x_train, y_train) [learning_curve] Training set sizes: [ 131 426 721 1016 1312] [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 50 out of 50 | elapsed: 1.1min finished stack&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在文章的开头我们对集成学习做了一个概述，其中省略了如何将子模型结果进行结合的方法，现在便对其做一个补充。在对具体的结合策略进行表述之前，先来说说使用结合策略可以带来哪些好处： 因为目标的假设空间很大，可能有多个假设在训练集上达到相同的性能，因此结合多个学习器可以减小单个学习器因误选而导致泛化性能不佳的风险 由于局部极小值的存在，而单个学习器在有的局部极小值对应的泛化性能很差，多次训练学习器并对结果进行结合可以降低陷入糟糕局部极小值的风险 目标的真实假设空间可能不在当前算法所考虑的假设空间中，多个学习器可以将相应的假设空间扩大从而摒除单个学习器无效的情况 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;综上所述，结合策略的使用可以提高泛化性能，扩大相应的假设空间，使得模型的整体效果变得更好。下面就让我们看看有哪些具体的策略吧。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;首先最容易想到的便是针对回归的平均法和针对分类的投票法。其中平均法又分为简单平均和加权平均。所谓简单平均就是求所有学习器的总平均值，加权平均就是对每一个分类器分配一个权重($\sum ^{T}_{i=1}w_i = 1$)再求和，而简单平均是加权平均权重为$\frac{1}{T}$的特例。投票法与平均法类似，分为绝对多数投票法、相对多数投票法和加权投票法，其中加权投票法和加权平均法类似，只是一个取的是加权后的计数最大值作为最终标记而另一个取的是平均值作为预测结果。绝对多数投票法和相对多数投票法可以说是兄弟了，它们都会通过标记计数大小来预测结果，但是绝对多数取的是预测过半的标记，如果没有则可以拒绝预测，而相对多数投票法就是少数服从多数的完美诠释，即取数量最多的标记作为预测结果，如果绝对多数投票法不允许拒绝预测，要求必须有一个预测值，那么它将退化为相对多数投票法。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;除此之外，还有一种更为强大的结合策略就是学习法。学习法通过额外的学习器(次级学习器)来结合基学习器(初级学习器)的预测结果。其中的典型代表就是下面使用的Stacking2，其一般步骤如下： 从数据集训练出基学习器 使用初级学习器生成新的数据集，新数据集中的特征为初级学习器的输出，标记不变 在新的数据集上使用次级学习器训练 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;学习法在使用过程中为了降低过拟合的风险，在训练初级学习器时使用交叉验证的方式，通过训练初级学习器时未使用的数据来生成新的数据集。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下面是Stacking的一个实现版本，其实我一开始也并没有太懂为什么要这样处理，直到我回过头再去审视这段代码，才有了一定的体会。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们首先来看一下它的构造。在初始化阶段(__init__)我们传入了base_models作为初级学习器，final_models作为次级学习器，即用base_models生成数据，final_model做最后的预测，n_folds表示交叉验证中使用的折数。 fit&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在这里fit不再只是单纯地去训练模型了，它还包括了生成数据集的步骤。前两行代码首先使用base_models生成了一个空列表用于存储训练使用的model，使用clone函数是因为对象传递进来是引用，如果直接在上面进行操作会影响外部的模型，所以要创建一个备份用于该类内使用。接下来便是创建交叉验证的划分折数，然后根据初级学习器的数量创建一个大小为(n, n_models)3的numpy数组用于存储新的数据集。第一层for循环遍历初级学习器，第二层循环使用交叉验证训练初级学习器，并保存每一折的训练后的模型，然后使用初级学习器对为参与训练的数据进行预测，最后使用预测值填充上面的numpy数组对应的位置。在数据生成之后，再用这些数据去训练次级学习器。 predict&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;预测阶段同样需要做一些预处理，顺序地从保存模型的列表中取出对应的模型列表(即对应模型每一折的训练成果)，然后计算该类模型预测的平均值，最后将所有的模型预测结合起来作为最后预测需要的数据，然后再调用次级学习器进行预测。1234567891011121314151617181920212223242526272829303132from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clonefrom sklearn.model_selection import KFoldclass StackModel(BaseEstimator, TransformerMixin, RegressorMixin): def __init__(self, base_models, final_model, n_folds=5): self.base_models = base_models self.final_model = final_model self.n_folds = n_folds def fit(self, X, y): self.base_models_ = [list() for i in self.base_models] self.final_model_ = clone(self.final_model) kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=42) out_predictions = np.zeros((X.shape[0], len(self.base_models))) for i, model in enumerate(self.base_models): for train_index, holdout_index in kfold.split(X, y): instance = clone(model) self.base_models_[i].append(instance) instance.fit(X[train_index], y[train_index]) y_pred = instance.predict(X[holdout_index]) out_predictions[holdout_index, i] = y_pred self.final_model_.fit(out_predictions, y) return self def predict(self, X): final_feature = np.column_stack([ np.column_stack([model.predict(X) for model in base_models]).mean(axis=1) for base_models in self.base_models_ ]) return self.final_model_.predict(final_feature) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们已经证实在简单模型中Lasso在该数据集上的表现更好，同时由于Stacking用多响应线性回归作为次级学习算法效果会比较好，所以这里选用该模型作为次级学习器。其它的初级学习器为我们之前使用或调参之后的模型。12345678910111213from sklearn.linear_model import Lasso, ElasticNetfrom sklearn.kernel_ridge import KernelRidgefrom sklearn.ensemble import GradientBoostingRegressorridge = KernelRidge(degree=2, alpha=0.05, kernel=&apos;polynomial&apos;)lasso = Lasso(alpha=0.0005)en = ElasticNet(max_iter=5000, selection=&apos;random&apos;)gbdt = GradientBoostingRegressor(n_estimators=49, max_depth=5, max_features=&apos;auto&apos;, min_samples_split=2, max_leaf_nodes=7, min_weight_fraction_leaf=0, min_samples_leaf=1, subsample=1)stack_models = StackModel(base_models=(ridge, gbdt, en), final_model=lasso)stack_models.fit(x_train, y_train) StackModel(base_models=(KernelRidge(alpha=0.05, coef0=1, degree=2, gamma=None, kernel=&#39;polynomial&#39;, kernel_params=None), GradientBoostingRegressor(alpha=0.9, criterion=&#39;friedman_mse&#39;, init=None, learning_rate=0.1, loss=&#39;ls&#39;, max_depth=5, max_features=&#39;auto&#39;, max_leaf_nodes=7, ...False, precompute=False, random_state=None, selection=&#39;random&#39;, tol=0.0001, warm_start=False)), final_model=Lasso(alpha=0.0005, copy_X=True, fit_intercept=True, max_iter=1000, normalize=False, positive=False, precompute=False, random_state=None, selection=&#39;cyclic&#39;, tol=0.0001, warm_start=False), n_folds=5) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;显而易见的，使用Stacking之后的均方误差更小，而其得分也达到了90%以上，可以说相当于之前的集成模型或者单一模型有着飞跃性的提升。12345from sklearn.metrics import mean_squared_errorfrom sklearn.model_selection import cross_val_predictstack_pred = stack_models.predict(x_train)mse = mean_squared_error(stack_pred, y_train)np.sqrt(mse) 0.08064604349182854 1plot_learning_curve(stack_models, x_train, y_train) [learning_curve] Training set sizes: [ 131 426 721 1016 1312] [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 50 out of 50 | elapsed: 49.9s finished 预测&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;终于到了验证成果的时候了，在做预测之前，我们先加载需要预测的数据，并使用与处理训练数据相同的规则对数据进行特征处理。1234test = pd.read_csv(&apos;house_price/test.csv&apos;)index = np.array(test[[&apos;Id&apos;]])[:,0]test = test.set_index([&apos;Id&apos;])x_test = full_pipeline.transform(test) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一如既往地，训练模型、数据预测，结合，最后将预测结果按照指定的格式输出到文件。这里需要稍微说一下的是结合策略，因为在上面的模型中，XGBoost应该是除了Stacking之外的表现最好的模型，所以在这里将这两者的结果进行加权平均，得出最后的预测值。这里的比例是随机选取的，当然也可以通过构建新的模型来训练这个比重，这里便不再多做赘述。最后还有一个需要注意的地方，那就是对模型预测的结果还取了指数，这是因为在之前分析训练集时为了解决目标值分布偏移的问题而对其进行了取对数操作，这使得所有的预测结果其实都是以此为基准的，为了得到真实的结果那么自然需要进行还原。12345678stack_models.fit(x_train, y_train)stack_pre = stack_models.predict(x_test)xg.fit(x_train, y_train)xg_pre = xg.predict(x_test)ensemble_pred = stack_pre*0.8 + xg_pre*0.2pred_df = pd.DataFrame(&#123;&apos;Id&apos;:index, &apos;SalePrice&apos;:np.expm1(ensemble_pred)&#125;)pred_df.to_csv(&apos;./house_price/prediction.csv&apos;, index=&apos;&apos;) 写在最后&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;转眼之间两个月已经过去了，总算是把内容都呈现了出来。虽然说两篇文章没有什么实质性的内容，但对我而言已经是一个良好的总结了。在这两个月多月里，我基本上是坚持每天码字。有时候写到不懂的地方就需要花费大量的时间去查资料，这导致最后行文可能只有一两行。特别是最后这一段时间，工作繁忙而没有时间做一个很好的梳理并保持之前的连续性。虽然有诸多困难，但好在功夫不负有心人，也算是圆满完成了当时定下的目标，这点让我倍感欣慰。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在这个过程中，我查阅了大量的资料，从书本到网络博客，参考了很多牛人的思想。其中有两本书可以说是我整个实践过程中的基石，其中一本是周志华老师的《机器学习》，它丰富和完善了我的理论知识，另一本是之前提到的 Aurélien Géron的《Hands-On Machine Learning with Scikit-Learn &amp; TensorFlow》，它教会我如何去使用Scikit-Learn。在此我衷心地感谢在这个过程中给我提供帮助的书和博客的作者。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;最后便是接下来的一些规划了，这已经算是一个实战项目了，但由于基础理论知识的相对薄弱，导致在训练过程中对各个参数的意义一知半解。我接下来打算对我接触到的所有基础内容做一个概览似的总结，方便后序查漏补缺。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;感谢您的阅读！！！ 1.又称离散系数，这里是标准差系数，其反应的是单位均值上的各指标观测值的离散程度 ↩2.本身是一种集成学习方法，这里作为一种特殊的结合策略 ↩3.n为样本数量，n_models为初级学习器个数 ↩]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[House-Prices-Advanced-Regression-Techniques-1]]></title>
    <url>%2F2019%2F05%2F25%2FHouse-Prices-Advanced-Regression-Techniques-1%2F</url>
    <content type="text"><![CDATA[前言&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我真正接触Kaggle是在做《Hands-On Machine Learning with Scikit-Learn and TensorFlow》的一道练习题的时候，那道练习题使用的数据是Kaggle上一个分类数据集——Titanic: Machine Learning from Disaster，当我登录这个页面的时候后发现这是一个非常热门的项目，其参与团队(个人)已经达到了11223个，这对我这样一个初来乍到的人是一个不小的冲击，抱着决定在这个平台试一试的心态我开始寻找适合我的项目。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;House-Prices-Advanced-Regression-Techniques是Kaggle上的一个知识性竞赛，作为一个回归问题，它提供了适量的数据以及特征供学习者使用；而作为机器学习的入门项目它帮助了很多人完成了从0到1的过程，现在上面有4746个团队(个人)提交了自己的预测结果。我作为一名学习者，也通过自己的努力在上面获得了自己的分数——0.12702，这是使用KernelRidge实现的模型进行预测的结果，这并不算一个很好的评分，大概排在1757名左右(前40%)，但对我来说确实一个很大的进步，这标示着从无到有的过程。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kaggle对于数据初学者来说确实是一个非常适合的平台，kaggler们都不吝啬自己的知识，发布着自己的kernel，表述自己的想法，借此帮助每一个需要帮助的社区成员。能完成这个项目对我来说意义非凡，在这里我特别感谢kaggle上的两位kaggler以及他们的对自己项目的无私奉献，他们分别是@Pedro Marcelino和他的kenel——Comprehensive data exploration with Python,他对数据的分析以及把控让现在的我难以望其项背，给了我非常大的启发；以及@Serigne和他的kernel——Stacked Regressions : Top 4% on LeaderBoard,他同样用了@Pedro Marcelino的数据分析方法，但是他在数据分析的基础上增加了模型的训练以及分析过程，帮助我学会把控自己的模型。再次对他们表示真挚的感谢。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;虽然这个项目的准确程度还可以有很大的提升，但就我现在的能力而言我决定让它暂且休息一下，好回头看看，总结总结得失。 准备工作&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在我个人的学习过程中，对于机器学习的理解一直都是觉得算法有多么多么重要，但当我真正着手去做的时候，发现事实其实与我想象中的大相径庭。不可否认，算法是构建模型的关键步骤，是不可逾越的一道关隘，但是其对最终模型的起到的作用其实并没有想象中的那么重要。这不经让我想到微软发布的那个关于数据和算法对模型影响的论文——The Unreasonable Effectiveness of Data，这篇论文说明了当数据量达到一定程度时，算法的优劣将被摒弃，颇有一些殊途同归的意思。而按我的理解，当数据的质量达到一定程度时，算法的优劣差异也会被一定程度上的减弱。我想这也是那么多前辈强调特征工程重要性的原因。 数据篇&nbsp;&nbsp;&nbsp;&nbsp;当我们拿到一份数据的时侯不是将它直接塞入算法，让算法产出一个模型，而应该是先对数据有一个全局的insight，了解数据的组成成分，包括以下几点： 目标&nbsp;&nbsp;&nbsp;&nbsp;即分类任务或者回归任务，观察需要预测的目标的分布，可以借用seaborn库进行可视化 特征值&nbsp;&nbsp;&nbsp;&nbsp;即实例的属性，确定属性值是离散的类别标签还是连续的数值型数据，对不同类型的数据需要进行不同的处理 异常值&nbsp;&nbsp;&nbsp;&nbsp;不是所有的数据分布都是合理的，在数据集中可能存在部分数据的分布超过一定的阈值，这部分数据被称作异常值或者离群点，对异常值的处理需要非常谨慎，这部分数据对模型的好坏起着非常关键的作用。 空值&nbsp;&nbsp;&nbsp;&nbsp;空值一般是指那种数据集中缺失的值，这部分值可能代表数据的特性，如表示某种特征，也可能只是单纯的缺失值。如果是单纯的值缺失，数值类型可以使用中位数进行填充，而类别标签可以用当前特征表示某种类别数量最多的类别值填充；而如果是表示某种特征，则可以使用None填充类别标签，用0填充数值特征，表示该缺失类别观测&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;kaggle有一个非常友好的地方，那就是在每一个项目里面都有关于这个数据集的描述，这个项目的描述文件可以在data_description.txt中看到。结合描述文件和一些python代码可以对数据有一个比较清晰的认知，了解特征的特性、取值和分布，了解目标的特性以及特征和目标之间的直接关系，下面的代码用表的方式直观地展现了数据的一些特性。1234567891011121314151617# 导入相关数据包import pandas as pdimport numpy as npimport seaborn as sbnimport matplotlib.pyplot as pltimport warningsdef ignore_warn(*args, **kwargs): # 忽略警告输出 passwarnings.warn = ignore_warn# 设置最大显示列数pd.set_option("display.max_columns",500)# 导入数据data = pd.read_csv('./house_price/train.csv')#展示数据的前5行data.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Id MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities LotConfig LandSlope Neighborhood Condition1 Condition2 BldgType HouseStyle OverallQual OverallCond YearBuilt YearRemodAdd RoofStyle RoofMatl Exterior1st Exterior2nd MasVnrType MasVnrArea ExterQual ExterCond Foundation BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinSF1 BsmtFinType2 BsmtFinSF2 BsmtUnfSF TotalBsmtSF Heating HeatingQC CentralAir Electrical 1stFlrSF 2ndFlrSF LowQualFinSF GrLivArea BsmtFullBath BsmtHalfBath FullBath HalfBath BedroomAbvGr KitchenAbvGr KitchenQual TotRmsAbvGrd Functional Fireplaces FireplaceQu GarageType GarageYrBlt GarageFinish GarageCars GarageArea GarageQual GarageCond PavedDrive WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition SalePrice 0 1 60 RL 65.0 8450 Pave NaN Reg Lvl AllPub Inside Gtl CollgCr Norm Norm 1Fam 2Story 7 5 2003 2003 Gable CompShg VinylSd VinylSd BrkFace 196.0 Gd TA PConc Gd TA No GLQ 706 Unf 0 150 856 GasA Ex Y SBrkr 856 854 0 1710 1 0 2 1 3 1 Gd 8 Typ 0 NaN Attchd 2003.0 RFn 2 548 TA TA Y 0 61 0 0 0 0 NaN NaN NaN 0 2 2008 WD Normal 208500 1 2 20 RL 80.0 9600 Pave NaN Reg Lvl AllPub FR2 Gtl Veenker Feedr Norm 1Fam 1Story 6 8 1976 1976 Gable CompShg MetalSd MetalSd None 0.0 TA TA CBlock Gd TA Gd ALQ 978 Unf 0 284 1262 GasA Ex Y SBrkr 1262 0 0 1262 0 1 2 0 3 1 TA 6 Typ 1 TA Attchd 1976.0 RFn 2 460 TA TA Y 298 0 0 0 0 0 NaN NaN NaN 0 5 2007 WD Normal 181500 2 3 60 RL 68.0 11250 Pave NaN IR1 Lvl AllPub Inside Gtl CollgCr Norm Norm 1Fam 2Story 7 5 2001 2002 Gable CompShg VinylSd VinylSd BrkFace 162.0 Gd TA PConc Gd TA Mn GLQ 486 Unf 0 434 920 GasA Ex Y SBrkr 920 866 0 1786 1 0 2 1 3 1 Gd 6 Typ 1 TA Attchd 2001.0 RFn 2 608 TA TA Y 0 42 0 0 0 0 NaN NaN NaN 0 9 2008 WD Normal 223500 3 4 70 RL 60.0 9550 Pave NaN IR1 Lvl AllPub Corner Gtl Crawfor Norm Norm 1Fam 2Story 7 5 1915 1970 Gable CompShg Wd Sdng Wd Shng None 0.0 TA TA BrkTil TA Gd No ALQ 216 Unf 0 540 756 GasA Gd Y SBrkr 961 756 0 1717 1 0 1 0 3 1 Gd 7 Typ 1 Gd Detchd 1998.0 Unf 3 642 TA TA Y 0 35 272 0 0 0 NaN NaN NaN 0 2 2006 WD Abnorml 140000 4 5 60 RL 84.0 14260 Pave NaN IR1 Lvl AllPub FR2 Gtl NoRidge Norm Norm 1Fam 2Story 8 5 2000 2000 Gable CompShg VinylSd VinylSd BrkFace 350.0 Gd TA PConc Gd TA Av GLQ 655 Unf 0 490 1145 GasA Ex Y SBrkr 1145 1053 0 2198 1 0 2 1 4 1 Gd 9 Typ 1 TA Attchd 2000.0 RFn 3 836 TA TA Y 192 84 0 0 0 0 NaN NaN NaN 0 12 2008 WD Normal 250000 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上表将所有的特征列都展示了出来，通过和描述文件结合，可以大概知道特征存在数值和类别两种不同形式的数据。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;但仅有这些明显是不够的，在远距离观察之后，让我们走近一点，细细品一品这些有趣的数据。在接近它之前，还有一点点额外的工作需要做。我们知道在用pandas导入数据的时候，DataFrame会自动为我们创建index,而通过对数据的遥望，我们发现数据集中有一个叫Id的特征列是按照有序递增的方式排列的，这个特征列在描述文件中并没有提及，由此我们可以相信这是一个与数据集分布无关的列，只是每个实例的唯一标志——当然，得出这个结论其实不需要这么复杂的分析，因为它是显而易见的。因此，我们可以放弃自动生成的index而使用Id作为新的index。1data = data.set_index(['Id']) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在，可以说是万事具备只欠东风了。是时候深入了解我们的数据了。我想没有什么能比图片更具有表现力了吧，现在就让我们来通过图来观察它。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数值型的特征有一个类别特征不具备的好处，它可以直接和目标值绘制出相关性，而接下来的代码就是做这件事的。我们首先将数值型的特征筛选出来，然后分别和SalePrice进行关联，SalePrice作为Y轴是因变量，特征值作为X轴是自变量12345678910fig = plt.figure(figsize=(24, 36))count = 1for x in data[data.columns[data.dtypes != 'object']]: ax = fig.add_subplot(8,5, count) ax.scatter(y=data['SalePrice'], x=data[x]) ax.set_xlabel(x, fontsize=13) ax.set_ylabel('SalePrice', fontsize=13) ax.set_title(x) count += 1plt.subplots_adjust(hspace=0.9, bottom=0.1, wspace=0.4) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过这些图，可以看出一些特征和目标有比较明显的线性关系，例如TotalBsmtSF、1stFlrSF和2ndFlrSF等，在进行特征值处理的时候就可以在这些数据上做些文章。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当然，除了数据的相关性以外，还可以看出数据的一些其它情况，例如离群点。从下面的代码中可以看出我将大部分的离群点都做过处理，但是在这里我把它注释掉了。至于原因，当然是因为这么处理之后模型训练并不理想，这是因为在训练集中虽然可以删除所有的异常值，让数据看起来非常完美，让模型的训练准确率变得很高，但是这样做是没有意义的，因为这将导致在测试的时候效果变得很差，对于测试的数据，我们总不能也将这些异常值删去不做预测吧，就像在业务场景中我们不可能抛弃一部分看起来不太合理但实际存在的客户一样，所以后面采用了其它方式处理训练集和测试集的离群点.至于保留这部分注释，也是为了保留这个思考过程。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在众多删除异常值的代码中，唯独有一行并没有删去。这一行删除的数据是[0, 3]这张图的那两个异常点，是参考@Pedro Marcelino的kenel后选择保留的。刚开始保留的时候我其实并不太清楚为何@Pedro Marcelino独独要删去这两个离群点而对其它的视而不见，后来我阅读关于这个特征的描述——Above grade (ground) living area square feet以及后面的相关性矩阵发现这个特征和大多数的面积特征都有关系(毕竟它表示地上生活面积)，并且在去掉这两个离群点之后重新画了图，看到其它面积的离群点也一起消失了。这无疑证明了@Pedro Marcelino这种处理方式的合理性。 123456789101112131415161718# data.drop(data[data['LotFrontage'] &gt; 300].index, inplace=True)# data.drop(data[data['LotArea'] &gt; 100000].index, inplace=True)# data.drop(data[data['MasVnrArea'] &gt; 1500].index, inplace=True)data.drop(data[(data['GrLivArea'] &gt; 4000) &amp; (data['SalePrice']&lt;300000)].index, inplace=True)# data.drop(data[data['BsmtFullBath'] == 3].index, inplace=True)# data.drop(data[data['EnclosedPorch'] &gt; 400].index, inplace=True)# data.drop(data[data['PoolArea'] &gt; 200].index, inplace=True)# data.drop(data[data['MiscVal'] &gt; 5000].index, inplace=True)# fig = plt.figure(figsize=(24, 36))# count = 1# for x in data[data.columns[data.dtypes != 'object']]:# ax = fig.add_subplot(8,5, count)# ax.scatter(y=data['SalePrice'], x=data[x])# ax.set_xlabel(x, fontsize=13)# ax.set_ylabel('SalePrice', fontsize=13)# ax.set_title(x)# count += 1# plt.subplots_adjust(hspace=0.9, bottom=0.1, wspace=0.4) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;处理完了明显的异常点之后，换个方向，看看我们的目标值。我们知道线性模型往往最简单的回归模型，而根据奥卡姆剃刀原理，当两个模型拥有一致的性能的时候，选取相对简单的那个。适用线性模型的数据往往具有正态分布的特性，为了明白线性模型是否适用于当前数据集，我们可以通过查看目标值是否满足这一特性。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过调用seaborn库的distplot函数，能够很简单明了的发现数据存在比较明显的偏斜。为了让数据分布更加符合正态分布，可以尝试对其进行取对数操作，因为取对数并不会影响数据的相对关系，并且可以减弱数据的异方差。通过下方的第二张图可以发现，在进行对数操作之后的数据图形更加符合正态分布。 原始12#查看售价分布sbn.distplot(data['SalePrice']) 取对数1sbn.distplot(np.log1p(data['SalePrice'])) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;现在，我们对数据集已经有了一个大概的了解，包括特征值和目标值，并知道了需要对目标值做相应的处理(上面的取对数操作)，是时候将目标值取出来放在旁边了。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;使用DataFrame提供的切片操作，将数据的特征和目标划分开并分别定义为x_train(特征)和y_train(目标)。在完成这个步骤之后，通过info()函数查看特征值的类型和数量关系，明确哪类特征有多少缺失值，方便后序处理。当我看见特征列中有部分数据存在大量缺失的时候，如PoolQC、Fence等，第一反应是直接删除这些数据，当然这种方式是欠考虑的；正如前文所述，在处理缺失值的时候我们应该考虑它是否代表该特征的部分特性以便做特殊处理。(后面未被删去的注释代码体现了这一思考过程)。12x_train, y_train = data.loc[:,:'SaleCondition'], np.log1p(data['SalePrice']).get_values()x_train.info() &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 1458 entries, 1 to 1460 Data columns (total 79 columns): MSSubClass 1458 non-null int64 MSZoning 1458 non-null object LotFrontage 1199 non-null float64 LotArea 1458 non-null int64 Street 1458 non-null object Alley 91 non-null object LotShape 1458 non-null object LandContour 1458 non-null object Utilities 1458 non-null object LotConfig 1458 non-null object LandSlope 1458 non-null object Neighborhood 1458 non-null object Condition1 1458 non-null object Condition2 1458 non-null object BldgType 1458 non-null object HouseStyle 1458 non-null object OverallQual 1458 non-null int64 OverallCond 1458 non-null int64 YearBuilt 1458 non-null int64 YearRemodAdd 1458 non-null int64 RoofStyle 1458 non-null object RoofMatl 1458 non-null object Exterior1st 1458 non-null object Exterior2nd 1458 non-null object MasVnrType 1450 non-null object MasVnrArea 1450 non-null float64 ExterQual 1458 non-null object ExterCond 1458 non-null object Foundation 1458 non-null object BsmtQual 1421 non-null object BsmtCond 1421 non-null object BsmtExposure 1420 non-null object BsmtFinType1 1421 non-null object BsmtFinSF1 1458 non-null int64 BsmtFinType2 1420 non-null object BsmtFinSF2 1458 non-null int64 BsmtUnfSF 1458 non-null int64 TotalBsmtSF 1458 non-null int64 Heating 1458 non-null object HeatingQC 1458 non-null object CentralAir 1458 non-null object Electrical 1457 non-null object 1stFlrSF 1458 non-null int64 2ndFlrSF 1458 non-null int64 LowQualFinSF 1458 non-null int64 GrLivArea 1458 non-null int64 BsmtFullBath 1458 non-null int64 BsmtHalfBath 1458 non-null int64 FullBath 1458 non-null int64 HalfBath 1458 non-null int64 BedroomAbvGr 1458 non-null int64 KitchenAbvGr 1458 non-null int64 KitchenQual 1458 non-null object TotRmsAbvGrd 1458 non-null int64 Functional 1458 non-null object Fireplaces 1458 non-null int64 FireplaceQu 768 non-null object GarageType 1377 non-null object GarageYrBlt 1377 non-null float64 GarageFinish 1377 non-null object GarageCars 1458 non-null int64 GarageArea 1458 non-null int64 GarageQual 1377 non-null object GarageCond 1377 non-null object PavedDrive 1458 non-null object WoodDeckSF 1458 non-null int64 OpenPorchSF 1458 non-null int64 EnclosedPorch 1458 non-null int64 3SsnPorch 1458 non-null int64 ScreenPorch 1458 non-null int64 PoolArea 1458 non-null int64 PoolQC 6 non-null object Fence 281 non-null object MiscFeature 54 non-null object MiscVal 1458 non-null int64 MoSold 1458 non-null int64 YrSold 1458 non-null int64 SaleType 1458 non-null object SaleCondition 1458 non-null object dtypes: float64(3), int64(33), object(43) memory usage: 911.2+ KB 数据分类&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;全局的审视已经告一段落了，是时候关注一些局部的细节了。正如前文已经提到的，特征一般分为两类，而现在就是要对两类数据分开讨论了。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在这里，我们将会分别讨论数值型和类别型特征，包括数据分布，数值特征、数量关系等。 数值型数据&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在之前的分析中，我们已经其实已经对数值型的数据有一个大概的了解了，现在我们需要进行具体的数值分析。数值型的数据有很多的指标可以进行分析，通过平均数、中位数、分位数、总数、标准差等可以得到不少有用的信息。而这些指标可以使用describe方法直接求的，下面便是计算之后的结果：1x_train.describe() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } MSSubClass LotFrontage LotArea OverallQual OverallCond YearBuilt YearRemodAdd MasVnrArea BsmtFinSF1 BsmtFinSF2 BsmtUnfSF TotalBsmtSF 1stFlrSF 2ndFlrSF LowQualFinSF GrLivArea BsmtFullBath BsmtHalfBath FullBath HalfBath BedroomAbvGr KitchenAbvGr TotRmsAbvGrd Fireplaces GarageYrBlt GarageCars GarageArea WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea MiscVal MoSold YrSold count 1458.000000 1199.000000 1458.000000 1458.000000 1458.000000 1458.000000 1458.000000 1450.000000 1458.000000 1458.000000 1458.000000 1458.000000 1458.000000 1458.000000 1458.000000 1458.000000 1458.000000 1458.000000 1458.000000 1458.00000 1458.000000 1458.000000 1458.000000 1458.000000 1377.000000 1458.000000 1458.000000 1458.000000 1458.000000 1458.000000 1458.000000 1458.000000 1458.000000 1458.000000 1458.000000 1458.000000 mean 56.893004 69.797331 10459.936900 6.093964 5.576132 1971.218107 1984.834019 102.753793 438.827160 46.613169 567.096708 1052.537037 1158.851166 345.762003 5.852538 1510.465706 0.423868 0.057613 1.563786 0.38203 2.866255 1.046639 6.510974 0.611111 1978.464052 1.766118 472.050069 94.084362 46.245542 21.984225 3.414266 15.081619 2.433471 43.548697 6.323045 2007.816187 std 42.329437 23.203458 9859.198156 1.376369 1.113359 30.193754 20.641760 179.442156 432.969094 161.420729 442.087187 414.982320 372.039498 435.423924 48.655960 507.878508 0.517404 0.238907 0.549891 0.50271 0.816323 0.220483 1.615880 0.641988 24.682879 0.747104 212.239248 125.350021 65.312932 61.155666 29.337173 55.792877 38.209947 496.460799 2.700167 1.328826 min 20.000000 21.000000 1300.000000 1.000000 1.000000 1872.000000 1950.000000 0.000000 0.000000 0.000000 0.000000 0.000000 334.000000 0.000000 0.000000 334.000000 0.000000 0.000000 0.000000 0.00000 0.000000 0.000000 2.000000 0.000000 1900.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 2006.000000 25% 20.000000 59.000000 7544.500000 5.000000 5.000000 1954.000000 1967.000000 0.000000 0.000000 0.000000 223.000000 795.250000 882.000000 0.000000 0.000000 1128.500000 0.000000 0.000000 1.000000 0.00000 2.000000 1.000000 5.000000 0.000000 1961.000000 1.000000 331.500000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 5.000000 2007.000000 50% 50.000000 69.000000 9475.000000 6.000000 5.000000 1972.500000 1994.000000 0.000000 382.000000 0.000000 477.500000 991.000000 1086.000000 0.000000 0.000000 1461.500000 0.000000 0.000000 2.000000 0.00000 3.000000 1.000000 6.000000 1.000000 1980.000000 2.000000 479.500000 0.000000 24.500000 0.000000 0.000000 0.000000 0.000000 0.000000 6.000000 2008.000000 75% 70.000000 80.000000 11600.000000 7.000000 6.000000 2000.000000 2004.000000 164.750000 711.000000 0.000000 808.000000 1296.750000 1390.750000 728.000000 0.000000 1776.000000 1.000000 0.000000 2.000000 1.00000 3.000000 1.000000 7.000000 1.000000 2002.000000 2.000000 576.000000 168.000000 68.000000 0.000000 0.000000 0.000000 0.000000 0.000000 8.000000 2009.000000 max 190.000000 313.000000 215245.000000 10.000000 9.000000 2010.000000 2010.000000 1600.000000 2188.000000 1474.000000 2336.000000 3206.000000 3228.000000 2065.000000 572.000000 4476.000000 3.000000 2.000000 3.000000 2.00000 8.000000 3.000000 14.000000 3.000000 2010.000000 4.000000 1390.000000 857.000000 547.000000 552.000000 508.000000 480.000000 738.000000 15500.000000 12.000000 2010.000000 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过count行可以比上面的全局更直观地看到哪些数据存在缺失，哪些没有；mean可以知道相应特征的平均值；std是指的标准差；min和max分别表示最小值和最大值；剩下对应的行是相对应的分位数。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过最大最小值可以得到数据的分布区间，比如LotArea的最大值为 215245而最小值为1300而Fireplaces最大值为3最小值为0。不同的区间会导致模型的性能不佳，所以需要进行标准化。标准化的方法有很多，有Min-Max标准化1和Z-score标准化，而这里用的是StandardScaler提供的Z-score标准化: z = \frac{x - u}{s} z: 标准化后的值 x: 原始值 u: 对应特征列的平均值 s: 对应特征列的标准差 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上述的的标准化已经基本使用到了除了分位数的所有指标，那么分位数又有什么用呢？先让我们画出箱线图吧。1234567fig = plt.figure(figsize=(24, 24))count = 1for x in x_train[x_train.columns[x_train.dtypes != 'object']]: ax = fig.add_subplot(8,5, count) ax.boxplot(x_train[x]) ax.set_title(x) count += 1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上面的图就是是所有数值型特征的箱线图，我想你也看到了这些图基本上都具有一箱三线多点的特征。提到箱线图就不得不说一下它的依据了，它是根据计算IQR(四分位距)来绘制的，这里先列出它的计算公式： IQR = Q_3 -Q_1 $Q_3$表示上四分位数即$\frac{3}{4}$分位数 $Q_1$表示下四分位数即$\frac{1}{4}$分位数 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;箱线图中的矩形表示的就是IQR，矩形的上下边界分别就是指的$Q_3$和$Q_1$，而矩形中间的一条线是中位数；上下的线段分别叫做上极限和下极限，而超出这个界限的那些点就被视为异常点，下面是上下极限的计算方法： 上极限$upper = Q_3+ 1.5IQR$ 下极限$down = Q_1 - 1.5IQR$下面是我在网上找的一张关于箱线图的详解&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;箱线图可以比较直观的观察数据的分布，知道其是否出现异常值，有多少异常值，数据是否偏斜等。通过上面的图，可以发现几乎每个特征都存在异常值，还有部分数据存在严重的偏斜。对于异常值，我们有多种处理方式： 删除异常值 用平均数或中位数修正 采用处理缺失值的方法 取对数减少极值影响 压缩极值到上下极限 不处理 可以根据不同情况选取上述方式。而偏斜的数据我们之前其实已经做过处理，便是使用自然对数。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们已经通过箱线图观测到了异常值，也发现部分数据存在偏斜，那么接下来使用直方图来观察数据，明晰具体数据都存在怎样的偏斜。1x_train.hist(figsize=(24, 24)) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过上图我们可以看到部分数据具有明显的正态分布，比如OverallQual、TotRmsAbvGrd，再结合箱线图发现他们的中位数缺失是靠近数据的中点的，这样的数据除了标准化就不用进行太多处理；但是像BsmtUnfSF和2ndFlrSF等就存在数据偏斜的问题，这部分数据除了需要进行归一化，可能还需要进行取对数平滑等操作来降低其可能带来的误差；除了上述的分布以外，我们还可以看到诸如YrSold的均匀分布和3SsnPorch这样的比较极端的分布，对于均匀分布我们不需要做太多处理，而对于那些比较极端分布可以根据数据的数量关系和对目标的影响来决定是否保留或做其它处理。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;之前我们使用散点图观察了数值型特征和目标之间的相关性并且排除了两个异常点，现在我们需要了解的是特征之间的相关性。关于相关性我们依然可以沿用散点图来进行观测，但是这里一共有36个特征，如果要绘制出两两之间的散点图，那么一共需要绘制1260张图，这是非常不利于观察和总结的。因此我们这里摒弃了这种方式，而是使用seaborn的heatmap方法来绘制出相关性矩阵的热力图。12plt.figure(figsize=(24, 24))sbn.heatmap(x_train.corr(), linewidths=0.5, annot=True) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过上图我们可以看到有部分特征具有很高的相关性，比如YearBuilt和GarageYrBlt``TotRmsAbvGrd和GrLiveArea等。对于相关性高的数据，我们可以采用整合数据为新的特征列、留一去一等方式，这需要根据实际情况来决定。但是相关性矩阵有一个缺点就是其只能表示线性相关，而不能体现其它相关性，如多项式、指数等，但是往往线性相关就已经能能够说明问题。对于诸如多项式之类的可以在训练中使用核技巧(如果必要)来弥补这部分缺陷。 非数值型数据&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如你所见，数值型特征有很多分析方法，通过图表可以对数据有很清晰的认知，同时基于认识可以规划出相应的处理方式。说完了数值型特征，那么就要处理非数值型的特征了。虽然非数值型的数据没有那么多分析方法，但是其复杂度却并不少，同样需要考虑异常值、数据分布等。下面先让我们看看数据的分布。1234567891011121314151617181920fig = plt.figure(figsize=(24, 48))count = 1for x in x_train.columns[x_train.dtypes == 'object']: ax = fig.add_subplot(15, 3, count) temp_feature = x_train[x].value_counts() feature_bar = ax.bar(range(temp_feature.shape[0]), temp_feature.values, align='center') ax.set_xticks(np.arange(temp_feature.shape[0])) if temp_feature.shape[0] &gt; 10: indexs = [index[-2:] for index in temp_feature.index] ax.set_xticklabels(indexs) else: ax.set_xticklabels(temp_feature.index) for bar in feature_bar: height = bar.get_height() ax.text(bar.get_x()+bar.get_width()/2-0.1, 1.1*height, str(height))# ax.set_ylim(0, 1.2 * temp_feature.values[0]) ax.set_title(x+'('+str(np.sum(temp_feature))+')') count+=1 plt.subplots_adjust(hspace=0.9, bottom=0.1) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;柱状图以每个特征的取值为横坐标，相应取值的数量为纵坐标，可以比较清晰地看到数据的数量关系。其中Utilities中共有1457条取值为AllPub的数据，1条取值为NoSeWa的数据，对训练并不会有什么帮助，因此可以删去这个特征。而其它的特征虽然也存在明显的偏斜，但是为了保证训练出来的模型不至于过拟合，还是应该适当保留这些特征而不应为了使数据完美而删去这部分特征。 处理数据&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于数据的分析已经告一段落，接下来需要做的是结合之前的分析确定数据的处理方式了。这里使用创建Transformers类的方式来统筹数据处理，让我们依此来看下面类的作用： FeaturePreProcessing这个类用于对所有的特征进行预处理，主要是转换数据的类别，因为有些数据虽然是数值类型，但是其表示的意义是一种类别关系，如果把其当成数字特征就隐含了大小关系，而这种特征是没有这种关系的，就会使模型进度下降，所以需要将这类数据的类型从float或int转换为str。同时，这里添加了使用TotalBsmtSF、1stFlrSF和2ndFlrSF添加了一个TotalSF组合特征，这是因为通过描述文件提供的信息可以得知这三个特征包含了一个的所有楼层建面信息。 FeatureSelectFeatureSelect类的作用比较简单，其目的只是单纯地为了分离数值型特征和非数值型特征以便后面分别进行处理。 NumericalImputer对于数值型的数据，在之前进行处理的时候一开始是直接使用sklearn的SimpleImputer进行处理的，但后来比对描述文件，发现对空值不能这么一概而论，所以增加了这个类用于处理特定的数值特征空值(用0填充) StringImputer犹如分析时一样，处理完了数值型的特征便是处理非数值型的了。非数值型的特征缺失值有两种处理方式，第一种是根据描述文件表述的意思将NA替换为None，第二种是根据频率最高的填充。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243from sklearn.base import BaseEstimator, TransformerMixinfrom sklearn.model_selection import learning_curveclass FeaturePreProcessing(BaseEstimator, TransformerMixin): """预处理所有特征""" def fit(self, X, y=None): return self def transform(self, X, y=None): X['MSSubClass'] = X['MSSubClass'].astype('str') X['YrSold'] = X['YrSold'].astype('str') X['MoSold'] = X['MoSold'].astype('str') X['OverallQual'] = X['OverallQual'].astype('str') X['OverallCond'] = X['OverallCond'].astype('str') X['TotalSF'] = X['TotalBsmtSF'] + X['1stFlrSF'] + X['2ndFlrSF'] return Xclass FeatureSelect(BaseEstimator, TransformerMixin): """特征选取""" def __init__(self, obj=True): self.obj = obj def fit(self, X, y=None): return self def transform(self, X, y=None): return X[X.columns[X.dtypes == 'object']] if self.obj else X[X.columns[X.dtypes != 'object']] class NumericalImputer(BaseEstimator, TransformerMixin): """数值型特征填充空值""" def fit(self, X, y=None): self.attributes = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'Fireplaces', 'MasVnrArea', 'GarageCars', 'GarageArea', 'GarageYrBlt'] return self def transform(self, X, y=None): for attribute in self.attributes: X[attribute].fillna(0.0, inplace=True) # 添加总面积特征 # X['TotalSF'] = X['TotalBsmtSF'] + X['1stFlrSF'] + X['2ndFlrSF'] return Xclass StringImputer(BaseEstimator, TransformerMixin): """填充String类型的空值""" def fit(self, X, y=None): self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X], index=X.columns) return self def transform(self, X, y=None): """ Alley: Type of alley access to property Grvl Gravel Pave Paved NA No alley access """ X['Alley'].fillna('None', inplace=True) """ MasVnrType: Masonry veneer type BrkCmn Brick Common BrkFace Brick Face CBlock Cinder Block None None Stone Stone """ X['MasVnrType'].fillna('None', inplace=True) """ BsmtCond: Evaluates the general condition of the basement Ex Excellent Gd Good TA Typical - slight dampness allowed Fa Fair - dampness or some cracking or settling Po Poor - Severe cracking, settling, or wetness NA No Basement """ X['BsmtCond'].fillna('None', inplace=True) """ BsmtExposure: Refers to walkout or garden level walls Gd Good Exposure Av Average Exposure (split levels or foyers typically score average or above) Mn Mimimum Exposure No No Exposure NA No Basement """ X['BsmtExposure'].fillna('None', inplace=True) """ BsmtFinType1: Rating of basement finished area GLQ Good Living Quarters ALQ Average Living Quarters BLQ Below Average Living Quarters Rec Average Rec Room LwQ Low Quality Unf Unfinshed NA No Basement """ X['BsmtFinType1'].fillna('None', inplace=True) """ BsmtFinType2: Rating of basement finished area (if multiple types) GLQ Good Living Quarters ALQ Average Living Quarters BLQ Below Average Living Quarters Rec Average Rec Room LwQ Low Quality Unf Unfinshed NA No Basement """ X['BsmtFinType2'].fillna('None', inplace=True) """ FireplaceQu: Fireplace quality Ex Excellent - Exceptional Masonry Fireplace Gd Good - Masonry Fireplace in main level TA Average - Prefabricated Fireplace in main living area or Masonry Fireplace in basement Fa Fair - Prefabricated Fireplace in basement Po Poor - Ben Franklin Stove NA No Fireplace """ X['FireplaceQu'].fillna('None', inplace=True) """ GarageType: Garage location 2Types More than one type of garage Attchd Attached to home Basment Basement Garage BuiltIn Built-In (Garage part of house - typically has room above garage) CarPort Car Port Detchd Detached from home NA No Garage """ X['GarageType'].fillna('None', inplace=True) """ GarageFinish: Interior finish of the garage Fin Finished RFn Rough Finished Unf Unfinished NA No Garage """ X['GarageFinish'].fillna('None', inplace=True) """ GarageQual: Garage quality Ex Excellent Gd Good TA Typical/Average Fa Fair Po Poor NA No Garage """ X['GarageQual'].fillna('None', inplace=True) """ GarageCond: Garage condition Ex Excellent Gd Good TA Typical/Average Fa Fair Po Poor NA No Garage """ X['GarageCond'].fillna('None', inplace=True)# X['GarageYrBlt'].fillna('None', inplace=True) """ PoolQC: Pool quality Ex Excellent Gd Good TA Average/Typical Fa Fair NA No Pool """ X['PoolQC'].fillna('None', inplace=True) """ Fence: Fence quality GdPrv Good Privacy MnPrv Minimum Privacy GdWo Good Wood MnWw Minimum Wood/Wire NA No Fence """ X['Fence'].fillna('None', inplace=True) """ MiscFeature: Miscellaneous feature not covered in other categories Elev Elevator Gar2 2nd Garage (if not described in garage section) Othr Other Shed Shed (over 100 SF) TenC Tennis Court NA None """ X['MiscFeature'].fillna('None', inplace=True) return X.fillna(self.most_frequent_) class DropFeature(BaseEstimator, TransformerMixin): """删除部分特征""" def __init__(self, features): self.features = features def fit(self, X, y=None): return self def transform(self, X, y=None): return X.drop(self.features, axis=1)class RemoveOutlier(BaseEstimator, TransformerMixin): """处理异常值""" def fit(self, X, y=None): q1 = X.quantile(0.25) q3 = X.quantile(0.75) iqr = q3 - q1 self.upper = q3 + 1.5 * iqr self.down = q1 - 1.5 * iqr self.median = X.median() return self def transform(self, X, y=None): X.where(X &lt;= self.upper, self.upper, axis=1, inplace=True) X.where(X &gt;= self.down, self.down, axis=1, inplace=True)# X['MiscVal'].where(X['MiscVal'] &lt;= 5000, 5000, inplace=True)# X['LotFrontage'].where(X['LotFrontage'] &lt;= 300, 300, inplace=True)# X['LotArea'].where(X['LotArea'] &lt;= 100000, 100000, inplace=True )# X['MasVnrArea'].where(X['MasVnrArea'] &lt;= 1500, 1500, inplace=True)# X['GrLivArea'].where(X['GrLivArea'] &lt;= 4000, 4000, inplace=True)# X['EnclosedPorch'].where(X['EnclosedPorch'] &lt;= 400, 400, inplace=True)# X['MiscVal'].where(X['MiscVal'] &lt;= 5000, 5000, inplace=True) return X def plot_learning_curve(model, X, y): train_size, train_scores, test_scores = learning_curve(model, X, y, n_jobs=-1, verbose=True, cv=10, random_state=42) train_scores_mean = np.mean(train_scores, axis=1) train_scores_std = np.std(train_scores, axis=1) test_scores_mean = np.mean(test_scores, axis=1) test_scores_std = np.std(test_scores, axis=1) plt.grid() plt.fill_between(train_size, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color='r') plt.fill_between(train_size, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color='b') plt.plot(train_size, train_scores_mean, 'r-', label='train') plt.plot(train_size, test_scores_mean, 'b--',label='val') plt.ylim(0.5, 1.05) plt.yticks( np.linspace(0.5, 1, 11)) plt.xlabel('Train Size', fontsize=14) plt.ylabel('acc', fontsize=14) plt.legend(loc='lower right') DropFeature在进行数据分析的时候，我们有提到特征删除的情况，这个类就是为了处理这种情况，将筛选出需要删除的特征列作为它的处理对象，最后返回去除对应特征的特征 RemoveOutlier正如整个代码所呈现的，该类中存在大量被注释的代码，这些就是前文所提到的删除异常值的尝试，但由于其不理想的效果，最后替换为根据IQR来处理异常值，即将界限外的值统一压缩到界限上。 plot_learning_curve最后的是一个公用方法，其采用的是sklearn.model_selection的learning_curve方法计算出测试分数和训练分数，并根据这两个值画出对应的学习曲线，用这个曲线可以直观地评估模型的优劣，以便对模型做进一步分析。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;之前提到数据的处理是通过创建相应的Transformers来完成的，而为什么需要这么做却没有进行说明。Transformers的作用主要是对数据集进行处理和转换，具体的可以查看sklearn的官方文档。Transformers可以统一训练集、测试集乃至最后的预测数据处理方式，而不用单独对每一个数据集建立重复的处理代码，这显著地提高了代码的复用性；同时Transformers可以与Pipeline有效结合在一起，将整个数据处理简化成一个管道顺序进行，从而使数据处理更加简洁易懂。123456789101112131415161718192021222324252627282930313233from sklearn.pipeline import Pipelinefrom sklearn.pipeline import FeatureUnionfrom sklearn.impute import SimpleImputerfrom sklearn.preprocessing import StandardScalerfrom sklearn.preprocessing import OneHotEncoderall_pipeline = Pipeline([ ('featurePre', FeaturePreProcessing())])numeric_pipeline = Pipeline([# ('drop', DropFeature(['PoolQC','YearBuilt', 'TotRmsAbvGrd', '1stFlrSF'])), ('selector', FeatureSelect(False)), ('impute1', NumericalImputer()),# ('outlier', RemoveOutlier()), ('impute', SimpleImputer(strategy='median')), ('standard', StandardScaler())])cat_pipeline = Pipeline([ ('drop', DropFeature([ 'Utilities'])), ('selector', FeatureSelect()), ('impute', StringImputer()), ('oneHot', OneHotEncoder(sparse=False, handle_unknown='ignore'))])full_pipeline = Pipeline([ ('all_pipeline', all_pipeline), ('featureunion',FeatureUnion([ ('numeric_pipeline', numeric_pipeline), ('cat_pipeline', cat_pipeline)]))])x_train = full_pipeline.fit_transform(x_train) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上面的代码便是使用Pipeline对不同特征进行不同的处理，其中我们创建的自不必多说，我们需要重点解释的是sklearn.preprocessing中导入的StandardScaler和OneHotEncoder以及sklearn.impute的SimpleImputer。 StandardScalerStandardScaler是一个用于标准化的Transformer，其使用的是我们前面提到的Z-score标准化 OneHotEncoder在讨论分数值型特征的时候我们只看了数据的分布情况，而没有谈到对数据的处理方式。其实对于非数值型的特征一般分为两类，一种是将有明显大小关系的的特征转化为数值类特征，而另一种就是OneHotEncoder做的——将特征转化为OneHot向量。OneHot向量可以摒弃数值类数据的相关性，而只是单纯地作为不相关的类别进行使用。 SimpleImputer和我们自己创建的StringImputer功能类似，只不过其是为了填充剩下(除NumericalImputer处理之外的)的数值类型特征，这里使用中位数median作为填充策略。 模型选择&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;到目前为止我们已经花了大量的时间去分析处理数据，希望这些工作没有让你忘记我们一开始的目的——训练模型，之前所有的特征工程都是为了在最后能得到一个最优化模型，让我们能够使用这个模型预测未知的数据集。下面我将我进行过尝试的模型全部罗列了出来，让我们来一一地解读它们吧。 随机梯度下降&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在使用随机梯度下降算法之前，我最先训练的是基于最小二乘法2的LinearRegression，但是其效果实在是差强人意，而sklearn提供的LinearRegression是基于最简单的最小二乘——scipy.linalg.lstsq——实现的，没有什么可以调节的参数来使模型具有更好的性能，因此我便想到使用SGDRegressor来替换。12345678910111213141516171819202122232425262728from sklearn.linear_model import SGDRegressorfrom sklearn.model_selection import cross_val_predictfrom sklearn.metrics import mean_squared_errorfrom sklearn.preprocessing import PolynomialFeaturesfrom sklearn.pipeline import Pipelinefrom sklearn.model_selection import RandomizedSearchCV# lr_pipline = Pipeline([# ('poly', PolynomialFeatures(degree=2)),# ('lr', SGDRegressor(alpha=0.001, eta0=0.01,penalty='None', learning_rate='constant'))# ])# lr_pipline.fit(x_train, y_train)# lr_pred = cross_val_predict(lr_pipline, x_train, y_train, # verbose=True, n_jobs=-1, cv=3)# lr_mse = mean_squared_error(lr_pred, y_train)# np.sqrt(lr_mse)sgd = SGDRegressor(loss='huber', early_stopping=True, max_iter=5000)sgd_grid_params = &#123; 'penalty': ['None', 'l1', 'l2', 'elasticnet'], 'alpha': np.linspace(1e-3, 0.01), 'l1_ratio': np.linspace(0, 1), 'learning_rate': ['constant', 'optimal', 'invscaling', 'adaptive'], 'eta0': np.linspace(1e-3, 1), 'power_t': np.linspace(1e-3, 1), 'epsilon': np.linspace(1e-3, 1)&#125;sgd_rnd_cv = RandomizedSearchCV(sgd, param_distributions=sgd_grid_params, cv=5, verbose=True, n_jobs=-1)sgd_rnd_cv.fit(x_train, y_train) Fitting 5 folds for each of 10 candidates, totalling 50 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 3.4min [Parallel(n_jobs=-1)]: Done 50 out of 50 | elapsed: 4.2min finished RandomizedSearchCV(cv=5, error_score=&#39;raise-deprecating&#39;, estimator=SGDRegressor(alpha=0.0001, average=False, early_stopping=True, epsilon=0.1, eta0=0.01, fit_intercept=True, l1_ratio=0.15, learning_rate=&#39;invscaling&#39;, loss=&#39;huber&#39;, max_iter=5000, n_iter=None, n_iter_no_change=5, penalty=&#39;l2&#39;, power_t=0.25, random_state=None, shuffle=True, tol=None, validation_fraction=0.1, verbose=0, warm_start=False), fit_params=None, iid=&#39;warn&#39;, n_iter=10, n_jobs=-1, param_distributions={&#39;penalty&#39;: [&#39;None&#39;, &#39;l1&#39;, &#39;l2&#39;, &#39;elasticnet&#39;], &#39;alpha&#39;: array([0.001 , 0.00118, 0.00137, 0.00155, 0.00173, 0.00192, 0.0021 , 0.00229, 0.00247, 0.00265, 0.00284, 0.00302, 0.0032 , 0.00339, 0.00357, 0.00376, 0.00394, 0.00412, 0.00431, 0.00449, 0.00467, 0.0048...51, 0.8369 , 0.85729, 0.87767, 0.89806, 0.91845, 0.93884, 0.95922, 0.97961, 1. ])}, pre_dispatch=&#39;2*n_jobs&#39;, random_state=None, refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=True) 1sgd_rnd_cv.best_score_ 0.9010049934685431 1sgd_rnd_cv.best_estimator_ SGDRegressor(alpha=0.0017346938775510204, average=False, early_stopping=True, epsilon=0.6941836734693878, eta0=0.1437142857142857, fit_intercept=True, l1_ratio=0.5510204081632653, learning_rate=&#39;optimal&#39;, loss=&#39;huber&#39;, max_iter=5000, n_iter=None, n_iter_no_change=5, penalty=&#39;l1&#39;, power_t=0.5922448979591837, random_state=None, shuffle=True, tol=None, validation_fraction=0.1, verbose=0, warm_start=False) 1234sgd_pred = cross_val_predict(sgd_rnd_cv.best_estimator_, x_train, y_train, verbose=True, n_jobs=-1, cv=3)sgd_mse = mean_squared_error(sgd_pred, y_train)np.sqrt(sgd_mse) [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 3 out of 3 | elapsed: 20.7s finished 0.12857487720273592 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SGDRegressor使用随机梯度下降来最小化损失函数，它拥有多种参数可供调节，具体可以参考官方文档。在我直接使用SGDRegressor默认参数进行训练的时候，发现误差非常大，后来加了各种参数，使用RandomizedSearchCV来进行参数搜索之后依然没有降低这种误差；直到我增加max_iter这一参数，才使得学习曲线逼近并趋近于0.9，这让我意识到之前之所以会出现这种量级的误差可能是训练数据集过小，导致结果无法收敛，而在文档中也确实明确说明了随机梯度下降适用于实例数量大于$10^4$的数据集。 1plot_learning_curve(sgd_rnd_cv.best_estimator_, x_train, y_train) [learning_curve] Training set sizes: [ 131 426 721 1016 1312] [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 50 out of 50 | elapsed: 3.2min finished 比对线性回归&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;之前提到LinearRegression是我尝试的第一个训练算法，但由于其不太好的表现效果被我弃置不顾，在我训练完SGDRegressor之后我又对LinearRegression进行了一些不同的尝试，比如这里将线性模型通过PolynomialFeatures将其变成一个二次型的多项式模型，这样的改变取得还算不错的成绩，只是其学习曲线反应出了它的明显缺陷，比如过拟合、验证结果波动等。这说明只是简单地使用多项式模型还是有不小的瑕疵的。12345678910111213from sklearn.linear_model import LinearRegressionfrom sklearn.model_selection import cross_val_predictfrom sklearn.metrics import mean_squared_errorfrom sklearn.preprocessing import PolynomialFeatureslr_pipline = Pipeline([ ('poly', PolynomialFeatures(degree=2)), ('lr', LinearRegression())])lr_pipline.fit(x_train, y_train)lr_pred = cross_val_predict(lr_pipline, x_train, y_train, verbose=True, n_jobs=-1, cv=3)lr_mse = mean_squared_error(lr_pred, y_train)np.sqrt(lr_mse) [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 3 out of 3 | elapsed: 52.8s finished 0.14167592407063873 1plot_learning_curve(lr_pipline, x_train, y_train) [learning_curve] Training set sizes: [ 131 426 721 1016 1312] [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 50 out of 50 | elapsed: 12.8min finished 岭回归&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;既然说到最小二乘法，就不得不提一下它的变体了。最小二乘法通过最小化均方误差来最小化损失函数，而它的变体体现在使用不同的正则化方法。岭回归便是使用二范数3来进行正则化(即l2正则)： \min_{w} || X w - y||_2^2 + \alpha ||w||_2^2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;通过选取不同的$\alpha$来对系数做不同的限制$w$，当$\alpha$足够小时，会使得一些作用不大的系数变得非常小但又不会为0，这即减小了过拟合的风险又保证了模型的复杂性，不至于损失过多的特征。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;这里选用的是sklearn.kernel_ridge的KernelRidge，该种实现使用到了核技巧4来使数据线性可分。由于前面的线性回归中使用多项式回归取得了一定的效果，所以这里选用了二次的多项式核。1234567from sklearn.kernel_ridge import KernelRidgeridge = KernelRidge(degree=2, alpha=0.05, kernel='polynomial')ridge_pred = cross_val_predict(ridge, x_train, y_train, cv=3, verbose=True, n_jobs=-1)ridge_mse = mean_squared_error(y_train, ridge_pred)np.sqrt(ridge_mse) [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 3 out of 3 | elapsed: 1.5s finished 0.11650866851364311 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;下面便是岭回归的学习曲线，我们可以发现其表现效果远好于上面的线性回归，就准确度而言也高于自己调试的随机梯度下降，但是也可以发现它的训练曲线和验证曲线相比梯度下降来说有一定间隙，可以判定其存在一定程度的过拟合。解决过拟合的方法有很多，比如增加数据量，降低模型复杂度等。1plot_learning_curve(ridge, x_train, y_train) [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [learning_curve] Training set sizes: [ 131 426 721 1016 1312] [Parallel(n_jobs=-1)]: Done 50 out of 50 | elapsed: 1.3s finished LASSO回归&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;既然讨论了使用l2正则的Ridge，又怎么能忘了使用l1正则的Lasso呢。Lasso和Ridge*唯一不同的地方是它使用了一范数来代替而范数也就是使用绝对值来代替平方： \min_{w} { \frac{1}{2n_{\text{samples}}} ||X w - y||_2 ^ 2 + \alpha ||w||_1}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;l1正则在训练过程中会使得相关的系数择一保留，这使得最后的系数保留了大量的0而产生一个稀疏向量，这能有效地减少特征，降低纬度。虽然这能有效缓解过拟合的问题，但是也可能造成精度丧失，而使得模型的泛化能力不足的情况。 123456from sklearn.linear_model import Lassolasso = Lasso(alpha=0.0005)lasso_pred = cross_val_predict(lasso, x_train, y_train, cv=3, verbose=True, n_jobs=-1)lasso_mse = mean_squared_error(lasso_pred, y_train)np.sqrt(lasso_mse) [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 3 out of 3 | elapsed: 0.2s finished 0.1165640368257087 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们已经对Lasso的原理做了一个简单的介绍，下面来看看它的效果如何。在上面的代码中我们只设置了alpha这个关键参数，这个参数起的作用和Ridge中一样，在此便不在多做赘述。Lasso和Ridge最后的结果似乎非常接近，但是就模型复杂度而言Ridge不仅保留了所有的系数，同时还使用了核技巧将模型转化为二次型多项式，而Lasso完全没有任何其它的操作，根据奥卡姆剃刀原理，如果就两者中选择似乎应该优先选择Lasso。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;我们再看看学习曲线，Lasso和Ridge的学习曲线表现出的准确度和上面的均方误差一样都很接近，但是也可以明显地看到Lasso训练和验证两条线更为靠近，这有理由让我们相信它已经优化了Ridge的过拟合问题。1plot_learning_curve(lasso, x_train, y_train) [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [learning_curve] Training set sizes: [ 131 426 721 1016 1312] [Parallel(n_jobs=-1)]: Done 50 out of 50 | elapsed: 2.5s finished Elastic Net&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;l1和l2我们都已经单独进行训练和分析，并且发现两种方式都能取得不错的效果，既然如此，何不将两种方式结合起来试试呢。Elastic Net就是将两者结合起来的产物，其公式如下： \min_{w} { \frac{1}{2n_{\text{samples}}} ||X w - y||_2 ^ 2 + \alpha \rho ||w||_1 + \frac{\alpha(1-\rho)}{2} ||w||_2 ^ 2}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中$\rho$用于控制l1和l2的权衡，这样的方式使得训练过程中如果有多个相关性很高的系数会可能保留多个而不是像Lasso那样随机选取一个，同时也能拥有Ridge在应对数据旋转时的稳定性。123456789101112131415161718from sklearn.linear_model import ElasticNetfrom sklearn.metrics import mean_squared_errorfrom sklearn.model_selection import cross_val_predictfrom sklearn.model_selection import GridSearchCVen = ElasticNet(max_iter=5000, selection='random')en_param = &#123; 'l1_ratio': np.linspace(0.01, 1, 11), 'alpha': np.linspace(0.0005, 0.1,11)&#125;en_grid_cv = GridSearchCV(en, param_grid=en_param, verbose=True, cv=5, n_jobs=-1)en_grid_cv.fit(x_train, y_train)# en = ElasticNet(alpha=0.0005, copy_X=True, fit_intercept=True,# l1_ratio=0.3, max_iter=1000, normalize=False, positive=False,# precompute=False, random_state=None, selection='cyclic', tol=0.0001,# warm_start=False)# en.fit(x_train, y_train) Fitting 5 folds for each of 121 candidates, totalling 605 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 42 tasks | elapsed: 13.3s [Parallel(n_jobs=-1)]: Done 500 tasks | elapsed: 19.2s [Parallel(n_jobs=-1)]: Done 605 out of 605 | elapsed: 19.9s finished GridSearchCV(cv=5, error_score=&#39;raise-deprecating&#39;, estimator=ElasticNet(alpha=1.0, copy_X=True, fit_intercept=True, l1_ratio=0.5, max_iter=5000, normalize=False, positive=False, precompute=False, random_state=None, selection=&#39;random&#39;, tol=0.0001, warm_start=False), fit_params=None, iid=&#39;warn&#39;, n_jobs=-1, param_grid={&#39;l1_ratio&#39;: array([0.01 , 0.109, 0.208, 0.307, 0.406, 0.505, 0.604, 0.703, 0.802, 0.901, 1. ]), &#39;alpha&#39;: array([0.0005 , 0.01045, 0.0204 , 0.03035, 0.0403 , 0.05025, 0.0602 , 0.07015, 0.0801 , 0.09005, 0.1 ])}, pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=True) 12en = en_grid_cv.best_estimator_en_grid_cv.best_estimator_ ElasticNet(alpha=0.0005, copy_X=True, fit_intercept=True, l1_ratio=1.0, max_iter=5000, normalize=False, positive=False, precompute=False, random_state=None, selection=&#39;random&#39;, tol=0.0001, warm_start=False) 1# en_grid_cv.best_score_ 1234en_pred = cross_val_predict(en, x_train, y_train, verbose=True, n_jobs=-1, cv=3)mse = mean_squared_error(y_train, en_pred)np.sqrt(mse) [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [Parallel(n_jobs=-1)]: Done 3 out of 3 | elapsed: 0.1s finished 0.11655822747866447 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;虽然上面说了Elastic Net的优点，但是在针对这个数据集的时候其最后得出的模型却与预期背道而驰。它最后选择的l1_ration也就是$\rho$为1，这也就意味着这个模型完全没有使用l2，而是只使用了l1，最后退化成了Lasso。从最后的均方误差和学习曲线也可以看出其和Lasso非常接近。1plot_learning_curve(en, x_train, y_train) [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers. [learning_curve] Training set sizes: [ 131 426 721 1016 1312] [Parallel(n_jobs=-1)]: Done 50 out of 50 | elapsed: 2.8s finished 小结&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;说是小结，其实是对突然决定分篇的一个阐述。因为后面的模型都是集成模型，但是在训练时都有明显的过拟合问题，因此我又尝试重新训练这些模型，而最后当然是以失败告终了。在不停地尝试，搜索最佳参数的过程中，我突然意识到或许我只是在碰运气，而没有具体地去分析这些参数对模型最后的影响，所以失败是不言而喻的。基于此，我决定先写到这儿，待我将剩下的模型都一一分析透彻之后再单开一篇专门来专门叙述。 1.$z = \frac{m - min}{max - min}$ ↩2.$\min_{w} || X w - y||_2^2$ ↩3.空间上两个向量矩阵的直线距离 ↩4.将低维空间的数据通过核函数映射到高维空间 ↩]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Reinforcement Learning]]></title>
    <url>%2F2019%2F01%2F08%2FReinforcement-Learning%2F</url>
    <content type="text"><![CDATA[引入OpenAI gym12345678import numpy as npimport osimport sys%matplotlib nbaggimport matplotlibimport matplotlib.animation as animationimport matplotlib.pyplot as plt 1import gym 1env = gym.make("MsPacman-v0") 1obs = env.reset() 1obs.shape (210, 160, 3) 1img = env.render(mode="rgb_array") 1234plt.figure(figsize=(5, 4))plt.imshow(img)plt.axis("off")plt.show() &lt;IPython.core.display.Javascript object&gt; 1(img == obs).all() True 1234567def plot_environment(env, figsize=(5, 4)): plt.close() plt.figure(figsize=figsize) img = env.render(mode="rgb_array") plt.imshow(img) plt.axis("off") plt.show() 1env.action_space Discrete(9) 12env.reset()plot_environment(env) &lt;IPython.core.display.Javascript object&gt; 1234for step in range(110): env.step(3) #leftfor step in range(40): env.step(8) #lower-left 1plot_environment(env) &lt;IPython.core.display.Javascript object&gt; 1obs, reward, done, info = env.step(0) 12#observation告诉代理环境是什么样的obs.shape #210*160RGB图像 (210, 160, 3) 12#最后一步的得分reward 10.0 12#游戏结束 done=Truedone False 12#额外信息info {&#39;ale.lives&#39;: 3} 随机游玩12345678910111213frames = []n_max_steps = 1000n_change_steps = 10obs = env.reset()for step in range(n_max_steps): img = env.render(mode="rgb_array") frames.append(img) if step % n_change_steps == 0: action = env.action_space.sample() obs, reward, done, info = env.step(action) if done: break 1234567891011def update_scene(num, frames, patch): patch.set_data(frames[num]) return patchdef plot_animation(frames, repeat=False, interval=40): plt.close() fig = plt.figure() patch = plt.imshow(frames[0]) plt.axis("off") return animation.FuncAnimation(fig, update_scene, fargs=(frames, patch), frames=len(frames), repeat=repeat, interval=interval) 12video = plot_animation(frames)plt.show() &lt;IPython.core.display.Javascript object&gt; 1env.close() Cart-Pole123env = gym.make("CartPole-v0")obs = env.reset()obs array([ 0.01760912, 0.02315423, 0.03459495, -0.03431416]) 修复渲染问题1234567891011121314151617181920212223242526272829303132333435363738394041424344from PIL import Image, ImageDrawtry: from pyglet.gl import gl_info openai_cart_pole_rendering = Trueexcept Exception: openai_cart_pole_rendering = Falsedef render_cart_pole(env, obs): if openai_cart_pole_rendering: return env.render(mode="rgb_array") else: img_w = 600 img_h = 400 cart_w = img_w // 12 cart_h = img_h // 15 pole_len = img_h // 3.5 pole_w = img_w // 80 + 1 x_width = 2 max_ang = 0.2 bg_col = (255, 255, 255) cart_col = 0x000000 pole_col = 0x669acc pos, vel, ang, ang_vel = obs img = Image.new('RGB', (img_w, img_h), bg_col) draw = ImageDraw.Draw(img) cart_x = pos * img_w // x_width + img_w // x_width cart_y = img_h * 95 // 100 top_pole_x = cart_x + pole_len // 2 - pole_len * np.cos(ang) top_pole_y = cart_y - cart_h // 2 - pole_len * np.cos(ang) draw.line((0, cart_y, img_w, cart_y), fill=0) draw.rectangle((cart_x - cart_w // 2, cart_y - cart_h // 2, cart_x + cart_w // 2, cart_y + cart_h // 2), fill=cart_col) draw.line((cart_x, cart_y - cart_h // 2, top_pole_x, top_pole_y), fill=pole_col, width=pole_w) return np.array(img)def plot_cart_pole(env, obs): plt.close() img = render_cart_pole(env, obs) plt.imshow(img) plt.axis("off") plt.show() 1plot_cart_pole(env, obs) &lt;IPython.core.display.Javascript object&gt; 1env.action_space Discrete(2) 12345obs = env.reset()while True: obs, reward, done, info = env.step(0) if done: break 1234plt.close()img = render_cart_pole(env, obs)plt.imshow(img)plt.axis("off") &lt;IPython.core.display.Javascript object&gt; (-0.5, 599.5, 399.5, -0.5) 1img.shape (400, 600, 3) 12345obs = env.reset()while True: obs, reward, done, info = env.step(1) if done: break 1plot_cart_pole(env, obs) &lt;IPython.core.display.Javascript object&gt; 硬编码策略123456789101112131415161718frames = []n_max_steps = 1000n_change_steps = 10obs = env.reset()for step in range(n_max_steps): img = render_cart_pole(env, obs) frames.append(img) position, velocity, angle, angular_velocity = obs if angle &lt; 0: action = 0 else: action = 1 obs, reward, done, info = env.step(action) if done: break 12video = plot_animation(frames)plt.show() &lt;IPython.core.display.Javascript object&gt; 神经网络策略1234567891011121314151617import tensorflow as tfn_inputs = 4n_hidden = 4n_outputs = 1initializer = tf.variance_scaling_initializer()X = tf.placeholder(tf.float32, shape=[None, n_inputs])hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.elu, kernel_initializer=initializer)outputs = tf.layers.dense(hidden, n_outputs, activation=tf.nn.sigmoid, kernel_initializer=initializer)p_left_and_right = tf.concat(axis=1, values=[outputs, 1 - outputs])action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)init = tf.global_variables_initializer() 1234567891011121314n_max_steps = 1000frames = []with tf.Session() as sess: init.run() obs = env.reset() for step in range(n_max_steps): img = render_cart_pole(env, obs) frames.append(img) action_val = action.eval(feed_dict=&#123;X: obs.reshape(1, n_inputs)&#125;) obs, reward, done, info = env.step(action_val[0][0]) if done: breakenv.close() 12video = plot_animation(frames)plt.show() &lt;IPython.core.display.Javascript object&gt; 改进1234def reset_graph(seed=42): tf.reset_default_graph() tf.set_random_seed(seed) np.random.seed(seed) 12345678910111213141516171819202122232425262728import tensorflow as tfreset_graph()n_inputs = 4n_hidden = 4n_outputs = 1learning_rate = 0.01initializer = tf.variance_scaling_initializer()X = tf.placeholder(tf.float32, shape=[None, n_inputs])y = tf.placeholder(tf.float32, shape=[None, n_outputs])hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.elu, kernel_initializer=initializer)logits = tf.layers.dense(hidden, n_outputs)outputs = tf.nn.sigmoid(logits)p_left_and_right = tf.concat(axis=1, values=[outputs, 1 - outputs])action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)optimizer = tf.train.AdamOptimizer(learning_rate)training_op = optimizer.minimize(cross_entropy)init = tf.global_variables_initializer()saver = tf.train.Saver() 1234567891011121314151617181920n_environments = 10n_iterations = 1000envs = [gym.make("CartPole-v0") for _ in range(n_environments)]observations = [env.reset() for env in envs]with tf.Session() as sess: init.run() for iteration in range(n_iterations): target_probas = np.array([([1.] if obs[2] &lt; 0 else [0.]) for obs in observations]) action_val, _ = sess.run([action, training_op], feed_dict=&#123;X: np.array(observations), y: target_probas&#125;) for env_index, env in enumerate(envs): obs, reward, done, info = env.step(action_val[env_index][0]) observations[env_index] = obs if not done else env.reset() saver.save(sess, "reinforce/my_policy_net_basic.ckpt")for env in envs: env.close() 123456789101112131415def render_policy_net(model_path, action, X, n_max_steps = 1000): frames = [] env = gym.make("CartPole-v0") obs = env.reset() with tf.Session() as sess: saver.restore(sess, model_path) for step in range(n_max_steps): img = render_cart_pole(env, obs) frames.append(img) action_val = action.eval(feed_dict=&#123;X: obs.reshape(1, n_inputs)&#125;) obs, reward, done, info = env.step(action_val[0][0]) if done: break env.close() return frames 123frames = render_policy_net("reinforce/my_policy_net_basic.ckpt", action, X)video = plot_animation(frames)plt.show() INFO:tensorflow:Restoring parameters from reinforce/my_policy_net_basic.ckpt &lt;IPython.core.display.Javascript object&gt; Policy Gradients123456789101112131415161718192021222324252627282930313233343536import tensorflow as tfreset_graph()n_inputs = 4n_hidden = 4n_outputs = 1learning_rate = 0.01initializer = tf.variance_scaling_initializer()X = tf.placeholder(tf.float32, shape=[None, n_inputs])hidden = tf.layers.dense(X, n_hidden, activation=tf.nn.elu, kernel_initializer=initializer)logits = tf.layers.dense(hidden, n_outputs)outputs = tf.nn.sigmoid(logits)p_left_and_right = tf.concat(axis=1, values=[outputs, 1 - outputs])action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)y = 1. - tf.to_float(action)cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits)optimizer = tf.train.AdamOptimizer(learning_rate)grads_and_vars = optimizer.compute_gradients(cross_entropy)gradients = [grad for grad, variable in grads_and_vars]gradient_plachholders = []grads_and_vars_feed = []for grad, variable in grads_and_vars: gradient_plachholder = tf.placeholder(tf.float32, shape=grad.get_shape()) gradient_plachholders.append(gradient_plachholder) grads_and_vars_feed.append((gradient_plachholder, variable))training_op = optimizer.apply_gradients(grads_and_vars_feed)init = tf.global_variables_initializer()saver = tf.train.Saver() 12345678910111213141516def discount_rewards(rewards, discount_rate): discount_rewards = np.zeros(len(rewards)) cumulative_rewards = 0 for step in reversed(range(len(rewards))): cumulative_rewards = rewards[step] + cumulative_rewards * discount_rate discount_rewards[step] = cumulative_rewards return discount_rewardsdef discount_and_normalize_rewards(all_rewards, discount_rate): all_discounted_rewards = [discount_rewards(rewards, discount_rate) for rewards in all_rewards] flat_rewards = np.concatenate(all_discounted_rewards) reward_mean = flat_rewards.mean() reward_std = flat_rewards.std() return [(discounted_rewards - reward_mean)/reward_std for discounted_rewards in all_discounted_rewards] 1discount_rewards([10, 0, -50], discount_rate=0.8) array([-22., -40., -50.]) 1discount_and_normalize_rewards([[10, 0, -50],[10, 20]], discount_rate=0.8) [array([-0.28435071, -0.86597718, -1.18910299]), array([1.26665318, 1.0727777 ])] 123456789101112131415161718192021222324252627282930313233343536373839env = gym.make("CartPole-v0")n_games_per_update = 10n_max_steps = 1000n_iterations = 250save_iterations = 10discount_rate = 0.95with tf.Session() as sess: init.run() for iteration in range(n_iterations): print("\rIteration:&#123;&#125;".format(iteration), end ="") all_rewards = [] all_gradients = [] for game in range(n_games_per_update): current_rewards = [] current_gradients = [] obs = env.reset() for step in range(n_max_steps): action_val, gradients_val = sess.run([action, gradients], feed_dict=&#123;X: obs.reshape(1, n_inputs)&#125;) obs, reward, done, info = env.step(action_val[0][0]) current_rewards.append(reward) current_gradients.append(gradients_val) if done: break all_rewards.append(current_rewards) all_gradients.append(current_gradients) all_rewards = discount_and_normalize_rewards(all_rewards, discount_rate=discount_rate) feed_dict = &#123;&#125; for var_index, gradient_plachholder in enumerate(gradient_plachholders): mean_gradients = np.mean([reward * all_gradients[game_index][step][var_index] for game_index,rewards in enumerate(all_rewards) for step, reward in enumerate(rewards)], axis=0) feed_dict[gradient_plachholder] = mean_gradients sess.run(training_op, feed_dict=feed_dict) if iteration % save_iterations == 0: saver.save(sess, "reinforce/nmy_policy_net_pg.ckpt") Iteration:249 1env.close() 123frames = render_policy_net("reinforce/nmy_policy_net_pg.ckpt", action, X, n_max_steps=1000)video = plot_animation(frames)plt.show() INFO:tensorflow:Restoring parameters from reinforce/nmy_policy_net_pg.ckpt &lt;IPython.core.display.Javascript object&gt; 马尔科夫链(Markov Chains)1234567891011121314151617181920212223trainsition_probabilities = [ [0.7, 0.2, 0.0, 0.1], # from s0 to s0, s1, s2, s3 [0.0, 0.0, 0.9, 0.1], # from s1 to ... [0.0, 1.0, 0.0, 0.0], # from s2 to ... [0.0, 0.0, 0.0, 1.0], # from s3 to ...]n_max_steps = 50def print_sequence(start_state=0): current_state = start_state print("States:", end=" ") for step in range(n_max_steps): print(current_state, end=" ") if current_state == 3: break current_state = np.random.choice(range(4), p=trainsition_probabilities[current_state]) else: print("...", end="") print()for _ in range(10): print_sequence() States: 0 0 3 States: 0 1 2 1 2 1 2 1 2 1 3 States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 States: 0 3 States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 States: 0 1 3 States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 ... States: 0 0 3 States: 0 0 0 1 2 1 2 1 3 States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 马尔科夫决策过程(MDP)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667transition_probabilities = [ [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]], # in s0, if action a0 then proba 0.7 to state s0 and 0.3 to state s1, etc. [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]], [None, [0.8, 0.1, 0.1], None], ]rewards = [ [[+10, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, -50]], [[0, 0, 0], [+40, 0, 0], [0, 0, 0]], ]possible_actions = [[0, 1, 2], [0, 2], [1]]def policy_fire(state): return [0, 2, 1][state]def policy_random(state): return np.random.choice(possible_actions[state])def policy_safe(state): return [0, 0, 1][state]class MDPEnvironment(object): def __init__(self, start_state=0): self.start_state=start_state self.reset() def reset(self): self.total_rewards = 0 self.state = self.start_state def step(self, action): next_state = np.random.choice(range(3), p=transition_probabilities[self.state][action]) reward = rewards[self.state][action][next_state] self.state = next_state self.total_rewards += reward return self.state, rewarddef run_episode(policy, n_steps, start_state=0, display=True): env = MDPEnvironment() if display: print("States(+rewards:)", end=" ") for step in range(n_steps): if display: if step == 10: print("...", end=" ") elif step &lt; 10: print(env.state, end=" ") action = policy(env.state) state, reward = env.step(action) if display and step &lt; 10: if reward: print("(&#123;&#125;)".format(reward), end=" ") if display: print("Total rewards=", env.total_rewards) return env.total_rewardsfor policy in (policy_fire, policy_random, policy_safe): all_totals = [] print(policy.__name__) for episode in range(1000): all_totals.append(run_episode(policy, n_steps=100, display=(episode&lt;5))) print("Summary:mean=&#123;:.1f&#125;, std=&#123;:.1f&#125;, min=&#123;&#125;, max&#123;&#125;".format( np.mean(all_totals), np.std(all_totals), np.min(all_totals), np.max(all_totals) )) print() policy_fire States(+rewards:) 0 (10) 0 (10) 0 1 (-50) 2 2 2 (40) 0 (10) 0 (10) 0 (10) ... Total rewards= 210 States(+rewards:) 0 1 (-50) 2 (40) 0 (10) 0 (10) 0 1 (-50) 2 2 (40) 0 (10) ... Total rewards= 70 States(+rewards:) 0 (10) 0 1 (-50) 2 (40) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) ... Total rewards= 70 States(+rewards:) 0 1 (-50) 2 1 (-50) 2 (40) 0 (10) 0 1 (-50) 2 (40) 0 ... Total rewards= -10 States(+rewards:) 0 1 (-50) 2 (40) 0 (10) 0 (10) 0 1 (-50) 2 (40) 0 (10) 0 (10) ... Total rewards= 290 Summary:mean=121.1, std=129.3, min=-330, max470 policy_random States(+rewards:) 0 1 (-50) 2 1 (-50) 2 (40) 0 1 (-50) 2 2 (40) 0 ... Total rewards= -60 States(+rewards:) 0 (10) 0 0 0 0 0 (10) 0 0 0 (10) 0 ... Total rewards= -30 States(+rewards:) 0 1 1 (-50) 2 (40) 0 0 1 1 1 1 ... Total rewards= 10 States(+rewards:) 0 (10) 0 (10) 0 0 0 0 1 (-50) 2 (40) 0 0 ... Total rewards= 0 States(+rewards:) 0 0 (10) 0 1 (-50) 2 (40) 0 0 0 0 (10) 0 (10) ... Total rewards= 40 Summary:mean=-22.1, std=88.2, min=-380, max200 policy_safe States(+rewards:) 0 1 1 1 1 1 1 1 1 1 ... Total rewards= 0 States(+rewards:) 0 1 1 1 1 1 1 1 1 1 ... Total rewards= 0 States(+rewards:) 0 (10) 0 (10) 0 (10) 0 1 1 1 1 1 1 ... Total rewards= 30 States(+rewards:) 0 (10) 0 1 1 1 1 1 1 1 1 ... Total rewards= 10 States(+rewards:) 0 1 1 1 1 1 1 1 1 1 ... Total rewards= 0 Summary:mean=22.3, std=26.2, min=0, max170 Q-Learning1234567891011121314151617n_states = 3n_actions = 3n_steps = 20000alpha = 0.01gamma = 0.99exploration_policy = policy_randomq_values = np.full((n_states, n_actions), -np.inf)for state, actions in enumerate(possible_actions): q_values[state][actions]=0env = MDPEnvironment()for step in range(n_steps): action = exploration_policy(env.state) state = env.state next_state, reward = env.step(action) next_value = np.max(q_values[next_state]) q_values[state, action] = (1-alpha)*q_values[state, action] + alpha*(reward + gamma * next_value) 12def optimal_policy(state): return np.argmax(q_values[state]) 1q_values array([[39.13508139, 38.88079412, 35.23025716], [18.9117071 , -inf, 20.54567816], [ -inf, 72.53192111, -inf]]) 1234567all_totals = []for episode in range(1000): all_totals.append(run_episode(optimal_policy, n_steps=100, display=(episode&lt;5)))print("Summary:mean=&#123;:.1f&#125;, std=&#123;:.1f&#125;, min=&#123;&#125;, max&#123;&#125;".format( np.mean(all_totals), np.std(all_totals), np.min(all_totals), np.max(all_totals) ))print() States(+rewards:) 0 (10) 0 (10) 0 1 (-50) 2 (40) 0 (10) 0 1 (-50) 2 (40) 0 (10) ... Total rewards= 230 States(+rewards:) 0 (10) 0 (10) 0 (10) 0 1 (-50) 2 2 1 (-50) 2 (40) 0 (10) ... Total rewards= 90 States(+rewards:) 0 1 (-50) 2 (40) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) ... Total rewards= 170 States(+rewards:) 0 1 (-50) 2 (40) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) 0 (10) ... Total rewards= 220 States(+rewards:) 0 1 (-50) 2 (40) 0 (10) 0 1 (-50) 2 (40) 0 (10) 0 (10) 0 (10) ... Total rewards= -50 Summary:mean=125.6, std=127.4, min=-290, max500 使用DQN算法创建MsPacman123env = gym.make("MsPacman-v0")obs = env.reset()obs.shape (210, 160, 3) 1env.action_space Discrete(9) 预处理12345678910mspacman_color = 210 + 164 + 74def preprocess_observation(obs): img = obs[1:176:2, ::2] img = img.sum(axis=2) img[img==mspacman_color] = 0 img = (img // 3 - 128).astype(np.int8) return img.reshape(88, 80, 1)img = preprocess_observation(obs) 1234567891011plt.figure(figsize=(11, 7))plt.subplot(121)plt.title("Original observation (160*210 RGB)")plt.imshow(obs)plt.axis("off")plt.subplot(122)plt.title("Preprocessed observation (88*80 greyscale)")plt.imshow(img.reshape(88, 80), interpolation="nearest", cmap="gray")plt.axis("off")plt.show() &lt;IPython.core.display.Javascript object&gt; 创建DQN12345678910111213141516171819202122232425262728293031323334reset_graph()input_height = 88input_width = 80input_channels = 1conv_n_maps = [32, 64, 64]conv_kernel_sizes = [(8, 8), (4, 4), (3, 3)]conv_strides = [4, 2, 1]conv_paddings = ["SAME"] * 3conv_activation = [tf.nn.relu] * 3n_hidden_in = 64 * 11 * 10n_hidden = 512hidden_activation = tf.nn.relun_outputs = env.action_space.ninitializer = tf.variance_scaling_initializer()def q_network(X_state, name): prev_layer = X_state / 128.0 with tf.variable_scope(name) as scope: for n_maps, kernel_size, strides, padding, activation in zip( conv_n_maps, conv_kernel_sizes, conv_strides, conv_paddings, conv_activation ): prev_layer = tf.layers.conv2d(prev_layer, filters=n_maps, kernel_size=kernel_size, strides=strides, padding=padding, activation=activation, kernel_initializer=initializer) last_conv_layers_flat = tf.reshape(prev_layer, shape=[-1, n_hidden_in]) hidden = tf.layers.dense(last_conv_layers_flat, n_hidden, activation=hidden_activation, kernel_initializer=initializer) outputs = tf.layers.dense(hidden, n_outputs, kernel_initializer=initializer) trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name) trainable_vars_by_name = &#123;var.name[len(scope.name):]: var for var in trainable_vars&#125; return outputs, trainable_vars_by_name 1234567X_state = tf.placeholder(tf.float32, shape=[None, input_height, input_width, input_channels])online_q_values, online_vars = q_network(X_state, name="q_networks/online")target_q_values, target_vars = q_network(X_state, name="q_networks/target")copy_ops = [target_var.assign(online_vars[var_name]) for var_name, target_var in target_vars.items()]copy_online_to_target = tf.group(*copy_ops) 1online_vars {&#39;/conv2d/bias:0&#39;: &lt;tf.Variable &#39;q_networks/online/conv2d/bias:0&#39; shape=(32,) dtype=float32_ref&gt;, &#39;/conv2d/kernel:0&#39;: &lt;tf.Variable &#39;q_networks/online/conv2d/kernel:0&#39; shape=(8, 8, 1, 32) dtype=float32_ref&gt;, &#39;/conv2d_1/bias:0&#39;: &lt;tf.Variable &#39;q_networks/online/conv2d_1/bias:0&#39; shape=(64,) dtype=float32_ref&gt;, &#39;/conv2d_1/kernel:0&#39;: &lt;tf.Variable &#39;q_networks/online/conv2d_1/kernel:0&#39; shape=(4, 4, 32, 64) dtype=float32_ref&gt;, &#39;/conv2d_2/bias:0&#39;: &lt;tf.Variable &#39;q_networks/online/conv2d_2/bias:0&#39; shape=(64,) dtype=float32_ref&gt;, &#39;/conv2d_2/kernel:0&#39;: &lt;tf.Variable &#39;q_networks/online/conv2d_2/kernel:0&#39; shape=(3, 3, 64, 64) dtype=float32_ref&gt;, &#39;/dense/bias:0&#39;: &lt;tf.Variable &#39;q_networks/online/dense/bias:0&#39; shape=(512,) dtype=float32_ref&gt;, &#39;/dense/kernel:0&#39;: &lt;tf.Variable &#39;q_networks/online/dense/kernel:0&#39; shape=(7040, 512) dtype=float32_ref&gt;, &#39;/dense_1/bias:0&#39;: &lt;tf.Variable &#39;q_networks/online/dense_1/bias:0&#39; shape=(9,) dtype=float32_ref&gt;, &#39;/dense_1/kernel:0&#39;: &lt;tf.Variable &#39;q_networks/online/dense_1/kernel:0&#39; shape=(512, 9) dtype=float32_ref&gt;} 123456789101112131415161718learning_rate = 0.001momentum = 0.95with tf.variable_scope("train"): X_action = tf.placeholder(tf.int32, shape=[None]) y = tf.placeholder(tf.float32, shape=[None, 1]) q_value = tf.reduce_sum(online_q_values * tf.one_hot(X_action, n_outputs), axis=1, keepdims=True) error = tf.abs(y -q_value) clipped_error = tf.clip_by_value(error, 0.0, 1.0) linear_error = 2 * (error - clipped_error) loss = tf.reduce_mean(tf.square(clipped_error) + learning_rate) global_step = tf.Variable(0, trainable=False, name="global_step") optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True) training_op = optimizer.minimize(loss, global_step=global_step)init = tf.global_variables_initializer()saver = tf.train.Saver() 123456789101112131415161718class ReplayMemory: def __init__(self, maxlen): self.maxlen = maxlen self.buf = np.empty(shape=maxlen, dtype=np.object) self.index = 0 self.length = 0 def append(self, data): self.buf[self.index] = data self.length = min(self.length + 1, self.maxlen) self.index = (self.index + 1) % self.maxlen def sample(self, batch_size, with_replacement=True): if with_replacement: indices = np.random.randint(self.length, size=batch_size) else: indices = np.random.randint(self.length)[:batch_size] return self.buf[indices] 12replay_memory_size = 500000replay_memory = ReplayMemory(replay_memory_size) 1234567def sample_memories(batch_size): cols = [[], [], [], [], []] for memory in replay_memory.sample(batch_size): for col, value in zip(cols, memory): col.append(value) cols = [np.array(col) for col in cols] return cols[0], cols[1], cols[2].reshape(-1, 1), cols[3], cols[4].reshape(-1, 1) 12345678910eps_min = 0.1eps_max = 1.0eps_decay_steps = 2000000def epsilon_greedy(q_values, step): epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps) if np.random.rand() &lt; epsilon: return np.random.randint(n_outputs) else: return np.argmax(q_values) 1234567891011n_steps = 4000000training_start = 10000training_interval = 4save_steps = 1000copy_steps = 10000discount_rate = 0.99skip_start = 90batch_size = 50iteration = 0checkpoint_path = "reinforce/dqn/my_dqn.ckpt"done = True 1234loss_val = np.inftygame_length = 0total_max_q = 0mean_max_q = 0.0 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354with tf.Session() as sess: if os.path.isfile(checkpoint_path + ".index"): saver.restore(sess, checkpoint_path) else: init.run() copy_online_to_target.run() while True: step = global_step.eval() if step &gt;= n_steps: brea; iteration += 1 print("\rIteration &#123;&#125;\tTraining step &#123;&#125;/&#123;&#125; (&#123;:.1f&#125;)%\tLoss &#123;:5f&#125;\tMean Max-Q &#123;:5f&#125; ".format( iteration, step, n_steps, step * 100 / n_steps, loss_val, mean_max_q ), end="") if done: obs = env.reset() for skip in range(skip_start): obs, reward, done, info = env.step(0) state = preprocess_observation(obs) q_values = online_q_values.eval(feed_dict=&#123;X_state: [state]&#125;) action = epsilon_greedy(q_values, step) obs, reward, done, info = env.step(action) next_state = preprocess_observation(obs) replay_memory.append((state, action, reward, next_state, 1.0 - done)) state = next_state total_max_q += q_values.max() game_length += 1 if done: mean_max_q = total_max_q / game_length total_max_q = 0.0 game_length = 0 if iteration &lt; training_start or iteration % training_interval != 0: continue X_state_val, X_action_val, rewards, X_next_state_val, continues = (sample_memories(batch_size)) next_q_values = target_q_values.eval(feed_dict=&#123;X_state: X_next_state_val&#125;) max_next_q_values = np.max(next_q_values, axis=1, keepdims=True) y_val = rewards + continues * discount_rate * max_next_q_values _, loss_val = sess.run([training_op, loss], feed_dict=&#123;X_state: X_state_val, X_action: X_action_val, y: y_val&#125;) if step % copy_steps == 0: copy_online_to_target.run() if step % save_steps == 0: saver.save(sess, checkpoint_path) INFO:tensorflow:Restoring parameters from reinforce/dqn/my_dqn.ckpt Iteration 47872 Training step 8408/4000000 (0.2)% Loss 0.001121 Mean Max-Q 0.096694 12345678910111213141516171819frames = []n_max_steps = 10000with tf.Session() as sess: saver.restore(sess, checkpoint_path) obs = env.reset() for step in range(n_max_steps): state = preprocess_observation(obs) q_values = online_q_values.eval(feed_dict=&#123;X_state: [state]&#125;) action = np.argmax(q_values) obs, reward, done, info = env.step(action) img = env.render(mode="rgb_array") frames.append(img) if done: break INFO:tensorflow:Restoring parameters from reinforce/dqn/my_dqn.ckpt 1plot_animation(frames) &lt;IPython.core.display.Javascript object&gt; &lt;matplotlib.animation.FuncAnimation at 0x1d45d850320&gt; 源文件]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>Hands-On Machine Learning with Scikit-Learn and TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Autoencoder]]></title>
    <url>%2F2019%2F01%2F03%2FAutoencoder%2F</url>
    <content type="text"><![CDATA[准备12345678910111213141516171819202122232425262728import osimport sysimport numpy as np%matplotlib inlineimport matplotlibimport matplotlib.pyplot as pltimport tensorflow as tfdef reset_graph(seed=42): tf.reset_default_graph() tf.set_random_seed(seed) np.random.seed(seed)def plot_image(image, shape=[28, 28]): plt.imshow(image.reshape(shape), cmap="Greys", interpolation="nearest") plt.axis("off")def plt_multiple_images(images, n_rows, n_cols, pad=2): images = images - images.min() w, h = images.shape[1:] image = np.zeros(((w+pad)*n_rows+pad, (h+pad)*n_cols+pad)) for y in range(n_rows): for x in range(n_cols): image[(y*(h+pad)+pad):(y*(h+pad)+pad+h), (x*(w+pad)+pad):(x*(w+pad)+pad+w)]=images[y*n_cols+x] plt.imshow(image, cmap="Greys", interpolation="nearest") plt.axis("off") 使用线性Autoencoder进行主成分分析(PCA)123456789101112import numpy.random as rndrnd.seed(4)m = 200w1, w2 = 0.1, 0.3noise = 0.1angles = rnd.rand(m) * 3 * np.pi / 2 - 0.5data = np.empty((m, 3))data[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * rnd.randn(m) / 2data[:, 1] = np.sin(angles) * 0.7 + noise * rnd.randn(m) / 2data[:, 2] = data[:, 0] * w1 + data[:, 1] * w2 + noise * rnd.randn(m) 1234from sklearn.preprocessing import StandardScalerscaler = StandardScaler()X_train = scaler.fit_transform(data[:100])X_test = scaler.transform(data[100:]) 1234567891011121314151617181920import tensorflow as tfreset_graph()n_inputs = 3n_hidden = 2n_outputs = n_inputslearning_rate = 0.01X = tf.placeholder(tf.float32, shape=[None, n_inputs])hidden = tf.layers.dense(X, n_hidden)outputs = tf.layers.dense(hidden, n_outputs)reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))optimizer = tf.train.AdamOptimizer(learning_rate)training_op = optimizer.minimize(reconstruction_loss)init = tf.global_variables_initializer() 12345678n_iterations = 1000codings = hiddenwith tf.Session() as sess: init.run() for iteration in range(n_iterations): training_op.run(feed_dict=&#123;X: X_train&#125;) coding_val = codings.eval(feed_dict=&#123;X: X_test&#125;) 12345fig = plt.figure(figsize=(4,3))plt.plot(coding_val[:, 0], coding_val[:, 1], "b.")plt.xlabel("$z_1$", fontsize=18)plt.ylabel("$z_2$", fontsize=18, rotation=0)plt.show() Stacked Autoencoders12from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets("tmp/data/") WARNING:tensorflow:From &lt;ipython-input-7-39eecf39f555&gt;:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version. Instructions for updating: Please use alternatives such as official/mnist/dataset.py from tensorflow/models. WARNING:tensorflow:From e:\python\python36\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version. Instructions for updating: Please write your own downloading logic. WARNING:tensorflow:From e:\python\python36\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version. Instructions for updating: Please use tf.data to implement this functionality. Extracting tmp/data/train-images-idx3-ubyte.gz WARNING:tensorflow:From e:\python\python36\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version. Instructions for updating: Please use tf.data to implement this functionality. Extracting tmp/data/train-labels-idx1-ubyte.gz Extracting tmp/data/t10k-images-idx3-ubyte.gz Extracting tmp/data/t10k-labels-idx1-ubyte.gz WARNING:tensorflow:From e:\python\python36\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version. Instructions for updating: Please use alternatives such as official/mnist/dataset.py from tensorflow/models. 一次性训练所有层1234567891011121314151617181920212223242526272829303132333435363738reset_graph()from functools import partialn_inputs = 28 * 28n_hidden1 = 300n_hidden2 = 150n_hidden3 = n_hidden1n_outputs = n_inputslearning_rate = 0.01l2_reg = 0.0001X = tf.placeholder(tf.float32, shape=[None, n_inputs])he_init = tf.contrib.layers.variance_scaling_initializer()l2_regularizer = tf.contrib.layers.l2_regularizer(l2_reg)my_dense_layer = partial(tf.layers.dense, activation=tf.nn.elu, kernel_initializer=he_init, kernel_regularizer=l2_regularizer)hidden1 = my_dense_layer(X, n_hidden1)hidden2 = my_dense_layer(hidden1, n_hidden2)hidden3 = my_dense_layer(hidden2, n_hidden3)outputs = my_dense_layer(hidden3, n_outputs, activation=None)reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)loss = tf.add_n([reconstruction_loss] + reg_losses)optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)training_op = optimizer.minimize(loss)init = tf.global_variables_initializer()saver = tf.train.Saver() 12345678910111213141516n_epochs = 5batch_size = 150with tf.Session() as sess: init.run() for epoch in range(n_epochs): n_batches = mnist.train.num_examples // batch_size for iteration in range(n_batches): print("\r&#123;&#125;%".format(100 * iteration // n_batches), end="") sys.stdout.flush() X_batch, y_batch = mnist.train.next_batch(batch_size) sess.run(training_op, feed_dict=&#123;X: X_batch&#125;) loss_train = reconstruction_loss.eval(feed_dict=&#123;X: X_batch&#125;) sys.stdout.flush() print("\r&#123;&#125;".format(epoch), "Train MSE:", loss_train) saver.save(sess, "autoencoder/my_model_all_layers.ckpt") 0 Train MSE: 0.020302307 1 Train MSE: 0.011643166 2 Train MSE: 0.010225781 3 Train MSE: 0.009899946 4 Train MSE: 0.010377405 123456789101112def show_reconstruction_digits(X, outputs, model_path=None, n_test_digits=2): with tf.Session() as sess: if model_path: saver.restore(sess, model_path) X_test = mnist.test.images[:n_test_digits] outputs_val = outputs.eval(feed_dict=&#123;X: X_test&#125;) fig = plt.figure(figsize=(8, 3 * n_test_digits)) for digit_index in range(n_test_digits): plt.subplot(n_test_digits, 2, digit_index * 2 + 1) plot_image(X_test[digit_index]) plt.subplot(n_test_digits, 2, digit_index * 2 + 2) plot_image(outputs_val[digit_index]) 1show_reconstruction_digits(X, outputs, "autoencoder/my_model_all_layers.ckpt") INFO:tensorflow:Restoring parameters from autoencoder/my_model_all_layers.ckpt 调整权重12345678910reset_graph()n_inputs = 28 * 28n_hidden1 = 300n_hidden2 = 150n_hidden3 = n_hidden1n_outputs = n_inputslearning_rate = 0.01l2_reg = 0.0005 1234567891011121314151617181920212223242526272829303132activation = tf.nn.eluregularizer = tf.contrib.layers.l2_regularizer(l2_reg)initializer = tf.contrib.layers.variance_scaling_initializer()X = tf.placeholder(tf.float32, shape=[None, n_inputs])weights1_init = initializer([n_inputs, n_hidden1])weights2_init = initializer([n_hidden1, n_hidden2])weights1 = tf.Variable(weights1_init, dtype=tf.float32, name="weights1")weights2 = tf.Variable(weights2_init, dtype=tf.float32, name="weights2")weights3 = tf.transpose(weights2, name="weights3")weights4 = tf.transpose(weights1, name="weights4")biases1 = tf.Variable(tf.zeros(n_hidden1), name="biases1")biases2 = tf.Variable(tf.zeros(n_hidden2), name="biases2")biases3 = tf.Variable(tf.zeros(n_hidden3), name="biases3")biases4 = tf.Variable(tf.zeros(n_outputs), name="biases4")hidden1 = activation(tf.matmul(X, weights1) + biases1)hidden2 = activation(tf.matmul(hidden1, weights2) + biases2)hidden3 = activation(tf.matmul(hidden2, weights3) + biases3)outputs = tf.matmul(hidden3, weights4) + biases4reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))reg_loss = regularizer(weights1) + regularizer(weights2)loss = reconstruction_loss + reg_lossoptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)training_op = optimizer.minimize(loss)init = tf.global_variables_initializer() 1saver = tf.train.Saver() 12345678910111213141516n_epochs = 5batch_size = 150with tf.Session() as sess: init.run() for epoch in range(n_epochs): n_batches = mnist.train.num_examples // batch_size for iteration in range(n_batches): print("\r&#123;&#125;%".format(100 * iteration // n_batches), end="") sys.stdout.flush() X_batch, y_batch = mnist.train.next_batch(batch_size) sess.run(training_op, feed_dict=&#123;X: X_batch&#125;) loss_train = reconstruction_loss.eval(feed_dict=&#123;X: X_batch&#125;) sys.stdout.flush() print("\r&#123;&#125;".format(epoch), "Train MSE:", loss_train) saver.save(sess, "autoencoder/my_model_tying_weights.ckpt") 0 Train MSE: 0.015066842 1 Train MSE: 0.016488561 2 Train MSE: 0.017375803 3 Train MSE: 0.016878227 4 Train MSE: 0.015587644 1show_reconstruction_digits(X, outputs, "autoencoder/my_model_tying_weights.ckpt") INFO:tensorflow:Restoring parameters from autoencoder/my_model_tying_weights.ckpt Training one Autoencoder at a time in multiple graphs1reset_graph() 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849from functools import partialdef train_autoencoder(X_train, n_neurons, n_epochs, batch_size, learning_rate=0.01, l2_reg=0.0005, seed=42, hidden_activation=tf.nn.elu, output_activation=tf.nn.elu): graph = tf.Graph() with graph.as_default(): tf.set_random_seed(seed) n_inputs = X_train.shape[1] X = tf.placeholder(tf.float32, shape=[None, n_inputs]) my_dense_layer = partial( tf.layers.dense, kernel_initializer=tf.contrib.layers.variance_scaling_initializer(), kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_reg) ) hidden = my_dense_layer(X, n_neurons, activation=hidden_activation, name="hidden") outputs = my_dense_layer(hidden, n_inputs, activation=output_activation, name="outputs") reconstruction_loss = tf.reduce_mean(tf.square(outputs - X)) reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES) loss = tf.add_n([reconstruction_loss] + reg_losses) optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) training_op = optimizer.minimize(loss) init = tf.global_variables_initializer() with tf.Session() as sess: init.run() for epoch in range(n_epochs): n_batches = len(X_train) // batch_size for iteration in range(n_batches): print("\r&#123;&#125;%".format(100 * iteration // n_batches), end="") sys.stdout.flush() indices = rnd.permutation(len(X_train))[:batch_size] X_batch = X_train[indices] sess.run(training_op, feed_dict=&#123;X: X_batch&#125;) loss_train = reconstruction_loss.eval(feed_dict=&#123;X: X_batch&#125;) sys.stdout.flush() print("\r&#123;&#125;".format(epoch), "Train MSE:", loss_train) params = dict([(var.name, var.eval()) for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)]) hidden_val = hidden.eval(feed_dict=&#123;X: X_train&#125;) return hidden_val, params["hidden/kernel:0"],params["hidden/bias:0"], params["outputs/kernel:0"], params["outputs/bias:0"] 12345hidden_output, W1, b1, W4, b4 = train_autoencoder(mnist.train.images, n_neurons=300, n_epochs=4, batch_size=150, output_activation=None)_, W2,b2, W3, b3 = train_autoencoder(hidden_output, n_neurons=150, n_epochs=4, batch_size=150) 0 Train MSE: 0.018517738 1 Train MSE: 0.0186826 2 Train MSE: 0.018467626 3 %Train MSE: 0.019231623 0 Train MSE: 0.0042361086 1 Train MSE: 0.0048326803 2 Train MSE: 0.004668748 3 Train MSE: 0.0044039097 123456789reset_graph()n_inputs = 28 * 28X = tf.placeholder(tf.float32, shape=[None, n_inputs])hidden1 = tf.nn.elu(tf.matmul(X, W1) + b1)hidden2 = tf.nn.elu(tf.matmul(hidden1, W2) + b2)hidden3 = tf.nn.elu(tf.matmul(hidden2, W3) + b3)outputs = tf.matmul(hidden3, W4) + b4 1show_reconstruction_digits(X, outputs) Training one Autoencoder at a time in a single graph1234567891011121314151617181920212223242526272829303132333435363738reset_graph()n_inputs = 28 * 28n_hidden1 = 300n_hidden2 = 150n_hidden3 = n_hidden1n_outputs = n_inputslearning_rate = 0.01l2_reg = 0.0001activation = tf.nn.eluregularizer = tf.contrib.layers.l2_regularizer(l2_reg)initializer = tf.contrib.layers.variance_scaling_initializer()X = tf.placeholder(tf.float32, shape=[None, n_inputs])weights1_init = initializer([n_inputs, n_hidden1])weights2_init = initializer([n_hidden1, n_hidden2])weights3_init = initializer([n_hidden2, n_hidden3])weights4_init = initializer([n_hidden3, n_outputs])weights1 = tf.Variable(weights1_init, dtype=tf.float32, name="weights1")weights2 = tf.Variable(weights2_init, dtype=tf.float32, name="weigths2")weights3 = tf.Variable(weights3_init, dtype=tf.float32, name="weights3")weights4 = tf.Variable(weights4_init, dtype=tf.float32, name="weights4")biases1 = tf.Variable(tf.zeros(n_hidden1), name="biases1")biases2 = tf.Variable(tf.zeros(n_hidden2), name="biases2")biases3 = tf.Variable(tf.zeros(n_hidden3), name="biases3")biases4 = tf.Variable(tf.zeros(n_outputs), name="biases4")hidden1 = activation(tf.matmul(X, weights1) + biases1)hidden2 = activation(tf.matmul(hidden1, weights2) + biases2)hidden3 = activation(tf.matmul(hidden2, weights3) + biases3)outputs = tf.matmul(hidden3, weights4) + biases4reconstruction_loss = tf.reduce_mean(tf.square(outputs - X)) 123456789101112131415optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)with tf.name_scope("phase1"): phasel_outputs = tf.matmul(hidden1, weights4) + biases4 phase1_reconstruction_loss = tf.reduce_mean(tf.square(phasel_outputs - X)) phase1_reg_loss = regularizer(weights1) + regularizer(weights4) phase1_loss = phase1_reconstruction_loss + phase1_reg_loss phase1_training_op = optimizer.minimize(phase1_loss)with tf.name_scope("phase2"): phase2_reconstruction_loss = tf.reduce_mean(tf.square(hidden3 - hidden1)) phase2_reg_loss = regularizer(weights2) + regularizer(weights3) phase2_loss = phase2_reconstruction_loss + phase2_reg_loss train_vars = [weights2, biases2, weights3, biases3] phase2_training_op = optimizer.minimize(phase2_loss, var_list=train_vars) 12init = tf.global_variables_initializer()saver = tf.train.Saver() 12345678910111213141516171819202122training_ops = [phase1_training_op, phase2_training_op]reconstruction_loesses = [phase1_reconstruction_loss, phase2_reconstruction_loss]n_epochs = [4, 4]batch_sizes = [150, 150]with tf.Session() as sess: init.run() for phase in range(2): print("Training phase #&#123;&#125;".format(phase + 1)) for epoch in range(n_epochs[phase]): n_batches = mnist.train.num_examples // batch_sizes[phase] for iteration in range(n_batches): print("\r&#123;&#125;%".format(100 * iteration // n_batches), end="") sys.stdout.flush() X_batch, y_batch = mnist.train.next_batch(batch_sizes[phase]) sess.run(training_ops[phase],feed_dict=&#123;X: X_batch&#125;) loss_train = reconstruction_loesses[phase].eval(feed_dict=&#123;X: X_batch&#125;) sys.stdout.flush() print("\r&#123;&#125;".format(epoch), "Training MSE:", loss_train) saver.save(sess, "autoencoder/my_model_one_at_a_time.ckpt") loss_test = reconstruction_loss.eval(feed_dict=&#123;X: mnist.test.images&#125;) print("Test MSE:", loss_test) Training phase #1 0 Training MSE: 0.0074068382 1 Training MSE: 0.0078287525 2 Training MSE: 0.007728059 3 Training MSE: 0.0074089756 Training phase #2 0 Training MSE: 0.3257823 1 Training MSE: 0.00573954 2 Training MSE: 0.0029418417 39% Training MSE: 0.0024437662 Test MSE: 0.009793411 缓存冻结的层输出12345678910111213141516171819202122232425262728293031training_ops = [phase1_training_op, phase2_training_op]reconstruction_loesses = [phase1_reconstruction_loss, phase2_reconstruction_loss]n_epochs = [4, 4]batch_sizes = [150, 150]with tf.Session() as sess: init.run() for phase in range(2): print("Training phase #&#123;&#125;".format(phase + 1)) if phase == 1: hidden1_cache = hidden1.eval(feed_dict=&#123;X: mnist.train.images&#125;) for epoch in range(n_epochs[phase]): n_batches = mnist.train.num_examples // batch_sizes[phase] for iteration in range(n_batches): print("\r&#123;&#125;%".format(100 * iteration // n_batches), end="") sys.stdout.flush() if phase == 1: indices = rnd.permutation(mnist.train.num_examples) hidden1_batch = hidden1_cache[indices[:batch_sizes[phase]]] feed_dict = &#123;hidden1: hidden1_batch&#125; sess.run(training_ops[phase], feed_dict=feed_dict) else: X_batch, y_batch = mnist.train.next_batch(batch_sizes[phase]) feed_dict = &#123;X: X_batch&#125; sess.run(training_ops[phase], feed_dict=feed_dict) loss_train = reconstruction_loesses[phase].eval(feed_dict=feed_dict) sys.stdout.flush() print("\r&#123;&#125;".format(epoch), "Train MSE:", loss_train) saver.save(sess, "autoencoder/my_mode_cache_frozen.ckpt") loss_test = reconstruction_loss.eval(feed_dict=&#123;X: mnist.test.images&#125;) print("Test MSE:", loss_test) Training phase #1 0 Train MSE: 0.007538227 1 Train MSE: 0.007754632 2 Train MSE: 0.007343643 3 Train MSE: 0.007837738 Test MSE: 0.10728952 Training phase #2 0 Train MSE: 0.16884093 1 Train MSE: 0.0044883126 2 Train MSE: 0.0024808452 3 Train MSE: 0.0020300867 Test MSE: 0.009770555 可视化重构123456789101112n_test_digits = 2X_test = mnist.test.images[:n_test_digits]with tf.Session() as sess: saver.restore(sess, "autoencoder/my_model_one_at_a_time.ckpt") outputs_val = outputs.eval(feed_dict=&#123;X: X_test&#125;)for digit_index in range(n_test_digits): plt.subplot(n_test_digits, 2, digit_index * 2 + 1) plot_image(X_test[digit_index]) plt.subplot(n_test_digits, 2, digit_index * 2 + 2) plot_image(outputs_val[digit_index]) INFO:tensorflow:Restoring parameters from autoencoder/my_model_one_at_a_time.ckpt 可视化另外的特征1234567with tf.Session() as sess: saver.restore(sess, "autoencoder/my_model_one_at_a_time.ckpt") weights_val = weights1.eval()for i in range(5): plt.subplot(1, 5, i + 1) plot_image(weights_val.T[i]) INFO:tensorflow:Restoring parameters from autoencoder/my_model_one_at_a_time.ckpt 非监督的预训练123456789101112131415161718192021222324252627282930313233343536373839404142434445reset_graph()n_inputs = 28 * 28n_hidden1 = 300n_hidden2 = 150n_outputs = 10learning_rate = 0.01l2_reg = 0.0005activation = tf.nn.eluregularizer = tf.contrib.layers.l2_regularizer(l2_reg)initializer = tf.contrib.layers.variance_scaling_initializer()X = tf.placeholder(tf.float32, shape=[None, n_inputs])y = tf.placeholder(tf.int32, shape=[None])weights1_init = initializer([n_inputs, n_hidden1])weights2_init = initializer([n_hidden1, n_hidden2])weights3_init = initializer([n_hidden2, n_hidden3])weights1 = tf.Variable(weights1_init, dtype=tf.float32, name="weights1")weights2 = tf.Variable(weights2_init, dtype=tf.float32, name="weights2")weights3 = tf.Variable(weights3_init, dtype=tf.float32, name="weights3")biases1 = tf.Variable(tf.zeros(n_hidden1), name="biases1")biases2 = tf.Variable(tf.zeros(n_hidden2), name="biases2")biases3 = tf.Variable(tf.zeros(n_hidden3), name="biases3")hidden1 = activation(tf.matmul(X, weights1) + biases1)hidden2 = activation(tf.matmul(hidden1, weights2) + biases2)logits = tf.matmul(hidden2, weights3) + biases3cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)reg_loss = regularizer(weights1) + regularizer(weights2) + regularizer(weights3)loss = cross_entropy + reg_lossoptimizer = tf.train.AdamOptimizer(learning_rate)training_op = optimizer.minimize(loss)correct = tf.nn.in_top_k(logits, y, 1)accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))init = tf.global_variables_initializer()pretrain_saver = tf.train.Saver([weights1, weights2, biases1, biases2])saver = tf.train.Saver() 1234567891011121314151617181920n_epochs = 4batch_size = 150n_labeled_instance = 20000with tf.Session() as sess: init.run() for epoch in range(n_epochs): n_batches = n_labeled_instance // batch_size for iteration in range(n_batches): print("\r&#123;&#125;%".format(100 * iteration // n_batches), end="") sys.stdout.flush() indices = rnd.permutation(n_labeled_instance)[:batch_size] X_batch, y_batch = mnist.train.images[indices], mnist.train.labels[indices] sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;) accuracy_val = accuracy.eval(feed_dict=&#123;X: X_batch, y: y_batch&#125;) sys.stdout.flush() print("\r&#123;&#125;".format(epoch), "Traing accuracy:", accuracy_val, end="") saver.save(sess, "autoencoder/my_model_supervised.ckpt") accuracy_val = accuracy.eval(feed_dict=&#123;X: mnist.test.images, y: mnist.test.labels&#125;) print("Test accuracy:", accuracy_val) 0 Traing accuracy: 0.94Test accuracy: 0.9247 1 Traing accuracy: 0.97333336Test accuracy: 0.9328 2 Traing accuracy: 0.9866667Test accuracy: 0.9406 3 Traing accuracy: 0.98Test accuracy: 0.9423 1234567891011121314151617181920n_epochs = 4batch_size = 150n_labeled_instance = 2000with tf.Session() as sess: init.run() pretrain_saver.restore(sess, "autoencoder/my_model_supervised.ckpt") for epoch in range(n_epochs): n_batches = n_labeled_instance // batch_size for iteration in range(n_batches): print("\r&#123;&#125;%".format(100 * iteration // n_batches), end="") sys.stdout.flush() indices = rnd.permutation(n_labeled_instance)[:batch_size] X_batch, y_batch = mnist.train.images[indices], mnist.train.labels[indices] sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;) accuracy_val = accuracy.eval(feed_dict=&#123;X: X_batch, y: y_batch&#125;) sys.stdout.flush() print("\r&#123;&#125;".format(epoch), "Traing accuracy:", accuracy_val, end="") saver.save(sess, "autoencoder/my_model_supervised.ckpt") accuracy_val = accuracy.eval(feed_dict=&#123;X: mnist.test.images, y: mnist.test.labels&#125;) print("Test accuracy:", accuracy_val) INFO:tensorflow:Restoring parameters from autoencoder/my_model_supervised.ckpt 0 Traing accuracy: 0.9533333Test accuracy: 0.8857 1 Traing accuracy: 0.9866667Test accuracy: 0.929 2 Traing accuracy: 1.0Test accuracy: 0.936 3 Traing accuracy: 1.0Test accuracy: 0.9339 Denoising Autoencoders123456789reset_graph()n_inputs = 28 * 28n_hidden1 = 300n_hidden2 = 150n_hidden3 = n_hidden1n_outputs = n_inputslearning_rate = 0.01 1234567891011121314noise_level = 1.0X = tf.placeholder(tf.float32, shape=[None, n_inputs])X_noisy = X + noise_level * tf.random_normal(tf.shape(X))hidden1 = tf.layers.dense(X_noisy, n_hidden1, activation=tf.nn.elu, name="hidden1")hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.elu, name="hidden2")hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.elu, name="hidden3")outputs = tf.layers.dense(hidden3, n_outputs, name="outputs")reconstruction_loss = tf.reduce_mean(tf.square(outputs - X)) 12345optimizer = tf.train.AdamOptimizer(learning_rate)training_op = optimizer.minimize(reconstruction_loss)init = tf.global_variables_initializer()saver = tf.train.Saver() 12345678910111213141516n_epochs = 10batch_size = 150with tf.Session() as sess: init.run() for epoch in range(n_epochs): n_batches = mnist.train.num_examples // batch_size for iteration in range(n_batches): print("\r&#123;&#125;%".format(100 * iteration // n_batches), end="") sys.stdout.flush() X_batch, y_batch = mnist.train.next_batch(batch_size) sess.run(training_op, feed_dict=&#123;X: X_batch&#125;) loss_train = reconstruction_loss.eval(feed_dict=&#123;X: X_batch&#125;) sys.stdout.flush() print("\r&#123;&#125;".format(epoch), "Train MSE:", loss_train) saver.save(sess, "autoencoder/my_model_stacked_denosing_gaussian.ckpt") 0 Train MSE: 0.044155695 1 Train MSE: 0.041478604 2 Train MSE: 0.0402305 3 Train MSE: 0.03867544 4 Train MSE: 0.036705464 5 Train MSE: 0.035445902 6 Train MSE: 0.03501453 7 Train MSE: 0.03678643 8 Train MSE: 0.2745152 9 Train MSE: 0.09069635 使用dropout123456789reset_graph()n_inputs = 28 * 28n_hidden1 = 300n_hidden2 = 150n_hidden3 = n_hidden1n_outputs = n_inputslearning_rate = 0.01 12345678910111213141516dropout_rate = 0.3training = tf.placeholder_with_default(False, shape=(), name="training")X = tf.placeholder(tf.float32, [None, n_inputs])X_drop = tf.layers.dropout(X, dropout_rate, training=training)hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu, name="hidden1")hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name="hidden2")hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name="hidden3")outputs = tf.layers.dense(hidden3, n_outputs, name="outputs")reconstruction_loss = tf.reduce_mean(tf.square(outputs - X)) 12345optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)training_op = optimizer.minimize(reconstruction_loss)init = tf.global_variables_initializer()saver = tf.train.Saver() 12345678910111213141516n_epochs = 10batch_size = 150with tf.Session() as sess: init.run() for epoch in range(n_epochs): n_batches = mnist.train.num_examples // batch_size for iteration in range(n_batches): print("\r&#123;&#125;%".format(100 * iteration // n_batches), end="") sys.stdout.flush() X_batch, y_batch = mnist.train.next_batch(batch_size) sess.run(training_op, feed_dict=&#123;X: X_batch, training:True&#125;) loss_train = reconstruction_loss.eval(feed_dict=&#123;X: X_batch&#125;) sys.stdout.flush() print("\r&#123;&#125;".format(epoch), "Train MSE:", loss_train) saver.save(sess, "autoencoder/my_model_stacked_denoising_dropout.ckpt") 0 Train MSE: 0.032365285 1 Train MSE: 0.028017173 2 Train MSE: 0.027088877 3 Train MSE: 0.027295204 4 Train MSE: 0.024870027 5 Train MSE: 0.026465423 6 Train MSE: 0.025175484 7 Train MSE: 0.026861466 8 Train MSE: 0.023711765 9 Train MSE: 0.026963389 1show_reconstruction_digits(X, outputs, "autoencoder/my_model_stacked_denoising_dropout.ckpt") INFO:tensorflow:Restoring parameters from autoencoder/my_model_stacked_denoising_dropout.ckpt Sparse Autoencoder12345678910111213p = 0.1q = np.linspace(0.001, 0.999, 500)kl_div = p * np.log(p / q) + (1 - p) * np.log((1-p) / (1 - q))mse = (p - q) ** 2plt.plot([p, p], [0, 0.3], "k:")plt.text(0.05, 0.32, "Target\nsparsity", fontsize=14)plt.plot(q, kl_div, "b-", label="KL divergence")plt.plot(q, mse, "r--", label="MSE")plt.legend(loc="upper left")plt.xlabel("Actual sparsity")plt.ylabel("Cost", rotation=0)plt.axis([0, 1, 0, 0.95])plt.show() 12345reset_graph()n_inputs = 28 * 28n_hidden1 = 1000n_outputs = n_inputs 12345678910111213141516171819def kl_divergence(p, q): return p * tf.log(p / q) + (1 - p) * tf.log((1 - p) / (1 - q))learning_rate = 0.01sparsity_target = 0.1sparsity_weight = 0.2X = tf.placeholder(tf.float32, shape=[None, n_inputs])hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.sigmoid)outputs = tf.layers.dense(hidden1, n_outputs)hidden1_mean = tf.reduce_mean(hidden1, axis=0)sparsity_loss = tf.reduce_sum(kl_divergence(sparsity_target, hidden1_mean))reconstruction_loss = tf.reduce_mean(tf.square(outputs - X))loss = reconstruction_loss + sparsity_weight * sparsity_lossoptimizer = tf.train.AdamOptimizer(learning_rate)training_op = optimizer.minimize(loss) 12init = tf.global_variables_initializer()saver = tf.train.Saver() 123456789101112131415161718192021n_epochs = 30batch_size = 1000with tf.Session() as sess: init.run() for epoch in range(n_epochs): n_batches = mnist.train.num_examples // batch_size for iteration in range(n_batches): print("\r&#123;&#125;%".format(100 * iteration // n_batches), end="") sys.stdout.flush() X_batch, y_batch = mnist.train.next_batch(batch_size) sess.run(training_op, feed_dict=&#123;X: X_batch&#125;) reconstruction_loss_val, sparsity_loss_val, loss_val = sess.run( [reconstruction_loss, sparsity_loss, loss], feed_dict=&#123;X: X_batch&#125; ) sys.stdout.flush() print("\r&#123;&#125;".format(epoch), "Train MSE:", reconstruction_loss_val, "\t Sparsity loss:", sparsity_loss_val, "\tTotal loss:", loss_val) saver.save(sess, "autoencoder/my_model_sparse.ckpt") 0 Train MSE: 0.13597836 Sparsity loss: 0.34833914 Total loss: 0.20564619 1 Train MSE: 0.05859924 Sparsity loss: 0.010568377 Total loss: 0.060712915 2 Train MSE: 0.05301216 Sparsity loss: 0.0274893 Total loss: 0.05851002 3 Train MSE: 0.048152912 Sparsity loss: 0.02638663 Total loss: 0.053430237 4 Train MSE: 0.04519025 Sparsity loss: 0.028350491 Total loss: 0.050860345 5 Train MSE: 0.043140396 Sparsity loss: 0.015900817 Total loss: 0.04632056 6 Train MSE: 0.03955833 Sparsity loss: 0.06444825 Total loss: 0.05244798 7 Train MSE: 0.035567418 Sparsity loss: 0.03933688 Total loss: 0.043434795 8 Train MSE: 0.032602772 Sparsity loss: 0.049595118 Total loss: 0.042521797 9 Train MSE: 0.029823603 Sparsity loss: 0.08126265 Total loss: 0.046076134 10 Train MSE: 0.027128985 Sparsity loss: 0.010442203 Total loss: 0.029217426 11 Train MSE: 0.025444714 Sparsity loss: 0.16434407 Total loss: 0.058313526 12 Train MSE: 0.02411371 Sparsity loss: 0.27343276 Total loss: 0.07880026 13 Train MSE: 0.02250252 Sparsity loss: 0.6099131 Total loss: 0.14448515 14 Train MSE: 0.02092416 Sparsity loss: 0.070890054 Total loss: 0.03510217 15 Train MSE: 0.020057669 Sparsity loss: 0.08326996 Total loss: 0.036711663 16 Train MSE: 0.019493314 Sparsity loss: 0.09606047 Total loss: 0.03870541 17 Train MSE: 0.018581435 Sparsity loss: 0.055874564 Total loss: 0.029756349 18 Train MSE: 0.017862516 Sparsity loss: 0.18050222 Total loss: 0.05396296 19 Train MSE: 0.017116258 Sparsity loss: 0.2812312 Total loss: 0.0733625 20 Train MSE: 0.015459475 Sparsity loss: 0.10458441 Total loss: 0.036376357 21 Train MSE: 0.015748186 Sparsity loss: 0.18440554 Total loss: 0.052629292 22 Train MSE: 0.016706262 Sparsity loss: 0.23294647 Total loss: 0.06329556 23 Train MSE: 0.01546143 Sparsity loss: 0.08982499 Total loss: 0.033426426 24 Train MSE: 0.015510442 Sparsity loss: 0.050992575 Total loss: 0.025708957 25 Train MSE: 0.01604331 Sparsity loss: 0.3630172 Total loss: 0.088646755 26 Train MSE: 0.014945716 Sparsity loss: 0.13651599 Total loss: 0.042248912 27 Train MSE: 0.014371737 Sparsity loss: 0.08159548 Total loss: 0.030690834 28 Train MSE: 0.013957049 Sparsity loss: 0.042383395 Total loss: 0.022433728 29 Train MSE: 0.014041565 Sparsity loss: 0.11076699 Total loss: 0.036194965 1show_reconstruction_digits(X, outputs, "autoencoder/my_model_sparse.ckpt") INFO:tensorflow:Restoring parameters from autoencoder/my_model_sparse.ckpt 1hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.sigmoid) 12345logits = tf.layers.dense(hidden1, n_outputs)outputs = tf.nn.sigmoid(logits)xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=X, logits=logits)reconstruction_loss = tf.reduce_mean(xentropy) Variational Autoencoder12345678910111213141516171819202122232425262728293031323334353637reset_graph()from functools import partialn_inputs = 28 * 28n_hidden1 = 500n_hidden2 = 500n_hidden3 = 20n_hidden4 = n_hidden2n_hidden5 = n_hidden1n_outputs = n_inputslearning_rate = 0.001initializer = tf.contrib.layers.variance_scaling_initializer()my_dense_layer = partial( tf.layers.dense, activation=tf.nn.elu, kernel_initializer=initializer)X = tf.placeholder(tf.float32, [None, n_inputs])hidden1 = my_dense_layer(X, n_hidden1)hidden2 = my_dense_layer(hidden1, n_hidden2)hidden3_mean = my_dense_layer(hidden2, n_hidden3, activation=None)hidden3_sigma = my_dense_layer(hidden2, n_hidden3, activation=None)noise = tf.random_normal(tf.shape(hidden3_sigma), dtype=tf.float32)hidden3 = hidden3_mean + hidden3_sigma * noisehidden4 = my_dense_layer(hidden3, n_hidden4)hidden5 = my_dense_layer(hidden4, n_hidden5)logits = my_dense_layer(hidden5, n_outputs, activation=None)outputs = tf.sigmoid(logits)xentropy =tf.nn.sigmoid_cross_entropy_with_logits(labels=X, logits=logits)reconstruction_loss = tf.reduce_sum(xentropy) 123456eps = 1e-10latent_loss = 0.5 * tf.reduce_sum( tf.square(hidden3_sigma) + tf.square(hidden3_mean) - 1 - tf.log(eps + tf.square(hidden3_sigma))) 1234567loss = reconstruction_loss + latent_lossoptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)training_op = optimizer.minimize(loss)init = tf.global_variables_initializer()saver = tf.train.Saver() 123456789101112131415161718192021n_epochs = 30batch_size = 150with tf.Session() as sess: init.run() for epoch in range(n_epochs): n_batches = mnist.train.num_examples // batch_size for iteration in range(n_batches): print("\r&#123;&#125;%".format(100 * iteration // n_batches), end="") sys.stdout.flush() X_batch, y_batch = mnist.train.next_batch(batch_size) sess.run(training_op, feed_dict=&#123;X: X_batch&#125;) loss_val, reconstruction_loss_val, latent_loss_val = sess.run( [loss, reconstruction_loss, latent_loss], feed_dict=&#123;X: X_batch&#125; ) sys.stdout.flush() print("\r&#123;&#125;".format(epoch), "Train total loss:", loss_val, "\tReconstruction loss:", reconstruction_loss_val, "\tLatent loss:", latent_loss_val) saver.save(sess, "autoencoder/my_mode_variational.ckpt") 0 Train total loss: 28618.115 Reconstruction loss: 24168.121 Latent loss: 4449.9946 1 Train total loss: 30913.555 Reconstruction loss: 24000.023 Latent loss: 6913.5303 2 Train total loss: 29388.672 Reconstruction loss: 23002.838 Latent loss: 6385.834 3 Train total loss: 24904.79 Reconstruction loss: 21492.559 Latent loss: 3412.2295 4 Train total loss: 23107.95 Reconstruction loss: 19479.098 Latent loss: 3628.8516 5 Train total loss: 20817.688 Reconstruction loss: 18077.043 Latent loss: 2740.6455 6 Train total loss: 18226.535 Reconstruction loss: 15215.681 Latent loss: 3010.855 7 Train total loss: 18172.998 Reconstruction loss: 15165.561 Latent loss: 3007.438 8 Train total loss: 16930.88 Reconstruction loss: 13802.411 Latent loss: 3128.4705 9 Train total loss: 16704.396 Reconstruction loss: 13389.804 Latent loss: 3314.5933 10 Train total loss: 16542.025 Reconstruction loss: 13393.953 Latent loss: 3148.0718 11 Train total loss: 16166.852 Reconstruction loss: 12910.099 Latent loss: 3256.753 12 Train total loss: 16105.586 Reconstruction loss: 12837.162 Latent loss: 3268.4233 13 Train total loss: 16308.055 Reconstruction loss: 12895.441 Latent loss: 3412.6133 14 Train total loss: 16188.68 Reconstruction loss: 12905.858 Latent loss: 3282.8215 15 Train total loss: 15944.889 Reconstruction loss: 12597.764 Latent loss: 3347.1245 16 Train total loss: 16157.871 Reconstruction loss: 12752.218 Latent loss: 3405.6528 17 Train total loss: 16119.484 Reconstruction loss: 12944.701 Latent loss: 3174.7832 18 Train total loss: 16498.84 Reconstruction loss: 13098.1875 Latent loss: 3400.6523 19 Train total loss: 16263.916 Reconstruction loss: 12844.331 Latent loss: 3419.5847 20 Train total loss: 15123.937 Reconstruction loss: 11745.332 Latent loss: 3378.6045 21 Train total loss: 15629.256 Reconstruction loss: 12185.464 Latent loss: 3443.7915 22 Train total loss: 15720.427 Reconstruction loss: 12303.7295 Latent loss: 3416.6973 23 Train total loss: 16194.514 Reconstruction loss: 12576.848 Latent loss: 3617.6655 24 Train total loss: 33406.54 Reconstruction loss: 21884.143 Latent loss: 11522.3955 25 Train total loss: 31271.346 Reconstruction loss: 24857.89 Latent loss: 6413.4556 26 Train total loss: 27312.076 Reconstruction loss: 23121.59 Latent loss: 4190.486 27 Train total loss: 32417.924 Reconstruction loss: 23142.03 Latent loss: 9275.895 28 Train total loss: 19135.332 Reconstruction loss: 15877.373 Latent loss: 3257.9597 29 Train total loss: 17062.746 Reconstruction loss: 13949.103 Latent loss: 3113.644 1234567891011121314151617181920212223242526272829303132333435363738394041424344reset_graph()from functools import partialn_inputs = 28 * 28n_hidden1 = 500n_hidden2 = 500n_hidden3 = 20n_hidden4 = n_hidden2n_hidden5 = n_hidden1n_outputs = n_inputslearning_rate = 0.001initializer = tf.contrib.layers.variance_scaling_initializer()my_dense_layer = partial( tf.layers.dense, activation=tf.nn.elu, kernel_initializer=initializer)X = tf.placeholder(tf.float32, [None, n_inputs])hidden1 = my_dense_layer(X, n_hidden1)hidden2 = my_dense_layer(hidden1, n_hidden2)hidden3_mean = my_dense_layer(hidden2, n_hidden3, activation=None)hidden3_gamma = my_dense_layer(hidden2, n_hidden3, activation=None)noise = tf.random_normal(tf.shape(hidden3_gamma), dtype=tf.float32)hidden3 = hidden3_mean + tf.exp(0.5 * hidden3_gamma) * noisehidden4 = my_dense_layer(hidden3, n_hidden4)hidden5 = my_dense_layer(hidden4, n_hidden5)logits = my_dense_layer(hidden5, n_outputs, activation=None)outputs = tf.sigmoid(logits)xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=X, logits=logits)reconstruction_loss = tf.reduce_sum(xentropy)latent_loss = 0.5 * tf.reduce_sum( tf.exp(hidden3_gamma) + tf.square(hidden3_mean) - 1 - hidden3_gamma)loss = reconstruction_loss + latent_lossoptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)training_op = optimizer.minimize(loss)init = tf.global_variables_initializer()saver = tf.train.Saver() 生成数字123456789101112131415161718192021222324252627import numpy as npn_digits = 60n_epochs = 50batch_size = 150with tf.Session() as sess: init.run() for epoch in range(n_epochs): n_batches = mnist.train.num_examples // batch_size for iteration in range(n_batches): print("\r&#123;&#125;%".format(100 * iteration // n_batches), end="") sys.stdout.flush() X_batch, y_batch = mnist.train.next_batch(batch_size) sess.run(training_op, feed_dict=&#123;X: X_batch&#125;) loss_val, reconstruction_loss_val, latent_loss_val = sess.run( [loss, reconstruction_loss, latent_loss], feed_dict=&#123;X: X_batch&#125; ) sys.stdout.flush() print("\r&#123;&#125;".format(epoch), "Train total loss:", loss_val, "\tReconstruction loss:", reconstruction_loss_val, "\tLatent loss:", latent_loss_val) saver.save(sess, "autoencoder/my_mode_variational.ckpt") codings_rnd = np.random.normal(size=[n_digits, n_hidden3]) outputs_val = outputs.eval(feed_dict=&#123;hidden3: codings_rnd&#125;) 0 Train total loss: 17901.604 Reconstruction loss: 14327.854 Latent loss: 3573.75 1 Train total loss: 17533.953 Reconstruction loss: 13797.998 Latent loss: 3735.9556 2 Train total loss: 17169.412 Reconstruction loss: 13212.218 Latent loss: 3957.1948 3 Train total loss: 16717.809 Reconstruction loss: 12906.914 Latent loss: 3810.895 4 Train total loss: 16044.961 Reconstruction loss: 12291.742 Latent loss: 3753.2183 5 Train total loss: 15961.359 Reconstruction loss: 12190.328 Latent loss: 3771.031 6 Train total loss: 15915.147 Reconstruction loss: 12045.509 Latent loss: 3869.6387 7 Train total loss: 16101.495 Reconstruction loss: 12314.455 Latent loss: 3787.04 8 Train total loss: 16155.182 Reconstruction loss: 12248.786 Latent loss: 3906.395 9 Train total loss: 15280.502 Reconstruction loss: 11493.895 Latent loss: 3786.6072 10 Train total loss: 15289.568 Reconstruction loss: 11527.297 Latent loss: 3762.272 11 Train total loss: 15314.567 Reconstruction loss: 11661.371 Latent loss: 3653.1965 12 Train total loss: 16085.252 Reconstruction loss: 12162.346 Latent loss: 3922.9058 13 Train total loss: 15621.995 Reconstruction loss: 11872.033 Latent loss: 3749.9617 14 Train total loss: 15449.285 Reconstruction loss: 11592.572 Latent loss: 3856.7124 15 Train total loss: 15287.921 Reconstruction loss: 11499.002 Latent loss: 3788.9192 16 Train total loss: 15538.403 Reconstruction loss: 11705.019 Latent loss: 3833.3848 17 Train total loss: 15225.552 Reconstruction loss: 11402.747 Latent loss: 3822.8047 18 Train total loss: 15403.612 Reconstruction loss: 11579.656 Latent loss: 3823.956 19 Train total loss: 15133.092 Reconstruction loss: 11382.73 Latent loss: 3750.361 20 Train total loss: 14997.918 Reconstruction loss: 11217.044 Latent loss: 3780.8738 21 Train total loss: 14820.831 Reconstruction loss: 11080.611 Latent loss: 3740.22 22 Train total loss: 15386.146 Reconstruction loss: 11490.824 Latent loss: 3895.3218 23 Train total loss: 15093.336 Reconstruction loss: 11351.523 Latent loss: 3741.813 24 Train total loss: 14981.201 Reconstruction loss: 11242.91 Latent loss: 3738.2905 25 Train total loss: 15472.315 Reconstruction loss: 11591.695 Latent loss: 3880.62 26 Train total loss: 15147.291 Reconstruction loss: 11353.133 Latent loss: 3794.1577 27 Train total loss: 14945.247 Reconstruction loss: 11207.519 Latent loss: 3737.7285 28 Train total loss: 15122.988 Reconstruction loss: 11392.824 Latent loss: 3730.1636 29 Train total loss: 14832.348 Reconstruction loss: 11220.064 Latent loss: 3612.2834 30 Train total loss: 14930.701 Reconstruction loss: 11258.225 Latent loss: 3672.4766 31 Train total loss: 14597.051 Reconstruction loss: 10906.106 Latent loss: 3690.944 32 Train total loss: 14825.398 Reconstruction loss: 11091.76 Latent loss: 3733.6384 33 Train total loss: 15294.322 Reconstruction loss: 11642.213 Latent loss: 3652.1094 34 Train total loss: 15126.83 Reconstruction loss: 11404.553 Latent loss: 3722.2778 35 Train total loss: 15034.021 Reconstruction loss: 11319.728 Latent loss: 3714.2944 36 Train total loss: 14350.993 Reconstruction loss: 10798.258 Latent loss: 3552.735 37 Train total loss: 14759.768 Reconstruction loss: 11103.823 Latent loss: 3655.9438 38 Train total loss: 14463.07 Reconstruction loss: 10842.15 Latent loss: 3620.92 39 Train total loss: 15347.902 Reconstruction loss: 11575.847 Latent loss: 3772.0554 40 Train total loss: 14960.492 Reconstruction loss: 11185.019 Latent loss: 3775.4731 41 Train total loss: 14601.76 Reconstruction loss: 10891.197 Latent loss: 3710.563 42 Train total loss: 15046.1045 Reconstruction loss: 11385.711 Latent loss: 3660.3936 43 Train total loss: 15596.535 Reconstruction loss: 11777.922 Latent loss: 3818.6138 44 Train total loss: 15113.572 Reconstruction loss: 11316.323 Latent loss: 3797.249 45 Train total loss: 14451.59 Reconstruction loss: 10892.8955 Latent loss: 3558.6946 46 Train total loss: 14933.413 Reconstruction loss: 11258.839 Latent loss: 3674.5745 47 Train total loss: 15393.2705 Reconstruction loss: 11576.03 Latent loss: 3817.2402 48 Train total loss: 15296.997 Reconstruction loss: 11525.246 Latent loss: 3771.751 49 Train total loss: 14890.031 Reconstruction loss: 11069.418 Latent loss: 3820.6135 1234plt.figure(figsize=(8, 50))for iteration in range(n_digits): plt.subplot(n_digits, 10, iteration + 1) plot_image(outputs_val[iteration]) 1latent_loss = 0.5 * tf.reduce_sum(tf.exp(hidden3_gamma) + tf.square(hidden3_mean) - 1 - hidden3_gamma ) Encode &amp; DecodeEncode1234567n_digits = 3X_test, y_test = mnist.train.next_batch(batch_size)codings = hidden3with tf.Session() as sess: saver.restore(sess, "autoencoder/my_mode_variational.ckpt") codings_val = codings.eval(feed_dict=&#123;X: X_test&#125;) INFO:tensorflow:Restoring parameters from autoencoder/my_mode_variational.ckpt Decode123with tf.Session() as sess: saver.restore(sess, "autoencoder/my_mode_variational.ckpt") outputs_val = outputs.eval(feed_dict=&#123;codings: codings_val&#125;) INFO:tensorflow:Restoring parameters from autoencoder/my_mode_variational.ckpt 123456fig = plt.figure(figsize=(8, 2.5 * n_digits))for iteration in range(n_digits): plt.subplot(n_digits, 2, 1 + 2 * iteration ) plot_image(X_test[iteration]) plt.subplot(n_digits, 2, 2 + 2 * iteration) plot_image(outputs_val[iteration]) Interpolate digits123456789101112131415n_iterations = 3n_digits = 6codings_rnd = np.random.normal(size=[n_digits, n_hidden3])with tf.Session() as sess: saver.restore(sess, "autoencoder/my_mode_variational.ckpt") target_codings = np.roll(codings_rnd, -1, axis=0) for iteration in range(n_iterations + 1): codings_interpolate = codings_rnd + (target_codings - codings_rnd) * iteration / n_iterations outputs_val = outputs.eval(feed_dict=&#123;codings: codings_interpolate&#125;) plt.figure(figsize=(11, 1.5*n_iterations)) for digit_index in range(n_digits): plt.subplot(1, n_digits, digit_index + 1) plot_image(outputs_val[digit_index]) plt.show() INFO:tensorflow:Restoring parameters from autoencoder/my_mode_variational.ckpt 源文件]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>Hands-On Machine Learning with Scikit-Learn and TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Recurrent Neural Networks]]></title>
    <url>%2F2018%2F12%2F28%2FRecurrent-Neural-Networks%2F</url>
    <content type="text"><![CDATA[准备123456789101112import numpy as np%matplotlib inlineimport matplotlibimport matplotlib.pyplot as pltimport tensorflow as tfdef reset_graph(seed=42): tf.reset_default_graph() tf.set_random_seed(seed) np.random.seed(seed) RNN基础手动实现RNN12345678910111213141516reset_graph()n_inputs = 3n_neurons = 5X0 = tf.placeholder(tf.float32, [None, n_inputs])X1 = tf.placeholder(tf.float32, [None, n_inputs])Wx = tf.Variable(tf.random_normal(shape=[n_inputs, n_neurons], dtype=tf.float32))Wy = tf.Variable(tf.random_normal(shape=[n_neurons, n_neurons], dtype=tf.float32))b = tf.Variable(tf.zeros([1, n_neurons], dtype=tf.float32))Y0 = tf.tanh(tf.matmul(X0, Wx) + b)Y1 = tf.tanh(tf.matmul(Y0, Wy) + tf.matmul(X1, Wx) + b)init = tf.global_variables_initializer() 12345678910111213141516X0_batch = np.array([ [0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]])# t=0X1_batch = np.array([ [9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]])# t=1with tf.Session() as sess: init.run() Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict=&#123;X0: X0_batch, X1: X1_batch&#125;) 1print(Y0_val) [[-0.0664006 0.9625767 0.68105793 0.7091854 -0.898216 ] [ 0.9977755 -0.719789 -0.9965761 0.9673924 -0.9998972 ] [ 0.99999774 -0.99898803 -0.9999989 0.9967762 -0.9999999 ] [ 1. -1. -1. -0.99818915 0.9995087 ]] 1print(Y1_val) [[ 1. -1. -1. 0.4020025 -0.9999998 ] [-0.12210419 0.62805265 0.9671843 -0.9937122 -0.2583937 ] [ 0.9999983 -0.9999994 -0.9999975 -0.85943305 -0.9999881 ] [ 0.99928284 -0.99999815 -0.9999058 0.9857963 -0.92205757]] 使用static_rnn()12n_inputs = 3n_neurons = 5 12345678reset_graph()X0 = tf.placeholder(tf.float32, [None, n_inputs])X1 = tf.placeholder(tf.float32, [None, n_inputs])basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons)out_put_seqs, states = tf.nn.static_rnn(basic_cell, [X0, X1], dtype=tf.float32)Y0, Y1 = out_put_seqs WARNING:tensorflow:From &lt;ipython-input-7-64acfd881dc3&gt;:6: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version. Instructions for updating: This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0. 1init = tf.global_variables_initializer() 12345678910111213141516X0_batch = np.array([ [0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 0, 1]])# t=0X1_batch = np.array([ [9, 8, 7], [0, 0, 0], [6, 5, 4], [3, 2, 1]])# t=1with tf.Session() as sess: init.run() Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict=&#123;X0: X0_batch, X1: X1_batch&#125;) 1Y0_val array([[ 0.30741334, -0.32884315, -0.6542847 , -0.9385059 , 0.52089024], [ 0.99122757, -0.9542541 , -0.7518079 , -0.9995208 , 0.9820235 ], [ 0.9999268 , -0.99783254, -0.8247353 , -0.9999963 , 0.99947774], [ 0.996771 , -0.68750614, 0.8419969 , 0.9303911 , 0.8120684 ]], dtype=float32) 1Y1_val array([[ 0.99998885, -0.99976057, -0.0667929 , -0.9999803 , 0.99982214], [-0.6524943 , -0.51520866, -0.37968948, -0.5922594 , -0.08968379], [ 0.99862397, -0.99715203, -0.03308626, -0.9991566 , 0.9932902 ], [ 0.99681675, -0.9598194 , 0.39660627, -0.8307606 , 0.79671973]], dtype=float32) 打包序列123n_steps = 2n_inputs = 3n_neurons = 5 12345678reset_graph()X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])X_seqs = tf.unstack(tf.transpose(X, perm=[1, 0, 2]))basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons)out_put_seqs, states = tf.nn.static_rnn(basic_cell, X_seqs, dtype=tf.float32)outputs = tf.transpose(tf.stack(out_put_seqs), perm=[1, 0, 2]) 1init = tf.global_variables_initializer() 12345678910X_batch = np.array([ [[0, 1, 2], [9, 8, 7]], [[3, 4, 5], [0, 0, 0]], [[6, 7, 8], [6, 5, 4]], [[9, 0, 1], [3, 2, 1]],])with tf.Session() as sess: init.run() outputs_val = outputs.eval(feed_dict=&#123;X: X_batch&#125;) 1print(outputs_val) [[[-0.45652324 -0.68064123 0.40938237 0.63104504 -0.45732826] [-0.9428799 -0.9998869 0.94055814 0.9999985 -0.9999997 ]] [[-0.8001535 -0.9921827 0.7817797 0.9971032 -0.9964609 ] [-0.637116 0.11300927 0.5798437 0.4310559 -0.6371699 ]] [[-0.93605185 -0.9998379 0.9308867 0.9999815 -0.99998295] [-0.9165386 -0.9945604 0.896054 0.99987197 -0.9999751 ]] [[ 0.9927369 -0.9981933 -0.55543643 0.9989031 -0.9953323 ] [-0.02746338 -0.73191994 0.7827872 0.9525682 -0.9781773 ]]] 1print(np.transpose(outputs_val, axes=[1, 0, 2])[1]) [[-0.9428799 -0.9998869 0.94055814 0.9999985 -0.9999997 ] [-0.637116 0.11300927 0.5798437 0.4310559 -0.6371699 ] [-0.9165386 -0.9945604 0.896054 0.99987197 -0.9999751 ] [-0.02746338 -0.73191994 0.7827872 0.9525682 -0.9781773 ]] 使用dynamic_rnn()123n_steps = 2n_inputs = 3n_neurons = 5 123456reset_graph()X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons)outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32) 1init = tf.global_variables_initializer() 12345678910X_batch = np.array([ [[0, 1, 2], [9, 8, 7]], [[3, 4, 5], [0, 0, 0]], [[6, 7, 8], [6, 5, 4]], [[9, 0, 1], [3, 2, 1]],])with tf.Session() as sess: init.run() outputs_val = outputs.eval(feed_dict=&#123;X: X_batch&#125;) 1print(outputs_val) [[[-0.85115266 0.87358344 0.5802911 0.8954789 -0.0557505 ] [-0.999996 0.99999577 0.9981815 1. 0.37679607]] [[-0.9983293 0.9992038 0.98071456 0.999985 0.25192663] [-0.7081804 -0.0772338 -0.85227895 0.5845349 -0.78780943]] [[-0.9999827 0.99999535 0.9992863 1. 0.5159072 ] [-0.9993956 0.9984095 0.83422637 0.99999976 -0.47325212]] [[ 0.87888587 0.07356028 0.97216916 0.9998546 -0.7351168 ] [-0.9134514 0.3600957 0.7624866 0.99817705 0.80142 ]]] 设置序列长度12345678n_steps = 2n_inputs = 3n_neurons = 5reset_graph()X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons) 12seq_length = tf.placeholder(tf.int32, [None])outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32, sequence_length=seq_length) 1init = tf.global_variables_initializer() 1234567X_batch = np.array([ [[0, 1, 2], [9, 8, 7]], [[3, 4, 5], [0, 0, 0]], [[6, 7, 8], [6, 5, 4]], [[9, 0, 1], [3, 2, 1]],])seq_length_batch = np.array([2, 1, 2, 2]) 12345with tf.Session() as sess: init.run() outputs_val, states_val = sess.run( [outputs, states], feed_dict=&#123;X: X_batch, seq_length: seq_length_batch&#125; ) 1print(outputs_val) [[[-0.9123188 0.16516446 0.5548655 -0.39159346 0.20846416] [-1. 0.956726 0.99831694 0.99970174 0.96518576]] [[-0.9998612 0.6702289 0.9723653 0.6631046 0.74457586] [ 0. 0. 0. 0. 0. ]] [[-0.99999976 0.8967997 0.9986295 0.9647514 0.93662 ] [-0.9999526 0.9681953 0.96002865 0.98706263 0.85459226]] [[-0.96435434 0.99501586 -0.36150697 0.9983378 0.999497 ] [-0.9613586 0.9568762 0.7132288 0.97729224 -0.0958299 ]]] 1print(states_val) [[-1. 0.956726 0.99831694 0.99970174 0.96518576] [-0.9998612 0.6702289 0.9723653 0.6631046 0.74457586] [-0.9999526 0.9681953 0.96002865 0.98706263 0.85459226] [-0.9613586 0.9568762 0.7132288 0.97729224 -0.0958299 ]] 训练序列分类器12345678910111213141516171819202122232425reset_graph()n_steps = 28n_inputs = 28n_neurons = 150n_outputs = 10learning_rate = 0.001X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])y = tf.placeholder(tf.int32, [None])basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons)outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)logits = tf.layers.dense(states, n_outputs)xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)loss = tf.reduce_mean(xentropy)optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)training_op = optimizer.minimize(loss)correct = tf.nn.in_top_k(logits, y, 1)accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))init = tf.global_variables_initializer() 123456789(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0y_train = y_train.astype(np.int32)y_test = y_test.astype(np.int32)X_valid, X_train = X_train[:5000], X_train[5000:]y_valid, y_train = y_train[:5000], y_train[5000:] 123456def shuffle_batch(X, y, batch_size): rnd_idx = np.random.permutation(len(X)) n_batches = len(X) // batch_size for batch_idx in np.array_split(rnd_idx, n_batches): X_batch, y_batch = X[batch_idx], y[batch_idx] yield X_batch, y_batch 1X_test = X_test.reshape((-1, n_steps, n_inputs)) 123456789101112n_epochs = 30batch_size = 150with tf.Session() as sess: init.run() for epoch in range(n_epochs): for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): X_batch = X_batch.reshape(-1, n_steps, n_inputs) sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;) acc_batch = accuracy.eval(feed_dict=&#123;X: X_batch, y:y_batch&#125;) acc_test = accuracy.eval(feed_dict=&#123;X: X_test, y: y_test&#125;) print(epoch, "Last batch accuracy:", acc_batch, "Test accuracy:", acc_test) 0 Last batch accuracy: 0.9533333 Test accuracy: 0.9288 1 Last batch accuracy: 0.96 Test accuracy: 0.9471 2 Last batch accuracy: 0.96 Test accuracy: 0.9499 3 Last batch accuracy: 0.96 Test accuracy: 0.9563 4 Last batch accuracy: 0.98 Test accuracy: 0.9677 5 Last batch accuracy: 0.93333334 Test accuracy: 0.9651 6 Last batch accuracy: 0.98 Test accuracy: 0.9685 7 Last batch accuracy: 0.96666664 Test accuracy: 0.9678 8 Last batch accuracy: 0.97333336 Test accuracy: 0.9693 9 Last batch accuracy: 0.99333334 Test accuracy: 0.9714 10 Last batch accuracy: 0.98 Test accuracy: 0.9752 11 Last batch accuracy: 0.9866667 Test accuracy: 0.9743 12 Last batch accuracy: 0.94666666 Test accuracy: 0.9716 13 Last batch accuracy: 0.97333336 Test accuracy: 0.9658 14 Last batch accuracy: 1.0 Test accuracy: 0.9772 15 Last batch accuracy: 0.98 Test accuracy: 0.974 16 Last batch accuracy: 0.99333334 Test accuracy: 0.9779 17 Last batch accuracy: 0.9866667 Test accuracy: 0.9775 18 Last batch accuracy: 0.9866667 Test accuracy: 0.9713 19 Last batch accuracy: 0.98 Test accuracy: 0.9724 20 Last batch accuracy: 0.9866667 Test accuracy: 0.9702 21 Last batch accuracy: 0.98 Test accuracy: 0.9758 22 Last batch accuracy: 0.98 Test accuracy: 0.9782 23 Last batch accuracy: 0.99333334 Test accuracy: 0.9778 24 Last batch accuracy: 0.9866667 Test accuracy: 0.9745 25 Last batch accuracy: 0.9866667 Test accuracy: 0.9741 26 Last batch accuracy: 0.9866667 Test accuracy: 0.9784 27 Last batch accuracy: 1.0 Test accuracy: 0.9808 28 Last batch accuracy: 0.99333334 Test accuracy: 0.9787 29 Last batch accuracy: 0.98 Test accuracy: 0.9813 多层RNN123456789reset_graph()n_steps = 28n_inputs = 28n_outputs = 10learning_rate = 0.001X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])y = tf.placeholder(tf.int32, [None]) 12345678n_neurons = 100n_layers = 3layers = [tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu) for layer in range(n_layers)]multi_layer_cell = tf.nn.rnn_cell.MultiRNNCell(layers)outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32) 12345678910states_concat = tf.concat(axis=1, values=states)logits = tf.layers.dense(states_concat, n_outputs)xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)loss = tf.reduce_mean(xentropy)optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)training_op = optimizer.minimize(loss)correct = tf.nn.in_top_k(logits, y, 1)accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))init = tf.global_variables_initializer() 12345678910111213n_epochs = 10batch_size = 150with tf.Session() as sess: init.run() for epoch in range(n_epochs): for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): X_batch = X_batch.reshape((-1, n_steps, n_inputs)) sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;) acc_batch = accuracy.eval(feed_dict=&#123;X: X_batch, y: y_batch&#125;) acc_test = accuracy.eval(feed_dict=&#123;X: X_test, y: y_test&#125;) print(epoch, "Last batch accuracy:", acc_batch, "Test accuracy:", acc_test)file_writer = tf.summary.FileWriter('rnn/multiRNN', tf.get_default_graph()) 0 Last batch accuracy: 0.94 Test accuracy: 0.9318 1 Last batch accuracy: 0.96 Test accuracy: 0.9563 2 Last batch accuracy: 0.96 Test accuracy: 0.9709 3 Last batch accuracy: 0.9866667 Test accuracy: 0.9701 4 Last batch accuracy: 0.9866667 Test accuracy: 0.9773 5 Last batch accuracy: 0.9533333 Test accuracy: 0.9747 6 Last batch accuracy: 0.99333334 Test accuracy: 0.9763 7 Last batch accuracy: 0.9866667 Test accuracy: 0.9804 8 Last batch accuracy: 0.98 Test accuracy: 0.9755 9 Last batch accuracy: 0.96666664 Test accuracy: 0.9804 时间序列1234567891011t_min, t_max = 0, 30resolution = 0.1def time_series(t): return t * np.sin(t) / 3 + 2 * np.sin(t*5)def next_batch(batch_size, n_steps): t0 = np.random.rand(batch_size, 1) * (t_max - t_min - n_steps * resolution) Ts = t0 + np.arange(0, n_steps + 1) * resolution ys = time_series(Ts) return ys[:, :-1].reshape(-1, n_steps, 1), ys[:, 1:].reshape(-1, n_steps, 1) 123456789101112131415161718192021222324t = np.linspace(t_min, t_max, int((t_max - t_min) / resolution))n_steps = 20t_instance = np.linspace(12.2, 12.2 + resolution * (n_steps + 1), n_steps + 1)plt.figure(figsize=(11, 4))plt.subplot(121)plt.title("A time series (generated)", fontsize=14)plt.plot(t, time_series(t), label=r"$ t. \sin(t)/3 + 2 . \sin(5t)$")plt.plot(t_instance[:-1], time_series(t_instance[:-1]), "b-", linewidth=3, label="A training instance")plt.legend(loc="lower left", fontsize=14)plt.axis([0, 30, -17, 13])plt.xlabel("Time")plt.ylabel("Value")plt.subplot(122)plt.title("A training instance", fontsize=14)plt.plot(t_instance[:-1], time_series(t_instance[:-1]), "bo", markersize=10, label="instance")plt.plot(t_instance[1:], time_series(t_instance[1:]), "w*", markersize=10, label="target")plt.legend(loc="upper left")plt.xlabel("Time")plt.show() 1X_batch, y_batch = next_batch(1, n_steps) 1np.c_[X_batch[0], y_batch[0]] array([[ 1.38452097, 2.05081182], [ 2.05081182, 2.29742291], [ 2.29742291, 2.0465599 ], [ 2.0465599 , 1.34009916], [ 1.34009916, 0.32948704], [ 0.32948704, -0.76115235], [-0.76115235, -1.68967022], [-1.68967022, -2.25492776], [-2.25492776, -2.34576159], [-2.34576159, -1.96789418], [-1.96789418, -1.24220428], [-1.24220428, -0.37478448], [-0.37478448, 0.39387907], [ 0.39387907, 0.84815766], [ 0.84815766, 0.85045064], [ 0.85045064, 0.3752526 ], [ 0.3752526 , -0.48422846], [-0.48422846, -1.53852738], [-1.53852738, -2.54795941], [-2.54795941, -3.28097239]]) 使用OutputProjectionWrapper123456789reset_graph()n_steps = 20n_inputs = 1n_neurons = 100n_outputs = 1X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])y = tf.placeholder(tf.float32, [None, n_steps, n_outputs]) 1234cell = tf.contrib.rnn.OutputProjectionWrapper( tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu), output_size=n_outputs) 1outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32) 1234567learning_rate = 0.001loss = tf.reduce_mean(tf.square(outputs -y))optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)training_op = optimizer.minimize(loss)init = tf.global_variables_initializer() 1saver = tf.train.Saver() 12345678910111213n_iterations = 1500batch_size = 50with tf.Session() as sess: init.run() for iteration in range(n_iterations): X_batch, y_batch = next_batch(batch_size, n_steps) sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;) if iteration % 100 == 0: mse = loss.eval(feed_dict=&#123;X: X_batch, y: y_batch&#125;) print(iteration, "\tMSE:", mse) saver.save(sess, "rnn/my_time_series_model") 0 MSE: 11.967254 100 MSE: 0.525841 200 MSE: 0.1495599 300 MSE: 0.07279411 400 MSE: 0.06158535 500 MSE: 0.05938873 600 MSE: 0.05470166 700 MSE: 0.047849063 800 MSE: 0.05107608 900 MSE: 0.047209196 1000 MSE: 0.047058314 1100 MSE: 0.047831465 1200 MSE: 0.04083041 1300 MSE: 0.047086805 1400 MSE: 0.041784383 12345with tf.Session() as sess: saver.restore(sess, "rnn/my_time_series_model") X_new = time_series(np.array(t_instance[:-1].reshape(-1, n_steps, n_inputs))) y_pred = sess.run(outputs, feed_dict=&#123;X: X_new&#125;) INFO:tensorflow:Restoring parameters from rnn/my_time_series_model 1y_pred array([[[-3.407753 ], [-2.4575484], [-1.1029298], [ 0.7815629], [ 2.2002175], [ 3.126768 ], [ 3.4037762], [ 3.3489153], [ 2.8798013], [ 2.2659323], [ 1.6447463], [ 1.5210768], [ 1.8972012], [ 2.7159088], [ 3.8894904], [ 5.140914 ], [ 6.142068 ], [ 6.666671 ], [ 6.6410103], [ 6.0725527]]], dtype=float32) 12345678910plt.title("Testing the model", fontsize=14)plt.plot(t_instance[:-1], time_series(t_instance[:-1]), "bo", markersize=10, label="instance")plt.plot(t_instance[1:], time_series(t_instance[1:]), "w*", markersize=10, label="target")plt.plot(t_instance[1:], y_pred[0, :, 0], "r.", markersize=10, label="prediction")plt.legend(loc="upper left")plt.xlabel("Time")plt.show() 不使用OutputProjectionWrapper()12345678reset_graph()n_steps = 20n_inputs = 1n_neurons = 100X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])y = tf.placeholder(tf.float32, [None, n_steps, n_outputs]) 12cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu)rnn_outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32) 12n_outputs = 1learning_rate = 0.001 123stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons])stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs]) 123456loss = tf.reduce_mean(tf.square(outputs - y))optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)training_op = optimizer.minimize(loss)init = tf.global_variables_initializer()saver = tf.train.Saver() 123456789101112131415n_iterations = 1500batch_size = 50with tf.Session() as sess: init.run() for iteration in range(n_iterations): X_batch, y_batch = next_batch(batch_size, n_steps) sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;) if iteration % 100 == 0: mse = loss.eval(feed_dict=&#123;X: X_batch, y: y_batch&#125;) print(iteration, "\tMSE:", mse) X_new = time_series(np.array(t_instance[:-1].reshape(-1, n_steps, n_inputs))) y_pred = sess.run(outputs, feed_dict=&#123;X: X_new&#125;) saver.save(sess, "rnn/my_time_series_model") 0 MSE: 13.907029 100 MSE: 0.5056698 200 MSE: 0.19735886 300 MSE: 0.101214476 400 MSE: 0.06850145 500 MSE: 0.06291986 600 MSE: 0.055129297 700 MSE: 0.049436502 800 MSE: 0.050434686 900 MSE: 0.0482007 1000 MSE: 0.04809868 1100 MSE: 0.04982501 1200 MSE: 0.041912545 1300 MSE: 0.049292978 1400 MSE: 0.043140374 1y_pred array([[[-3.4332483], [-2.4594698], [-1.1081185], [ 0.6882153], [ 2.1105688], [ 3.0585155], [ 3.5144088], [ 3.3531117], [ 2.808016 ], [ 2.1606152], [ 1.662645 ], [ 1.5578941], [ 1.9173537], [ 2.7210245], [ 3.8667865], [ 5.100083 ], [ 6.099999 ], [ 6.6480975], [ 6.6147423], [ 6.022089 ]]], dtype=float32) 12345678910plt.title("Testing the model", fontsize=14)plt.plot(t_instance[:-1], time_series(t_instance[:-1]), "bo", markersize=10, label="instance")plt.plot(t_instance[1:], time_series(t_instance[1:]), "w*", markersize=10, label="target")plt.plot(t_instance[1:], y_pred[0, :, 0], "r.", markersize=10, label="prediction")plt.legend(loc="upper left")plt.xlabel("Time")plt.show() 生成一个创造性的序列12345678with tf.Session() as sess: saver.restore(sess, "rnn/my_time_series_model") sequence = [0.] * n_steps for iteration in range(300): X_batch = np.array(sequence[-n_steps:]).reshape(1, n_steps, 1) y_pred = sess.run(outputs, feed_dict=&#123;X: X_batch&#125;) sequence.append(y_pred[0, -1, 0]) INFO:tensorflow:Restoring parameters from rnn/my_time_series_model 123456plt.figure(figsize=(8, 4))plt.plot(np.arange(len(sequence)), sequence, "b-")plt.plot(t[:n_steps], sequence[:n_steps], "b--", linewidth=3)plt.xlabel("Time")plt.ylabel("Value")plt.show() 123456789101112131415with tf.Session() as sess: saver.restore(sess, "rnn/my_time_series_model") sequence1 = [0. for i in range(n_steps)] for iteration in range(len(t) - n_steps): X_batch = np.array(sequence1[-n_steps:]).reshape(1, n_steps, 1) y_pred = sess.run(outputs, feed_dict=&#123;X: X_batch&#125;) sequence1.append(y_pred[0, -1, 0]) sequence2 = [time_series(i * resolution + t_min + (t_max - t_min/3)) for i in range(n_steps)] for iteration in range(len(t) - n_steps): X_batch = np.array(sequence2[-n_steps:]).reshape(1, n_steps, 1) y_pred = sess.run(outputs, feed_dict=&#123;X: X_batch&#125;) sequence2.append(y_pred[0, -1, 0]) INFO:tensorflow:Restoring parameters from rnn/my_time_series_model 123456789101112plt.figure(figsize=(11,4))plt.subplot(121)plt.plot(t, sequence1, "b-")plt.plot(t[:n_steps], sequence1[:n_steps], "b-", linewidth=3)plt.xlabel("Time")plt.ylabel("Value")plt.subplot(122)plt.plot(t, sequence2, "b-")plt.plot(t[:n_steps], sequence2[:n_steps], "b-", linewidth=3)plt.xlabel("Time")plt.show() Deep RNNMultiRNNCell123456reset_graph()n_inputs = 2n_steps = 5X = tf.placeholder(tf.float32, [None, n_steps, n_inputs]) 12345678n_neurons = 100n_layers = 3layers = [tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons) for layer in range(n_layers)]multi_layer_cell = tf.nn.rnn_cell.MultiRNNCell(layers)outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32) 1init = tf.global_variables_initializer() 1X_batch = np.random.rand(2, n_steps, n_inputs) 123with tf.Session() as sess: init.run() outputs_val, states_val = sess.run([outputs, states], feed_dict=&#123;X: X_batch&#125;) 1outputs_val.shape (2, 5, 100) Dropout1234567reset_graph()n_inputs = 1n_neurons = 100n_layers = 3n_steps = 20n_outputs = 1 12X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])y = tf.placeholder(tf.float32, [None, n_steps, n_outputs]) 1234567keep_prob = tf.placeholder_with_default(1.0, shape=())cells = [tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons) for layer in range(n_layers)]cells_drop = [tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=keep_prob) for cell in cells]multi_layer_cell = tf.nn.rnn_cell.MultiRNNCell(cells_drop)rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32) 1234567891011learning_rate = 0.01stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons])stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])loss = tf.reduce_mean(tf.square(outputs - y))optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)training_op = optimizer.minimize(loss)init = tf.global_variables_initializer()saver = tf.train.Saver() 1234567891011121314151617n_iterations = 1500batch_size = 50train_keep_prob = 0.5with tf.Session() as sess: init.run() for iteration in range(n_iterations): X_batch, y_batch = next_batch(batch_size, n_steps) _, mse = sess.run([training_op, loss], feed_dict=&#123; X: X_batch, y: y_batch, keep_prob: train_keep_prob &#125;) if iteration % 100 == 0: print(iteration, "Training MSE:", mse) saver.save(sess,"rnn/my_dropout_time_series_model") 0 Training MSE: 16.10992 100 Training MSE: 4.2036242 200 Training MSE: 3.7243023 300 Training MSE: 3.8051453 400 Training MSE: 3.1154072 500 Training MSE: 3.4736195 600 Training MSE: 3.4444861 700 Training MSE: 3.3598778 800 Training MSE: 4.1624136 900 Training MSE: 4.263299 1000 Training MSE: 3.5078833 1100 Training MSE: 4.2051315 1200 Training MSE: 2.7443748 1300 Training MSE: 4.583499 1400 Training MSE: 5.121917 12345with tf.Session() as sess: saver.restore(sess, "rnn/my_dropout_time_series_model") X_new = time_series(np.array(t_instance[:-1].reshape(-1, n_steps, n_inputs))) y_pred = sess.run(outputs, feed_dict=&#123;X: X_new&#125;) INFO:tensorflow:Restoring parameters from rnn/my_dropout_time_series_model 12345678910plt.title("Testing the model", fontsize=14)plt.plot(t_instance[:-1], time_series(t_instance[:-1]), "bo", markersize=10, label="instance")plt.plot(t_instance[1:], time_series(t_instance[1:]), "w*", markersize=10, label="target")plt.plot(t_instance[1:], y_pred[0, :, 0], "r.", markersize=10, label="prediction")plt.legend(loc="upper left")plt.xlabel("Time")plt.show() LSTM1234reset_graph()lstm_cell = tf.nn.rnn_cell.LSTMCell(name="basic_lstm_cell", num_units=n_neurons) 12345678910111213141516171819202122232425n_steps = 28n_inputs = 28n_neurons = 150n_outputs = 10n_layers = 3learning_rate = 0.001X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])y = tf.placeholder(tf.int32, [None])lstm_cells = [tf.nn.rnn_cell.LSTMCell(num_units=n_neurons, name="basic_lstm_cell") for layer in range(n_layers)]multi_cell = tf.nn.rnn_cell.MultiRNNCell(lstm_cells)outputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32)top_layer_h_state = states[-1][1]logits = tf.layers.dense(top_layer_h_state, n_outputs, name="softmax")xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)loss = tf.reduce_mean(xentropy, name="loss")optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)training_op = optimizer.minimize(loss)correct = tf.nn.in_top_k(logits, y, 1)accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))init = tf.global_variables_initializer() 1states (LSTMStateTuple(c=&lt;tf.Tensor &#39;rnn/while/Exit_3:0&#39; shape=(?, 150) dtype=float32&gt;, h=&lt;tf.Tensor &#39;rnn/while/Exit_4:0&#39; shape=(?, 150) dtype=float32&gt;), LSTMStateTuple(c=&lt;tf.Tensor &#39;rnn/while/Exit_5:0&#39; shape=(?, 150) dtype=float32&gt;, h=&lt;tf.Tensor &#39;rnn/while/Exit_6:0&#39; shape=(?, 150) dtype=float32&gt;), LSTMStateTuple(c=&lt;tf.Tensor &#39;rnn/while/Exit_7:0&#39; shape=(?, 150) dtype=float32&gt;, h=&lt;tf.Tensor &#39;rnn/while/Exit_8:0&#39; shape=(?, 150) dtype=float32&gt;)) 1top_layer_h_state &lt;tf.Tensor &#39;rnn/while/Exit_8:0&#39; shape=(?, 150) dtype=float32&gt; 123456789101112n_epochs = 10batch_size = 150with tf.Session() as sess: init.run() for epoch in range(n_epochs): for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): X_batch = X_batch.reshape((-1, n_steps, n_inputs)) sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;) acc_batch = accuracy.eval(feed_dict=&#123;X: X_batch, y: y_batch&#125;) acc_test = accuracy.eval(feed_dict=&#123;X: X_test, y: y_test&#125;) print(epoch, "Last batch accuracy:", acc_batch, "Test accuracy:", acc_test) 0 Last batch accuracy: 0.9533333 Test accuracy: 0.9481 1 Last batch accuracy: 0.96 Test accuracy: 0.9699 2 Last batch accuracy: 0.96 Test accuracy: 0.9639 3 Last batch accuracy: 1.0 Test accuracy: 0.9808 4 Last batch accuracy: 0.9866667 Test accuracy: 0.9826 5 Last batch accuracy: 1.0 Test accuracy: 0.986 6 Last batch accuracy: 1.0 Test accuracy: 0.9872 7 Last batch accuracy: 0.99333334 Test accuracy: 0.9882 8 Last batch accuracy: 1.0 Test accuracy: 0.9837 9 Last batch accuracy: 0.99333334 Test accuracy: 0.9883 1lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=n_neurons, use_peepholes=True) 1gru_cell = tf.nn.rnn_cell.GRUCell(num_units=n_neurons) 嵌入向量获取数据1234567891011121314151617181920212223242526from six.moves import urllibimport errnoimport osimport zipfileWORDS_PATH = "datasets/words"WORDS_URL = "http://mattmahoney.net/dc/text8.zip"def mkdir_p(path): try: os.makedirs(path=path) except OSError as exc: if esc.errno == errno.EEXIST and os.path.isdir(path): pass else: raisedef fetch_words_data(words_rul=WORDS_URL, words_path=WORDS_PATH): os.makedirs(words_path, exist_ok=True) zip_path = os.path.join(words_path, "words.zip") if not os.path.exists(zip_path): urllib.request.urlretrieve(words_rul, zip_path) with zipfile.ZipFile(zip_path) as f: data = f.read(f.namelist()[0]) return data.decode("ascii").split() 1words = fetch_words_data() 1words[:5] [&#39;anarchism&#39;, &#39;originated&#39;, &#39;as&#39;, &#39;a&#39;, &#39;term&#39;] 建立字典12345678from collections import Countervocabulary_size = 50000vocabulary = [("UNK", None)] + Counter(words).most_common(vocabulary_size - 1)vocabulary = np.array([word for word, _ in vocabulary])dictionary = &#123;word: code for code, word in enumerate(vocabulary)&#125;data = np.array([dictionary.get(word, 0) for word in words]) 1" ".join(words[:9]), data[:9] (&#39;anarchism originated as a term of abuse first used&#39;, array([5234, 3081, 12, 6, 195, 2, 3134, 46, 59])) 12" ".join(vocabulary[word_index] for word_index in [5234, 3081, 12, 6, 195, 2, 3134, 46, 59]) &#39;anarchism originated as a term of abuse first used&#39; 1words[24], data[24] (&#39;culottes&#39;, 0) Generate batches12345678910111213141516171819202122232425from collections import dequedef generate_batch(batch_size, num_skips, skip_window): global data_index assert batch_size % num_skips == 0 assert num_skips &lt;= 2 * skip_window batch = np.ndarray(shape=[batch_size], dtype=np.int32) labels = np.ndarray(shape=[batch_size, 1], dtype=np.int32) span = 2 * skip_window + 1 buffer = deque(maxlen=span) for _ in range(span): buffer.append(data[data_index]) data_index = (data_index + 1) % len(data) for i in range(batch_size // num_skips): target = skip_window targets_to_avoid = [skip_window] for j in range(num_skips): while target in targets_to_avoid: target = np.random.randint(0, span) targets_to_avoid.append(target) batch[i * num_skips + j] = buffer[skip_window] labels[i * num_skips + j] = buffer[target] buffer.append(data[data_index]) data_index = (data_index + 1) % len(data) return batch, labels 1np.random.seed(42) 12data_index = 0batch, labels = generate_batch(8, 2, 1) 1batch, [vocabulary[word] for word in batch] (array([3081, 3081, 12, 12, 6, 6, 195, 195]), [&#39;originated&#39;, &#39;originated&#39;, &#39;as&#39;, &#39;as&#39;, &#39;a&#39;, &#39;a&#39;, &#39;term&#39;, &#39;term&#39;]) 1labels, [vocabulary[word] for word in labels[:, 0]] (array([[ 12], [5234], [ 6], [3081], [ 12], [ 195], [ 2], [ 6]]), [&#39;as&#39;, &#39;anarchism&#39;, &#39;a&#39;, &#39;originated&#39;, &#39;as&#39;, &#39;term&#39;, &#39;of&#39;, &#39;a&#39;]) 创建模型1234567891011batch_size = 128embedding_size = 128skip_window = 1num_skips = 2valid_size = 16valid_window = 100valid_examples = np.random.choice(valid_window, valid_size, replace=False)num_sampled = 64learning_rate = 0.01 1234reset_graph()train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])valid_dataset = tf.constant(valid_examples, dtype=tf.int32) 12345vocabulary_size = 50000embedding_size = 150init_embeds = tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0)embeddings = tf.Variable(init_embeds) 12train_inputs = tf.placeholder(tf.int32, shape=[None])embed = tf.nn.embedding_lookup(embeddings, train_inputs) 12345nce_weights = tf.Variable( tf.truncated_normal([vocabulary_size, embedding_size], stddev= 1.0 / np.sqrt(embedding_size)))nce_biases = tf.Variable(tf.zeros([vocabulary_size])) 1234567891011121314loss = tf.reduce_mean( tf.nn.nce_loss(nce_weights, nce_biases, train_labels, embed, num_sampled, vocabulary_size))optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)training_op = optimizer.minimize(loss)norm = tf.sqrt(tf.reduce_mean(tf.square(embeddings), axis=1, keepdims=True))normalized_embedding = embeddings / normvalid_embeddings = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)similarity = tf.matmul(valid_embeddings, normalized_embedding, transpose_b=True)init = tf.global_variables_initializer() 训练模型1234567891011121314151617181920212223242526272829303132num_steps = 10001with tf.Session() as sess: init.run() average_loss = 0 for step in range(num_steps): print("\rIteration:&#123;&#125;".format(step), end="") batch_inputs, batch_labels = generate_batch(batch_size,num_skips, skip_window) feed_dict = &#123;train_inputs: batch_inputs, train_labels: batch_labels&#125; _, loss_val = sess.run([training_op, loss], feed_dict=feed_dict) average_loss += loss_val if step % 2000 == 0: if step &gt; 0: average_loss /= 2000 print("Average loss at step", step, ":", average_loss) average_loss = 0 if step % 10000 == 0: sim = similarity.eval() for i in range(valid_size): valid_word = vocabulary[valid_examples[i]] top_k = 8 nearest = (-sim[i, :]).argsort()[1: top_k+1] log_str = "Nearest to %s" % valid_word for k in range(top_k): close_word = vocabulary[nearest[k]] log_str = "%s %s," % (log_str, close_word) print(log_str) final_embeddings = normalized_embedding.eval() Iteration:0Average loss at step 0 : 290.5275573730469 Nearest to over tt, tuned, manichaeans, fractional, cambridge, balaguer, fluoride, strenuously, Nearest to one imagines, tijuana, hindrance, steadfastly, motorcyclist, lords, letting, adolfo, Nearest to were bezier, antibodies, nicknamed, panthers, compiler, tao, smarter, busy, Nearest to may failure, rna, efficacious, aspirin, lecompton, definitive, geese, amphibious, Nearest to two annihilate, bettors, wir, cindy, epinephrine, team, voluntarily, crystallize, Nearest to its knob, abeokuta, bracelet, bastards, ivens, objectivity, blanton, cold, Nearest to than lame, watts, stones, sram, elves, zarqawi, applets, cloves, Nearest to these pedro, condoned, neck, ssn, supervising, doug, thereto, melton, Nearest to they lowly, deportation, shrewd, reznor, tojo, decadent, occured, risotto, Nearest to is interests, golfers, dropouts, richards, egyptians, legionnaires, leonel, opener, Nearest to up clair, drives, steadfast, missed, nashville, kilowatts, anal, vinland, Nearest to he transitioned, winchell, resh, goldsmiths, standardised, markings, pursued, satirized, Nearest to people blissymbolics, mike, buffers, untouchables, carolingian, posted, ville, hypertalk, Nearest to more cactus, sta, reformation, poets, diligently, rsc, ravaged, nabokov, Nearest to was russo, rammed, investiture, glucagon, heck, adventurer, sharada, homing, Nearest to UNK reykjav, fi, rosalyn, mainline, archaeologist, armstrong, stevenage, ean, Iteration:2000Average loss at step 2000 : 133.45819056224823 Iteration:4000Average loss at step 4000 : 62.97674214935303 Iteration:6000Average loss at step 6000 : 40.385357957839965 Iteration:8000Average loss at step 8000 : 31.5875605533123 Iteration:10000Average loss at step 10000 : 25.615500225067137 Nearest to over tikal, seal, scriptores, felony, bougainville, chapter, dubrovnik, valdemar, Nearest to one eight, nine, six, two, seven, four, three, five, Nearest to were was, logan, antlia, anaximenes, songs, by, aga, hood, Nearest to may zero, to, theism, eight, can, packing, would, creativity, Nearest to two zero, one, five, four, six, three, eight, nine, Nearest to its the, mechanisms, antipope, alcmene, alemanni, alexandra, alder, topped, Nearest to than lit, but, quantity, barbados, asmara, proxima, constructing, floors, Nearest to these nur, antipopes, floors, nightclubs, mainly, and, other, aurelianus, Nearest to they that, cain, angilbert, nine, autoerotic, alexandrovich, three, some, Nearest to is yahya, are, tt, but, was, stamp, ttt, politican, Nearest to up refrigerant, incompleteness, rensselaer, four, persistence, astor, lumped, assigned, Nearest to he campylobacter, his, but, eight, later, antigens, UNK, in, Nearest to people autoerotic, stained, trigonometry, satrap, rijndael, equality, fiesta, songs, Nearest to more less, ahmad, ski, conjectures, zyklon, physically, rarely, ah, Nearest to was became, calcite, is, were, had, asphyxiation, suffixes, nmt, Nearest to UNK and, one, the, dmt, a, bromide, ananda, tile, 1np.save("rnn/my_final_embeddings.npy", final_embeddings) plot the embeddings123456789101112def plot_with_labels(low_dim_embs, labels): assert low_dim_embs.shape[0] &gt;= len(labels) , "More labels than embeddings" plt.figure(figsize=(18, 18)) for i, label in enumerate(labels): x, y = low_dim_embs[i, :] plt.scatter(x, y) plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom') 1234567from sklearn.manifold import TSNEtsne = TSNE(perplexity=30, n_components=2, init="pca", n_iter=5000)plot_only = 500low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])labels = [vocabulary[i] for i in range(plot_only)]plot_with_labels(low_dim_embs, labels) Machine Translation12345678910111213141516171819202122232425262728293031323334import tensorflow as tfreset_graph()n_steps = 50n_neurons = 200n_layers = 3num_encoder_symbols = 20000num_decoder_symbols = 20000embedding_size = 150learning_rate = 0.01X = tf.placeholder(tf.int32, [None, n_steps])Y = tf.placeholder(tf.int32, [None, n_steps])W = tf.placeholder(tf.float32, [None, n_steps - 1, 1])Y_input = Y[:, :-1]Y_target = Y[:, 1:]encoder_inputs = tf.unstack(tf.transpose(X))decoder_inputs = tf.unstack(tf.transpose(Y_input))lstm_cells = [tf.nn.rnn_cell.LSTMCell(num_units=n_neurons, name="basic_lstm_cell") for layer in range(n_layers)]cell = tf.nn.rnn_cell.MultiRNNCell(lstm_cells)outputs_seqs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq( encoder_inputs, decoder_inputs, cell, num_encoder_symbols, num_decoder_symbols, embedding_size)logits = tf.transpose(tf.unstack(outputs_seqs), perm=[1, 0, 2]) 12345678910logits_flat = tf.reshape(logits, [-1, num_decoder_symbols])Y_target_flat = tf.reshape(Y_target, [-1])W_flat = tf.reshape(W, [-1])xentropy = W_flat * tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y_target_flat, logits=logits_flat)loss = tf.reduce_mean(xentropy)optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)traninig_op = optimizer.minimize(loss)init = tf.global_variables_initializer() 源文件]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>Hands-On Machine Learning with Scikit-Learn and TensorFlow</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Convolutional Neural Networks]]></title>
    <url>%2F2018%2F12%2F24%2FConvolutional-Neural-Networks%2F</url>
    <content type="text"><![CDATA[准备12345%matplotlib inlineimport tensorflow as tfimport numpy as npimport matplotlibimport matplotlib.pyplot as plt 123456789101112def plot_image(image): plt.imshow(image, cmap="gray", interpolation="nearest") plt.axis("off")def plot_color_image(image): plt.imshow(image.astype(np.uint8), interpolation="nearest") plt.axis("off")def reset_graph(seed=42): tf.reset_default_graph() tf.set_random_seed(seed) np.random.seed(seed) 卷积层(Convolutional Layer)1234567891011from sklearn.datasets import load_sample_imagechina = load_sample_image("china.jpg")flower = load_sample_image("flower.jpg")image = china[150:220, 130:250]height, width, channels = image.shapeimage_gray_scale = image.mean(axis=2).astype(np.float32)images = image_gray_scale.reshape(1, height, width, 1) 123fmap = np.zeros(shape=(7, 7, 1, 2), dtype=np.float32)fmap[:, 3, 0, 0] = 1fmap[3, :, 0, 1] = 1 12plot_image(fmap[:, :, 0, 0])plt.show() 12plot_image(fmap[:, :, 0, 1])plt.show() 12345reset_graph()X = tf.placeholder(tf.float32, shape=(None, height, width, 1))feature_maps = tf.constant(fmap)covolution = tf.nn.conv2d(X, feature_maps, strides=[1, 1, 1, 1], padding="SAME") 12with tf.Session() as sess: output = covolution.eval(feed_dict=&#123;X: images&#125;) 12plot_image(images[0, :, :, 0])plt.show() 12plot_image(output[0, :, :, 0])plt.show() 12plot_image(output[0, :, :, 1])plt.show() 例子123456789101112131415161718192021import numpy as npfrom sklearn.datasets import load_sample_imageschina = load_sample_image("china.jpg")flower = load_sample_image("flower.jpg")dataset = np.array([china, flower], dtype=np.float32)batch_size, height, width, channels = dataset.shape# 创建两个过滤器filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)filters[:, 3, :, 0] = 1 #垂直filters[3, :, :, 1] = 1 #水平X = tf.placeholder(tf.float32, shape=(None, height, width, channels))convolution = tf.nn.conv2d(X, filters, strides=[1, 2, 2, 1], padding="SAME")with tf.Session() as sess: output = sess.run(convolution, feed_dict=&#123;X: dataset&#125;)plt.imshow(output[0, :, :, 1], cmap="gray")plt.show() 1234for image_index in (0, 1): for feature_map_index in (0, 1): plot_image(output[image_index, :, :, feature_map_index ]) plt.show() 1234reset_graph()X = tf.placeholder(shape=(None, height, width, channels), dtype=np.float32)conv = tf.layers.conv2d(X, filters=2, kernel_size=7, strides=[2, 2], padding="SAME") 12345init = tf.global_variables_initializer()with tf.Session() as sess: init.run() output = sess.run(conv, feed_dict=&#123;X: dataset&#125;) 12plt.imshow(output[0, :, :, 1], cmap="gray")plt.show() VALID 和 SAME填充123456789101112reset_graph()filter_primes = np.array([2., 3., 5., 7., 11., 13.], dtype=np.float32)x = tf.constant(np.arange(1, 14, dtype=np.float32).reshape([1, 1, 13, 1]))filters = tf.constant(filter_primes.reshape(1, 6, 1, 1))valid_conv = tf.nn.conv2d(x, filters, strides=[1, 1, 5, 1], padding="VALID")same_conv = tf.nn.conv2d(x, filters, strides=[1, 1, 5, 1], padding="SAME")with tf.Session() as sess: print("VALID:\n", valid_conv.eval()) print("SAME:\n", same_conv.eval()) VALID: [[[[184.] [389.]]]] SAME: [[[[143.] [348.] [204.]]]] 12345678print("VALID:")print(np.array([1,2,3,4,5,6]).T.dot(filter_primes))print(np.array([6,7,8,9,10,11]).T.dot(filter_primes))print("SAME:")print(np.array([0,1,2,3,4,5]).T.dot(filter_primes))print(np.array([5,6,7,8,9,10]).T.dot(filter_primes))print(np.array([10,11,12,13,0,0]).T.dot(filter_primes)) VALID: 184.0 389.0 SAME: 143.0 348.0 204.0 池化层(Pooling layer)12345batch_size, height, width, channels = dataset.shapefilters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)filters[:, 3, :, 0] = 1 #垂直filters[3, :, :, 1] = 1 #水平 12345678X = tf.placeholder(tf.float32, shape=(None, height, width, channels))max_pool = tf.nn.max_pool(X, ksize=[1,2,2,1], strides=[1,2,2,1], padding="VALID")with tf.Session() as sess: output = sess.run(max_pool, feed_dict=&#123;X: dataset&#125;)plt.imshow(output[0].astype(np.uint8))plt.show() 12plot_color_image(dataset[0])plt.show() 12plot_color_image(output[0])plt.show() MNIST123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657reset_graph()height = 28width = 28channels = 1n_inputs = height * widthconv1_fmaps = 32conv1_ksize = 3conv1_stride = 1conv1_pad = "SAME"conv2_fmaps = 64conv2_ksize = 3conv2_stride = 2conv2_pad = "SAME"pool3_fmaps = conv2_fmapsn_fcl = 64n_outputs = 10with tf.name_scope("inputs"): X = tf.placeholder(tf.float32, shape=[None, n_inputs], name="X") X_reshaped = tf.reshape(X, shape=[-1, height, width, channels]) y = tf.placeholder(tf.int32, shape=[None], name="y")conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_fmaps, kernel_size=conv1_ksize, strides=conv1_stride, padding=conv1_pad, activation=tf.nn.relu, name="conv1")conv2 = tf.layers.conv2d(conv1, filters=conv2_fmaps, kernel_size=conv2_ksize, strides=conv2_stride, padding=conv2_pad, activation=tf.nn.relu, name="conv2")with tf.name_scope("pool3"): pool3 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding="VALID") pool3_flat = tf.reshape(pool3, shape=[-1, pool3_fmaps * 7 * 7])with tf.name_scope("fc1"): fc1 = tf.layers.dense(pool3_flat, n_fcl, activation=tf.nn.relu, name="fc1")with tf.name_scope("output"): logits = tf.layers.dense(fc1, n_outputs, name="output") Y_proba = tf.nn.softmax(logits, name="Y_proba")with tf.name_scope("train"): xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y) loss = tf.reduce_mean(xentropy) optimizer = tf.train.AdamOptimizer() training_op = optimizer.minimize(loss)with tf.name_scope("eval"): correct = tf.nn.in_top_k(logits, y, 1) accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))with tf.name_scope("init_and_save"): init = tf.global_variables_initializer() saver = tf.train.Saver() 12from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets("./tmp/data") WARNING:tensorflow:From &lt;ipython-input-24-5263d3034815&gt;:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version. Instructions for updating: Please use alternatives such as official/mnist/dataset.py from tensorflow/models. WARNING:tensorflow:From e:\python\python36\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version. Instructions for updating: Please write your own downloading logic. WARNING:tensorflow:From e:\python\python36\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version. Instructions for updating: Please use tf.data to implement this functionality. Extracting ./tmp/data\train-images-idx3-ubyte.gz WARNING:tensorflow:From e:\python\python36\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version. Instructions for updating: Please use tf.data to implement this functionality. Extracting ./tmp/data\train-labels-idx1-ubyte.gz WARNING:tensorflow:From e:\python\python36\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\base.py:252: _internal_retry.&lt;locals&gt;.wrap.&lt;locals&gt;.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version. Instructions for updating: Please use urllib or similar directly. Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes. Extracting ./tmp/data\t10k-images-idx3-ubyte.gz Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes. Extracting ./tmp/data\t10k-labels-idx1-ubyte.gz WARNING:tensorflow:From e:\python\python36\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version. Instructions for updating: Please use alternatives such as official/mnist/dataset.py from tensorflow/models. 1234567891011121314n_epochs = 10batch_size = 100with tf.Session() as sess: init.run() for epoch in range(n_epochs): for iteration in range(mnist.train.num_examples // batch_size): X_batch, y_batch = mnist.train.next_batch(batch_size) sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;) acc_train = accuracy.eval(feed_dict=&#123;X: X_batch, y: y_batch&#125;) acc_test = accuracy.eval(feed_dict=&#123;X: mnist.test.images, y: mnist.test.labels&#125;) print(epoch, "Training accuracy:", acc_train, "Test accuracy:", acc_test) save_path = saver.save(sess, "./cnn/my_mnist_mode") 0 Training accuracy: 0.99 Test accuracy: 0.9771 1 Training accuracy: 0.96 Test accuracy: 0.98 2 Training accuracy: 1.0 Test accuracy: 0.986 3 Training accuracy: 0.99 Test accuracy: 0.9871 4 Training accuracy: 1.0 Test accuracy: 0.9882 5 Training accuracy: 0.99 Test accuracy: 0.9903 6 Training accuracy: 1.0 Test accuracy: 0.9882 7 Training accuracy: 0.99 Test accuracy: 0.9892 8 Training accuracy: 1.0 Test accuracy: 0.9884 9 Training accuracy: 1.0 Test accuracy: 0.9882 源文件]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>Hands-On Machine Learning with Scikit-Learn and TensorFlow</tag>
        <tag>神经网络</tag>
        <tag>卷积神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Training Deep Neural Nets]]></title>
    <url>%2F2018%2F12%2F20%2FTraining-Deep-Neural-Nets%2F</url>
    <content type="text"><![CDATA[准备123456789101112import numpy as npimport tensorflow as tf%matplotlib inlineimport matplotlib.pyplot as pltdef reset_graph(seed=42): tf.reset_default_graph() tf.set_random_seed(seed) np.random.seed(seed) Vanishing/Exploding Gradients Problems12def logit(z): return 1 / (1 + np.exp(-z)) 12345678910111213141516171819202122z = np.linspace(-5, 5, 200)plt.plot([-5, 5], [0, 0], 'k-')plt.plot([-5, 5], [1, 1], 'k--')plt.plot([0, 0], [-0.2, 1.2], 'k-')plt.plot([-5, 5], [-3/4, 7/4], 'g--')plt.plot(z, logit(z), 'b--', linewidth=2)props = dict(facecolor='black', shrink=0.1)plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha='center')plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha='center')plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha='center')plt.grid(True)plt.title("Sigmoid activation function", fontsize=14)plt.axis([-5, 5, -0.2, 1.2])plt.show() Xavier和 He 初始化12import tensorflow as tfreset_graph() 1234n_inputs = 28 * 28n_hidden1 = 300X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X") 123he_init = tf.variance_scaling_initializer()hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, kernel_initializer=he_init, name="hidden1") 不饱和激活函数Leaky ReLU12def leaky_relu(z, alpha=0.01): return np.maximum(alpha*z, z) 123456789plt.plot(z, leaky_relu(z,0.05), 'b-', linewidth=2)plt.plot([-5, 5], [0, 0], 'k-')plt.plot([0, 0], [-0.5, 4.2], 'k-')plt.annotate('Leak', xytext=(-4.5, 0.5), xy=(-5, -0.2), arrowprops=props, ha="center")plt.title("Leaky ReLU activation function", fontsize=14)plt.axis([-5, 5, -0.5, 4.2])plt.show() 123reset_graph()X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X") 1234def leaky_relu(z, name=None): return tf.maximum(0.01 * z, z, name=name)hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name="hidden1") 使用Leaky ReLU训练MNIST 123456reset_graph()n_inputs = 28 * 28n_hidden1 = 300n_hidden2 = 100n_outputs = 10 12X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")y = tf.placeholder(tf.int32, shape=(None), name="y") 1234with tf.name_scope("dnn"): hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name="hidden1") hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name="hidden2") logits = tf.layers.dense(hidden2, n_outputs, name="outputs") 123with tf.name_scope("loss"): xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) loss = tf.reduce_mean(xentropy, name="loss") 12345learning_rate = 0.01with tf.name_scope("train"): optimizer = tf.train.GradientDescentOptimizer(learning_rate) training_op = optimizer.minimize(loss) 123with tf.name_scope("eval"): correct = tf.nn.in_top_k(logits, y, 1) accurancy = tf.reduce_mean(tf.cast(correct, tf.float32)) 12init = tf.global_variables_initializer()saver = tf.train.Saver() 加载数据 12345678910(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0y_train = y_train.astype(np.int32)y_test = y_test.astype(np.int32)X_valid, X_train = X_train[:5000], X_train[5000:]y_valid, y_train = y_train[:5000], y_train[5000:] 123456def shuffle_batch(X, y, batch_size): rnd_idx = np.random.permutation(len(X)) n_batches = len(X) // batch_size for batch_idx in np.array_split(rnd_idx, n_batches): X_batch, y_batch = X[batch_idx], y[batch_idx] yield X_batch, y_batch 1234567891011121314n_epochs = 40batch_size = 50with tf.Session() as sess: init.run() for epoch in range(n_epochs): for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;) if epoch % 5 == 0: acc_batch = accurancy.eval(feed_dict=&#123;X: X_batch, y: y_batch&#125;) acc_valid = accurancy.eval(feed_dict=&#123;X: X_valid, y: y_valid&#125;) print(epoch, "Batch accuracy:", acc_batch, "Validation accurancy:", acc_valid) save_path = saver.save(sess, "dnn/leaky_relu_model_final.ckpt")file_write = tf.summary.FileWriter("dnn/leaky_relu", tf.get_default_graph()) 0 Batch accuracy: 0.86 Validation accurancy: 0.9044 5 Batch accuracy: 0.94 Validation accurancy: 0.9494 10 Batch accuracy: 0.92 Validation accurancy: 0.9656 15 Batch accuracy: 0.94 Validation accurancy: 0.971 20 Batch accuracy: 1.0 Validation accurancy: 0.9762 25 Batch accuracy: 1.0 Validation accurancy: 0.9772 30 Batch accuracy: 0.98 Validation accurancy: 0.9782 35 Batch accuracy: 1.0 Validation accurancy: 0.9788 ELU12def elu(z, alpha=1): return np.where(z &lt; 0, alpha * (np.exp(z)-1), z) 12345678plt.plot(z, elu(z), "b-", linewidth=2)plt.plot([-5, 5], [0, 0], "k-")plt.plot([0, 0], [-2.2, 3.2], "k-")plt.plot([-5, 5], [-1, -1], "k--")plt.title(r"ELU activation funtion($\alpha=1$)", fontsize=14)plt.axis([-5, 5, -2.2, 3.2])plt.grid(True)plt.show() 12345reset_graph()X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name="hidden1") 批量标准化(Batch Normalization)123456789101112131415161718192021reset_graph()import tensorflow as tfn_inputs = 28 * 28n_hidden1 = 300n_hidden2 = 100n_outputs = 10X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")training = tf.placeholder_with_default(False, shape=(), name="training")hidden1 = tf.layers.dense(X, n_hidden1, name="hidden1")bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)bn1_act = tf.nn.elu(bn1)hidden2 = tf.layers.dense(bn1_act, n_hidden2, name="hidden2")bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)bn2_act = tf.nn.elu(bn2)logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name="outputs")logits = tf.layers.batch_normalization(logits_before_bn, training=training, momentum=0.9) 1234reset_graph()X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")training = tf.placeholder_with_default(False, shape=(), name="training") 123456789101112131415from functools import partialmy_batch_norm_layer = partial(tf.layers.batch_normalization, training=training, momentum=0.9)hidden1 = tf.layers.dense(X, n_hidden1, name="hidden1")bn1 = my_batch_norm_layer(hidden1)bn1_act = tf.nn.elu(bn1)hidden2 = tf.layers.dense(bn1_act, n_hidden2, name="hidden2")bn2 = my_batch_norm_layer(hidden2)bn2_act = tf.nn.elu(bn2)logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name="outputs")logits = my_batch_norm_layer(logits_before_bn) 123456789101112131415161718192021222324252627282930313233343536373839404142434445reset_graph()batch_norm_momentum = 0.9X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")y = tf.placeholder(tf.int32, shape=(None), name="y")training = tf.placeholder_with_default(False, shape=(), name="training")with tf.name_scope("dnn"): he_init = tf.variance_scaling_initializer() my_batch_norm_layer = partial( tf.layers.batch_normalization, training=training, momentum=batch_norm_momentum ) my_dense_layer = partial( tf.layers.dense, kernel_initializer=he_init ) hidden1 = my_dense_layer(X, n_hidden1, name="hidden1") bn1 = tf.nn.elu(my_batch_norm_layer(hidden1)) hidden2 = my_dense_layer(bn1, n_hidden2, name="hidden2") bn2 = tf.nn.elu(my_batch_norm_layer(hidden2)) logits_before_bn = my_dense_layer(bn2, n_outputs, name="outputs") logits = my_batch_norm_layer(logits_before_bn)with tf.name_scope("loss"): xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) loss = tf.reduce_mean(xentropy, name="loss")with tf.name_scope("train"): optimizer = tf.train.GradientDescentOptimizer(learning_rate) training_op = optimizer.minimize(loss)with tf.name_scope("eval"): correct = tf.nn.in_top_k(logits, y, 1) accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))init = tf.global_variables_initializer()saver = tf.train.Saver() 12n_epochs = 20batch_size = 200 123456789101112extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)with tf.Session() as sess: init.run() for epoch in range(n_epochs): for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): sess.run([training_op, extra_update_ops], feed_dict=&#123;training: True, X: X_batch, y: y_batch&#125;) accuracy_val = accuracy.eval(feed_dict=&#123;X: X_valid, y: y_valid&#125;) print(epoch, "Validation accuracy:", accuracy_val) save_path = saver.save(sess, "dnn/elu_model_final.ckpt") file_writer = tf.summary.FileWriter("dnn/elu", tf.get_default_graph()) 0 Validation accuracy: 0.8952 1 Validation accuracy: 0.9202 2 Validation accuracy: 0.9318 3 Validation accuracy: 0.9422 4 Validation accuracy: 0.9468 5 Validation accuracy: 0.954 6 Validation accuracy: 0.9568 7 Validation accuracy: 0.96 8 Validation accuracy: 0.962 9 Validation accuracy: 0.9638 10 Validation accuracy: 0.9662 11 Validation accuracy: 0.9682 12 Validation accuracy: 0.9672 13 Validation accuracy: 0.9696 14 Validation accuracy: 0.9706 15 Validation accuracy: 0.9704 16 Validation accuracy: 0.9718 17 Validation accuracy: 0.9726 18 Validation accuracy: 0.9738 19 Validation accuracy: 0.9742 1[v.name for v in tf.trainable_variables()] [&#39;hidden1/kernel:0&#39;, &#39;hidden1/bias:0&#39;, &#39;batch_normalization/gamma:0&#39;, &#39;batch_normalization/beta:0&#39;, &#39;hidden2/kernel:0&#39;, &#39;hidden2/bias:0&#39;, &#39;batch_normalization_1/gamma:0&#39;, &#39;batch_normalization_1/beta:0&#39;, &#39;outputs/kernel:0&#39;, &#39;outputs/bias:0&#39;, &#39;batch_normalization_2/gamma:0&#39;, &#39;batch_normalization_2/beta:0&#39;] 1[v.name for v in tf.global_variables()] [&#39;hidden1/kernel:0&#39;, &#39;hidden1/bias:0&#39;, &#39;batch_normalization/gamma:0&#39;, &#39;batch_normalization/beta:0&#39;, &#39;batch_normalization/moving_mean:0&#39;, &#39;batch_normalization/moving_variance:0&#39;, &#39;hidden2/kernel:0&#39;, &#39;hidden2/bias:0&#39;, &#39;batch_normalization_1/gamma:0&#39;, &#39;batch_normalization_1/beta:0&#39;, &#39;batch_normalization_1/moving_mean:0&#39;, &#39;batch_normalization_1/moving_variance:0&#39;, &#39;outputs/kernel:0&#39;, &#39;outputs/bias:0&#39;, &#39;batch_normalization_2/gamma:0&#39;, &#39;batch_normalization_2/beta:0&#39;, &#39;batch_normalization_2/moving_mean:0&#39;, &#39;batch_normalization_2/moving_variance:0&#39;] 梯度裁剪123456789101112131415161718192021222324reset_graph()n_inputs = 28 * 28n_hidden1 = 300n_hidden2 = 50n_hidden3 = 50n_hidden4 = 50n_hidden5 = 50n_outputs = 10X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")y = tf.placeholder(tf.int32, shape=(None), name="y")with tf.name_scope("dnn"): hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name="hidden1") hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name="hidden2") hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name="hidden3") hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name="hidden4") hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name="hidden5") logits = tf.layers.dense(hidden5, n_outputs, name="outputs")with tf.name_scope("loss"): xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) loss = tf.reduce_mean(xentropy, name="loss") 1learning_rate = 0.01 1234567threhold = 1.0optimizer = tf.train.GradientDescentOptimizer(learning_rate)grads_and_vars = optimizer.compute_gradients(loss)capped_gvs = [(tf.clip_by_value(grad, -threhold, threhold), var) for grad, var in grads_and_vars]training_op = optimizer.apply_gradients(capped_gvs) 123with tf.name_scope("eval"): correct = tf.nn.in_top_k(logits, y, 1) accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name="accuracy") 12init = tf.global_variables_initializer()saver = tf.train.Saver() 12n_epochs = 20batch_size = 200 12345678910with tf.Session() as sess: init.run() for epoch in range(n_epochs): for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;) accuracy_val = accuracy.eval(feed_dict=&#123;X: X_valid, y: y_valid&#125;) print(epoch, "Validation accuracy:", accuracy_val) save_path = saver.save(sess, "dnn/clip_model_final.ckpt")filewriter = tf.summary.FileWriter("dnn/clip", tf.get_default_graph()) 0 Validation accuracy: 0.2906 1 Validation accuracy: 0.795 2 Validation accuracy: 0.8836 3 Validation accuracy: 0.9068 4 Validation accuracy: 0.9136 5 Validation accuracy: 0.9232 6 Validation accuracy: 0.93 7 Validation accuracy: 0.9342 8 Validation accuracy: 0.9384 9 Validation accuracy: 0.944 10 Validation accuracy: 0.9454 11 Validation accuracy: 0.9472 12 Validation accuracy: 0.9516 13 Validation accuracy: 0.9532 14 Validation accuracy: 0.9542 15 Validation accuracy: 0.9562 16 Validation accuracy: 0.9572 17 Validation accuracy: 0.9596 18 Validation accuracy: 0.9588 19 Validation accuracy: 0.9616 重利用之前训练的层重利用TensorFlow模型12reset_graph()saver = tf.train.import_meta_graph("dnn/clip_model_final.ckpt.meta") 12for op in tf.get_default_graph().get_operations(): print(op.name) X y hidden1/kernel/Initializer/random_uniform/shape hidden1/kernel/Initializer/random_uniform/min hidden1/kernel/Initializer/random_uniform/max hidden1/kernel/Initializer/random_uniform/RandomUniform hidden1/kernel/Initializer/random_uniform/sub hidden1/kernel/Initializer/random_uniform/mul hidden1/kernel/Initializer/random_uniform hidden1/kernel hidden1/kernel/Assign hidden1/kernel/read hidden1/bias/Initializer/zeros hidden1/bias hidden1/bias/Assign hidden1/bias/read dnn/hidden1/MatMul dnn/hidden1/BiasAdd dnn/hidden1/Relu hidden2/kernel/Initializer/random_uniform/shape hidden2/kernel/Initializer/random_uniform/min hidden2/kernel/Initializer/random_uniform/max hidden2/kernel/Initializer/random_uniform/RandomUniform hidden2/kernel/Initializer/random_uniform/sub hidden2/kernel/Initializer/random_uniform/mul hidden2/kernel/Initializer/random_uniform hidden2/kernel hidden2/kernel/Assign hidden2/kernel/read hidden2/bias/Initializer/zeros hidden2/bias hidden2/bias/Assign hidden2/bias/read dnn/hidden2/MatMul dnn/hidden2/BiasAdd dnn/hidden2/Relu hidden3/kernel/Initializer/random_uniform/shape hidden3/kernel/Initializer/random_uniform/min hidden3/kernel/Initializer/random_uniform/max hidden3/kernel/Initializer/random_uniform/RandomUniform hidden3/kernel/Initializer/random_uniform/sub hidden3/kernel/Initializer/random_uniform/mul hidden3/kernel/Initializer/random_uniform hidden3/kernel hidden3/kernel/Assign hidden3/kernel/read hidden3/bias/Initializer/zeros hidden3/bias hidden3/bias/Assign hidden3/bias/read dnn/hidden3/MatMul dnn/hidden3/BiasAdd dnn/hidden3/Relu hidden4/kernel/Initializer/random_uniform/shape hidden4/kernel/Initializer/random_uniform/min hidden4/kernel/Initializer/random_uniform/max hidden4/kernel/Initializer/random_uniform/RandomUniform hidden4/kernel/Initializer/random_uniform/sub hidden4/kernel/Initializer/random_uniform/mul hidden4/kernel/Initializer/random_uniform hidden4/kernel hidden4/kernel/Assign hidden4/kernel/read hidden4/bias/Initializer/zeros hidden4/bias hidden4/bias/Assign hidden4/bias/read dnn/hidden4/MatMul dnn/hidden4/BiasAdd dnn/hidden4/Relu hidden5/kernel/Initializer/random_uniform/shape hidden5/kernel/Initializer/random_uniform/min hidden5/kernel/Initializer/random_uniform/max hidden5/kernel/Initializer/random_uniform/RandomUniform hidden5/kernel/Initializer/random_uniform/sub hidden5/kernel/Initializer/random_uniform/mul hidden5/kernel/Initializer/random_uniform hidden5/kernel hidden5/kernel/Assign hidden5/kernel/read hidden5/bias/Initializer/zeros hidden5/bias hidden5/bias/Assign hidden5/bias/read dnn/hidden5/MatMul dnn/hidden5/BiasAdd dnn/hidden5/Relu outputs/kernel/Initializer/random_uniform/shape outputs/kernel/Initializer/random_uniform/min outputs/kernel/Initializer/random_uniform/max outputs/kernel/Initializer/random_uniform/RandomUniform outputs/kernel/Initializer/random_uniform/sub outputs/kernel/Initializer/random_uniform/mul outputs/kernel/Initializer/random_uniform outputs/kernel outputs/kernel/Assign outputs/kernel/read outputs/bias/Initializer/zeros outputs/bias outputs/bias/Assign outputs/bias/read dnn/outputs/MatMul dnn/outputs/BiasAdd loss/SparseSoftmaxCrossEntropyWithLogits/Shape loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits loss/Const loss/loss gradients/Shape gradients/grad_ys_0 gradients/Fill gradients/loss/loss_grad/Reshape/shape gradients/loss/loss_grad/Reshape gradients/loss/loss_grad/Shape gradients/loss/loss_grad/Tile gradients/loss/loss_grad/Shape_1 gradients/loss/loss_grad/Shape_2 gradients/loss/loss_grad/Const gradients/loss/loss_grad/Prod gradients/loss/loss_grad/Const_1 gradients/loss/loss_grad/Prod_1 gradients/loss/loss_grad/Maximum/y gradients/loss/loss_grad/Maximum gradients/loss/loss_grad/floordiv gradients/loss/loss_grad/Cast gradients/loss/loss_grad/truediv gradients/zeros_like gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1 gradients/dnn/outputs/MatMul_grad/MatMul gradients/dnn/outputs/MatMul_grad/MatMul_1 gradients/dnn/outputs/MatMul_grad/tuple/group_deps gradients/dnn/outputs/MatMul_grad/tuple/control_dependency gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1 gradients/dnn/hidden5/Relu_grad/ReluGrad gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1 gradients/dnn/hidden5/MatMul_grad/MatMul gradients/dnn/hidden5/MatMul_grad/MatMul_1 gradients/dnn/hidden5/MatMul_grad/tuple/group_deps gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1 gradients/dnn/hidden4/Relu_grad/ReluGrad gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1 gradients/dnn/hidden4/MatMul_grad/MatMul gradients/dnn/hidden4/MatMul_grad/MatMul_1 gradients/dnn/hidden4/MatMul_grad/tuple/group_deps gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1 gradients/dnn/hidden3/Relu_grad/ReluGrad gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1 gradients/dnn/hidden3/MatMul_grad/MatMul gradients/dnn/hidden3/MatMul_grad/MatMul_1 gradients/dnn/hidden3/MatMul_grad/tuple/group_deps gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1 gradients/dnn/hidden2/Relu_grad/ReluGrad gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1 gradients/dnn/hidden2/MatMul_grad/MatMul gradients/dnn/hidden2/MatMul_grad/MatMul_1 gradients/dnn/hidden2/MatMul_grad/tuple/group_deps gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1 gradients/dnn/hidden1/Relu_grad/ReluGrad gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1 gradients/dnn/hidden1/MatMul_grad/MatMul gradients/dnn/hidden1/MatMul_grad/MatMul_1 gradients/dnn/hidden1/MatMul_grad/tuple/group_deps gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1 clip_by_value/Minimum/y clip_by_value/Minimum clip_by_value/y clip_by_value clip_by_value_1/Minimum/y clip_by_value_1/Minimum clip_by_value_1/y clip_by_value_1 clip_by_value_2/Minimum/y clip_by_value_2/Minimum clip_by_value_2/y clip_by_value_2 clip_by_value_3/Minimum/y clip_by_value_3/Minimum clip_by_value_3/y clip_by_value_3 clip_by_value_4/Minimum/y clip_by_value_4/Minimum clip_by_value_4/y clip_by_value_4 clip_by_value_5/Minimum/y clip_by_value_5/Minimum clip_by_value_5/y clip_by_value_5 clip_by_value_6/Minimum/y clip_by_value_6/Minimum clip_by_value_6/y clip_by_value_6 clip_by_value_7/Minimum/y clip_by_value_7/Minimum clip_by_value_7/y clip_by_value_7 clip_by_value_8/Minimum/y clip_by_value_8/Minimum clip_by_value_8/y clip_by_value_8 clip_by_value_9/Minimum/y clip_by_value_9/Minimum clip_by_value_9/y clip_by_value_9 clip_by_value_10/Minimum/y clip_by_value_10/Minimum clip_by_value_10/y clip_by_value_10 clip_by_value_11/Minimum/y clip_by_value_11/Minimum clip_by_value_11/y clip_by_value_11 GradientDescent/learning_rate GradientDescent/update_hidden1/kernel/ApplyGradientDescent GradientDescent/update_hidden1/bias/ApplyGradientDescent GradientDescent/update_hidden2/kernel/ApplyGradientDescent GradientDescent/update_hidden2/bias/ApplyGradientDescent GradientDescent/update_hidden3/kernel/ApplyGradientDescent GradientDescent/update_hidden3/bias/ApplyGradientDescent GradientDescent/update_hidden4/kernel/ApplyGradientDescent GradientDescent/update_hidden4/bias/ApplyGradientDescent GradientDescent/update_hidden5/kernel/ApplyGradientDescent GradientDescent/update_hidden5/bias/ApplyGradientDescent GradientDescent/update_outputs/kernel/ApplyGradientDescent GradientDescent/update_outputs/bias/ApplyGradientDescent GradientDescent eval/in_top_k/InTopKV2/k eval/in_top_k/InTopKV2 eval/Cast eval/Const eval/accuracy init save/Const save/SaveV2/tensor_names save/SaveV2/shape_and_slices save/SaveV2 save/control_dependency save/RestoreV2/tensor_names save/RestoreV2/shape_and_slices save/RestoreV2 save/Assign save/Assign_1 save/Assign_2 save/Assign_3 save/Assign_4 save/Assign_5 save/Assign_6 save/Assign_7 save/Assign_8 save/Assign_9 save/Assign_10 save/Assign_11 save/restore_all 1234567#选择需要的X = tf.get_default_graph().get_tensor_by_name("X:0")y = tf.get_default_graph().get_tensor_by_name("y:0")accuracy = tf.get_default_graph().get_tensor_by_name("eval/accuracy:0")training_op = tf.get_default_graph().get_operation_by_name("GradientDescent") 123# 将重要的操作放在单独的集合中for op in (X, y, accuracy, training_op): tf.add_to_collection("my_important_ops", op) 12# 或许这些操作X, y, accuracy, training_op = tf.get_collection("my_important_ops") 123456789with tf.Session() as sess: saver.restore(sess, "dnn/clip_model_final.ckpt") for epoch in range(n_epochs): for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;) accuracy_val = accuracy.eval(feed_dict=&#123;X: X_valid, y: y_valid&#125;) print(epoch, "Validation accuracy:", accuracy_val) save_path = saver.save(sess, "dnn/my_new_model_final.ckpt") INFO:tensorflow:Restoring parameters from dnn/clip_model_final.ckpt 0 Validation accuracy: 0.9626 1 Validation accuracy: 0.963 2 Validation accuracy: 0.9632 3 Validation accuracy: 0.9658 4 Validation accuracy: 0.965 5 Validation accuracy: 0.9628 6 Validation accuracy: 0.966 7 Validation accuracy: 0.9678 8 Validation accuracy: 0.9672 9 Validation accuracy: 0.9678 10 Validation accuracy: 0.97 11 Validation accuracy: 0.97 12 Validation accuracy: 0.966 13 Validation accuracy: 0.9706 14 Validation accuracy: 0.972 15 Validation accuracy: 0.9708 16 Validation accuracy: 0.9724 17 Validation accuracy: 0.9706 18 Validation accuracy: 0.972 19 Validation accuracy: 0.9684 123456789101112131415161718192021222324252627282930# 重利用低层reset_graph()n_hidden4 = 20n_outputs = 10saver = tf.train.import_meta_graph("dnn/clip_model_final.ckpt.meta")X = tf.get_default_graph().get_tensor_by_name("X:0")y = tf.get_default_graph().get_tensor_by_name("y:0")hidden3 = tf.get_default_graph().get_tensor_by_name("dnn/hidden3/Relu:0")new_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name="new_hidden4")new_logits = tf.layers.dense(new_hidden4, n_outputs, name="new_outputs")with tf.name_scope("new_loss"): xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits) loss = tf.reduce_mean(xentropy, name="losss")with tf.name_scope("new_eval"): correct = tf.nn.in_top_k(new_logits, y, 1) accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name="accuracy")with tf.name_scope("new_train"): optimizer = tf.train.GradientDescentOptimizer(learning_rate) training_op = optimizer.minimize(loss)init = tf.global_variables_initializer()new_saver = tf.train.Saver() 12345678910with tf.Session() as sess: init.run() saver.restore(sess, "dnn/clip_model_final.ckpt") for epoch in range(n_epochs): for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;) accuracy_val = accuracy.eval(feed_dict=&#123;X: X_valid, y: y_valid&#125;) print(epoch, "Validation accuracy:", accuracy_val) save_path = new_saver.save(sess, "dnn/my_new_low_layer_model_final.ckpt") INFO:tensorflow:Restoring parameters from dnn/clip_model_final.ckpt 0 Validation accuracy: 0.9172 1 Validation accuracy: 0.9394 2 Validation accuracy: 0.9464 3 Validation accuracy: 0.95 4 Validation accuracy: 0.955 5 Validation accuracy: 0.9522 6 Validation accuracy: 0.9566 7 Validation accuracy: 0.9598 8 Validation accuracy: 0.9608 9 Validation accuracy: 0.9608 10 Validation accuracy: 0.9628 11 Validation accuracy: 0.9622 12 Validation accuracy: 0.9646 13 Validation accuracy: 0.9648 14 Validation accuracy: 0.9654 15 Validation accuracy: 0.9668 16 Validation accuracy: 0.9676 17 Validation accuracy: 0.9662 18 Validation accuracy: 0.9684 19 Validation accuracy: 0.968 冻结低层1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253reset_graph()n_inputs = 28 * 28n_hidden1 = 300n_hidden2 = 50n_hidden3 = 50n_hidden4 = 20n_outputs = 10X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")y = tf.placeholder(tf.int32, shape=(None), name="y")with tf.name_scope("dnn"): hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name="hidden1") hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name="hidden2") hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name="hidden3") hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name="hidden4") logits = tf.layers.dense(hidden4, n_outputs, name="outputs")with tf.name_scope("loss"): xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) loss = tf.reduce_mean(xentropy, name="loss")with tf.name_scope("eval"): correct = tf.nn.in_top_k(logits, y, 1) accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name="accuracy")with tf.name_scope("train"): optimizer = tf.train.GradientDescentOptimizer(learning_rate) train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope="hidden[34]|outputs") training_op = optimizer.minimize(loss, var_list=train_vars)init = tf.global_variables_initializer()new_saver = tf.train.Saver()reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope="hidden[123]")restore_saver = tf.train.Saver(reuse_vars)init = tf.global_variables_initializer()saver = tf.train.Saver()with tf.Session() as sess: init.run() restore_saver.restore(sess, "dnn/clip_model_final.ckpt") for epoch in range(n_epochs): for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;) accuracy_val = accuracy.eval(feed_dict=&#123;X: X_valid, y: y_valid&#125;) print(epoch, "Validation accuracy:", accuracy_val) save_path = new_saver.save(sess, "dnn/my_new_freeze_low_layer_model_final.ckpt") INFO:tensorflow:Restoring parameters from dnn/clip_model_final.ckpt 0 Validation accuracy: 0.893 1 Validation accuracy: 0.9234 2 Validation accuracy: 0.9358 3 Validation accuracy: 0.941 4 Validation accuracy: 0.947 5 Validation accuracy: 0.9484 6 Validation accuracy: 0.95 7 Validation accuracy: 0.954 8 Validation accuracy: 0.9542 9 Validation accuracy: 0.954 10 Validation accuracy: 0.955 11 Validation accuracy: 0.9548 12 Validation accuracy: 0.9572 13 Validation accuracy: 0.957 14 Validation accuracy: 0.9564 15 Validation accuracy: 0.957 16 Validation accuracy: 0.9576 17 Validation accuracy: 0.958 18 Validation accuracy: 0.9586 19 Validation accuracy: 0.9582 缓冲冻结的层12345678910111213141516171819202122232425262728293031323334353637reset_graph()n_inputs = 28 * 28n_hidden1 = 300n_hidden2 = 50n_hidden3 = 50n_hidden4 = 20n_outputs = 10X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")y = tf.placeholder(tf.int32, shape=(None), name="y")with tf.name_scope("dnn"): hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name="hidden1") hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name="hidden2") hidden2_stop = tf.stop_gradient(hidden2) hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu, name="hidden3") hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name="hidden4") logits = tf.layers.dense(hidden4, n_outputs, name="outputs")with tf.name_scope("loss"): xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) loss = tf.reduce_mean(xentropy, name="loss")with tf.name_scope("eval"): correct = tf.nn.in_top_k(logits, y, 1) accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name="accuracy")with tf.name_scope("train"): optimizer = tf.train.GradientDescentOptimizer(learning_rate) training_op = optimizer.minimize(loss)reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope="hidden[123]")restore_saver = tf.train.Saver(reuse_vars)init = tf.global_variables_initializer()saver = tf.train.Saver() 1234567891011121314151617181920import numpy as npn_batches = len(X_train) // batch_sizewith tf.Session() as sess: init.run() restore_saver.restore(sess, "dnn/clip_model_final.ckpt") h2_cache = sess.run(hidden2, feed_dict=&#123;X: X_train&#125;) h2_cache_valid = sess.run(hidden2, feed_dict=&#123;X: X_valid&#125;) for epoch in range(n_epochs): shuffled_idx = np.random.permutation(len(X_train)) hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches) y_batches = np.array_split(y_train[shuffled_idx], n_batches) for hidden2_batch, y_batch in zip(hidden2_batches, y_batches): sess.run(training_op, feed_dict=&#123;hidden2:hidden2_batch, y:y_batch&#125;) accuracy_val = accuracy.eval(feed_dict=&#123;hidden2: h2_cache_valid, y: y_valid&#125;) print(epoch, "Validation accuracy:", accuracy_val) save_path = saver.save(sess, "dnn/my_new_cache_freeze_low_layer_model_final.ckpt") INFO:tensorflow:Restoring parameters from dnn/clip_model_final.ckpt 0 Validation accuracy: 0.9006 1 Validation accuracy: 0.9346 2 Validation accuracy: 0.9444 3 Validation accuracy: 0.9478 4 Validation accuracy: 0.9516 5 Validation accuracy: 0.952 6 Validation accuracy: 0.9522 7 Validation accuracy: 0.9532 8 Validation accuracy: 0.9546 9 Validation accuracy: 0.9558 10 Validation accuracy: 0.955 11 Validation accuracy: 0.9554 12 Validation accuracy: 0.9558 13 Validation accuracy: 0.9572 14 Validation accuracy: 0.957 15 Validation accuracy: 0.9572 16 Validation accuracy: 0.9568 17 Validation accuracy: 0.9592 18 Validation accuracy: 0.958 19 Validation accuracy: 0.9598 更快的优化器Momentum optimization1optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9) Nesterov Accelerated Gradient12optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9, use_nesterov=True) AdaGrad1optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate) RMSProp12optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, momentum=0.9, decay=0.9, epsilon=1e-10) Adam Optimization1optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) Learning Rate Scheduling1234567891011121314151617181920212223242526272829303132333435363738394041424344454647reset_graph()n_inputs = 28 * 28n_hidden1 = 300n_hidden2 = 50n_outputs = 10X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")y = tf.placeholder(tf.int32, shape=(None), name="y")with tf.name_scope("dnn"): hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name="hidden1") hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name="hidden2") logits = tf.layers.dense(hidden2, n_outputs, name="outputs")with tf.name_scope("loss"): xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) loss = tf.reduce_mean(xentropy, name="loss")with tf.name_scope("eval"): correct = tf.nn.in_top_k(logits, y, 1) accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name="accuracy")with tf.name_scope("train"): initial_learning_rate = 0.1 decay_steps = 10000 decay_rate = 1/10 global_step = tf.Variable(0, trainable=False, name="global_step") learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, decay_steps, decay_rate) optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9) training_op = optimizer.minimize(loss, global_step=global_step)init = tf.global_variables_initializer()saver = tf.train.Saver()n_epochs = 5batch_size = 50with tf.Session() as sess: init.run() for epoch in range(n_epochs): for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;) accuracy_val = accuracy.eval(feed_dict=&#123;X: X_valid, y: y_valid&#125;) print(epoch, "Validation accuracy:", accuracy_val) save_path = saver.save(sess, "dnn/learning_rate_scheduling_model_final.ckpt") 0 Validation accuracy: 0.9574 1 Validation accuracy: 0.9716 2 Validation accuracy: 0.973 3 Validation accuracy: 0.9798 4 Validation accuracy: 0.9816 通过正规化避免过拟合$l_1$ 和 $l_2$ 正规化 手动实现 $l_1$ 正规化 123456789101112131415161718192021222324252627282930313233343536reset_graph()n_inputs = 28 * 28n_hidden1 = 300n_outputs = 10X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")y = tf.placeholder(tf.int32, shape=(None), name="y")with tf.name_scope("dnn"): hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name="hidden1") logits = tf.layers.dense(hidden1, n_outputs, name="outputs")W1 = tf.get_default_graph().get_tensor_by_name("hidden1/kernel:0")W2 = tf.get_default_graph().get_tensor_by_name("outputs/kernel:0")scale = 0.001with tf.name_scope("loss"): xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) base_loss = tf.reduce_mean(xentropy, name="avg_xentropy") reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2)) loss = tf.add(base_loss, scale * reg_losses, name="loss")with tf.name_scope("eval"): correct = tf.nn.in_top_k(logits, y, 1) accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name="accuracy")learning_rate = 0.01with tf.name_scope("train"): optimizer = tf.train.GradientDescentOptimizer(learning_rate) training_op = optimizer.minimize(loss)init = tf.global_variables_initializer()saver = tf.train.Saver() 1234567891011n_epoches = 20batch_size = 200with tf.Session() as sess: init.run() for epoch in range(n_epoches): for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;) accuracy_val = accuracy.eval(feed_dict=&#123;X: X_valid, y: y_valid&#125;) print(epoch, "Validation accuracy:", accuracy_val) save_path = saver.save(sess,"dnn/l_1_model_final.ckpt") 0 Validation accuracy: 0.831 1 Validation accuracy: 0.871 2 Validation accuracy: 0.8838 3 Validation accuracy: 0.8934 4 Validation accuracy: 0.8966 5 Validation accuracy: 0.8988 6 Validation accuracy: 0.9016 7 Validation accuracy: 0.9044 8 Validation accuracy: 0.9058 9 Validation accuracy: 0.906 10 Validation accuracy: 0.9068 11 Validation accuracy: 0.9054 12 Validation accuracy: 0.907 13 Validation accuracy: 0.9084 14 Validation accuracy: 0.9088 15 Validation accuracy: 0.9064 16 Validation accuracy: 0.9068 17 Validation accuracy: 0.9066 18 Validation accuracy: 0.9066 19 Validation accuracy: 0.9052 使用正规化方法 123456789101112131415161718192021222324252627282930313233343536reset_graph()n_inputs = 28 * 28n_hidden1 = 300n_outputs = 10X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")y = tf.placeholder(tf.int32, shape=(None), name="y")with tf.name_scope("dnn"): hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name="hidden1") logits = tf.layers.dense(hidden1, n_outputs, name="outputs")W1 = tf.get_default_graph().get_tensor_by_name("hidden1/kernel:0")W2 = tf.get_default_graph().get_tensor_by_name("outputs/kernel:0")scale = 0.001with tf.name_scope("loss"): xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) base_loss = tf.reduce_mean(xentropy, name="avg_xentropy") reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2)) loss = tf.add(base_loss, scale * reg_losses, name="loss")with tf.name_scope("eval"): correct = tf.nn.in_top_k(logits, y, 1) accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name="accuracy")learning_rate = 0.01with tf.name_scope("train"): optimizer = tf.train.GradientDescentOptimizer(learning_rate) training_op = optimizer.minimize(loss)init = tf.global_variables_initializer()saver = tf.train.Saver() 1234567891011n_epoches = 20batch_size = 200with tf.Session() as sess: init.run() for epoch in range(n_epoches): for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;) accuracy_val = accuracy.eval(feed_dict=&#123;X: X_valid, y: y_valid&#125;) print(epoch, "Validation accuracy:", accuracy_val) save_path = saver.save(sess,"dnn/l_1_model_final.ckpt") 0 Validation accuracy: 0.831 1 Validation accuracy: 0.871 2 Validation accuracy: 0.8838 3 Validation accuracy: 0.8934 4 Validation accuracy: 0.8966 5 Validation accuracy: 0.8988 6 Validation accuracy: 0.9016 7 Validation accuracy: 0.9044 8 Validation accuracy: 0.9058 9 Validation accuracy: 0.906 10 Validation accuracy: 0.9068 11 Validation accuracy: 0.9054 12 Validation accuracy: 0.907 13 Validation accuracy: 0.9084 14 Validation accuracy: 0.9088 15 Validation accuracy: 0.9064 16 Validation accuracy: 0.9068 17 Validation accuracy: 0.9066 18 Validation accuracy: 0.9066 19 Validation accuracy: 0.9052 12345678910111213141516171819202122232425262728293031323334353637383940reset_graph()from functools import partialn_inputs = 28 * 28n_hidden1 = 300n_hidden2 = 50n_outputs = 10X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")y = tf.placeholder(tf.int32, shape=(None), name="y")scale = 0.001my_dense_layer = partial( tf.layers.dense, activation=tf.nn.relu, kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))with tf.name_scope("dnn"): hidden1 = my_dense_layer(X, n_hidden1, name="hidden1") hidden2 = my_dense_layer(hidden1, n_hidden2, name="hidden2") logits = my_dense_layer(hidden2, n_outputs, activation=None, name="outputs")with tf.name_scope("loss"): xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) base_loss = tf.reduce_mean(xentropy, name="avg_xentropy") reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES) loss = tf.add_n([base_loss] + reg_losses, name="loss")with tf.name_scope("eval"): correct = tf.nn.in_top_k(logits, y, 1) accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name="accuracy")learning_rate = 0.01with tf.name_scope("train"): optimizer = tf.train.GradientDescentOptimizer(learning_rate) training_op = optimizer.minimize(loss)init = tf.global_variables_initializer()saver = tf.train.Saver() 1234567891011n_epoches = 20batch_size = 200with tf.Session() as sess: init.run() for epoch in range(n_epoches): for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;) accuracy_val = accuracy.eval(feed_dict=&#123;X: X_valid, y: y_valid&#125;) print(epoch, "Validation accuracy:", accuracy_val) save_path = saver.save(sess,"dnn/l_1_function_model_final.ckpt") 0 Validation accuracy: 0.8274 1 Validation accuracy: 0.8766 2 Validation accuracy: 0.8952 3 Validation accuracy: 0.9016 4 Validation accuracy: 0.908 5 Validation accuracy: 0.9096 6 Validation accuracy: 0.9124 7 Validation accuracy: 0.9154 8 Validation accuracy: 0.9178 9 Validation accuracy: 0.919 10 Validation accuracy: 0.92 11 Validation accuracy: 0.9224 12 Validation accuracy: 0.9212 13 Validation accuracy: 0.9228 14 Validation accuracy: 0.9222 15 Validation accuracy: 0.9218 16 Validation accuracy: 0.9218 17 Validation accuracy: 0.9228 18 Validation accuracy: 0.9216 19 Validation accuracy: 0.9214 DropOut1234reset_graph()X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")y = tf.placeholder(tf.int32, shape=(None), name="y") 123456789101112131415161718192021222324252627282930training = tf.placeholder_with_default(False, shape=(), name="training")dropout_rate = 0.5X_drop = tf.layers.dropout(X, dropout_rate, training=training)with tf.name_scope("dnn"): hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu, name="hidden1") hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training) hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu, name="hidden2") hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training) logits = tf.layers.dense(hidden2_drop, n_outputs, name="outputs")with tf.name_scope("loss"): xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) loss = tf.reduce_mean(xentropy, name="loss")with tf.name_scope("train"): optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9) training_op = optimizer.minimize(loss)with tf.name_scope("eval"): correct = tf.nn.in_top_k(logits, y, 1) accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))init = tf.global_variables_initializer()saver = tf.train.Saver() 1234567891011n_epoches = 20batch_size = 200with tf.Session() as sess: init.run() for epoch in range(n_epoches): for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;) accuracy_val = accuracy.eval(feed_dict=&#123;X: X_valid, y: y_valid&#125;) print(epoch, "Validation accuracy:", accuracy_val) save_path = saver.save(sess,"dnn/dropout_model_final.ckpt") 0 Validation accuracy: 0.923 1 Validation accuracy: 0.9438 2 Validation accuracy: 0.9504 3 Validation accuracy: 0.961 4 Validation accuracy: 0.9654 5 Validation accuracy: 0.9694 6 Validation accuracy: 0.9726 7 Validation accuracy: 0.9736 8 Validation accuracy: 0.9756 9 Validation accuracy: 0.975 10 Validation accuracy: 0.9768 11 Validation accuracy: 0.9782 12 Validation accuracy: 0.976 13 Validation accuracy: 0.9788 14 Validation accuracy: 0.9776 15 Validation accuracy: 0.9802 16 Validation accuracy: 0.9796 17 Validation accuracy: 0.9802 18 Validation accuracy: 0.9806 19 Validation accuracy: 0.9806 Max Norm12345678def max_norm_regularizer(threshold, axes=1, name="max_norm", collection="max_norm"): def max_norm(weights): clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes) clip_weights = tf.assign(weights, clipped, name=name) tf.add_to_collection(collection, clip_weights) return None return max_norm 123456789101112reset_graph()n_inputs = 28 * 28n_hidden1 = 300n_hidden2 = 50n_outputs = 10learning_rate = 0.01momentum = 0.9X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")y = tf.placeholder(tf.int32, shape=(None), name="y") 12345678max_norm_reg = max_norm_regularizer(threshold=1.0)with tf.name_scope("dnn"): hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, kernel_regularizer=max_norm_reg, name="hidden1") hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, kernel_regularizer=max_norm_reg, name="hidden2") logits = tf.layers.dense(hidden2, n_outputs, name="outputs") 1234567891011121314with tf.name_scope("loss"): xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) loss = tf.reduce_mean(xentropy, name="loss")with tf.name_scope("train"): optimizer = tf.train.MomentumOptimizer(learning_rate, momentum) training_op = optimizer.minimize(loss)with tf.name_scope("eval"): correct = tf.nn.in_top_k(logits, y, 1) accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))init = tf.global_variables_initializer()saver = tf.train.Saver() 12n_epoches = 20batch_size = 50 1234567891011clip_all_weights = tf.get_collection("max_norm")with tf.Session() as sess: init.run() for epoch in range(n_epoches): for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;) sess.run(clip_all_weights) acc_valid = accuracy.eval(feed_dict=&#123;X:X_valid, y: y_valid&#125;) print(epoch, "Validation accuracy:", acc_valid) save_path = saver.save(sess, "dnn/max_norm_model_final.ckpt") 0 Validation accuracy: 0.9556 1 Validation accuracy: 0.97 2 Validation accuracy: 0.973 3 Validation accuracy: 0.9758 4 Validation accuracy: 0.9762 5 Validation accuracy: 0.9788 6 Validation accuracy: 0.98 7 Validation accuracy: 0.9824 8 Validation accuracy: 0.9816 9 Validation accuracy: 0.981 10 Validation accuracy: 0.983 11 Validation accuracy: 0.982 12 Validation accuracy: 0.9808 13 Validation accuracy: 0.9828 14 Validation accuracy: 0.982 15 Validation accuracy: 0.9824 16 Validation accuracy: 0.9824 17 Validation accuracy: 0.9824 18 Validation accuracy: 0.982 19 Validation accuracy: 0.9824 源文件]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>Hands-On Machine Learning with Scikit-Learn and TensorFlow</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Introduction to Artificial Neural Networks]]></title>
    <url>%2F2018%2F12%2F17%2FIntroduction-to-Artificial-Neural-Networks%2F</url>
    <content type="text"><![CDATA[感知器1234567891011import numpy as npfrom sklearn.datasets import load_irisfrom sklearn.linear_model import Perceptroniris = load_iris()X = iris.data[:, (2, 3)].astype(np.float32)y = (iris.target == 0).astype(np.int)per_clf = Perceptron(random_state=42, max_iter=50, tol=1e-3)per_clf.fit(X, y)y_pred = per_clf.predict([[2, 0.5]]) 1y_pred array([1]) 123456from sklearn.linear_model import SGDClassifiersgd_clf = SGDClassifier(loss="perceptron", learning_rate="constant", eta0=1, penalty=None, max_iter=50, tol=1e-3)sgd_clf.fit(X, y)y_pred = per_clf.predict([[2, 0.5]]) 1y_pred array([1]) 使用TensorFlow高级API123456789import tensorflow as tf(X_train, y_train),(X_test, y_test) = tf.keras.datasets.mnist.load_data()X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0y_train = y_train.astype(np.int32)y_test = y_test.astype(np.int32)X_valid, X_train = X_train[:5000], X_train[5000:]y_valid, y_train = y_train[:5000], y_train[5000:] 1234feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(X_train)dnn_clf = tf.contrib.learn.DNNClassifier(hidden_units=[300, 100], n_classes=10, feature_columns=feature_columns)dnn_clf.fit(X_train, y_train, batch_size=50, steps=40000) INFO:tensorflow:Using default config. WARNING:tensorflow:Using temporary folder as model directory: C:\Users\deng.xj\AppData\Local\Temp\tmpoqlzthaw INFO:tensorflow:Using config: {&#39;_task_type&#39;: None, &#39;_task_id&#39;: 0, &#39;_cluster_spec&#39;: &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x000001BB66D724A8&gt;, &#39;_master&#39;: &#39;&#39;, &#39;_num_ps_replicas&#39;: 0, &#39;_num_worker_replicas&#39;: 0, &#39;_environment&#39;: &#39;local&#39;, &#39;_is_chief&#39;: True, &#39;_evaluation_master&#39;: &#39;&#39;, &#39;_train_distribute&#39;: None, &#39;_eval_distribute&#39;: None, &#39;_device_fn&#39;: None, &#39;_tf_config&#39;: gpu_options { per_process_gpu_memory_fraction: 1.0 } , &#39;_tf_random_seed&#39;: None, &#39;_save_summary_steps&#39;: 100, &#39;_save_checkpoints_secs&#39;: 600, &#39;_log_step_count_steps&#39;: 100, &#39;_protocol&#39;: None, &#39;_session_config&#39;: None, &#39;_save_checkpoints_steps&#39;: None, &#39;_keep_checkpoint_max&#39;: 5, &#39;_keep_checkpoint_every_n_hours&#39;: 10000, &#39;_model_dir&#39;: &#39;C:\\Users\\deng.xj\\AppData\\Local\\Temp\\tmpoqlzthaw&#39;} WARNING:tensorflow:From &lt;ipython-input-6-90ea1841712a&gt;:4: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01. Instructions for updating: Estimator is decoupled from Scikit Learn interface by moving into separate class SKCompat. Arguments x, y and batch_size are only available in the SKCompat class, Estimator will only accept input_fn. Example conversion: est = Estimator(...) -&gt; est = SKCompat(Estimator(...)) WARNING:tensorflow:From &lt;ipython-input-6-90ea1841712a&gt;:4: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01. Instructions for updating: Estimator is decoupled from Scikit Learn interface by moving into separate class SKCompat. Arguments x, y and batch_size are only available in the SKCompat class, Estimator will only accept input_fn. Example conversion: est = Estimator(...) -&gt; est = SKCompat(Estimator(...)) WARNING:tensorflow:From &lt;ipython-input-6-90ea1841712a&gt;:4: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with batch_size is deprecated and will be removed after 2016-12-01. Instructions for updating: Estimator is decoupled from Scikit Learn interface by moving into separate class SKCompat. Arguments x, y and batch_size are only available in the SKCompat class, Estimator will only accept input_fn. Example conversion: est = Estimator(...) -&gt; est = SKCompat(Estimator(...)) WARNING:tensorflow:From e:\python\python36\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\estimator.py:509: SKCompat.__init__ (from tensorflow.contrib.learn.python.learn.estimators.estimator) is deprecated and will be removed in a future version. Instructions for updating: Please switch to the Estimator interface. WARNING:tensorflow:From e:\python\python36\lib\site-packages\tensorflow\contrib\learn\python\learn\learn_io\data_feeder.py:102: extract_pandas_labels (from tensorflow.contrib.learn.python.learn.learn_io.pandas_io) is deprecated and will be removed in a future version. Instructions for updating: Please access pandas data directly. WARNING:tensorflow:From e:\python\python36\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\head.py:678: ModelFnOps.__new__ (from tensorflow.contrib.learn.python.learn.estimators.model_fn) is deprecated and will be removed in a future version. Instructions for updating: When switching to tf.estimator.Estimator, use tf.estimator.EstimatorSpec. You can use the `estimator_spec` method to create an equivalent one. INFO:tensorflow:Create CheckpointSaverHook. INFO:tensorflow:Graph was finalized. INFO:tensorflow:Running local_init_op. INFO:tensorflow:Done running local_init_op. INFO:tensorflow:Saving checkpoints for 0 into C:\Users\deng.xj\AppData\Local\Temp\tmpoqlzthaw\model.ckpt. INFO:tensorflow:loss = 2.292883, step = 1 INFO:tensorflow:global_step/sec: 238.165 INFO:tensorflow:loss = 0.32534814, step = 101 (0.422 sec) INFO:tensorflow:global_step/sec: 338.742 ... INFO:tensorflow:loss = 0.0007182892, step = 39601 (0.298 sec) INFO:tensorflow:global_step/sec: 333.114 INFO:tensorflow:loss = 0.00014897775, step = 39701 (0.300 sec) INFO:tensorflow:global_step/sec: 342.208 INFO:tensorflow:loss = 0.0010287359, step = 39801 (0.292 sec) INFO:tensorflow:global_step/sec: 333.116 INFO:tensorflow:loss = 0.0009217943, step = 39901 (0.300 sec) INFO:tensorflow:Saving checkpoints for 40000 into C:\Users\deng.xj\AppData\Local\Temp\tmpoqlzthaw\model.ckpt. INFO:tensorflow:Loss for final step: 0.00035940565. DNNClassifier(params={&#39;head&#39;: &lt;tensorflow.contrib.learn.python.learn.estimators.head._MultiClassHead object at 0x000001BB6821B668&gt;, &#39;hidden_units&#39;: [300, 100], &#39;feature_columns&#39;: (_RealValuedColumn(column_name=&#39;&#39;, dimension=784, default_value=None, dtype=tf.float32, normalizer=None),), &#39;optimizer&#39;: None, &#39;activation_fn&#39;: &lt;function relu at 0x000001BB6378FF28&gt;, &#39;dropout&#39;: None, &#39;gradient_clip_norm&#39;: None, &#39;embedding_lr_multipliers&#39;: None, &#39;input_layer_min_slice_size&#39;: None}) 123from sklearn.metrics import accuracy_scorey_pred = list(dnn_clf.predict(X_test))accuracy_score(y_test,y_pred) WARNING:tensorflow:From e:\python\python36\lib\site-packages\tensorflow\python\util\deprecation.py:553: calling DNNClassifier.predict (from tensorflow.contrib.learn.python.learn.estimators.dnn) with outputs=None is deprecated and will be removed after 2017-03-01. Instructions for updating: Please switch to predict_classes, or set `outputs` argument. WARNING:tensorflow:From e:\python\python36\lib\site-packages\tensorflow\contrib\learn\python\learn\estimators\dnn.py:463: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01. Instructions for updating: Estimator is decoupled from Scikit Learn interface by moving into separate class SKCompat. Arguments x, y and batch_size are only available in the SKCompat class, Estimator will only accept input_fn. Example conversion: est = Estimator(...) -&gt; est = SKCompat(Estimator(...)) INFO:tensorflow:Graph was finalized. INFO:tensorflow:Restoring parameters from C:\Users\deng.xj\AppData\Local\Temp\tmpoqlzthaw\model.ckpt-40000 INFO:tensorflow:Running local_init_op. INFO:tensorflow:Done running local_init_op. 0.9814 1dnn_clf.evaluate(X_test, y_test) WARNING:tensorflow:From &lt;ipython-input-8-862f84b3278e&gt;:1: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01. Instructions for updating: Estimator is decoupled from Scikit Learn interface by moving into separate class SKCompat. Arguments x, y and batch_size are only available in the SKCompat class, Estimator will only accept input_fn. Example conversion: est = Estimator(...) -&gt; est = SKCompat(Estimator(...)) WARNING:tensorflow:From &lt;ipython-input-8-862f84b3278e&gt;:1: calling BaseEstimator.evaluate (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01. Instructions for updating: Estimator is decoupled from Scikit Learn interface by moving into separate class SKCompat. Arguments x, y and batch_size are only available in the SKCompat class, Estimator will only accept input_fn. Example conversion: est = Estimator(...) -&gt; est = SKCompat(Estimator(...)) INFO:tensorflow:Starting evaluation at 2018-12-17-02:39:23 INFO:tensorflow:Graph was finalized. INFO:tensorflow:Restoring parameters from C:\Users\deng.xj\AppData\Local\Temp\tmpoqlzthaw\model.ckpt-40000 INFO:tensorflow:Running local_init_op. INFO:tensorflow:Done running local_init_op. INFO:tensorflow:Finished evaluation at 2018-12-17-02:39:23 INFO:tensorflow:Saving dict for global step 40000: accuracy = 0.9814, global_step = 40000, loss = 0.07299631 {&#39;accuracy&#39;: 0.9814, &#39;global_step&#39;: 40000, &#39;loss&#39;: 0.07299631} 手写DNN数据构造1tf.reset_default_graph() 1234567import tensorflow as tfimport tensorboard as tbn_inputs = 28*28n_hidden1 = 300n_hidden2 = 100n_outputs = 10 12X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")y = tf.placeholder(tf.int32, shape=(None), name="y") 12345678910111213def neuron_layer(X, n_neurons, name, activation=None): with tf.name_scope(name): # 每一层建立一个名字空间 n_inputs = int(X.get_shape()[1])# 获取特征数 stddev = 2 / np.sqrt(n_inputs) init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev) W = tf.Variable(init, name="weights") b = tf.Variable(tf.zeros([n_neurons]), name="biases") z = tf.matmul(X, W) + b if activation=="relu": return tf.nn.relu(z) else: return z 1234with tf.name_scope("dnn"): hidden1 = neuron_layer(X, n_hidden1, "hidden1", activation="relu") hidden2 = neuron_layer(hidden1, n_hidden2, "hidden2", activation="relu") logits =neuron_layer(hidden2, n_outputs, "outputs") 123with tf.name_scope("loss"): xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits) loss = tf.reduce_mean(xentropy, name="loss") 12345learning_rate = 0.01with tf.name_scope("train"): optimizer = tf.train.GradientDescentOptimizer(learning_rate) training_op = optimizer.minimize(loss) 123with tf.name_scope("eval"): correct = tf.nn.in_top_k(logits, y, 1) accuracy = tf.reduce_mean(tf.cast(correct, tf.float32)) 12init = tf.global_variables_initializer()saver = tf.train.Saver() 执行123456789n_epochs = 40batch_size = 50def shuffle_batch(X, y, batch_size): rnd_idx = np.random.permutation(len(X)) n_batches = len(X) // batch_size for batch_idx in np.array_split(rnd_idx, n_batches): X_batch, y_batch = X[batch_idx], y[batch_idx] yield X_batch, y_batch 123456789with tf.Session() as sess: init.run() for epoch in range(n_epochs): for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;) acc_batch = accuracy.eval(feed_dict=&#123;X: X_batch, y: y_batch&#125;) acc_val = accuracy.eval(feed_dict=&#123;X: X_valid, y: y_valid&#125;) print(epoch, "Batch accuracy:", acc_batch, "Val accuracy:", acc_val) save_path = saver.save(sess, "./dnn/my_model_final.ckpt") 0 Batch accuracy: 0.94 Val accuracy: 0.911 1 Batch accuracy: 1.0 Val accuracy: 0.9316 2 Batch accuracy: 0.9 Val accuracy: 0.943 3 Batch accuracy: 0.92 Val accuracy: 0.9484 4 Batch accuracy: 0.92 Val accuracy: 0.9528 5 Batch accuracy: 0.94 Val accuracy: 0.9556 6 Batch accuracy: 0.98 Val accuracy: 0.957 7 Batch accuracy: 0.94 Val accuracy: 0.961 8 Batch accuracy: 0.98 Val accuracy: 0.9636 9 Batch accuracy: 1.0 Val accuracy: 0.9662 10 Batch accuracy: 0.98 Val accuracy: 0.966 11 Batch accuracy: 0.98 Val accuracy: 0.9688 12 Batch accuracy: 1.0 Val accuracy: 0.9702 13 Batch accuracy: 0.96 Val accuracy: 0.9706 14 Batch accuracy: 0.98 Val accuracy: 0.9724 15 Batch accuracy: 0.98 Val accuracy: 0.972 16 Batch accuracy: 1.0 Val accuracy: 0.9724 17 Batch accuracy: 0.96 Val accuracy: 0.9752 18 Batch accuracy: 1.0 Val accuracy: 0.9746 19 Batch accuracy: 1.0 Val accuracy: 0.9736 20 Batch accuracy: 0.98 Val accuracy: 0.9766 21 Batch accuracy: 0.98 Val accuracy: 0.976 22 Batch accuracy: 0.98 Val accuracy: 0.9778 23 Batch accuracy: 0.98 Val accuracy: 0.976 24 Batch accuracy: 1.0 Val accuracy: 0.976 25 Batch accuracy: 1.0 Val accuracy: 0.9768 26 Batch accuracy: 0.98 Val accuracy: 0.976 27 Batch accuracy: 0.98 Val accuracy: 0.9774 28 Batch accuracy: 1.0 Val accuracy: 0.9782 29 Batch accuracy: 1.0 Val accuracy: 0.9772 30 Batch accuracy: 0.98 Val accuracy: 0.9764 31 Batch accuracy: 0.98 Val accuracy: 0.9778 32 Batch accuracy: 0.98 Val accuracy: 0.9792 33 Batch accuracy: 1.0 Val accuracy: 0.9776 34 Batch accuracy: 0.98 Val accuracy: 0.9786 35 Batch accuracy: 1.0 Val accuracy: 0.979 36 Batch accuracy: 1.0 Val accuracy: 0.9782 37 Batch accuracy: 1.0 Val accuracy: 0.9798 38 Batch accuracy: 1.0 Val accuracy: 0.9792 39 Batch accuracy: 1.0 Val accuracy: 0.9788 使用神经网络12345with tf.Session() as sess: saver.restore(sess, "./dnn/my_model_final.ckpt") X_new_scaled = X_test[:20] Z = logits.eval(feed_dict=&#123;X: X_new_scaled&#125;) y_pred = np.argmax(Z, axis=1) INFO:tensorflow:Restoring parameters from ./dnn/my_model_final.ckpt 12print("Predicted classes:", y_pred)print("Actual classes:", y_test[:20]) Predicted classes: [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4] Actual classes: [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4] 源文件]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>Hands-On Machine Learning with Scikit-Learn and TensorFlow</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Up and Runing with TensorFlow]]></title>
    <url>%2F2018%2F12%2F13%2FUp-and-Runing-with-TensorFlow%2F</url>
    <content type="text"><![CDATA[Hello World123456import tensorflow as tf# 非立即执行而是创建一个图x = tf.Variable(3, name="x")y = tf.Variable(4, name="y")f = x*x*y + y + 2 123456# 打开session执行sess = tf.Session()sess.run(x.initializer)sess.run(y.initializer)result = sess.run(f)print(result) 42 12# 关闭sessionsess.close() 123456# 使用块结构运行with tf.Session() as sess: x.initializer.run() y.initializer.run() result = f.eval() print(result) 42 1234567# 使用global_variables_initialzer创建全局初始化init = tf.global_variables_initializer()with tf.Session() as sess: init.run() result = f.eval() print(result) 42 123456# 使用交互式sessionsess = tf.InteractiveSession()init.run()result = f.eval()print(result)sess.close() 42 管理图12x1 = tf.Variable(1)x1.graph is tf.get_default_graph() True 1234# 创建独立的图graph = tf.Graph()with graph.as_default(): x2 = tf.Variable(2) 1x2.graph is graph True 1x2.graph is tf.get_default_graph() False 节点的生命周期1234w = tf.constant(3)x = w + 2y = x + 5z = x + 3 1234#未重复利用结果w和xwith tf.Session() as sess: print(y.eval()) print(z.eval()) 10 8 12345# 将y和z放在同一个图中以重复利用w和xwith tf.Session() as sess: y_val, z_val = sess.run([y, z]) print(y_val) print(z_val) 10 8 使用TensorFlow训练线性回归12345678910111213141516import numpy as npfrom sklearn.datasets import fetch_california_housinghousing = fetch_california_housing()m, n = housing.data.shapehousing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name="X")y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name="y")XT = tf.transpose(X)# 使用正规方程计算thetatheta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)with tf.Session() as sess: theta_value = theta.eval() print(theta_value[:5]) [[-3.7185181e+01] [ 4.3633747e-01] [ 9.3952334e-03] [-1.0711310e-01] [ 6.4479220e-01]] 实现梯度下降手动计算梯度123456789101112131415161718192021222324252627282930from sklearn.preprocessing import StandardScalerscaler = StandardScaler()scale_housing_data = scaler.fit_transform(housing.data)scale_housing_data_bias = np.c_[np.ones((m, 1)), scale_housing_data]n_epochs = 1000learning_rate = 0.01X = tf.constant(scale_housing_data_bias, dtype=tf.float32, name="X")y = tf.constant(housing.target.reshape(-1,1), dtype=tf.float32, name="y")theta = tf.Variable(tf.random_normal([n+1, 1], -1.0, 1.0), name="theta")y_pred = tf.matmul(X, theta, name="predictions")error = y_pred - ymse = tf.reduce_mean(tf.square(error), name="mse")gradients = 2/m * tf.matmul(tf.transpose(X), error)training_op = tf.assign(theta, theta-learning_rate*gradients)init = tf.global_variables_initializer()with tf.Session() as sess: sess.run(init) for epoch in range(n_epochs): if epoch % 100 == 0: print("Epoch", epoch, "MSE=", mse.eval()) sess.run(training_op) best_theta = theta.eval() print(best_theta[:5]) Epoch 0 MSE= 8.3278 Epoch 100 MSE= 0.78606343 Epoch 200 MSE= 0.64422286 Epoch 300 MSE= 0.6223913 Epoch 400 MSE= 0.6058846 Epoch 500 MSE= 0.59219676 Epoch 600 MSE= 0.58081675 Epoch 700 MSE= 0.57135147 Epoch 800 MSE= 0.5634769 Epoch 900 MSE= 0.5569242 [[2.0685523 ] [0.6491688 ] [0.08126644] [0.06857597] [0.03242581]] 使用自动微分1gradients = tf.gradients(mse, [theta])[0] 使用优化器123# 梯度下降优化器optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)training_op = optimizer.minimize(mse) 12# 动量优化器optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9) 输送数据123456# 使用占位符A = tf.placeholder(tf.float32, shape=(None, 3))B = A + 5with tf.Session() as sess: B_val_1 = B.eval(feed_dict=&#123;A: [[1, 2, 3]]&#125;) B_val_2 = B.eval(feed_dict=&#123;A: [[4, 5, 6], [7, 8, 9]]&#125;) 1print(B_val_1) [[6. 7. 8.]] 1print(B_val_2) [[ 9. 10. 11.] [12. 13. 14.]] 12345678910111213141516171819202122# 小批量梯度下降X = tf.placeholder(tf.float32, shape=(None, n+1), name="X")y = tf.placeholder(tf.float32, shape=(None, 1), name="y")batch_size = 100n_batches = int(np.ceil(m / batch_size))def fetch_batch(epoch, batch_index, batch_size): np.random.seed(epoch * n_batches + batch_index) indices = np.random.randint(m, size=batch_size) X_batch = scale_housing_data_bias[indices] y_batch = housing.target.reshape(-1, 1)[indices] return X_batch, y_batchwith tf.Session() as sess: sess.run(init) for epoch in range(n_epochs): for batch_index in range(n_batches): X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size) sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;) best_theta = theta.eval() 1best_theta array([[ 2.0685525 ], [ 0.8296056 ], [ 0.11874896], [-0.26550332], [ 0.3056771 ], [-0.00450377], [-0.03932568], [-0.89991784], [-0.87057173]], dtype=float32) 保存和加载模型1234567891011121314151617181920212223242526n_epochs = 1000learning_rate = 0.01X = tf.constant(scale_housing_data_bias, dtype=tf.float32, name="X")y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name="y")theta = tf.Variable(tf.random_normal([n+1, 1], -1.0, 1.0, seed=42), name="theta")y_pred = tf.matmul(X, theta, name="predictions")error = y_pred - ymse = tf.reduce_mean(tf.square(error), name="mse")optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)training_op = optimizer.minimize(mse)init = tf.global_variables_initializer()saver = tf.train.Saver()with tf.Session() as sess: sess.run(init) for epoch in range(n_epochs): if epoch % 100 == 0: print("epoch", epoch, "MSE=", mse.eval()) save_path = saver.save(sess, "tmp/my_model.ckpt") sess.run(training_op) best_theta = theta.eval() save_path = saver.save(sess, 'tmp/my_model_final.ckpt') epoch 0 MSE= 34.112576 epoch 100 MSE= 1.2404147 epoch 200 MSE= 0.62713087 epoch 300 MSE= 0.58235973 epoch 400 MSE= 0.5691915 epoch 500 MSE= 0.5602364 epoch 600 MSE= 0.55325645 epoch 700 MSE= 0.54772204 epoch 800 MSE= 0.54330814 epoch 900 MSE= 0.53977317 1best_theta array([[ 2.0685523 ], [ 0.68664634], [ 0.10857289], [ 0.03570269], [ 0.04292491], [-0.00635225], [-0.0354162 ], [-1.1083173 ], [-1.0608274 ]], dtype=float32) 1234# 加载模型with tf.Session() as sess: saver.restore(sess, "tmp/my_model_final.ckpt") best_theta_restored = theta.eval() INFO:tensorflow:Restoring parameters from tmp/my_model_final.ckpt 1np.allclose(best_theta, best_theta_restored) True 12#保存指定变量saver = tf.train.Saver(&#123;"weights": theta&#125;) 使用TensorBoard12345from datetime import datetimenow = datetime.utcnow().strftime("%Y%m%d%H%M%S")root_logdir = "tf_logs"logdir = "&#123;&#125;/run-&#123;&#125;/".format(root_logdir, now) 12345678910111213n_epochs = 1000learning_rate = 0.01X = tf.placeholder(tf.float32, shape=(None, n + 1), name="X")y = tf.placeholder(tf.float32, shape=(None, 1), name="y")theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name="theta")y_pred = tf.matmul(X, theta, name="predictions")error = y_pred - ymse = tf.reduce_mean(tf.square(error), name="mse")optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)training_op = optimizer.minimize(mse)init = tf.global_variables_initializer() 12mse_summary = tf.summary.scalar("MSE", mse)file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph()) 123n_epochs = 10batch_size = 100n_batches = int(np.ceil(m / batch_size)) 123456789101112with tf.Session() as sess: sess.run(init) for epoch in range(n_epochs): for batch_index in range(n_batches): X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size) if batch_index% 10 == 0: summary_str = mse_summary.eval(feed_dict=&#123;X: X_batch, y: y_batch&#125;) step = epoch * n_batches + batch_index file_writer.add_summary(summary_str, step) sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;) best_theta = theta.eval() 1file_writer.close() 1best_theta array([[ 2.070016 ], [ 0.8204561 ], [ 0.1173173 ], [-0.22739051], [ 0.3113402 ], [ 0.00353193], [-0.01126994], [-0.91643935], [-0.8795008 ]], dtype=float32) 名称作用域123with tf.name_scope("loss") as scope: error = y_pred - y mse = tf.reduce_mean(tf.square(error), name="mse") 1print(error.op.name) loss/sub 1print(mse.op.name) loss/mse 模块性1tf.reset_default_graph() 123456789101112131415n_features = 3X = tf.placeholder(tf.float32, shape=(None, n_features), name="X")w1 = tf.Variable(tf.random_normal((n_features, 1)), name="weights1")w2 = tf.Variable(tf.random_normal((n_features, 1)), name="weights2")b1 = tf.Variable(0.0, name="bias1")b2 = tf.Variable(0.0, name="bias2")z1 = tf.add(tf.matmul(X, w1), b1, name="z1")z2 = tf.add(tf.matmul(X, w2), b2, name="z2")relu1 = tf.maximum(z1, 0, name="relu1")relu2 = tf.maximum(z2, 0, name="relu2")output = tf.add(relu1, relu2, name="output") 12345678910111213#Don't Repeat Yourselftf.reset_default_graph()def relu(X): w_shape = (int(X.get_shape()[1]), 1) w = tf.Variable(tf.random_normal(w_shape), name="weights") b = tf.Variable(0.0, name="bias") z = tf.add(tf.matmul(X, w), b, name="z") return tf.maximum(z, 0, name="relu")n_features = 3X = tf.placeholder(tf.float32, shape=(None, n_features), name="X")relus = [relu(X) for i in range(5)]output = tf.add_n(relus, name="output") 1file_writer = tf.summary.FileWriter("logs/relu1", tf.get_default_graph()) 1234567891011121314tf.reset_default_graph()def relu(X): with tf.name_scope("relu"): w_shape = (int(X.get_shape()[1]), 1) w = tf.Variable(tf.random_normal(w_shape), name="weights") b = tf.Variable(0.0, name="bias") z = tf.add(tf.matmul(X, w), b, name="z") return tf.maximum(z, 0, name="max")n_features = 3X = tf.placeholder(tf.float32, shape=(None, n_features), name="X")relus = [relu(X) for i in range(5)]output = tf.add_n(relus, name="output")file_writer = tf.summary.FileWriter("logs/relu2", tf.get_default_graph()) 共享变量123456789101112131415tf.reset_default_graph()def relu(X, threshold): with tf.name_scope("relu"): w_shape = (int(X.get_shape()[1]), 1) w = tf.Variable(tf.random_normal(w_shape), name="weights") b = tf.Variable(0.0, name="bias") z = tf.add(tf.matmul(X, w), b, name="z") return tf.maximum(z, threshold, name="max")threshold = tf.Variable(0.0, name="threshold")X = tf.placeholder(tf.float32, shape=(None, n_features), name="X")relus = [relu(X, threshold) for i in range(5)]output = tf.add_n(relus, name="output")file_writer = tf.summary.FileWriter("logs/relu_threshold", tf.get_default_graph()) 12with tf.variable_scope("relu"): threshold = tf.get_variable("threshold", shape=(), initializer=tf.constant_initializer(0.0)) 12with tf.variable_scope("relu", reuse=True):#重复使用 threshold = tf.get_variable("threshold") 123with tf.variable_scope("relu") as scope: scope.reuse_variables() threshold = tf.get_variable("threshold") 123456789101112131415161718tf.reset_default_graph()def relu(X): with tf.variable_scope("relu", reuse=True): threshold = tf.get_variable("threshold") w_shape = int(X.get_shape()[1]), 1 w = tf.Variable(tf.random_normal(w_shape), name="weights") b = tf.Variable(0.0, name='bias') z = tf.add(tf.matmul(X, w), b, name='z') return tf.maximum(z, threshold, name="max")X = tf.placeholder(tf.float32, shape=(None, n_features), name="X")with tf.variable_scope("relu"): threshold = tf.get_variable("threshold", shape=(), initializer=tf.constant_initializer(0.0))relus = [relu(X) for relu_index in range(5)]output = tf.add_n(relus, name="output")file_writer = tf.summary.FileWriter("logs/relu6", tf.get_default_graph()) 1234567891011121314151617tf.reset_default_graph()def relu(X): threshold = tf.get_variable("threshold", shape=(), initializer=tf.constant_initializer()) w_shape = int(X.get_shape()[1]), 1 w = tf.Variable(tf.random_normal(w_shape), name="weights") b = tf.Variable(0.0, name='bias') z = tf.add(tf.matmul(X, w), b, name='z') return tf.maximum(z, threshold, name="max")X = tf.placeholder(tf.float32, shape=(None, n_features), name="X")relus = []for relu_index in range(5): with tf.variable_scope("relu", reuse=(relu_index &gt;= 1)) as scope: relus.append(relu(X))output = tf.add_n(relus, name="output")file_writer = tf.summary.FileWriter("logs/relu7", tf.get_default_graph()) 源文件]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Hands-On Machine Learning with Scikit-Learn and TensorFlow</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dimensionality Reduction]]></title>
    <url>%2F2018%2F12%2F12%2FDimensionality-Reduction%2F</url>
    <content type="text"><![CDATA[123456789101112import numpy as npnp.random.seed(4)m = 60w1, w2 = 0.1, 0.3noise = 0.1angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5X = np.empty((m, 3))X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m) PCA1234X_centred = X - X.mean(axis=0)U, s, V = np.linalg.svd(X_centred)c1 = V.T[:, 0]c2 = V.T[:, 1] 降维123W2 = V.T[:, :2]X2D = X_centred.dot(W2)X2D[:5] array([[-1.26203346, -0.42067648], [ 0.08001485, 0.35272239], [-1.17545763, -0.36085729], [-0.89305601, 0.30862856], [-0.73016287, 0.25404049]]) 使用sklearn12345from sklearn.decomposition import PCApca = PCA(n_components=2)X2D = pca.fit_transform(X)X2D[:5] array([[ 1.26203346, 0.42067648], [-0.08001485, -0.35272239], [ 1.17545763, 0.36085729], [ 0.89305601, -0.30862856], [ 0.73016287, -0.25404049]]) 可释方差1print(pca.explained_variance_ratio_) [0.84248607 0.14631839] 选择正确的维度1234pca = PCA()pca.fit(X)cumsum = np.cumsum(pca.explained_variance_ratio_)d = np.argmax(cumsum &gt;= 0.95) + 1 12pca = PCA(n_components=0.95)X_reduced = pca.fit_transform(X) PCA压缩1234567891011from six.moves import urllibfrom sklearn.datasets import fetch_mldatafrom sklearn.model_selection import train_test_splitmnist = fetch_mldata("MNIST original", data_home='./datasets/')X = mnist["data"]y = mnist["target"]X_train, X_test, y_train, y_test = train_test_split(X, y) 123pca = PCA(n_components=154)X_mnist_reduced = pca.fit_transform(X_train)X_mnist_recovered = pca.fit_transform(X_mnist_reduced) 增量PCA12345678from sklearn.decomposition import IncrementalPCAn_batches = 100inc_pca = IncrementalPCA(n_components=154)for X_batch in np.array_split(X_train, n_batches): inc_pca.partial_fit(X_batch)X_mnist_reduced = inc_pca.transform(X_train) 123456filename = "my_mnist.data"m, n = X_train.shapeX_mm = np.memmap(filename, dtype='float32', mode='write', shape=(m, n))X_mm[:] = X_traindel X_mm 12345X_mm = np.memmap(filename, dtype='float32', mode='readonly', shape=(m, n))batch_size = m // n_batchesinc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)inc_pca.fit(X_mm) IncrementalPCA(batch_size=525, copy=True, n_components=154, whiten=False) 随机PCA12rnd_pca = PCA(n_components=154, svd_solver='randomized')X_reduced = rnd_pca.fit_transform(X_train) Kernel PCA1234567# 使用径向核函数from sklearn.decomposition import KernelPCAfrom sklearn.datasets import make_swiss_rollX, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)y = t &gt; 6.9rbf_pca = KernelPCA(n_components=2, kernel='rbf', gamma=0.04)X_reduced = rbf_pca.fit_transform(X) 选取核函数和调整超参数12345678910111213141516from sklearn.model_selection import GridSearchCVfrom sklearn.linear_model import LogisticRegressionfrom sklearn.pipeline import Pipelineclf = Pipeline([ ("kpca", KernelPCA(n_components=2)), ("log_reg", LogisticRegression(solver='lbfgs'))])param_grid = [&#123; "kpca__gamma": np.linspace(0.03, 0.05, 10), "kpca__kernel": ["rbf", "sigmoid"]&#125;]grid_search = GridSearchCV(clf, param_grid, cv=3)grid_search.fit(X, y) GridSearchCV(cv=3, error_score=&#39;raise-deprecating&#39;, estimator=Pipeline(memory=None, steps=[(&#39;kpca&#39;, KernelPCA(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver=&#39;auto&#39;, fit_inverse_transform=False, gamma=None, kernel=&#39;linear&#39;, kernel_params=None, max_iter=None, n_components=2, n_jobs=None, random_state=None, remove_zero_eig=False, tol=0)), (&#39;log_reg&#39;, LogisticRe...enalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False))]), fit_params=None, iid=&#39;warn&#39;, n_jobs=None, param_grid=[{&#39;kpca__gamma&#39;: array([0.03 , 0.03222, 0.03444, 0.03667, 0.03889, 0.04111, 0.04333, 0.04556, 0.04778, 0.05 ]), &#39;kpca__kernel&#39;: [&#39;rbf&#39;, &#39;sigmoid&#39;]}], pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;, scoring=None, verbose=0) 1print(grid_search.best_params_) {&#39;kpca__gamma&#39;: 0.043333333333333335, &#39;kpca__kernel&#39;: &#39;rbf&#39;} 1234# 恢复rbf_pca = KernelPCA(n_components=2, kernel='rbf', gamma=0.0433, fit_inverse_transform=True)X_reduced = rbf_pca.fit_transform(X)X_preimage = rbf_pca.inverse_transform(X_reduced) 12from sklearn.metrics import mean_squared_errormean_squared_error(X, X_preimage) 32.78630879576614 LLE1234from sklearn.manifold import LocallyLinearEmbeddinglle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)X_reduced = lle.fit_transform(X) 源文件]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Hands-On Machine Learning with Scikit-Learn and TensorFlow</tag>
        <tag>sklearn</tag>
        <tag>PCA</tag>
        <tag>维度约简</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ensemble Learning and Random Forests]]></title>
    <url>%2F2018%2F12%2F11%2FEnsemble-Learning-and-Random-Forests%2F</url>
    <content type="text"><![CDATA[选举式分类器12345from sklearn.model_selection import train_test_splitfrom sklearn.datasets import make_moonsX, y = make_moons(n_samples=500, noise=0.30, random_state=42)X_train, X_test, y_train, y_test = train_test_split(X, y , random_state=42) 1234567891011121314from sklearn.ensemble import RandomForestClassifierfrom sklearn.ensemble import VotingClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.svm import SVClog_clf = LogisticRegression(solver='lbfgs')rnd_clf = RandomForestClassifier(n_estimators=10)svm_clf = SVC(gamma='auto')voting_clf = VotingClassifier( estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)], voting='hard')voting_clf.fit(X_train, y_train) VotingClassifier(estimators=[(&#39;lr&#39;, LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class=&#39;warn&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False)), (&#39;rf&#39;, RandomF..., max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False))], flatten_transform=None, n_jobs=None, voting=&#39;hard&#39;, weights=None) 123456from sklearn.metrics import accuracy_scorefor clf in (log_clf, rnd_clf, svm_clf, voting_clf): clf.fit(X_train, y_train) y_pred = clf.predict(X_test) print(clf.__class__.__name__, accuracy_score(y_test, y_pred)) LogisticRegression 0.864 RandomForestClassifier 0.896 SVC 0.888 VotingClassifier 0.896 放回取样和不放回取样1234567891011from sklearn.ensemble import BaggingClassifierfrom sklearn.tree import DecisionTreeClassifierbag_clf = BaggingClassifier( DecisionTreeClassifier(), n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1)bag_clf.fit(X_train, y_train)y_pred = bag_clf.predict(X_test)print(accuracy_score(y_test, y_pred)) 0.92 out-of-bag 评估123456bag_clf = BaggingClassifier( DecisionTreeClassifier(), n_estimators=500, bootstrap=True, n_jobs=-1, oob_score=True)bag_clf.fit(X_train, y_train)bag_clf.oob_score_ 0.8986666666666666 12y_pred = bag_clf.predict(X_test)accuracy_score(y_test, y_pred) 0.888 1bag_clf.oob_decision_function_ array([[0.4010989 , 0.5989011 ], .... [0.04278075, 0.95721925], [0.98876404, 0.01123596], [1. , 0. ], [0.03664921, 0.96335079], [0.62285714, 0.37714286]]) 随机森林123456from sklearn.ensemble import RandomForestClassifierrnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)rnd_clf.fit(X_train, y_train)y_pred_rf = rnd_clf.predict(X_test) 1234bag_clf = BaggingClassifier( DecisionTreeClassifier(splitter='random', max_leaf_nodes=16), n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1) 特征重要性1234567from sklearn.datasets import load_irisiris = load_iris()rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)rnd_clf.fit(iris["data"], iris["target"])for name, score in zip(iris["feature_names"], rnd_clf.feature_importances_): print(name, score) sepal length (cm) 0.10154330050026057 sepal width (cm) 0.02620789074908215 petal length (cm) 0.4514173964983005 petal width (cm) 0.42083141225235715 BoostingAdaBoost12345678910from sklearn.ensemble import AdaBoostClassifierada_clf = AdaBoostClassifier( DecisionTreeClassifier(max_depth=1), n_estimators=200, algorithm="SAMME.R", learning_rate=0.5)ada_clf.fit(X_train, y_train)y_pred = ada_clf.predict(X_test)accuracy_score(y_test, y_pred) 0.896 梯度Boosting1234567891011121314from sklearn.tree import DecisionTreeRegressortree_reg1 = DecisionTreeRegressor(max_depth=2)tree_reg1.fit(X_train, y_train)y2 = y_train - tree_reg1.predict(X_train)tree_reg2 = DecisionTreeRegressor(max_depth=2)tree_reg2.fit(X_train, y2)y3 = y2 - tree_reg2.predict(X_train)tree_reg3 = DecisionTreeRegressor(max_depth=2)tree_reg3.fit(X_train, y3)y_pred = sum(tree.predict(X_test) for tree in (tree_reg1, tree_reg2, tree_reg3)) 1234from sklearn.ensemble import GradientBoostingRegressorgbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)gbrt.fit(X,y) GradientBoostingRegressor(alpha=0.9, criterion=&#39;friedman_mse&#39;, init=None, learning_rate=1.0, loss=&#39;ls&#39;, max_depth=2, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=3, n_iter_no_change=None, presort=&#39;auto&#39;, random_state=None, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False) 12345678910111213141516# 防止过拟合，提前终止import numpy as npfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import mean_squared_errorX_train, X_val, y_train, y_val = train_test_split(X, y)gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)gbrt.fit(X_train, y_train)errors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)]bst_n_estimators = np.argmin(errors)gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=bst_n_estimators)gbrt_best.fit(X_train, y_train) GradientBoostingRegressor(alpha=0.9, criterion=&#39;friedman_mse&#39;, init=None, learning_rate=0.1, loss=&#39;ls&#39;, max_depth=2, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=65, n_iter_no_change=None, presort=&#39;auto&#39;, random_state=None, subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False) 12345678910111213141516gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)min_val_error = float("inf")error_going_up = 0for n_estimators in range(1, 120): gbrt.n_estimators = n_estimators gbrt.fit(X_train, y_train) y_pred = gbrt.predict(X_val) val_error = mean_squared_error(y_val, y_pred) if val_error &lt; min_val_error: min_val_error = val_error error_going_up = 0 else: error_going_up += 1 if error_going_up == 5: break 源文件]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Hands-On Machine Learning with Scikit-Learn and TensorFlow</tag>
        <tag>sklearn</tag>
        <tag>AdaBoost</tag>
        <tag>随机森林</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Decision Tree]]></title>
    <url>%2F2018%2F12%2F10%2FDecision-Tree%2F</url>
    <content type="text"><![CDATA[训练和显示决策树123456789from sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifieriris = load_iris()X = iris.data[:, 2:]y = iris.targettree_clf = DecisionTreeClassifier(max_depth=2)tree_clf.fit(X, y) DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=2, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter=&#39;best&#39;) 1234567891011121314# 生成dot文件from sklearn.tree import export_graphvizimport osdef image_path(fig_id): return os.path.join('.', "images", "decision_trees", fig_id)export_graphviz( tree_clf, out_file=image_path('iris_tree.dot'), feature_names=iris.feature_names[2:], class_names=iris.target_names, rounded=True, filled=True) 安装graphviz dot -Tpng .\images\decision_trees\iris_tree.dot -o .\images\decision_trees\iris_tree.png 预测和评估分类可能性1tree_clf.predict_proba([[5, 1.5]]) array([[0. , 0.90740741, 0.09259259]]) 1tree_clf.predict([[5, 1.5]]) array([1]) 回归1234from sklearn.tree import DecisionTreeRegressortree_reg = DecisionTreeRegressor(max_depth=2)tree_reg.fit(X, y) DecisionTreeRegressor(criterion=&#39;mse&#39;, max_depth=2, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter=&#39;best&#39;) 12345678export_graphviz( tree_clf, out_file=image_path('iris_tree_regression.dot'), feature_names=iris.feature_names[2:], class_names=iris.target_names, rounded=True, filled=True) 源文件]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>分类</tag>
        <tag>Hands-On Machine Learning with Scikit-Learn and TensorFlow</tag>
        <tag>sklearn</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Support Vector Machine]]></title>
    <url>%2F2018%2F12%2F06%2FSupport-Vector-Machine%2F</url>
    <content type="text"><![CDATA[线性SVM分类软间隔分类12345678910111213141516import numpy as npfrom sklearn import datasetsfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScalerfrom sklearn.svm import LinearSVCiris = datasets.load_iris()X = iris["data"][:, (2, 3)]y = (iris["target"] == 2).astype(np.float64)svm_clf = Pipeline(( ("scaler", StandardScaler()), ("linear_svc", LinearSVC(C=1, loss="hinge"))))svm_clf.fit(X, y) Pipeline(memory=None, steps=[(&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with_std=True)), (&#39;linear_svc&#39;, LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True, intercept_scaling=1, loss=&#39;hinge&#39;, max_iter=1000, multi_class=&#39;ovr&#39;, penalty=&#39;l2&#39;, random_state=None, tol=0.0001, verbose=0))]) 1svm_clf.predict([[5.5, 1.7]]) array([1.]) 非线性SVM分类1234567891011from sklearn.datasets import make_moonsfrom sklearn.pipeline import Pipelinefrom sklearn.preprocessing import PolynomialFeaturespolynomial_svm_clf = Pipeline(( ("poly_featrure", PolynomialFeatures(degree=3)), ("scaler", StandardScaler()), ("svm_clf", LinearSVC(C=10, loss="hinge"))))polynomial_svm_clf.fit(X, y) Pipeline(memory=None, steps=[(&#39;poly_featrure&#39;, PolynomialFeatures(degree=3, include_bias=True, interaction_only=False)), (&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with_std=True)), (&#39;svm_clf&#39;, LinearSVC(C=10, class_weight=None, dual=True, fit_intercept=True, intercept_scaling=1, loss=&#39;hinge&#39;, max_iter=1000, multi_class=&#39;ovr&#39;, penalty=&#39;l2&#39;, random_state=None, tol=0.0001, verbose=0))]) 多项式核1234567from sklearn.svm import SVCpolu_kernel_svm_clf = Pipeline(( ("scaler", StandardScaler()), ("svm_clf", SVC(kernel="poly", degree=3, coef0=1, C=5))))polu_kernel_svm_clf.fit(X, y) Pipeline(memory=None, steps=[(&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with_std=True)), (&#39;svm_clf&#39;, SVC(C=5, cache_size=200, class_weight=None, coef0=1, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=&#39;auto_deprecated&#39;, kernel=&#39;poly&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False))]) 高斯径向基函数核123456rbf_kernel_svm_clf = Pipeline(( ("scaler", StandardScaler()), ("svm_clf", SVC(kernel="rbf", gamma=5, C=0.001))))rbf_kernel_svm_clf.fit(X, y) Pipeline(memory=None, steps=[(&#39;scaler&#39;, StandardScaler(copy=True, with_mean=True, with_std=True)), (&#39;svm_clf&#39;, SVC(C=0.001, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&#39;ovr&#39;, degree=3, gamma=5, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False))]) SVM回归123from sklearn.svm import LinearSVRsvm_reg = LinearSVR(epsilon=1.5)svm_reg.fit(X, y) LinearSVR(C=1.0, dual=True, epsilon=1.5, fit_intercept=True, intercept_scaling=1.0, loss=&#39;epsilon_insensitive&#39;, max_iter=1000, random_state=None, tol=0.0001, verbose=0) 1234from sklearn.svm import SVRsvm_poly_reg = SVR(kernel="poly", degree=2, C=100, epsilon=0.1, gamma='auto')svm_poly_reg.fit(X, y) SVR(C=100, cache_size=200, coef0=0.0, degree=2, epsilon=0.1, gamma=&#39;auto&#39;, kernel=&#39;poly&#39;, max_iter=-1, shrinking=True, tol=0.001, verbose=False)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Hands-On Machine Learning with Scikit-Learn and TensorFlow</tag>
        <tag>sklearn</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Training Models]]></title>
    <url>%2F2018%2F11%2F29%2FTraining-Models%2F</url>
    <content type="text"><![CDATA[线性回归正规方程1234567891011121314# 构建类线性数据%matplotlib inlineimport matplotlibimport matplotlib.pyplot as pltimport numpy as npX = 2 * np.random.rand(100, 1)y = 4 + 3 * X + np.random.randn(100, 1)plt.plot(X, y, "b.")plt.xlabel("$x_1$", fontsize=18)plt.ylabel("$y$", rotation=0, fontsize=18)plt.axis([0, 2, 0, 15])plt.show() 1234#使用正规方程计算thetaX_b = np.c_[np.ones((100, 1)), X]theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)theta_best array([[3.97963397], [2.98594391]]) 12345# 使用theta进行预测X_new = np.array([[0],[2]])X_new_b = np.c_[np.ones((2,1)), X_new]y_predict = X_new_b.dot(theta_best)y_predict array([[3.97963397], [9.95152178]]) 1234567# 画出预测函数plt.plot(X_new, y_predict, "r-")plt.plot(X, y, "b.")plt.xlabel("$x_1$", fontsize=18)plt.ylabel("$y$", rotation=0, fontsize=18)plt.axis([0, 2, 0, 15])plt.show() 12345# 使用sklearn库的LinearRegression预测from sklearn.linear_model import LinearRegressionlin_reg = LinearRegression()lin_reg.fit(X, y)lin_reg.intercept_, lin_reg.coef_ (array([3.97963397]), array([[2.98594391]])) 1lin_reg.predict(X_new) array([[3.97963397], [9.95152178]]) 梯度下降批量梯度下降12345678910eta = 0.1#学习速率n_iterations = 1000m = 100theta = np.random.randn(2, 1)for iteration in range(n_iterations): gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y) theta = theta - eta * gradientstheta array([[3.97963397], [2.98594391]]) 12345678910111213141516171819202122232425262728293031323334353637# 画出不同学习速率的拟合过程theta_path_bgd = []def plot_gradient_descent(theta, eta, theta_path=None): m = len(X_b) plt.plot(X, y, "b.") n_iterations = 1000 for iteration in range(n_iterations): if iteration &lt; 10: y_predict = X_new_b.dot(theta) style = "b-" if iteration &gt; 0 else "r--" plt.plot(X_new, y_predict, style) gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y) theta = theta - eta * gradients if theta_path is not None: theta_path.append(theta) plt.xlabel("$x_1$", fontsize=18) plt.axis([0, 2, 0, 15]) plt.title(r"$\eta = &#123;&#125;$".format(eta), fontsize=16)np.random.seed(42)theta = np.random.randn(2, 1)plt.figure(figsize=(10, 4))plt.subplot(131)plot_gradient_descent(theta, eta=0.02)plt.ylabel("$y$", rotation=0, fontsize=18)plt.subplot(132)plot_gradient_descent(theta, eta=0.1, theta_path=theta_path_bgd)plt.subplot(133)plot_gradient_descent(theta, eta=0.5)plt.show() 随机梯度下降1234567891011121314151617181920212223242526272829# 实现theta_path_bgd = []n_epochs = 50t0, t1 = 5, 50def learning_schedule(t): return t0/(t + t1)theta = np.random.randn(2,1)for epoch in range(n_epochs): for i in range(m): if epoch == 0 and i&lt;20: y_predict = X_new_b.dot(theta) style = "b-" if i &gt; 0 else "r--" plt.plot(X_new, y_predict, style) random_index = np.random.randint(m) xi = X_b[random_index:random_index+1] yi = y[random_index:random_index+1] gradients = 2 * xi.T.dot(xi.dot(theta) - yi) eta = learning_schedule(epoch*m + i) theta = theta - eta * gradients theta_path_bgd.append(theta)plt.plot(X, y, "b.")plt.xlabel("$x_1$", fontsize=18)plt.ylabel("$y$", rotation=0, fontsize=18)plt.axis([0, 2, 0, 15])plt.show()theta array([[3.94377661], [2.97561711]]) 1234#使用sklearn的梯度下降from sklearn.linear_model import SGDRegressorsgd_reg = SGDRegressor(n_iter=50, penalty=None, eta0=0.1)sgd_reg.fit(X, y.ravel()) e:\python\python36\lib\site-packages\sklearn\linear_model\stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead. DeprecationWarning) SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1, eta0=0.1, fit_intercept=True, l1_ratio=0.15, learning_rate=&#39;invscaling&#39;, loss=&#39;squared_loss&#39;, max_iter=None, n_iter=50, n_iter_no_change=5, penalty=None, power_t=0.25, random_state=None, shuffle=True, tol=None, validation_fraction=0.1, verbose=0, warm_start=False) 1sgd_reg.intercept_, sgd_reg.coef_ (array([3.96293115]), array([2.9698392])) 多项式回归123456789# 构建数据集m = 100X = 6 * np.random.rand(m, 1) - 3y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)plt.plot(X, y, "b.")plt.xlabel("$x_1$", fontsize=18)plt.ylabel("$y$", rotation=0, fontsize=18)plt.axis([-3, 3, 0, 10])plt.show() 12345#为特征添加指数from sklearn.preprocessing import PolynomialFeaturespoly_features = PolynomialFeatures(degree=2, include_bias=False)X_poly = poly_features.fit_transform(X)X[0] array([2.15290063]) 1X_poly[0] array([2.15290063, 4.63498111]) 1234567891011121314lin_reg = LinearRegression()lin_reg.fit(X_poly, y)X_new = np.linspace(-3, 3, 100).reshape(100, 1)X_new_poly = poly_features.transform(X_new)y_new = lin_reg.predict(X_new_poly)plt.plot(X, y, "b.")plt.plot(X_new, y_new, "r-", linewidth=2, label="Predictions")plt.xlabel("$x_1$", fontsize=18)plt.ylabel("$y$", rotation=0, fontsize=18)plt.legend(loc="upper left", fontsize=14)plt.axis([-3, 3, 0, 10])lin_reg.intercept_, lin_reg.coef_ (array([2.28405711]), array([[0.88171323, 0.44446033]])) 学习曲线12345678910111213141516171819202122from sklearn.metrics import mean_squared_errorfrom sklearn.model_selection import train_test_splitdef plot_learning_curves(model, X, y): X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2) train_errors, val_errors = [], [] for m in range(1, len(X_train)): model.fit(X_train[:m], y_train[:m]) y_train_predict = model.predict(X_train[:m]) y_val_predict = model.predict(X_val) train_errors.append(mean_squared_error(y_train_predict, y_train[:m])) val_errors.append(mean_squared_error(y_val_predict, y_val)) plt.plot(np.sqrt(train_errors), "r-+", linewidth=2, label="train") plt.plot(np.sqrt(val_errors), "b-", linewidth=3, label="val") plt.xlabel("$X_1$", fontsize=18) plt.ylabel("$y$", rotation=0, fontsize=16) plt.axis([0,80,0,3]) plt.legend(loc="upper right")lin_reg = LinearRegression()plot_learning_curves(lin_reg, X, y) 1234567# 使用管道from sklearn.pipeline import Pipelinepolynomial_regression = Pipeline(( ("poly_features", PolynomialFeatures(degree=10, include_bias=False)), ("sgd_reg", LinearRegression())))plot_learning_curves(polynomial_regression, X, y) 正规化线性模型岭回归12345# 使用sklearn的Ridgefrom sklearn.linear_model import Ridgeridge_reg = Ridge(alpha=1, solver="cholesky")ridge_reg.fit(X, y)ridge_reg.predict([[1.5]]) array([[4.66784461]]) 123sgd_reg = SGDRegressor(max_iter=300,tol=1e-3,penalty="l2")sgd_reg.fit(X, y.ravel())sgd_reg.predict([[1.5]]) array([4.65328865]) Lasso回归1234from sklearn.linear_model import Lassolasso_reg = Lasso(alpha=0.1)lasso_reg.fit(X, y)lasso_reg.predict([[1.5]]) array([4.61400886]) 弹性网络1234from sklearn.linear_model import ElasticNetelastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)elastic_net.fit(X, y)elastic_net.predict([[1.5]]) array([4.62211027]) 及时停止1234567891011121314151617181920212223242526272829303132from sklearn.base import clonefrom sklearn.preprocessing import StandardScalernp.random.seed(42)m = 100X = 6 * np.random.rand(m, 1) - 3y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)X_train, X_val, y_train, y_val = train_test_split(X[:50], y[:50].ravel(), test_size=0.5, random_state=10)poly_scaler = Pipeline([ ("poly_features", PolynomialFeatures(degree=90, include_bias=False)), ("std_scaler", StandardScaler()), ])X_train_poly_scaled = poly_scaler.fit_transform(X_train)X_val_poly_scaled = poly_scaler.transform(X_val)sgd_reg = SGDRegressor(warm_start=True, penalty=None, max_iter=300, learning_rate="constant", eta0=0.0005, tol=1e-3)minimum_val_error = float("inf")best_epoch = Nonebest_model = Nonefor epoch in range(1000): sgd_reg.fit(X_train_poly_scaled, y_train) y_val_predict = sgd_reg.predict(X_val_poly_scaled) val_error = mean_squared_error(y_val_predict, y_val) if val_error &lt; minimum_val_error: minimum_val_error = val_error best_epoch = epoch best_model = clone(sgd_reg)epoch, sgd_reg (999, SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1, eta0=0.0005, fit_intercept=True, l1_ratio=0.15, learning_rate=&#39;constant&#39;, loss=&#39;squared_loss&#39;, max_iter=300, n_iter=None, n_iter_no_change=5, penalty=None, power_t=0.25, random_state=None, shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=True)) 逻辑回归123from sklearn import datasetsiris = datasets.load_iris()list(iris.keys()) [&#39;data&#39;, &#39;target&#39;, &#39;target_names&#39;, &#39;DESCR&#39;, &#39;feature_names&#39;, &#39;filename&#39;] 12X = iris["data"][:, 3:]y = (iris["target"] == 2).astype(np.int) 123from sklearn.linear_model import LogisticRegressionlog_reg = LogisticRegression(solver='lbfgs')log_reg.fit(X, y) LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class=&#39;warn&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False) 123456789101112131415X_new = np.linspace(0, 3, 1000).reshape(-1, 1)y_proba = log_reg.predict_proba(X_new)decision_boundary = X_new[y_proba[:, 1] &gt;= 0.5][0]plt.plot([decision_boundary, decision_boundary], [-1, 2], "k:", linewidth=2)plt.plot(X_new, y_proba[:, 1], label="Iris-Virginica")plt.plot(X_new, y_proba[:, 0], label="Not Iris-Virginica")plt.text(decision_boundary+0.02, 0.15, "Decision boundary", fontsize=14, color="k", ha="center")plt.arrow(decision_boundary, 0.08, -0.3, 0, head_width=0.05, head_length=0.1, fc='b', ec='b')plt.arrow(decision_boundary, 0.92, 0.3, 0, head_width=0.05, head_length=0.1, fc='g', ec='g')plt.xlabel("Petal width(cm)", fontsize=14)plt.ylabel("probability", fontsize=14)plt.legend(loc="center left", fontsize=14)plt.axis([0, 3, -0.02, 1.02])plt.show() 1log_reg.predict([[1.7], [1.5]]) array([1, 0]) 多项逻辑回归12X = iris["data"][:, (2, 3)]y = iris["target"] 12softmax_reg = LogisticRegression(multi_class="multinomial", solver="lbfgs", C=10)softmax_reg.fit(X, y) LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class=&#39;multinomial&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False) 1softmax_reg.predict([[5,2]]) array([2]) 1softmax_reg.predict_proba([[5,2]]) array([[6.38014896e-07, 5.74929995e-02, 9.42506362e-01]]) 源文件]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Hands-On Machine Learning with Scikit-Learn and TensorFlow</tag>
        <tag>sklearn</tag>
        <tag>回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Classification]]></title>
    <url>%2F2018%2F11%2F27%2FClassification%2F</url>
    <content type="text"><![CDATA[准备数据123from sklearn.datasets import fetch_mldatamnist = fetch_mldata("MNIST original", data_home='./datasets/')mnist e:\python\python36\lib\site-packages\sklearn\utils\deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22 warnings.warn(msg, category=DeprecationWarning) e:\python\python36\lib\site-packages\sklearn\utils\deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22 warnings.warn(msg, category=DeprecationWarning) {&#39;COL_NAMES&#39;: [&#39;label&#39;, &#39;data&#39;], &#39;DESCR&#39;: &#39;mldata.org dataset: mnist-original&#39;, &#39;data&#39;: array([[0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], ..., [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0], [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), &#39;target&#39;: array([0., 0., 0., ..., 9., 9., 9.])} 1X, y = mnist["data"], mnist["target"] 1X.shape (70000, 784) 1y.shape (70000,) 查看图片12345678910%matplotlib inlineimport matplotlibimport matplotlib.pyplot as pltsome_digit = X[36000]some_digit_image = some_digit.reshape(28, 28)plt.imshow(some_digit_image, cmap=matplotlib.cm.binary, interpolation="nearest")plt.axis("off")plt.show() 1y[36000] 5.0 准备训练集和测试集12#划分X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:] 12345#混洗import numpy as npshuffle_index = np.random.permutation(60000)X_train, y_train = X_train[shuffle_index], y_train[shuffle_index] 训练二元分类123# 数据集中所有为5的置为True,所有不为5的置为Falsey_train_5 = (y_train == 5)y_test_5 = (y_test == 5) 12345#训练from sklearn.linear_model import SGDClassifiersgd_clf = SGDClassifier(random_state=42, max_iter=300, tol=1e-3) #设置random_state来取得有复验性的结果sgd_clf.fit(X_train, y_train_5) SGDClassifier(alpha=0.0001, average=False, class_weight=None, early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15, learning_rate=&#39;optimal&#39;, loss=&#39;hinge&#39;, max_iter=300, n_iter=None, n_iter_no_change=5, n_jobs=None, penalty=&#39;l2&#39;, power_t=0.5, random_state=42, shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0, warm_start=False) 1sgd_clf.predict([some_digit]) array([ True]) 性能评估用交叉验证集测量精度123from sklearn.model_selection import cross_val_scorecross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring="accuracy") array([0.96385, 0.9663 , 0.96225]) 1234567891011# 预测所有的均为非5from sklearn.base import BaseEstimatorclass Never5Classifier(BaseEstimator): def fit(self, X, y=None): pass def predict(self, X): return np.zeros((len(X), 1), dtype=bool)never_5_clf = Never5Classifier()cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring="accuracy") array([0.90875, 0.9118 , 0.9084 ]) 混淆矩阵1234# 使用cross_val_predict()预测from sklearn.model_selection import cross_val_predicty_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)y_train_pred array([False, False, True, ..., False, False, True]) 123# 生成混淆矩阵from sklearn.metrics import confusion_matrixconfusion_matrix(y_train_5, y_train_pred) array([[53843, 736], [ 1416, 4005]], dtype=int64) 12y_train_perfect_predictions = y_train_5confusion_matrix(y_train_5, y_train_perfect_predictions) array([[54579, 0], [ 0, 5421]], dtype=int64) 准确率和召回率12from sklearn.metrics import precision_score, recall_scoreprecision_score(y_train_5, y_train_pred)# 4005/(4005+736) 0.8447584897700907 1recall_score(y_train_5, y_train_pred)# 4005/(4005+1416) 0.7387935805201993 123# f1scorefrom sklearn.metrics import f1_scoref1_score(y_train_5, y_train_pred) 0.7882306632552647 准确率/召回率权衡123# 返回该实例的度量分数y_scores = sgd_clf.decision_function([some_digit])y_scores array([4067.16672909]) 123threshold = 0y_som_digit_pred = (y_scores &gt; threshold)y_som_digit_pred array([ True]) 123threshold = 5000y_som_digit_pred = (y_scores &gt; threshold)y_som_digit_pred array([False]) 1y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method="decision_function")#使用decision_function代替predictions 123456789101112131415# 画出准确率和召回率的曲线from sklearn.metrics import precision_recall_curveprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)def plot_precision_recall_vs_threshold(precisions, recalls, thresholds): plt.plot(thresholds, precisions[:-1], "b--", label="Precision") plt.plot(thresholds, recalls[:-1], "g-", label="Recall") plt.xlabel("Threshold") plt.legend(loc="center left") plt.ylim([0, 1]) plt.xlim([-70000, 70000])plot_precision_recall_vs_threshold(precisions, recalls, thresholds)plt.show() 123#90%precisiony_train_pred_90 = (y_scores &gt; 20000)precision_score(y_train_5, y_train_pred_90) 0.9716981132075472 1recall_score(y_train_5, y_train_pred_90) 0.019000184467810367 ROC曲线12345678910111213from sklearn.metrics import roc_curvefpr, tpr, thresholds = roc_curve(y_train_5, y_scores)def plot_roc_curve(fpr, tpr, label=None): plt.plot(fpr, tpr, linewidth=2, label=label) plt.plot([0,1], [0,1], 'k--') plt.axis([0, 1, 0, 1]) plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate')plot_roc_curve(fpr, tpr)plt.show() 12from sklearn.metrics import roc_auc_scoreroc_auc_score(y_train_5, y_scores) 0.9665469033599001 12345678910111213#随机森林的ROC曲线from sklearn.ensemble import RandomForestClassifierforest_clf = RandomForestClassifier(random_state=42, n_estimators=10)y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3, method='predict_proba')y_scores_forest = y_probas_forest[:,1]fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5, y_scores_forest)plt.plot(fpr, tpr, 'b:', label="SGD")plot_roc_curve(fpr_forest, tpr_forest, "Random Forest")plt.legend(loc="lower right")plt.show() 1roc_auc_score(y_train_5, y_scores_forest) 0.9923774040313053 多级分类12sgd_clf.fit(X_train, y_train)#底层训练了10个二元分类器，选择最大的sgd_clf.predict([some_digit]) e:\python\python36\lib\site-packages\sklearn\linear_model\stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit. ConvergenceWarning) array([5.]) 12some_digit_scores = sgd_clf.decision_function([some_digit])some_digit_scores array([[ -1924.18040085, -24335.92781177, -8446.0176118 , -1942.26247443, -13280.95661159, 4067.16672909, -15844.49239355, -21310.04623554, -9717.41918688, -12970.31612817]]) 1np.argmax(some_digit_scores) 5 1sgd_clf.classes_ array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.]) 1sgd_clf.classes_[5] 5.0 123456#使用一对一的训练器from sklearn.multiclass import OneVsOneClassifierovo_clf = OneVsOneClassifier(SGDClassifier(random_state=42, max_iter=500, tol=1e-3))ovo_clf.fit(X_train, y_train)ovo_clf.predict([some_digit]) array([5.]) 1len(ovo_clf.estimators_) 45 12forest_clf.fit(X_train, y_train)forest_clf.predict([some_digit]) array([5.]) 1forest_clf.predict_proba([some_digit]) array([[0. , 0. , 0. , 0. , 0. , 0.9, 0. , 0. , 0.1, 0. ]]) 12cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring="accuracy") array([0.88242352, 0.8860443 , 0.87363104]) 12345# 缩减输入from sklearn.preprocessing import StandardScalerscaler = StandardScaler()X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring="accuracy") e:\python\python36\lib\site-packages\sklearn\linear_model\stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit. ConvergenceWarning) e:\python\python36\lib\site-packages\sklearn\linear_model\stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit. ConvergenceWarning) e:\python\python36\lib\site-packages\sklearn\linear_model\stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit. ConvergenceWarning) array([0.91106779, 0.91079554, 0.90988648]) 误差分析1234#首先查看混淆矩阵y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)conf_mx = confusion_matrix(y_train, y_train_pred)conf_mx e:\python\python36\lib\site-packages\sklearn\linear_model\stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit. ConvergenceWarning) e:\python\python36\lib\site-packages\sklearn\linear_model\stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit. ConvergenceWarning) e:\python\python36\lib\site-packages\sklearn\linear_model\stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit. ConvergenceWarning) array([[5657, 1, 16, 9, 12, 53, 40, 6, 125, 4], [ 1, 6451, 43, 24, 5, 46, 6, 9, 142, 15], [ 36, 29, 5339, 95, 74, 26, 78, 42, 226, 13], [ 38, 26, 128, 5334, 2, 226, 29, 48, 216, 84], [ 15, 15, 47, 14, 5323, 10, 43, 23, 148, 204], [ 39, 19, 33, 177, 60, 4602, 83, 24, 297, 87], [ 28, 22, 51, 2, 40, 96, 5591, 10, 78, 0], [ 19, 16, 58, 32, 53, 8, 8, 5764, 63, 244], [ 22, 73, 56, 111, 6, 138, 39, 12, 5318, 76], [ 26, 22, 30, 73, 132, 33, 2, 186, 189, 5256]], dtype=int64) 123#使用图像显示矩阵plt.matshow(conf_mx)plt.show() 123#画出误差row_nums =conf_mx.sum(axis=1, keepdims=True)#计算每一类实际的数量row_nums array([[5923], [6742], [5958], [6131], [5842], [5421], [5918], [6265], [5851], [5949]], dtype=int64) 12norm_conf_mx = conf_mx/row_nums# 计算错误比例norm_conf_mx array([[9.55090326e-01, 1.68833361e-04, 2.70133378e-03, 1.51950025e-03, 2.02600034e-03, 8.94816816e-03, 6.75333446e-03, 1.01300017e-03, 2.11041702e-02, 6.75333446e-04], [1.48323939e-04, 9.56837734e-01, 6.37792940e-03, 3.55977455e-03, 7.41619697e-04, 6.82290122e-03, 8.89943637e-04, 1.33491546e-03, 2.10619994e-02, 2.22485909e-03], [6.04229607e-03, 4.86740517e-03, 8.96106076e-01, 1.59449480e-02, 1.24202753e-02, 4.36388050e-03, 1.30916415e-02, 7.04934542e-03, 3.79321920e-02, 2.18194025e-03], [6.19801011e-03, 4.24074376e-03, 2.08775077e-02, 8.70004893e-01, 3.26211059e-04, 3.68618496e-02, 4.73006035e-03, 7.82906541e-03, 3.52307943e-02, 1.37008645e-02], [2.56761383e-03, 2.56761383e-03, 8.04519000e-03, 2.39643958e-03, 9.11160561e-01, 1.71174255e-03, 7.36049298e-03, 3.93700787e-03, 2.53337898e-02, 3.49195481e-02], [7.19424460e-03, 3.50488840e-03, 6.08743774e-03, 3.26508024e-02, 1.10680686e-02, 8.48920863e-01, 1.53108283e-02, 4.42722745e-03, 5.47869397e-02, 1.60486995e-02], [4.73132815e-03, 3.71747212e-03, 8.61777628e-03, 3.37952011e-04, 6.75904022e-03, 1.62216965e-02, 9.44744846e-01, 1.68976005e-03, 1.31801284e-02, 0.00000000e+00], [3.03272147e-03, 2.55387071e-03, 9.25778132e-03, 5.10774142e-03, 8.45969673e-03, 1.27693536e-03, 1.27693536e-03, 9.20031923e-01, 1.00558659e-02, 3.89465283e-02], [3.76004102e-03, 1.24764997e-02, 9.57101350e-03, 1.89711160e-02, 1.02546573e-03, 2.35857118e-02, 6.66552726e-03, 2.05093146e-03, 9.08904461e-01, 1.29892326e-02], [4.37048243e-03, 3.69810052e-03, 5.04286435e-03, 1.22709699e-02, 2.21886031e-02, 5.54715078e-03, 3.36190956e-04, 3.12657590e-02, 3.17700454e-02, 8.83509834e-01]]) 123np.fill_diagonal(norm_conf_mx, 0)#将对角线置为0，只显示误差plt.matshow(norm_conf_mx)#行越暗表示预测越正确，越亮表示误差越高plt.show() 1234567891011121314151617181920212223242526272829303132#画出所有的3和5cl_3, cl_5 = 3, 5X_33 = X_train[(y_train == cl_3) &amp; (y_train_pred == cl_3)]#实际为3和预测为3X_35 = X_train[(y_train == cl_3) &amp; (y_train_pred == cl_5)]#实际为3和预测为5X_53 = X_train[(y_train == cl_5) &amp; (y_train_pred == cl_3)]#实际为5和预测为3X_55 = X_train[(y_train == cl_5) &amp; (y_train_pred == cl_5)]#实际为5和预测为5def plot_digits(instances, images_per_row=10, **options): size = 28 images_per_row = min(len(instances), images_per_row) images = [instance.reshape(size,size) for instance in instances] n_rows = images_per_row+1 row_images = [] n_empty = n_rows * images_per_row - len(instances) images.append(np.zeros((size, size * n_empty))) for row in range(n_rows): rimages = images[row * images_per_row : (row + 1) * images_per_row] row_images.append(np.concatenate(rimages, axis=1)) image = np.concatenate(row_images, axis=0) plt.imshow(image, cmap=matplotlib.cm.binary, **options) plt.axis("off")plt.figure(figsize=(8,8))plt.subplot(221)plot_digits(X_33[:25], images_per_row=5)plt.subplot(222)plot_digits(X_35[:25], images_per_row=5)plt.subplot(223)plot_digits(X_53[:25], images_per_row=5)plt.subplot(224)plot_digits(X_55[:25], images_per_row=5) 多标签分类1234567from sklearn.neighbors import KNeighborsClassifiery_train_large = (y_train &gt;= 7)y_train_odd = (y_train %2 == 1)y_multilabel = np.c_[y_train_large, y_train_odd]knn_clf = KNeighborsClassifier()knn_clf.fit(X_train, y_multilabel) KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;, metric_params=None, n_jobs=None, n_neighbors=5, p=2, weights=&#39;uniform&#39;) 1knn_clf.predict([some_digit]) array([[False, True]]) 123#计算F1y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_train, cv=3)f1_score(y_train, y_train_knn_pred, average="macro") 0.9686571237993065 多路输出1234567891011#在图片数据上添加噪声import numpy.random as rndnoise = rnd.randint(0, 100, (len(X_train), 784))X_train_mod = X_train + noisenoise = rnd.randint(0, 100, (len(X_test), 784))X_test_mod = X_test + noisey_train_mod = X_trainy_test_mod = X_test 123456789def plot_digit(data): image = data.reshape(28, 28) plt.imshow(image, cmap = matplotlib.cm.binary, interpolation="nearest") plt.axis("off")knn_clf.fit(X_train_mod, y_train_mod)clean_digit = knn_clf.predict([X_test_mod[2]])plot_digit(clean_digit) 源文件]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>分类</tag>
        <tag>Hands-On Machine Learning with Scikit-Learn and TensorFlow</tag>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinearRegression]]></title>
    <url>%2F2018%2F11%2F27%2FLinearRegression%2F</url>
    <content type="text"><![CDATA[12345678910111213141516171819202122232425262728import osimport tarfilefrom six.moves import urllibDOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml/master/"HOUSING_PATH = "datasets/housing"HOUSING_URL = DOWNLOAD_ROOT + HOUSING_PATH + "/housing.tgz"def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH): """获取数据源并解压缩 :param housing_url 下载链接 :param housing_path 文件保存地址 :return """ if not os.path.isdir(housing_path): os.makedirs(housing_path) tgz_path = os.path.join(housing_path, "housing.tgz") print("正在从："+housing_url+"下载文件...") urllib.request.urlretrieve(housing_url, tgz_path) print("正在解压文件...") housing_tgz = tarfile.open(tgz_path) housing_tgz.extractall(path=housing_path) print("解压完成！") housing_tgz.close()fetch_housing_data() 正在从：https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.tgz下载文件... 正在解压文件... 解压完成！ 12345678910import pandas as pddef load_housing_data(housing_path=HOUSING_PATH): """加载数据文件 :param housing_path 数据地址 :return """ csv_path = os.path.join(housing_path, "housing.csv") return pd.read_csv(csv_path) 12housing = load_housing_data()housing.head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity 0 -122.23 37.88 41.0 880.0 129.0 322.0 126.0 8.3252 452600.0 NEAR BAY 1 -122.22 37.86 21.0 7099.0 1106.0 2401.0 1138.0 8.3014 358500.0 NEAR BAY 2 -122.24 37.85 52.0 1467.0 190.0 496.0 177.0 7.2574 352100.0 NEAR BAY 3 -122.25 37.85 52.0 1274.0 235.0 558.0 219.0 5.6431 341300.0 NEAR BAY 4 -122.25 37.85 52.0 1627.0 280.0 565.0 259.0 3.8462 342200.0 NEAR BAY 1housing.info() #获取数据的相关信息 &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 20640 entries, 0 to 20639 Data columns (total 10 columns): longitude 20640 non-null float64 latitude 20640 non-null float64 housing_median_age 20640 non-null float64 total_rooms 20640 non-null float64 total_bedrooms 20433 non-null float64 population 20640 non-null float64 households 20640 non-null float64 median_income 20640 non-null float64 median_house_value 20640 non-null float64 ocean_proximity 20640 non-null object dtypes: float64(9), object(1) memory usage: 1.6+ MB 1housing["ocean_proximity"].value_counts() #统计相关类别的信息(非数值信息) &lt;1H OCEAN 9136 INLAND 6551 NEAR OCEAN 2658 NEAR BAY 2290 ISLAND 5 Name: ocean_proximity, dtype: int64 1housing.describe()#数值信息的摘要(会去除空的数据) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value count 20640.000000 20640.000000 20640.000000 20640.000000 20433.000000 20640.000000 20640.000000 20640.000000 20640.000000 mean -119.569704 35.631861 28.639486 2635.763081 537.870553 1425.476744 499.539680 3.870671 206855.816909 std 2.003532 2.135952 12.585558 2181.615252 421.385070 1132.462122 382.329753 1.899822 115395.615874 min -124.350000 32.540000 1.000000 2.000000 1.000000 3.000000 1.000000 0.499900 14999.000000 25% -121.800000 33.930000 18.000000 1447.750000 296.000000 787.000000 280.000000 2.563400 119600.000000 50% -118.490000 34.260000 29.000000 2127.000000 435.000000 1166.000000 409.000000 3.534800 179700.000000 75% -118.010000 37.710000 37.000000 3148.000000 647.000000 1725.000000 605.000000 4.743250 264725.000000 max -114.310000 41.950000 52.000000 39320.000000 6445.000000 35682.000000 6082.000000 15.000100 500001.000000 1234%matplotlib inlineimport matplotlib.pylab as plthousing.hist(bins=50, figsize=(20,15))plt.show() 1234567891011121314import numpy as npdef split_train_test(data, test_ratio): """从数据中创建训练集和测试集 :param 数据集 :param 分割比例(测试集占全集的百分比) :return """ shuffled_indices = np.random.permutation(len(data)) test_set_size = int(len(data) * test_ratio) test_indices = shuffled_indices[:test_set_size] train_indices = shuffled_indices[test_set_size:] return data.iloc[train_indices], data.iloc[test_indices] 12train_set, tesst_set = split_train_test(housing, 0.2)print(len(train_set), "train +", len(tesst_set), "test") 16512 train + 4128 test 123456789101112131415161718192021222324import hashlibdef test_set_check(identifier, test_ratio, hash): """测试集检查 :param identifier 标识符 :param test_ratio 测试集比例 :param hash hash函数 :return """ return hash(np.int64(identifier)).digest()[-1] &lt; 256 * test_ratiodef split_train_test_by_id(data, test_ratio, id_column, hash=hashlib.md5): """通过ID分割 :param data 数据集 :param test_ratio 测试集的比例 :param id_column 作为id的列 :param hash hash函数 :return """ ids = data[id_column] in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio, hash)) return data.loc[~in_test_set], data.loc[in_test_set] 12housing_with_id = housing.reset_index() #housing数据无标识符，所以需要添加train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, "index") 12housing_with_id["id"] = housing["longitude"] * 1000 + housing["latitude"] #使用联合主键作为idtrain_set, tesst_set = split_train_test_by_id(housing_with_id, 0.2, "id") 12from sklearn.model_selection import train_test_splittrain_set, tesst_set = train_test_split(housing, test_size=0.2, random_state=42) #使用sklearn自带的分割函数 12housing["income_cat"] = np.ceil(housing["median_income"] / 1.5) # 压缩median_incomehousing["income_cat"].where(housing["income_cat"] &lt; 5, 5.0, inplace=True) 1234567from sklearn.model_selection import StratifiedShuffleSplitsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)for train_index, test_index in split.split(housing, housing["income_cat"]):#分层抽样 strat_train_set = housing.loc[train_index] strat_test_set = housing.loc[test_index] 1housing["income_cat"].value_counts() / len(housing) #每一层样本在总数据集中的比例 3.0 0.350581 2.0 0.318847 4.0 0.176308 5.0 0.114438 1.0 0.039826 Name: income_cat, dtype: float64 12for set in (strat_test_set, strat_train_set):#删除incom_cat列 set.drop(["income_cat"], axis=1, inplace=True) 1housing = strat_train_set.copy() 1housing.plot(kind="scatter", x="longitude", y="latitude")#画出散点图 &lt;matplotlib.axes._subplots.AxesSubplot at 0x11b8dc4a8&gt; 1housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.1) &lt;matplotlib.axes._subplots.AxesSubplot at 0x11bdadda0&gt; 12345housing.plot(kind="scatter", x="longitude", y="latitude", alpha=0.4, s=housing["population"] / 100, label="population", c="median_house_value", cmap=plt.get_cmap("jet"), colorbar=True, )plt.legend() &lt;matplotlib.legend.Legend at 0x11bef7cc0&gt; 12corr_matrix = housing.corr()#计算相关系数corr_matrix["median_house_value"].sort_values(ascending=False) median_house_value 1.000000 median_income 0.687160 total_rooms 0.135097 housing_median_age 0.114110 households 0.064506 total_bedrooms 0.047689 population -0.026920 longitude -0.047432 latitude -0.142724 Name: median_house_value, dtype: float64 12345from pandas.plotting import scatter_matrixattributes = ["median_house_value", "median_income", "total_rooms","housing_median_age"]scatter_matrix(housing[attributes], figsize=(12,8)) #从数据图获取相关性 array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1166ded68&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1166b4e80&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1162bac18&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x11677d198&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x11676c940&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x11643ec88&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1161eb048&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x116220940&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1162208d0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1165e6fd0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x11672f9e8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x115fcdac8&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x1165824e0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x116828a58&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x116815fd0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x116602588&gt;]], dtype=object) 1housing.plot(kind="scatter", x="median_income", y="median_house_value", alpha=0.1)# 画出收入中位数与房价中位数的关系 &lt;matplotlib.axes._subplots.AxesSubplot at 0x116240e10&gt; 1234567#尝试组合属性查看相关性housing["rooms_per_household"] = housing["total_rooms"]/housing["households"]housing["bedrooms_per_room"] = housing["total_bedrooms"]/housing["total_rooms"]housing["population_per_houshold"] = housing["population"]/housing["households"]corr_matrix = housing.corr()corr_matrix["median_house_value"].sort_values(ascending=False) median_house_value 1.000000 median_income 0.687160 rooms_per_household 0.146285 total_rooms 0.135097 housing_median_age 0.114110 households 0.064506 total_bedrooms 0.047689 population_per_houshold -0.021985 population -0.026920 longitude -0.047432 latitude -0.142724 bedrooms_per_room -0.259984 Name: median_house_value, dtype: float64 准备训练数据12housing = strat_train_set.drop("median_house_value", axis=1)housing_labels = strat_train_set["median_house_value"].copy() 123456789from sklearn.impute import SimpleImputer#使用中值填充空值#median = housing["total_bedrooms"].median()#housing["total_bedrooms"].fillna(median)imputer = SimpleImputer(strategy="median")housing_num = housing.drop("ocean_proximity", axis=1)#删除非数值项imputer.fit(housing_num)X = imputer.transform(housing_num)housing_tr = pd.DataFrame(X, columns=housing_num.columns) 12print(housing_num.median().values)imputer.statistics_#每一个属性的中值保存在statistics_ [-118.51 34.26 29. 2119.5 433. 1164. 408. 3.5409] array([-118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409]) 1234567#处理非数值属性from sklearn.preprocessing import LabelEncoderencoder = LabelEncoder()housing_cat = housing["ocean_proximity"]housing_cathousing_cat_encoded = encoder.fit_transform(housing_cat)housing_cat_encoded array([0, 0, 4, ..., 1, 0, 3]) 12# 使用该映射方式会使两个相近的值比更相隔更远的值更相似print(encoder.classes_)#查看映射值 [&#39;&lt;1H OCEAN&#39; &#39;INLAND&#39; &#39;ISLAND&#39; &#39;NEAR BAY&#39; &#39;NEAR OCEAN&#39;] 12345#使用one-hot encoding解决上述问题，即每一组只有一个属性被映射为1，其它均为0from sklearn.preprocessing import OneHotEncoderencoder = OneHotEncoder()housing_cat_1hot = encoder.fit_transform(housing_cat.values.reshape(-1,1))housing_cat_1hot#scipy的稀疏矩阵 &lt;16512x5 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39; with 16512 stored elements in Compressed Sparse Row format&gt; 1housing_cat_1hot.toarray() array([[1., 0., 0., 0., 0.], [1., 0., 0., 0., 0.], [0., 0., 0., 0., 1.], ..., [0., 1., 0., 0., 0.], [1., 0., 0., 0., 0.], [0., 0., 0., 1., 0.]]) 1234from sklearn.preprocessing import LabelBinarizerencoder = LabelBinarizer()#LabelBinarizer(sparse_output=True)housing_cat_1hot = encoder.fit_transform(housing_cat)housing_cat_1hot #默认输出NumPy数组，使用sparse_output=True在LabelBinarizer中 array([[1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 0, 1], ..., [0, 1, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 1, 0]]) 自定义转换器1234567891011121314151617181920212223242526from sklearn.base import BaseEstimator, TransformerMixinrooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6class CombinedAttributesAddr(BaseEstimator, TransformerMixin): def __init__(self, add_bedrooms_per_room = True): self.add_bedrooms_per_room = add_bedrooms_per_room def fit(self, X, y=None): return self def transform(self, X, y=None): rooms_per_household = X[:, rooms_ix] / X[:, household_ix] popilation_per_household = X[:, population_ix] / X[:, household_ix] if self.add_bedrooms_per_room: bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix] return np.c_[X, rooms_per_household, popilation_per_household, bedrooms_per_room] else: return np.c_[X, rooms_per_household, popilation_per_household]attr_adder = CombinedAttributesAddr(add_bedrooms_per_room=False)housing_extra_attribs = attr_adder.transform(housing.values) 转换管道123456789# 通过管道定义转换步骤和顺序from sklearn.pipeline import Pipelinefrom sklearn.preprocessing import StandardScalernum_pipeline = Pipeline([ ('imputer', SimpleImputer(strategy='median')), ('attribs_adder', CombinedAttributesAddr()), ('std_scaler', StandardScaler()),])housing_num_tr = num_pipeline.fit_transform(housing_num) 123456789101112131415161718192021222324252627282930313233343536from sklearn.pipeline import FeatureUnionclass DataFrameSelector(BaseEstimator, TransformerMixin): def __init__(self, attribute_names): self.attribute_names = attribute_names def fit(self, X, y=None): return self def transform(self, X, y=None): return X[self.attribute_names].valuesnum_attribs = list(housing_num)cat_attribs = ["ocean_proximity"]# 多管道并行运行，直到都到达最终输出进行合并num_pipeline = Pipeline([ ('selector', DataFrameSelector(num_attribs)), ('imputer', SimpleImputer(strategy='median')), ('attribs_adder', CombinedAttributesAddr()), ('std_scaler', StandardScaler()),])cat_pipeline = Pipeline([ ('selector', DataFrameSelector(cat_attribs)), ('label_binarizer', OneHotEncoder()),])full_pipeline = FeatureUnion(transformer_list=[ ('num_pipeline', num_pipeline), ('cat_pipeline', cat_pipeline),])housing_prepared = full_pipeline.fit_transform(housing)housing_prepared &lt;16512x16 sparse matrix of type &#39;&lt;class &#39;numpy.float64&#39;&gt;&#39; with 198144 stored elements in Compressed Sparse Row format&gt; 训练模型1234from sklearn.linear_model import LinearRegressionlin_reg = LinearRegression()lin_reg.fit(housing_prepared, housing_labels) LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) 123some_data = housing.iloc[:5]some_labels = housing_labels.iloc[:5]some_data_prepared = full_pipeline.transform(some_data) 12print("Predictions:\t", lin_reg.predict(some_data_prepared))print("Labels:\t\t", list(some_labels)) Predictions: [210644.60465462 317768.80729055 210956.43320428 59218.98857356 189747.55852838] Labels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0] 计算误差12345from sklearn.metrics import mean_squared_errorhousing_predictions = lin_reg.predict(housing_prepared)lin_mse = mean_squared_error(housing_labels, housing_predictions)lin_rmse = np.sqrt(lin_mse)lin_rmse 68628.19819848923 使用决策树训练模型123from sklearn.tree import DecisionTreeRegressortree_reg = DecisionTreeRegressor()tree_reg.fit(housing_prepared, housing_labels) DecisionTreeRegressor(criterion=&#39;mse&#39;, max_depth=None, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter=&#39;best&#39;) 1234housing_predictions = tree_reg.predict(housing_prepared)tree_mse = mean_squared_error(housing_labels, housing_predictions)tree_rmse = np.sqrt(tree_mse)tree_rmse#过拟合 0.0 使用交叉验证集1234from sklearn.model_selection import cross_val_scorescores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring="neg_mean_squared_error", cv=10)rmse_scores = np.sqrt(-scores) 123456def display_scores(scores): print("Scores:", scores) print("Mean:", scores.mean()) print("Standard deviation:", scores.std())display_scores(rmse_scores) Scores: [69387.06653153 67286.59471614 70828.67634657 68637.08950797 71446.95561512 75698.68535069 72708.74498639 71492.80752186 76248.48154091 68464.80592567] Mean: 71219.99080428517 Standard deviation: 2844.881736248285 1234lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring="neg_mean_squared_error", cv=10)lin_rmse_scores = np.sqrt(-lin_scores)display_scores(lin_rmse_scores) Scores: [66782.73844317 66960.11809222 70347.95243904 74739.57055258 68031.13389115 71193.84181183 64969.63056023 68281.61137907 71552.91565846 67665.10081739] Mean: 69052.46136451427 Standard deviation: 2731.674003577556 使用随机森林1234567from sklearn.ensemble import RandomForestRegressorforest_reg = RandomForestRegressor()forest_reg.fit(housing_prepared, housing_labels)housing_predictions = forest_reg.predict(housing_prepared)forest_rmse = mean_squared_error(housing_labels, housing_predictions)forest_rmse = np.sqrt(forest_rmse)forest_rmse /usr/local/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22. &quot;10 in version 0.20 to 100 in 0.22.&quot;, FutureWarning) 22163.130551903472 12345from sklearn.model_selection import cross_val_scoreforest_rmse_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring="neg_mean_squared_error", cv=10)forest_rmse_scores = np.sqrt(-forest_rmse_scores)display_scores(forest_rmse_scores) Scores: [52527.47724318 49900.17270529 53256.69381502 54127.96523378 52565.69742633 55149.84634202 51794.92861894 51150.56514327 55913.16499668 52788.63800042] Mean: 52917.51495249358 Standard deviation: 1712.2101214708819 调整模型网格查找123456789101112#网格查找用于指定想要组合的属性，使用交叉验证集进行验证from sklearn.model_selection import GridSearchCVparam_grid = [ &#123;'n_estimators':[3, 10, 30], 'max_features': [2, 4, 6, 8]&#125;, &#123;'bootstrap': [False], 'n_estimators': [3, 10], 'max_features':[2, 3, 4]&#125;]forest_reg = RandomForestRegressor()grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error')grid_search.fit(housing_prepared, housing_labels)grid_search.best_params_ {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 30} 1grid_search.best_estimator_ RandomForestRegressor(bootstrap=True, criterion=&#39;mse&#39;, max_depth=None, max_features=6, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False) 123cvres = grid_search.cv_results_for mean_score, params in zip(cvres["mean_test_score"], cvres["params"]): print(np.sqrt(-mean_score), params) 64228.003388102516 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3} 55454.00031053945 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10} 53155.36771883186 {&#39;max_features&#39;: 2, &#39;n_estimators&#39;: 30} 60238.28891932349 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3} 53044.27139651894 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10} 50650.62627231678 {&#39;max_features&#39;: 4, &#39;n_estimators&#39;: 30} 58658.342221629726 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 3} 52358.51674007991 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 10} 49887.67768112314 {&#39;max_features&#39;: 6, &#39;n_estimators&#39;: 30} 58730.283997131446 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 3} 52432.46120216881 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 10} 49923.33236627694 {&#39;max_features&#39;: 8, &#39;n_estimators&#39;: 30} 63281.22274071418 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 3} 54214.06059814892 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 2, &#39;n_estimators&#39;: 10} 59820.193646740416 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_estimators&#39;: 3} 52310.6659263699 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 3, &#39;n_estimators&#39;: 10} 58663.61528699336 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_estimators&#39;: 3} 51510.14057731367 {&#39;bootstrap&#39;: False, &#39;max_features&#39;: 4, &#39;n_estimators&#39;: 10} 随机查找12#当查找范围小时使用GridSearch#范围大时使用随机查找 123#分析模型和误差fearture_importances = grid_search.best_estimator_.feature_importances_fearture_importances array([7.69331738e-02, 6.97499695e-02, 4.36568780e-02, 1.82661833e-02, 1.66823710e-02, 1.80076874e-02, 1.62985023e-02, 3.12925764e-01, 6.02271320e-02, 1.09872594e-01, 7.56850138e-02, 1.48263701e-02, 1.56744680e-01, 4.83159596e-05, 3.56162952e-03, 6.51373535e-03]) 1234extra_attribs = ["rooms_per_hhold", "pop_per_hhhold", "bedrooms_per_room"]cat_one_hot_attribs = list(encoder.classes_)attributes = num_attribs + extra_attribs + cat_one_hot_attribssorted(zip(fearture_importances, attributes), reverse=True) [(0.31292576369427055, &#39;median_income&#39;), (0.15674468046305143, &#39;INLAND&#39;), (0.1098725938995592, &#39;pop_per_hhhold&#39;), (0.07693317375274458, &#39;longitude&#39;), (0.07568501377495424, &#39;bedrooms_per_room&#39;), (0.06974996951561971, &#39;latitude&#39;), (0.06022713204358833, &#39;rooms_per_hhold&#39;), (0.043656877966590804, &#39;housing_median_age&#39;), (0.01826618328336908, &#39;total_rooms&#39;), (0.01800768738966343, &#39;population&#39;), (0.01668237100450264, &#39;total_bedrooms&#39;), (0.016298502278259873, &#39;households&#39;), (0.014826370100255391, &#39;&lt;1H OCEAN&#39;), (0.006513735353629898, &#39;NEAR OCEAN&#39;), (0.0035616295203113124, &#39;NEAR BAY&#39;), (4.831595962946045e-05, &#39;ISLAND&#39;)] 在测试集上评估123456789final_model = grid_search.best_estimator_X_test = strat_test_set.drop("median_house_value", axis=1)y_test = strat_test_set["median_house_value"].copy()X_test_prepared = full_pipeline.transform(X_test)final_prediction = final_model.predict(X_test_prepared)final_mse = mean_squared_error(y_test, final_prediction)final_rmse = np.sqrt(final_mse)final_rmse 47932.162242297694 源文件]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Hands-On Machine Learning with Scikit-Learn and TensorFlow</tag>
        <tag>sklearn</tag>
        <tag>回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVD]]></title>
    <url>%2F2018%2F05%2F31%2FSVD%2F</url>
    <content type="text"><![CDATA[SVD概述 从噪声数据中抽取相关特征 SVD优缺点 优点：简化数据，去除噪声，提高算法的结果 缺点：数据的转换可能难以理解 适用数据类型：数值型数据 SVD工作原理不同于特征值分解只能用于方阵，奇异值分解可以用于任何矩阵，奇异值分解的公式为： A_{m\times n}=U_{m\times m}\Sigma_{m\times n}V^T_{n \times n}其中$A$是一个$m\times n$的矩阵，$U$是一个$m\times m$的正定矩阵(向量正交)，$\Sigma$是一个$m\times n$的从大到小排列的对角矩阵(对角线的值为奇异值)，$V^T$是一个$n \times n$的正定矩阵(向量正交)。在某个奇异值的数目(r个)之后，其他的奇异值都置为0。即数据集拥有r个重要特征其余特征都是噪声或冗余特征。确定保留多少个奇异值可以通过以下公式: \frac{\sum_{ii}^k\Sigma_{ii}}{\sum_{ii}^n\Sigma_{ii}}\ge90\%其中$k$表示保留的奇异值，$n$表示总的奇异值，压缩后的数据表示为 A_{m\times n}\approx U_{m\times k}\Sigma_{k\times k}V^T_{k\times n} 奇异值分解 实现SVD基于协同过滤的推荐系统协同过滤(collaborative filtering) 是通过将用户和其他用户的数据进行对比来实现推荐的。当数据采用矩阵形式进行组织时，可以通过用户或物品之间的相似度利用已有的数据预测用户喜好。 相似度计算 用于相似度计算的矩阵通过用户对物品的意见计算相似度 欧氏距离手撕猪肉(Pulled Pork) 和 烤牛肉(Tri Tip) 之间的欧氏距离为： \sqrt{(4-4)^2+(3-3)^2+(2-1)^2}=1手撕猪肉(Pulled Pork) 和 鳗鱼饭(Unagi Don) 之间的欧氏距离为： \sqrt{(4-2)^2+(3-5)^2+(2-2)^2}=2.83从上面的数据中可以看出 手撕猪肉(Pulled Pork) 和 烤牛肉(Tri Tip) 之间的距离小于 手撕猪肉(Pulled Pork) 和 鳗鱼饭(Unagi Don) 之间的距离，因此 手撕猪肉(Pulled Pork) 和 烤牛肉(Tri Tip) 比与 鳗鱼饭(Unagi Don) 更相似。为了使相似度在$0$到$1$之间变化，并且物品越相似，它们相似度越大，相似度公式改写为： similarity=\frac{1}{1+distance} 皮尔逊相关系数皮尔逊相关系数(Pearson correlation) 度量的事两个向量之间的相似度。其相对于欧氏距离的优势在于它对用户评级的量级并不敏感。在NumPy中，皮尔逊相关系数的计算是由函数corrcoef()进行，它的取值范围是从$-1$到$+1$,为了将其归一化到$0$到$1$之间，所以相似度计算为： 0.5+0.5*corrcoef 余弦相似度余弦相似度(cosine similarity) 计算的事两个向量夹角的余弦值。如果夹角为$90^{\circ}$，则相似度为$0$；如果两个向量方向相同，相似度为$1.0$。余弦相似度的取值范围也在$-1$到$+1$之间，因此也要将其归一化到$0$到$1$。两个向量$A$和$B$夹角的余弦相似度如下: cos\theta=\frac{A \cdot B}{\mid\mid A\mid\mid\;\mid\mid B\mid\mid}其中$\mid\mid A\mid\mid$和$\mid\mid B\mid\mid$表示向量$A$、$B$的2范数。向量$[4,2,2]$的2范数为: \sqrt{4^2+2^2+2^2}在NumPy的线性代数工具箱中提供了范数的计算方法linalg.norm() 相似度计算 1234567891011121314151617181920212223242526272829303132def eclud_sim(in_a, in_b): """欧式距离计算相似度 相似度=1/(1+距离) :param in_a: 矩阵A :param in_b: 矩阵B :return: """ return 1.0/(1.0+la.norm(in_a-in_b))def pears_sim(in_a, in_b): """皮尔逊相关系数计算相似度 相似度=0.5+0.5*相关系数 :param in_a: 矩阵A :param in_b: 矩阵B :return: """ if len(in_a) &lt; 3: return 1.0 return 0.5+0.5*np.corrcoef(in_a, in_b, rowvar=0)[0][1]def cos_sim(in_a, in_b): """余弦相似度 相似度=向量乘积/范数乘积 :param in_a: 矩阵A :param in_b: 矩阵B :return: """ num = np.float(in_a.T*in_b) denom = la.norm(in_a)*la.norm(in_b) return 0.5+0.5*(num/denom) 上面的相似度计算都是假设数据采用了列向量方式进行表示，这里使用列向量暗示将利用基于物品的相似度计算方法。使用 基于物品(item-based) 的相似度还是 基于用户(user-based) 的相似度1取决于用户或物品的数目。基于物品相似度计算的时间会随物品数量的增加而增加；基于用户的相似度计算的时间则会随用户数量的增加而增加。对于大部分产品导向的推荐引擎，用户的数量往往大于物品的数量。 评价推荐引擎通过交叉测试的方法，将已知评分值去掉，然后对它们进行预测，最后计算预测值和真实值之间的差异。用于推荐引擎评价的指标是称为 最小均方根误差(Root Mean Squared Error, RMSE) 的指标，它首先计算均方误差的平均值然后取其平方根。2 餐馆菜肴推荐引擎基于物品相似度的推荐引擎推荐引擎工作过程：给定一个用户，系统会为此用户返回N个最好的推荐菜。 寻找用户没有评级的菜肴，即在用户-物品矩阵中的0值 在用户没有评级的所有物品中，对每个物品预计一个可能的评级分数 对这些物品的评分从高到低进行排序，返回前N个物品 基于物品相似度的推荐引擎12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667def stand_est(data_mat, user, sim_meas, item): """计算用户对物品的估计评分值 :param data_mat: 数据矩阵 :param user: 用户编号 :param sim_meas: 相似度计算方法 :param item: 物品编号 :return: """ # 获取物品总数 n = np.shape(data_mat)[1] # 总的相似度 sim_total = 0.0 # 总的相似度评分 rat_sim_total = 0.0 for j in range(n): # 对每一个物品 # 计算用户的评分 user_rating = data_mat[user, j] if user_rating == 0: # 如果用户没有对该物品评分 # 跳过该物品 continue # 找到重合的元素 # 即寻找用户都评级的两个物品 over_lap = np.nonzero(np.logical_and(data_mat[:, item].A &gt; 0, data_mat[:, j].A &gt; 0))[0] if len(over_lap) == 0: # 如果没有重合的元素 similarity = 0 else: # 计算重合物品的相似度 similarity = sim_meas(data_mat[over_lap, item], data_mat[over_lap, j]) print('%d 和 %d 相似度为 %f' % (item, j, similarity)) # 累加相似度 sim_total += similarity # 计算相似度和当前用户评分的乘积 rat_sim_total += similarity*user_rating if sim_total == 0: # 直接退出 return 0 else: # 对评分进行归一化 return rat_sim_total/sim_totaldef recommend(data_mat, user, n=3, sim_meas=cos_sim, est_method=stand_est): """推荐 :param data_mat: 数据矩阵 :param user: 用户编号 :param n: 推荐结果数量 :param sim_meas: 相似度计算函数 :param est_method: 估计方法 :return: """ # 找到用户未评级的武平 unrated_items = np.nonzero(data_mat[user, :].A == 0)[1] if len(unrated_items) == 0: return '你对所有物品都进行了评级' item_scores = [] for item in unrated_items: # 对每一个未评级的物品 # 计算估计评分 estimated_score = est_method(data_mat, user, sim_meas, item) # 将物品编号和对应评分存入列表 item_scores.append((item, estimated_score)) # 排序后返回前n个值 return sorted(item_scores, key=lambda j: j[1], reverse=True)[:n] 使用SVD提高推荐效果 用户菜肴矩阵 基于SVD的评分估计1234567891011121314151617181920212223242526272829303132333435def svd_est(data_mat, user, sim_meas, item): """基于SVD的评分估计 :param data_mat: :param user: :param sim_meas: :param item: :return: """ n = np.shape(data_mat)[1] sim_total = 0.0 rat_sim_total = 0.0 # 使用SVD分解 u, sigma, vt = la.svd(data_mat) # 使用包含90%能量值的奇异值 # 将其转换为一个对角矩阵 sig4 = np.mat(np.eye(4)*sigma[:4]) # 使用U矩阵将物品转换到低维空间 x_formed_items = data_mat.T*u[:, :4]*sig4.I for j in range(n): # 对所有物品 # 获取用户的评分值 user_rating = data_mat[user, j] if user_rating == 0 or j == item: continue # 计算低维空间下的相似度 similarity = sim_meas(x_formed_items[item, :].T, x_formed_items[j, :].T) print('%d 和 %d 相似度为 %f' % (item, j, similarity)) # 累加相似度 sim_total += similarity # 计算相似度和当前用户评分的乘积 rat_sim_total += similarity*user_rating if sim_total == 0: return 0 else: return rat_sim_total/sim_total 基于SVD的图像压缩 基于SVD的图像压缩1234567891011121314151617181920212223def img_compress(num_sv=3, thresh=0.8): """图像压缩 :param num_sv: 奇异值数目 :param thresh: 阈值 :return: """ myl = [] with open('data/0_5.txt', 'r', encoding='utf-8') as f: for line in f.readlines(): new_row = [] for i in range(32): new_row.append(np.int(line[i])) myl.append(new_row) my_mat = np.mat(myl) print('原始矩阵') print_mat(my_mat, thresh) u, sigma, v_t = la.svd(my_mat) sig_recon = np.mat(np.zeros((num_sv, num_sv))) for k in range(num_sv): sig_recon[k, k] = sigma[k] recon_mat = u[:, :num_sv]*sig_recon*v_t[:num_sv, :] print('使用%d个奇异值重构矩阵' % num_sv) print_mat(recon_mat, thresh) 示例代码 1.在[用于相似度计算的矩阵]这张图中，行与行之间比较的是基于用户的相似度，列与列之间比较的事基于物品的相似度。 ↩2.如果评级在1星到5星，而RMSE为1，意味着预测值和真实评价相差一个星级 ↩]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实战</tag>
        <tag>机器学习工具</tag>
        <tag>降维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PCA]]></title>
    <url>%2F2018%2F05%2F30%2FPCA%2F</url>
    <content type="text"><![CDATA[PCA(主成分分析法)概述1 数据从原来的坐标系转换到新的坐标系，新坐标系的选择是由数据本身决定的。第一个坐标轴选择的是原始数据中方差最大的方向，第二个坐标轴的选择和第一个坐标轴正交且具有最大方差的方向 PCA的优缺点 优点：降低数据的复杂性，识别最重要的多个特征 缺点：不一定需要，且可能损失有用信息 适用数据类型：数值型数据 工作原理 覆盖整个数据集的直线其中直线B最长，并给出了数据集中差异化最大的方向 在PCA中，对数据的坐标进行了旋转，该旋转的过程取决于数据的本身。第一条坐标轴旋转到覆盖数据的最大方差位置，即图中的直线B。数据的最大方差给出了数据的最重要的信息。在选择了覆盖数据最大差异性的坐标轴后，选择第二条坐标轴。第二条坐标轴是与第一条坐标轴正交的直线，它是覆盖数据次大差异性的坐标轴，即图中直线C。 二维空间的三个类别当在数据集上应用PCA时，可以去掉一维，从而使分类问题变得更容易 PCA的实现第一个主成分是数据差异性最大(即方差最大)的方向提取出来的，第二个主成分则来自数据差异性次大的方向且与第一个主成分方向正交。通过数据集的协方差矩阵及其特征值分析，可以求得主成分的值。一旦得到协方差矩阵的特征向量，就可以保留最大的N个值。这些特征向量给出了N个最重要特征的真实结构。可以通过将数据乘上这N个特征向量而将它转换到新的空间。 伪代码 123456减去平均值计算协方差矩阵计算协方差矩阵的特征值和特征向量将特征值从大到小排序保留最上面的N个特征向量将数据转换到上述N个特征向量构建的新空间中 主成分分析 1234567891011121314151617181920212223def pca(data_mat, top_n_feat=9999999): """主成分分析法 :param data_mat: 数据集 :param top_n_feat: 最大的特征值的个数 :return: """ # 计算并减去原始数据的平均值 mean_vals = np.mean(data_mat, axis=0) mean_removed = data_mat-mean_vals # 计算协方差矩阵及其特征值 cov_mat = np.cov(mean_removed, rowvar=0) eig_vals, eig_vects = np.linalg.eig(np.mat(cov_mat)) # 根据特征值排序 eig_vals_ind = np.argsort(eig_vals) # 获取前top_n_feat个特征值的下标 eig_vals_ind = eig_vals_ind[:-(top_n_feat+1):-1] # 获取特征向量 red_eig_vects = eig_vects[:, eig_vals_ind] # 将数据转换到新空间中 low_dem_data_mat = mean_removed*red_eig_vects recon_mat = (low_dem_data_mat*red_eig_vects.T)+mean_vals # 返回降维后的数据和重构后的数据 return low_dem_data_mat, recon_mat 运行 12345678910111213141516In [2]: data_mat = pca.load_data_set('data/testSet.txt')In [3]: low_d_mat,recon_mat = pca.pca(data_mat, 1)In [4]: shape(low_d_mat)Out[4]: (1000, 1)In [5]: fig = plt.figure()In [6]: ax = fig.add_subplot(111)In [9]: ax.scatter(data_mat[:,0].flatten().A[0], data_mat[:,1].flatten().A[0],marker='^',s=50,c='red')Out[9]: &lt;matplotlib.collections.PathCollection at 0x1fdf83e9860&gt;In [10]: ax.scatter(recon_mat[:,0].flatten().A[0], recon_mat[:,1].flatten().A[0],marker='o',s=50,c='blue')Out[10]: &lt;matplotlib.collections.PathCollection at 0x1fd83a3a470&gt; 运行结果 1.有其他两种降维方法。一种是因子分析(Factor Analysis)，假设在观察数据的生成中有一些观察不到的隐变量，假设观察数据是这些隐变量和某些噪声的线型组合，就可以通过找到隐变量实现数据的降维；另一种是独立成分分析(Independent Component Analysis,ICA)，ICA假设是从N个数据源生成，数据为多个数据源的混合观察结果，这些数据源之间在统计上是相互独立的，如果数据源的数目少于观察数据的数目，就可以实现降维过程。 ↩]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实战</tag>
        <tag>PCA</tag>
        <tag>维度约简</tag>
        <tag>机器学习工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FP-growth算法]]></title>
    <url>%2F2018%2F05%2F30%2FFP-growth%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[FP-growth算法概述 基于Apriori构建，将数据集存储在一个特定的称作FP树的结构之后发现频繁项集或者频繁项对，即常在一块出现的元素项的集合FP树 FP-growth算法的优缺点 优点：一般要快于Apriori 缺点：实现比较困难，在某些数据集上性能会下降 适用数据类型：标称型数据 工作原理FP-growth算法将数据存储在一种称为FP树的紧凑数据结构中。FP代表 频繁模式(Frequent Pattern)。一棵FP树与计算机科学中的其他树结构类似，但是它通过连接(link)来连接相似元素，被连起来的元素项可以看成一个链表。一个元素项可以在一棵FP树中出现多次。FP树会存储项集的出现频率，而每个项集会以路径的方式存储在树中。存在相似元素的集合会共享树的一部分。只有当集合之间完全不同时，树才会分叉。树节点上给出集合中的单个元素及其在序列中的出现次数，路径会给出该序列的出现次数。相似项之间的连接即 节点连接(node link)，用于快速发现相似项的位置。 简单示例数据 一棵FP树上图中元素项$z$出现了5次，集合$\{r,z\}$出现了1次，$\{t,s,y,x,z\}$出现了2次，$\{t,r,y,x,z\}$出现了一次。元素$z$的右边标的是5，表示z出现了5次，刚才的集合已经给出了4次出现，所以$z$单独出现1次。 FP-growth算法工作流程如下： 构建FP树 扫描第一遍数据，对所有元素项的出现次数进行计数(如果某元素不频繁，那么该元素的超集也不频繁) 第二遍扫描，利用FP树挖掘频繁项集 FP-growth算法的一般流程 收集数据：用任何方法 准备数据：由于存储的是集合，所以需要离散数据。处理连续数据需要将它们量化为离散值 分析数据：使用任意方法 训练算法：构建一个FP树，并对树进行挖掘 测试算法：没有测试过程 使用算法：可用于识别经常出现的元素项，从而用于指定决策、推荐元素或进行预测 FP-growth算法实现构建FP树 FP树类定义123456789101112131415161718192021222324252627282930# 树结构class TreeNode: def __init__(self, name_value, num_occur, parent_node): """初始化方法 :param name_value: 节点名称 :param num_occur: 对应节点元素的个数 :param parent_node: 父节点 """ self.name = name_value self.count = num_occur # 链接相似的元素项 self.node_link = None self.parent = parent_node self.children = &#123;&#125; def inc(self, num_occur): """计数值 :param num_occur: 数量 :return: """ self.count += num_occur def display(self, ind=1): """显示树结构 :param ind: 子树锁进个数 :return: """ print(' '*ind, self.name, ' ', self.count) for child in self.children.values(): child.display(ind+1) 除了给出的FP树之外，还需要一个头指针表来指向给定类型的第一个实例。利用头指针表，可以快速访问FP树中一个给定类型的所有元素。这里使用一个字典作为数据结构来保存头指针表，除了存放指针外，头指针表还可以用来保存FP树中每类元素的总数。 带头指针表的FP树 第一次遍历数据集会获得每个元素项的出现频率，然后去掉不满足最小支持度的元素项。下一步构建FP树，在构建时，读入每个项集并将其添加到一条已存在的路径中，如果该路径不存在，则创建一条新路径。由于每个事务都是无序集合，假设有集合$\{z,x,y\}$和$\{y,z,r\}$，在FP树中相同项($y$)只表示一次。为了解决这个问题，在将集合添加到树之前，需要对每个集合基于元素项的绝对出现频率来进行排序。 将非频繁项移除并且重排序后的事务数据集 对事务记录过滤和排序后，就可以构建FP树。从空集($\phi$)开始，向其中不断添加频繁项集。过滤、排序后的事务依次添加到树中，如果树中已存在现有元素，则增加现有元素的值；如果元素不存在，则向树添加一个分枝。 FP树构建过程 FP树构建函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687create_tree(data_set, min_sup=1): """构建FP树 :param data_set: 数据集 :param min_sup: 最小支持度 :return: """ # 用字典创建头指针表 header_table = &#123;&#125; for trans in data_set: # 扫描数据集 for item in trans: # 统计每个元素项出现的频度 # 将信息存储在头指针标中 header_table[item] = header_table.get(item, 0) + data_set[trans] keys = header_table.keys() for key in list(keys): if header_table[key] &lt; min_sup: # 删除头指针表中出现次数少于最小支持度的项 del(header_table[key]) # 对头指针表中的数据去重 freq_item_set = set(header_table.keys()) if len(freq_item_set) == 0: # 如果去重后的数据为空则返回None return None, None for key in header_table: # 扩展头指针表 # 用于保存元素项的计数值及指向第一个元素项的指针 header_table[key] = [header_table[key], None] # 创建只包含空集合的根节点 ret_tree = TreeNode('Null Set', 1, None) for tran_set, count in data_set.items(): # 遍历数据集 local_d = &#123;&#125; for item in tran_set: if item in freq_item_set: # 统计元素全局频率 local_d[item] = header_table[item][0] if len(local_d) &gt; 0: # 如果元素大于0 # 根据全局频率对每个事务中的元素进行排序 ordered_items = [v[0] for v in sorted(local_d.items(), key=lambda p: p[1], reverse=True)] # 更新树 update_tree(ordered_items, ret_tree, header_table, count) return ret_tree, header_tabledef update_tree(items, in_tree, header_table, count): """更新树 :param items: 一个项集 :param in_tree: 树 :param header_table: 头指针表 :param count: 该项集对应的计数 :return: """ if items[0] in in_tree.children: # 测试事务的第一恶搞元素是否作为子节点 # 如果是，更新该元素项的计数 in_tree.children[items[0]].inc(count) else: # 如果不存在 # 创一个新的子节点添加到树中 in_tree.children[items[0]] = TreeNode(items[0], count, in_tree) if header_table[items[0]][1] is None: # 如果头指针没有指向 # 将其添加到头指针指向 header_table[items[0]][1] = in_tree.children[items[0]] else: # 否则在该元素项头指针列表后面进行更新 update_header(header_table[items[0]][1], in_tree.children[items[0]]) if len(items) &gt; 1: # 迭代调用 # 每次去掉列表中的第一个元素 update_tree(items[1::], in_tree.children[items[0]], header_table, count)def update_header(node_to_test, target_node): """更新头指针列表 :param node_to_test: 指针为止 :param target_node: 当前节点 :return: """ while node_to_test.node_link is not None: # 更新指针为止直到成为尾指针 node_to_test = node_to_test.node_link # 将该节点添加到尾指针后面 node_to_test.node_link = target_node 简单数据集及包装器 12345678910111213141516171819202122def load_simple_dat(): """创建简单数据集 :return: """ simple_dat = [['r', 'z', 'h', 'j', 'p'], ['z', 'y', 'x', 'w', 'v', 'u', 't', 's'], ['z'], ['r', 'x', 'n', 'o', 's'], ['y', 'r', 'x', 'z', 'q', 't', 'p'], ['y', 'z', 'x', 'e', 'q', 's', 't', 'm']] return simple_datdef create_init_set(data_set): """包装数据为字典 :param data_set: 数据集 :return: """ ret_dict = &#123;&#125; for trans in data_set: ret_dict[frozenset(trans)] = 1 return ret_dict 挖掘频繁项集抽取条件模式基从FP树中抽取频繁项集的步骤： 从FP树中获得条件模式基 利用条件模式基，构建一个条件FP树 迭代重复步骤(1)和步骤(2)，直到树包含一个元素项为止 条件模式基(conditional pattern base) 是以查找元素项为结尾的路径集合。每一条路径都是一条 前缀路径(prefix path)，即介于所查找元素项与根节点之间的所有内容。 带头指针表的FP树上图中符号$r$的前缀路径是$\{x,s\}$、$\{z,x,y\}$和$\{z\}$。每一条前缀路径都与一个计数值关联。该计数值等于起始元素项的计数值，该计数值给了每条路径上r的数目 每个频繁项的前缀路径 查找前缀路径123456789101112131415161718192021222324252627282930313233def ascend_tree(leaf_node, prefix_path): """上溯FP树 :param leaf_node: 树节点 :param prefix_path: 前缀路径 :return: """ if leaf_node.parent is not None: # 如果树节点的父节点不为空 # 添加前缀路径 prefix_path.append(leaf_node.name) # 上溯父节点 ascend_tree(leaf_node.parent, prefix_path)def find_prefix_path(base_path, tree_node): """寻找前缀路径 :param base_path: 基本路径 :param tree_node: 树节点 :return: """ # 条件模式基字典 cond_paths = &#123;&#125; while tree_node is not None: # 如果树节点不为空 prefix_path = [] # 从该节点上溯 ascend_tree(tree_node, prefix_path) if len(prefix_path) &gt; 1: # 更新该条件模式基的数目 cond_paths[frozenset(prefix_path[1:])] = tree_node.count # 指向链表的下一个元素 tree_node = tree_node.node_link return cond_paths 构建条件FP树对于每一个频繁项都要创建一棵条件FP树，使用每一个频繁项的条件模式基作为输入数据，通过相同的建树代码构建条件树。然后递归发现频繁项、发现条件模式基以及另外的条件数 元素项t的条件FP树构建过程最初树以空集作为根节点。接下来原始的集合$\{y,x,s,z\}$中的集合$\{y,x,z\}$被添加进来。因为不满足最小支持度要求，字符$s$并没有加入。类似地$\{y,x,z\}$也从原始集合$\{y,x,r,z\}$中添加进来 查找频繁项集12345678910111213141516171819202122232425def mine_tree(in_tree, header_table, min_sup, pre_fix, freq_item_list): """查找频繁项集 :param in_tree: 树 :param header_table: 头指针列表 :param min_sup: 最小支持度 :param pre_fix: 前缀路径 :param freq_item_list: 频繁项列表 :return: """ # 对头指针表中的元素按照其出现频率进行排序 big_list = [v[0] for v in sorted(header_table.items(), key=lambda p: p[0])] for base_pat in big_list: # 将频繁项添加到频繁项列表 new_freq_set = pre_fix.copy() new_freq_set.add(base_pat) freq_item_list.append(new_freq_set) # 查找条件模式基 cond_patt_bases = find_prefix_path(base_pat, header_table[base_pat][1]) # 创建条件基对应的FP树 my_cond_tree, my_head = create_tree(cond_patt_bases, min_sup) if my_head is not None: # 如果树中有元素项递归调用 print('conditional tree for: ', new_freq_set) my_cond_tree.display() mine_tree(my_cond_tree, my_head, min_sup, new_freq_set, freq_item_list) 示例：从新闻网站点击流中挖掘 提取新闻频繁项集1234567891011121314151617181920212223242526272829303132333435363738In [10]: parsed_dat = [line.split() for line in open('data/kosarak.dat').readlines()]#将数据集导入列表In [11]: init_set = fp_growth.create_init_set(parsed_dat)#初始集合格式化In [12]: my_fp_tree, my_header = fp_growth.create_tree(init_set, 100000)#构建FP树，寻找至少被10万人浏览过的新闻In [13]: my_freq_list = []#创建空列表保存频繁项集In [14]: fp_growth.mine_tree(my_fp_tree,my_header,100000,set([]),my_freq_list)conditional tree for: &#123;'1'&#125; Null Set 1 6 107404conditional tree for: &#123;'11'&#125; Null Set 1 6 261773conditional tree for: &#123;'3'&#125; Null Set 1 6 186289 11 117401 11 9718conditional tree for: &#123;'11', '3'&#125; Null Set 1 6 117401In [15]: len(my_freq_list)Out[15]: 9In [16]: my_freq_listOut[16]:[&#123;'1'&#125;, &#123;'1', '6'&#125;, &#123;'11'&#125;, &#123;'11', '6'&#125;, &#123;'3'&#125;, &#123;'11', '3'&#125;, &#123;'11', '3', '6'&#125;, &#123;'3', '6'&#125;, &#123;'6'&#125;]]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实战</tag>
        <tag>频繁项集</tag>
        <tag>无监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apriori算法]]></title>
    <url>%2F2018%2F05%2F29%2FApriori%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Apriori算法概述 更智能的方法在合理的时间范围内找到频繁项集 Apriori算法的优缺点 优点：易编码实现 缺点：在大数据集上可能较慢 适用数据类型：数值型或者标称型数据 工作原理从大规模数据集中寻找物品间的隐含关系被称作 关联分析(association analysis) 或者 关联规则学习(association rule learning)。关联分析是一种在大规模数据集上寻找关系的任务，这些关系有两种形式： 频繁项集(frequent item sets)：经常出现在一起的物品的集合 关联规则(association rules)：暗示两种物品之间可能存在很强的关系 某杂货部交易清单 交易号码 商品 0 豆奶，莴苣 1 莴苣，尿布，葡萄酒，甜菜 2 豆奶，尿布，葡萄酒，橙汁 3 莴苣，豆奶，尿布，葡萄酒 4 莴苣，豆奶，尿布，橙汁 {豆奶，尿布，葡萄酒} 指一个频繁项集；尿布-&gt;葡萄酒 是一个关联规则目标是如果有人买了尿布，他很可能也会买葡萄酒 支持度(support) 被定义为数据集中包含该项集的记录所占的比例。上表中，{豆奶} 的支持度为$\frac{4}{5}$，{豆奶，尿布} 的支持度为$\frac{3}{5}$。支持度是针对项集来说，可以定义一个最小支持度，而只保留满足最小支持度的项集。可信度(confidence) 是针对一条关联规则来定义的。{尿布}-&gt;{葡萄酒} 这条关联规则的可信度被定义为 支持度({尿布，葡萄酒})/支持度({尿布}) 。上表中 {尿布，葡萄酒} 的支持度为$\frac{3}{5}$，尿布的支持度为$\frac{4}{5}$，所以{尿布}-&gt;{葡萄酒} 的可信度为$\frac{3}{4}=0.75$支持度和可信度是用来量化关联分析是否成功的方法。 集合{0,1,2,3}的所有可能图中从上往下的第一个集合是$\phi$，表示空集或不包含任何物品的集合。物品集合之间的连线表名两个或者更多集合可以组合形成一个更大的集合。这里对于4种物品的集合遍历了14遍数据。Apriori原理是指如果某个项集是频繁的，那么它的所有自己也是频繁的。意味着如果${0,1}$是频繁的，那么${0}$、${1}$也一定是频繁的。反过来说，如果一个项集是非频繁集，那么它的所有超集也是非频繁的。 标识出非频繁项集的所有项集图中给出了所有可能的项集，其中非频繁项集用灰色表示。由于集合${2,3}$是非频繁的，因此${0,2,3}$、$1,2,3$和$0,1,2,3$也是非频繁的，它们的支持度不需要计算 Apriori算法的一般流程 收集数据：使用任意方法 准备数据：任何数据类型都行，只保存集合 分析数据：任何方法 训练算法：使用Apriori算法来找到频繁项集 测试算法：不需要测试过程 使用算法：用于发现频繁项集以及物品之间的关联规则 实现Apriori算法发现频繁集 数据集扫描伪代码 123456789对数据集中的每条记录tran: 对每个候选集can: 检查can是否是tran的子集： 如果是： 增加can的计数值对每个候选项集： 如果其支持度不低于最小值： 保留该项集返回所有频繁项集列表 辅助函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def load_data_set(): """生成数据集 :return: """ return [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]def create_c1(data_set): """构建大小为1的所有候选项集的集合 :param data_set: :return: """ # 构建集合C1 candidate_1 = [] for transaction in data_set: for item in transaction: # 提取候选集 if not [item] in candidate_1: candidate_1.append([item]) candidate_1.sort() # 创建一个不可改变的集合 return list(map(frozenset, candidate_1))def scan_d(d, candidate_k, min_support): """创建满足最小支持度的列表 :param d: 数据集 :param candidate_k: 候选集合列表 :param min_support: 最小支持度 :return: """ ss_cnt = &#123;&#125; for tid in d: # 遍历所有交易记录 for can in candidate_k: # 遍历所有候选集 if can.issubset(tid): # 如果候选集是记录的一部分 if can not in ss_cnt.keys(): # 为记载则记为1 ss_cnt[can] = 1 else: # 否则记录+1 ss_cnt[can] += 1 # 记录的大小 num_items = np.float(len(d)) ret_list = [] support_data = &#123;&#125; for key in ss_cnt: # 扫描所有候选集 # 计算候选集的支持度 support = ss_cnt[key]/num_items if support &gt;= min_support: # 候选集的支持度大于最小支持度 # 将候选集加入列表 ret_list.insert(0, key) # 记录候选集的支持度 support_data[key] = support return ret_list, support_data Apriori算法伪代码 1234当集合中项的个数大于0时： 构建一个k个项组成的候选项集的列表 检查数据以确认每个项集是频繁的 保留频繁项集并构建k+1项组成的候选项集的列表 Apriori算法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def apriori_gen(list_k, k): """构建大小为k的所有候选项集的集合 :param list_k: 频繁项列表 :param k: 项集元素个数 :return: """ ret_list = [] # 计算l_k的元素个数 len_list_k = len(list_k) for i in range(len_list_k): for j in range(i+1, len_list_k): # 比较l_k中的每个元素和其他元素 # 前k-2个元素相同 l_1 = list(list_k[i])[:k-2] l_2 = list(list_k[j])[:k-2] l_1.sort() l_2.sort() if l_1 == l_2: # 求并并添加到列表中 ret_list.append(list_k[i] | list_k[j]) return ret_listdef apriori(data_set, min_support=0.5): """主函数 :param data_set: :param min_support: :return: """ # 创建大小为1的所有候选项集的集合 candidate_1 = create_c1(data_set) # 数据去重 d = list(map(set, data_set)) # 计算候选项集为1的满足最小支持度的列表 list_1, support_data = scan_d(d, candidate_1, min_support) lis = [list_1] k = 2 while len(lis[k-2]) &gt; 0: # 创建大小为k的所有候选项集的集合 candidate_k = apriori_gen(lis[k-2], k) # 计算候选项集为k的满足最小支持度的列表 list_k, support_k = scan_d(d, candidate_k, min_support) # 更新支持度字典 support_data.update(support_k) # 添加候选集为k的满足最小支持度的列表 lis.append(list_k) k += 1 return lis, support_data 从频繁项集挖掘关联规则某个元素或者某个元素集合可能会推导出另一个元素。如果有一个频繁项集 {豆奶,莴苣}，那么就有一条关联规则”豆奶—&gt;莴苣“。这意味着如果有人购买了豆奶，那么在统计上他会购买莴苣的概率较大。但反之不一定成立，也就是说即使”豆奶—&gt;莴苣“统计上显著，那么”莴苣—&gt;豆奶“也不一定成立。(从逻辑研究上讲，箭头左边的集合称作前件，箭头邮编的集合称为后件)对于关联规则，使用可信度来进行量化。一条规则$P\to H$的可信度定义为$\frac{support(P\mid H)}{support(P)}$。其中$P\mid H$是指所有出现在集合P或者集合H中的元素。 对于频繁项集{0,1,2,3}的关联规则网格示意图阴影区域给出的事低可信度的规则。如果发现$0,1,2\to 3$是一条低可信度规则，那么所有其他以$3$作为后件的规则可信度也会较低类似于频繁项集生成。如果某条规则不满足最小可信度要求，那么该规则的所有子集也不会满足最小可信度要求。假设规则$0,1,2\to 3$并不满足最小可信度要求，那么任何左部为${0,1,2}$子集的规则也不会满足最小可信度要求。 从一个频繁项集开始 创建一个规则列表，其中规则右部只包含一个元素 然后对这些规则进行测试 合并所有剩余规则来创建一个新的规则列表，其中规则右部包含两个元素这种方法被称为分级法 关联规则生成123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566def rule_from_conseq(freq_set, h, support_data, big_rule_list, min_conf=0.7): """关联规则生成 :param freq_set: 频繁项集 :param h: 每个频繁项集只包含单个元素集合的列表 :param support_data: 频繁项集支持数据的字典 :param big_rule_list: 包含可信度的规则列表 :param min_conf: 最小可信度 :return: """ # 计算h中频繁集大小 m = len(h[0]) if len(freq_set) &gt; m+1: # 频繁项集大到可以移除大小为m的子集 # 生成h中元素的无重复组合 hmp1 = apriori_gen(h, m+1) # 计算可信度 hmp1 = calc_conf(freq_set, hmp1, support_data, big_rule_list, min_conf) if len(hmp1) &gt; 1: # 如果不止一条规则满足要求 # 进行迭代 rule_from_conseq(freq_set, hmp1, support_data, big_rule_list, min_conf)def calc_conf(freq_set, h, support_data, big_rule_list, min_conf=0.7): """计算可信度 :param freq_set: 频繁项集 :param h:每个频繁项集只包含单个元素集合的列表 :param support_data: 频繁项集支持数据的字典 :param big_rule_list: 包含可信度的规则列表 :param min_conf: 最小可信度 :return: """ pruned_h = [] for conseq in h: # 计算所有项集的可信度 conf = support_data[freq_set]/support_data[freq_set-conseq] if conf &gt;= min_conf: # 可信度大于最小可信度 print(freq_set-conseq, '--&gt;', conseq, 'conf:', conf) # 加入列表 big_rule_list.append((freq_set-conseq, conseq, conf)) pruned_h.append(conseq) return pruned_hdef generate_rules(lis, support_data, min_conf=0.7): """关联规则生成主函数 :param lis: 频繁项集列表 :param support_data: 频繁项集支持数据的字典 :param min_conf: 最小可信度阈值 :return: """ # 包含可信度的规则列表 big_rule_list = [] for i in range(1, len(lis)): # 获取两个或更多元素的列表 for freq_set in lis[i]: # 对每个频繁项集创建只包含单个元素集合的列表 h_1 = [frozenset([item]) for item in freq_set] if i &gt; 1: # 如果频繁项集的元素数目超过2就对它进行进一步合并 rule_from_conseq(freq_set, h_1, support_data, big_rule_list, min_conf) else: # 只有两个元素则计算可信度 calc_conf(freq_set, h_1, support_data, big_rule_list, min_conf) return big_rule_list 示例代码]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实战</tag>
        <tag>频繁项集</tag>
        <tag>无监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[K-均值]]></title>
    <url>%2F2018%2F05%2F28%2FK-%E5%9D%87%E5%80%BC%2F</url>
    <content type="text"><![CDATA[K-均值算法概述 发现k个不同的簇，且每个簇的中心采用簇中所含值的均值计算而成 k-均值算法的优缺点 优点：容易实现 缺点：可能收敛到局部最小值，在大规模数据集上收敛较慢 适用数据类型：数值型数据 工作原理 随机确定k个初始点作为质心 将数据集中的每个点分配到一个簇中(为每个点找距其最近的质心，将其分配给该质心所对应的簇) 每个簇的质心更新为该簇所有点的平均值 k-均值的一般流程 收集数据：使用任意方法 准备数据：需要数值型数据来计算距离，可以将标称型数据映射为二值型数据再用于距离计算 分析数据：使用任意方法 训练数据：无监督学习无训练过程 测试算法：应用聚类算法、观察结果。可以使用量化的误差指标如误差平方和来评价算法的结果 使用算法：通常情况下，簇质心可以代表整个簇的数据来做出决策 实现k-均值算法k-均值算法 伪代码 12345678创建k个点作为起始质心(随机选择)当任意一个点的簇分配结果发生改变时： 对数据集中的每个数据点： 对每个质心： 计算质心与数据点之间的距离 将数据点分配到距其最近的簇 对每个簇： 计算簇中所有点的均值并将均值作为质心 支持函数 1234567891011121314151617181920212223242526272829303132333435363738def load_data_set(filename): """加载数据集 :param filename: 文件名 :return: """ data_mat = [] with open(filename, 'r', encoding='utf-8') as f: for line in f.readlines(): cur_line = line.strip().split('\t') flt_line = list(map(float, cur_line)) data_mat.append(flt_line) return data_matdef dist_eclud(vec_a, vec_b): """计算向量欧式距离 :param vec_a: 向量a :param vec_b: 向量b :return: """ return np.sqrt(np.sum(np.power(vec_a-vec_b, 2)))def rand_cent(data_set, k): """构造质心 :param data_set: 数据集 :param k: 质心个数 :return: """ n = np.shape(data_set)[1] centroids = np.mat(np.zeros((k, n))) for j in range(n): # 生成质心位于每个数据点的最大值和最小值之间 min_j = np.min(data_set[:, j]) max_j = np.max(data_set[:, j]) range_j = np.float(max_j-min_j) centroids[:, j] = min_j + range_j * np.random.rand(k, 1) return centroids k-均值聚类算法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def k_means(data_set, k, dist_meas=dist_eclud, create_cent=rand_cent): """k-means算法 :param data_set: 数据集 :param k: 质心个数 :param dist_meas: 距离计算方法，默认欧式距离 :param create_cent: 创建质心方法 :return: """ # 数据点的个数 m = np.shape(data_set)[0] # 每个点的簇分配结果 # 第一列记录簇的索引值 # 第二列记录存储误差 cluster_assment = np.mat(np.zeros((m, 2))) # 生成k个质心 centroids = create_cent(data_set, k) # 簇是否改变 cluster_changed = True while cluster_changed: # 当簇发生改变 cluster_changed = False for i in range(m): # 对每个数据点 min_dist = np.inf min_index = -1 for j in range(k): # 对每个质心 # 计算距离 dist_j_i = dist_meas(centroids[j, :], data_set[i, :]) if dist_j_i &lt; min_dist: # 如果距离小于最小距离 # 更新最小距离 # 更新该数据点对应的质心下标 min_dist = dist_j_i min_index = j if cluster_assment[i, 0] != min_index: # 如果对应数据点质心改变 # 更新标识 cluster_changed = True # 更新该数据点的对应质心下标和对应的误差值 cluster_assment[i, :] = min_index, min_dist**2 # print(centroids) for cent in range(k): # 遍历所有质心 pts_in_clust = data_set[np.nonzero(cluster_assment[:, 0].A == cent)[0]] # 更新取值,沿数据点列的方向进行均值计算 centroids[cent, :] = np.mean(pts_in_clust, axis=0) return centroids, cluster_assment 通过后处理提高聚类性能SSE(Sum of Squared Error，误差平方和)是一种用于度量聚类效果的指标。SSE值越小表示数据点越接近它们的质心，聚类效果也越好。因为聚类的目标是在保持簇数目不变的情况下提高簇的质量，所以不能通过增加簇的个数来降低SSE值。有两种可以量化的方法： 合并最近的质心，通过计算所有质心之间的距离，然后合并距离最近的两个点来实现 合并两个使得SSE增幅最小的质心，合并两个簇后计算总的SSE值 必须在所有可能的两个簇上重复执行上述处理过程，直到找到合并最佳的两个簇为止。 二分k-均值算法 伪代码1234567将所有点看成一个簇当簇数目小于k时： 对每个簇： 计算总误差 在给定的簇上面进行k-均值聚类(k=2) 计算将该簇一分为二后的总误差 选择使得误差最小的那个簇进行划分操作 另一种做法是选择SSE最大的簇进行划分，直到簇数目达到指定数目为止。 二分k-均值聚类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354def bi_k_means(data_set, k, dis_meas=dist_eclud): """二分k-means :param data_set: 数据集 :param k: 质心个数 :param dis_meas: 距离计算方法 :return: """ # 回去数据集大小 m = np.shape(data_set)[0] # 每个点的簇分配结果 # 第一列记录簇的索引值 # 第二列记录存储误差 cluster_assment = np.mat(np.zeros((m, 2))) # 计算整个数据集的质心 centroid_0 = np.mean(data_set, axis=0).tolist()[0] cent_list = [centroid_0] for j in range(m): # 计算每个点到质心的误差值 cluster_assment[j, 1] = dis_meas(np.mat(centroid_0), data_set[j, :])**2 while len(cent_list) &lt; k: # 对簇进行划分直到得到k个簇 lowest_sse = np.inf for i in range(len(cent_list)): # 遍历列表中的每一个簇 # 得到当前簇的数据 pts_in_curr_cluster = data_set[np.nonzero(cluster_assment[:, 0].A == i)[0], :] # 对当前簇进行划分 centroid_mat, split_cluster_ass = k_means(pts_in_curr_cluster, 2, dis_meas) # 计算划分数据的误差 sse_split = np.sum(split_cluster_ass[:, 1]) # 计算剩余数据集的误差 sse_not_split = np.sum(cluster_assment[np.nonzero(cluster_assment[:, 0].A != i)[0], 1]) print('sse_split,sse_not_split', sse_split, sse_not_split) if sse_split + sse_not_split &lt; lowest_sse: # 如果误差小于最小误差 # 更新质心的下标 best_cent_to_split = i # 更新质心 best_new_cents = centroid_mat # 更新簇数据 best_clust_ass = split_cluster_ass.copy() # 更新最小误差 lowest_sse = sse_split + sse_not_split # 将编号为0和1的结果簇修改为划分簇及新加簇的编号 best_clust_ass[np.nonzero(best_clust_ass[:, 0].A == 1)[0], 0] = len(cent_list) best_clust_ass[np.nonzero(best_clust_ass[:, 0].A == 0)[0], 0] = best_cent_to_split print('the best_cent_to_split is:', best_cent_to_split) print('the len of best_clust_ass is', len(best_clust_ass)) # 新的质心添加到cent_list cent_list[best_cent_to_split] = best_new_cents[0, :] cent_list.append(best_new_cents[1, :]) cluster_assment[np.nonzero(cluster_assment[:, 0].A == best_cent_to_split)[0], :] = best_clust_ass print('cent_list:', cent_list) return np.mat(np.array(cent_list)), cluster_assment 对地图上的点进行聚类1 对地图坐标聚类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def dist_slc(vec_a, vec_b): """计算球面距离 :param vec_a: 向量a :param vec_b: 向量b :return: """ # 给定两点的经纬度，使用球面余弦定理计算两点距离 # sin和cos函数需先将角度转换为弧度 a = np.sin(vec_a[0, 1]*np.pi/180)*np.sin(vec_b[0, 1]*np.pi/180) b = np.cos(vec_a[0, 1]*np.pi/180)*np.cos(vec_b[0, 1]*np.pi/180)\ * np.cos(np.pi*(vec_b[0, 0]-vec_a[0, 0])/180) return np.arccos(a+b)*6371.0def cluster_clubs(num_clust=5): """画出聚类结果 :param num_clust: :return: """ import matplotlib.pyplot as plt data_list = [] with open('data/places.txt', 'r', encoding='utf-8') as f: # 提取文件中的经纬度 for line in f.readlines(): line_arr = line.split('\t') data_list.append([np.float(line_arr[4]), np.float(line_arr[3])]) data_mat = np.mat(data_list) # 运行bi_k_means函数得到聚类中心以及聚类对应的数据点 my_centroids, clust_assing = bi_k_means(data_mat, num_clust, dis_meas=dist_slc) # 创建一幅图和一个矩形 fig = plt.figure() rect = [0.1, 0.1, 0.8, 0.8] # 标记形状的列表用于绘制散点图 scatter_markers = ['s', 'o', '^', '8', 'p', 'd', 'v', 'h', '&gt;', '&lt;'] axprops = dict(xticks=[], yticks=[]) # 基于一副图像来创建矩阵 ax0 = fig.add_axes(rect, label='ax0', **axprops) img_p = plt.imread('data/Portland.png') ax0.imshow(img_p) ax1 = fig.add_axes(rect, label='ax1', frameon=False) for i in range(num_clust): # 用索引创建标记形状 pts_in_curr_cluster = data_mat[np.nonzero(clust_assing[:, 0].A == i), :] marker_style = scatter_markers[i % len(scatter_markers)] ax1.scatter(pts_in_curr_cluster[:, 0].flatten().A[0], pts_in_curr_cluster[:, 1].flatten().A[0], marker=marker_style, s=90) # 创建质心的形状 ax1.scatter(my_centroids[:, 0].flatten().A[0], my_centroids[:, 1].flatten().A[0], marker='+', s=300) plt.show() 示例代码 1.地理坐标 ↩]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实战</tag>
        <tag>无监督学习</tag>
        <tag>聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树回归]]></title>
    <url>%2F2018%2F05%2F28%2F%E6%A0%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[树回归概述 使用二元切分来处理连续数据 树回归的优缺点 优点：可以对复杂和非线性的数据建模 缺点：结果不易理解 适用数据类型：数值型和标称型数据 树回归的工作原理CART(Classification And Regression Trees，分类回归树) 采用二元切分来处理连续型变量。如果特征值大于给定值就走左子树，否则就走右子树，树构建一般是离线完成。其中回归树是在每个叶节点构建一个数值，而模型树是在每个叶节点构建一个线性模型。 树回归的一般流程 收集数据：采用任意方法收集数据 准备数据：需要数值型的数据，标称型数据应该映射成二值型数据 分析数据：绘出数据的二维可视化显示结果，以字典方式生成树 训练算法：大部分时间都花费在叶节点树模型的构建上 测试算法：使用测试数据上的R2值来分析模型的效果 使用算法：使用训练出的树做预测 树回归的实现在Python中，由于其简单的数据结构，可以使用字典来存储构建的树，该字典包含四个元素: 待切分的特征 待切分的特征值 右子树。当不再需要切分的时候，也可以是单个值 左子树。当不再需要切分的时候，也可以是单个值 CART算法实现 creat_tree()伪代码 12345找到最佳的待切分特征： 如果该节点不能再分，将该节点存为叶节点 执行二元切分 在右子树调用create_tree()方法 在左子树调用create_tree()方法 CART实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152def load_data_set(file_name): """加载数据 :param file_name: 文件名 :return: """ data_mat = [] with open(file_name, 'r', encoding='utf-8') as f: for line in f.readlines(): cur_line = line.strip().split('\t') # 将每行映射成为浮点数 flt_line = list(map(np.float, cur_line)) data_mat.append(flt_line) return data_matdef bin_split_data_set(data_set, feature, value): """将数据切分成两个子集并返回 :param data_set: 数据集 :param feature: 待切分的特征 :param value: 该特征的某个值 :return: """ # nonzero返回的是index mat_0 = data_set[np.nonzero(data_set[:, feature] &gt; value)[0], :] mat_1 = data_set[np.nonzero(data_set[:, feature] &lt;= value)[0], :] return mat_0, mat_1def create_tree(data_set, leaf_type=reg_leaf, err_type=reg_err, ops=(1, 4)): """创建树 :param data_set: 数据集 :param leaf_type: 建立叶节点的函数 :param err_type: 误差计算函数 :param ops: 树构建所需要的其他元组 :return: """ # 切分数据集 feat, val = choose_best_split(data_set, leaf_type, err_type, ops) if feat is None: # 如果满足条件 # 则返回叶节点 return val # 不满足则创建新的字典 ret_tree = dict(&#123;&#125;) ret_tree['sp_ind'] = feat ret_tree['sp_val'] = val # 切分数据集 l_set, r_set = bin_split_data_set(data_set, feat, val) # 递归调用创建树 ret_tree['left'] = create_tree(l_set, leaf_type, err_type, ops) ret_tree['right'] = create_tree(r_set, leaf_type, err_type, ops) return ret_tree 回归树构建回归树假设叶节点是常数值。如果需要成功构建以常数分段为叶节点的树，需要度量出数据的一致性。使用树分类时需要在给定节点计算数据的混乱度。计算连续型数值的混乱度首先计算数据的均值，然后计算每条数据的值到均值的差值(使用绝对值或平方值) 最佳二元切分伪代码 1234567对每个特征： 对每个特征值： 将数据集切分成两份 计算切分的误差 如果当前误差小于当前最小误差： 将当前切分设定为最佳切分并更新最小误差返回最佳切分的特征和阈值 回归树切分 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667def reg_leaf(data_set): """生成叶节点 :param data_set: 数据集 :return: """ # 回归树中是目标变量的均值 return np.mean(data_set[:, -1])def reg_err(data_set): """误差估计函数 :param data_set: :return: """ # 总方差等于均方差乘以样本个数 return np.var(data_set[:, -1]) * np.shape(data_set)[0]def choose_best_split(data_set, leaf_type=reg_leaf, err_type=reg_err, ops=(1, 4)): """寻找数据的最佳二元切分方式 :param data_set: 数据集 :param leaf_type: 生成叶节点函数 :param err_type: 误差函数 :param ops: 用于控制函数的停止时机 :return: """ # 容许的误差下降值 tol_s = ops[0] # 切分的最少样本树 tol_n = ops[1] if len(set(data_set[:, -1].T.tolist()[0])) == 1: # 统计不同特征的数目 # 如果为1就不再切分直接返回 return None, leaf_type(data_set) # 计算当前数据集的大小和误差 m, n = np.shape(data_set) # 误差用于与新切分的误差进行对比 # 来检查新切分能否降低误差 s = err_type(data_set) best_s = np.inf best_index = 0 best_value = 0 for feat_index in range(n-1): # 遍历所有特征 for split_val in set(data_set[:, feat_index].T.tolist()[0]): # 遍历该特征上的所有取值 mat_0, mat_1 = bin_split_data_set(data_set, feat_index, split_val) if np.shape(mat_0)[0] &lt; tol_n or np.shape(mat_1)[0] &lt; tol_n: # 如果切分后子集不够大 # 进入下一次循环 continue new_s = err_type(mat_0) + err_type(mat_1) if new_s &lt; best_s: # 如果新子集的误差小于最好误差 # 更新特征下标/切分值/最小误差 best_index = feat_index best_value = split_val best_s = new_s if s-best_s &lt; tol_s: # 如果切分数据集后提升不大 # 则不在进行切分而直接创建叶节点 return None, leaf_type(data_set) mat_0, mat_1 = bin_split_data_set(data_set, best_index, best_value) if np.shape(mat_0)[0] &lt; tol_n or np.shape(mat_1)[0] &lt; tol_n: # 如果切分出的数据集很小则退出 return None, leaf_type(data_set) return best_index, best_value 树剪枝当一棵树的节点过多，表明该模型可能对数据进行了”过拟合”。通过降低决策树的复杂度来避免过拟合的过程称为 剪枝(pruning)。在函数choose_best_split()中提前终止条件是一种 预剪枝(prepruning)；使用测试集和训练集的方式是 后剪枝(postpruning)。 预剪枝通过不断修改停止条件tol_s来得到合理结果。 使用默认ops项 1234567891011In [5]: my_data = mat(reg_trees.load_data_set('data/ex2.txt'))In [6]: reg_trees.create_tree(my_data)Out[6]:&#123;'left': &#123;'left': &#123;'left': &#123;'left': 105.24862350000001, 'right': 112.42895575000001, 'sp_ind': 0, 'sp_val': 0.958512&#125;, ... 'sp_ind': 0, 'sp_val': 0.499171&#125; 使用较大的tol_s 123456In [8]: reg_trees.create_tree(my_data, ops=(10000,4))Out[8]:&#123;'left': 101.35815937735848, 'right': -2.6377193297872341, 'sp_ind': 0, 'sp_val': 0.499171&#125; 后剪枝使用后剪枝需要将数据集分成测试集和训练集。首先指定参数，使构建出的树足够大、足够复杂，便于剪枝。接下来从上而下找到叶节点，用测试集判断这些叶节点合并是否能降低测试误差 伪代码 123456基于已有的树切分测试数据： 如果存在任一子集是一棵树，在该子集递归剪枝过程 计算将当前两个叶节点合并后的误差 计算不合并的误差 如果合并会降低误差： 合并叶节点 剪枝函数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253def is_tree(obj): """判断是否是树 :param obj: 需要判断的对象 :return: """ return type(obj).__name__ == 'dict'def get_mean(tree): """计算叶节点的平均值 :param tree: 树 :return: """ if is_tree(tree['right']): tree['right'] = get_mean(tree['right']) if is_tree(tree['left']): tree['left'] = get_mean(tree['left']) return (tree['left']+tree['right'])/2.0def prune(tree, test_data): """ 树剪枝 :param tree: 待剪枝的树 :param test_data: 剪枝所需的测试数据 :return: """ if np.shape(test_data)[0] == 0: # 确认数据集是否是空 return get_mean(tree) # 如果是子树就对该子树进行剪枝 if is_tree(tree['right']) or is_tree(tree['left']): l_set, r_set = bin_split_data_set(test_data, tree['sp_ind'], tree['sp_val']) if is_tree(tree['left']): tree['left'] = prune(tree['left'], l_set) if is_tree(tree['right']): tree['right'] = prune(tree['right'], r_set) if not is_tree(tree['left']) and not is_tree(tree['right']): # 如果不是子树就进行合并 l_set, r_set = bin_split_data_set(test_data, tree['sp_ind'], tree['sp_val']) error_no_merge = np.sum(np.power(l_set[:, -1]-tree['left'], 2))+np.sum(np.power(r_set[:, -1]-tree['right'], 2)) tree_mean = (tree['left']+tree['right'])/2.0 error_merge = np.sum(np.power(test_data[:, -1] - tree_mean, 2)) if error_merge &lt; error_no_merge: # 比较合并前后的误差 # 合并后的误差比合并前的误差小 # 就返回合并的树 print("merging") return tree_mean else: # 否则直接返回 return tree else: return tree 模型树模型树将叶节点设定为分段线性函数，分段线性函数(piecewise linear)是指模型由多个线性片段组成。模型树具有可解释性和更高的预测准确度。模型树的误差通过线性的模型对指定的数据集进行拟合，然后计算真实的目标值与模型预测值间的差值。最后将这些差值的平方求和就得到所需的误差。 模型树叶节点1234567891011121314151617181920212223242526272829303132333435# 模型树def linear_solve(data_set): """目标变量转换 将数据集格式化成目标变量Y和自变量X :param data_set: 数据集 :return: """ m, n = np.shape(data_set) x = np.mat(np.ones((m, n))) x[:, 1:n] = data_set[:, 0:n-1] y = data_set[:, -1] x_t_x = x.T*x if np.linalg.det(x_t_x) == 0.0: raise NameError('该矩阵不能求逆，尝试提高ops参数的第二个值') ws = x_t_x.I*(x.T*y) return ws, x, ydef model_leaf(data_set): """生成叶子节点的回归系数 :param data_set: 数据集 :return: """ ws, x, y = linear_solve(data_set) return wsdef model_err(data_set): """计算误差 :param data_set: 数据集 :return: """ ws, x, y = linear_solve(data_set) y_hat = x*ws return np.sum(np.power(y-y_hat, 2)) 示例：树回归与标准回归的比较1 树回归预测 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354def reg_tree_eval(model, in_dat): """数据转换 :param model: 树结构 :param in_dat: 输入数据 :return: """ return np.float(model)def model_tree_eval(model, in_data): """数据转换 :param model: 树结构 :param in_data: 输入数据 :return: """ n = np.shape(in_data)[1] x = np.mat(np.ones((1, n+1))) x[:, 1:n+1] = in_data return np.float(x*model)def tree_fore_cast(tree, in_data, model_eval=reg_tree_eval): """预测函数 :param tree: 树结构 :param in_data: 输入数据 :param model_eval: 树的类型 :return: """ if not is_tree(tree): return model_err(tree, in_data) if in_data[tree['sp_ind']] &gt; tree['sp_val']: if is_tree(tree['left']): return tree_fore_cast(tree['left'], in_data, model_eval) else: return model_eval(tree['left'], in_data) else: if is_tree(tree['right']): return tree_fore_cast(tree['right'], in_data, model_eval) else: return model_eval(tree['right'], in_data)def create_for_cast(tree, test_data, model_eval=reg_tree_eval): """ 测试函数 :param tree: 树结构 :param test_data: 测试数据 :param model_eval: 树类型 :return: """ m = len(test_data) y_hat = np.mat(np.zeros((m, 1))) for i in range(m): y_hat[i, 0] = tree_fore_cast(tree, np.mat(test_data[i]), model_eval) return y_hat 创建回归树 12345678910In [13]: train_mat = mat(reg_trees.load_data_set('data/bikeSpeedVsIq_train.txt'))In [14]: test_mat = mat(reg_trees.load_data_set('data/bikeSpeedVsIq_test.txt'))In [15]: my_tree = reg_trees.create_tree(train_mat, ops=(1,20))In [16]: y_hat = reg_trees.create_for_cast(my_tree, test_mat[:,0])In [17]: corrcoef(y_hat, test_mat[:,1], rowvar=0)[0,1]Out[17]: 0.96408523182221506 创建模型树 123456In [18]: my_tree = reg_trees.create_tree(train_mat, reg_trees.model_leaf,reg_trees.model_err, (1,20))In [19]: y_hat = reg_trees.create_for_cast(my_tree, test_mat[:,0], reg_trees.model_tree_eval)In [20]: corrcoef(y_hat, test_mat[:,1], rowvar=0)[0,1]Out[20]: 0.97604121913806285 线性回归 12345678In [21]: ws, x, y = reg_trees.linear_solve(train_mat)In [22]: for i in range(shape(test_mat)[0]): ...: y_hat[i]=test_mat[i,0]*ws[1,0]+ws[0,0] ...:In [23]: corrcoef(y_hat, test_mat[:,1], rowvar=0)[0,1]Out[23]: 0.94346842356747662 因为相关系数越接近1越好，所以可以看出在这里对数据的预测模型树&gt;回归树&gt;线性回归。 使用Tkinter创建GUI 回归树界面 模型树界面 示例代码 1.数据集是骑自行车的速度和人的智商之间的关系 ↩]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实战</tag>
        <tag>监督学习</tag>
        <tag>回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性回归]]></title>
    <url>%2F2018%2F05%2F25%2F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[线性回归概述 依赖输入写出一个目标值的公式 线性回归的优缺点 优点：结果易于理解，计算上不复杂 缺点：对非线性的数据拟合不好 适用数据类型：数值型和标称型数据 线性回归的工作原理回归 是指寻找 回归方程(regression equation) 的回归系数的过程。线性回归是指可以将输入项分别乘以一些常量，再将结果加起来得到输出。1输入数据存放在矩阵$x$中，而回归系数存放在向量$w$中。对于给定的数据$x_1$中，预测结果将会通过$y_1=x_1^Tw$给出。而确定系数$w$需要使得预测$y$值和真实$y$值之间的差值最小，为了防止误差简单累加将使正差值和负差值相互抵消，所以使用平方误差。平方误差写做： \sum_{i=1}^m(y_i-x_i^Tw)^2用矩阵表示还可以写做$(y-Xw)^T(y-Xw)$。对$w$求导后得到$x^T(y-xw)$，令其等于0，解出$w$如下： \hat{w}=(x^Tx)^{-1}x^Ty其中$\hat{w}$表示当前可以估计出的$w$的最优解。对矩阵$(x^Tx)^{-1}$求逆的部分表示这个方程只能在逆矩阵存在的时候适用。除了适用矩阵方法外，还可以使用最小二乘法 线性回归的一般流程 收集数据：采用任意方法收集数据 准备数据：回归需要数值型数据，标称型数据将被转成二值型数据 分析数据：绘出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘在图上作为对比 训练算法：找到回归系数 测试算法：使用R2或者预测值和数据的拟合度，来分析模型的效果 使用算法：使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，这样可以预测连续型的数据 回归算法的实现线性回归 数据导入 1234567891011121314151617def load_data_set(file_name): """加载数据 :param file_name: 文件名 :return: """ num_feat = len(open(file_name).readline().split('\t')) -1 data_mat = [] label_mat = [] with open(file_name, 'r', encoding='utf-8') as f: for line in f.readlines(): line_arr = [] cur_line = line.strip().split('\t') for i in range(num_feat): line_arr.append(float(cur_line[i])) data_mat.append(line_arr) label_mat.append(float(cur_line[-1])) return data_mat, label_mat 标准回归函数 12345678910111213141516def stand_regress(x_arr, y_arr): """计算最佳拟合直线 :param x_arr: 数据集 :param y_arr: 结果集 :return: """ x_mat = np.mat(x_arr) y_mat = np.mat(y_arr).T x_t_x = x_mat.T*x_mat if np.linalg.det(x_t_x) == 0.0: # 判断x_t_x行列式是否为0(是否可逆) print('奇异矩阵不能求逆') return ws = x_t_x.I*(x_mat.T*y_mat) # 返回参数向量 return ws 绘制回归曲线 1234567891011121314151617181920212223In [13]: x_arr,y_arr = regression.load_data_set('data/ex0.txt') #加载数据In [14]: ws = regression.stand_regress(x_arr,y_arr)#训练回归系数In [15]: x_mat = mat(x_arr)In [16]: y_mat = mat(y_arr).TIn [17]: fig = plt.figure()In [18]: ax = fig.add_subplot(111)In [19]: ax.scatter(x_mat[:,1].flatten().A[0],y_mat[:,0].flatten().A[0])Out[19]: &lt;matplotlib.collections.PathCollection at 0x1735a89bcf8&gt;In [20]: x_copy = x_mat.copy()In [21]: x_copy.sort(0)In [22]: y_hat = x_copy*wsIn [23]: ax.plot(x_copy[:,1],y_hat)Out[23]: [&lt;matplotlib.lines.Line2D at 0x1735b407e48&gt;] 数据集合最佳拟合直线 可以通过计算预测值$\hat{y}$序列和真实值$y$序列的匹配程度，也就是计算两个序列的相关系数来判断模型的好坏。 计算相关系数123456In [33]: y_hat = x_mat*wsIn [34]: corrcoef(y_hat.T, y_mat.T)Out[34]:array([[ 1. , 0.98647356], [ 0.98647356, 1. ]]) 局部加权线性回归由于线性回归求的是具有最小均方误差的无偏估计，所以有可能出现欠拟合现象。对于这种情况可以在估计中引入一些偏差来降低预测的均方误差。局部加权线性回归(Locally Weighted Linear Regression,LWLR) 算法在待预测点附近的每个点赋予一定的权重，然后在这个子集上基于最小均方差来进行普通的回归。这种算法每次预测都需要实现选取出对应的数据子集。其解出回归系数$w$的形式如下： \hat{w}=(w^Twx)^{-1}x^Twy其中$w$是一个矩阵，用来给每个数据点赋予权重。LWLR使用“核”来对附近的点赋予更高的权重。最常用的核是高斯核，其对应的权重如下: w(i,i)=e^{\frac{\mid x^{(i)}-x\mid}{-2 k^2}}这构建了一个只含对角元素的权重矩阵$w$,并且点$x$与$x(i)$越近，$w(i,i)$将会越大。参数$k$决定了对附近的点赋予多大的权重。 局部加权线性回归 12345678910111213141516171819202122232425262728293031323334353637383940def lwlr(test_point, x_arr, y_arr, k=1.0): """局部加权线性回归 给出x空间的任意一点，计算出对应的预测值y_hat :param test_point: 测试数据点 :param x_arr: 数据集 :param y_arr: 结果集 :param k: 权重参数，决定对附近的点赋予多大权重，控制衰减速度 :return: """ x_mat = np.mat(x_arr) y_mat = np.mat(y_arr).T m = np.shape(x_mat)[0] # 创建对角权重矩阵 weights = np.mat(np.eye(m)) for j in range(m): # 计算高斯核对应的权重 # 随着样本点与待预测点距离的递增，权重将以指数基衰减 diff_mat = test_point - x_mat[j, :] weights[j, j] = np.exp(diff_mat*diff_mat.T/(-2.0*k**2)) x_t_x = x_mat.T*(weights*x_mat) if np.linalg.det(x_t_x) == 0.0: print('奇异矩阵不能求逆') return ws = x_t_x.I*(x_mat.T*(weights*y_mat)) return test_point*wsdef lwlr_test(test_arr, x_arr, y_arr, k=1.0): """测试局部加权线性回归 :param test_arr: 测试数据集 :param x_arr: 数据集 :param y_arr: 结果集 :param k: 权重参数，决定对附近的点赋予多大权重，控制衰减速度 :return: """ m = np.shape(test_arr)[0] y_hat = np.zeros((m,1)) for i in range(m): y_hat[i] = lwlr(test_arr[i], x_arr, y_arr, k) return y_hat 绘制不同k值的拟合曲线 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152In [3]: import regressionIn [4]: x_arr,y_arr = regression.load_data_set('data/ex0.txt') #加载数据In [5]: fig = plt.figure()In [6]: ax1= fig.add_subplot(3,1,1)In [7]: y_hat_1 = regression.lwlr_test(x_arr,x_arr,y_arr,0.01)In [8]: x_mat = mat(x_arr)In [9]: srt_ind = x_mat[:,1].argsort(0)In [10]: x_sort = x_mat[srt_ind][:,0,:]In [11]: ax1.plot(x_sort[:,1], y_hat_1[srt_ind][:,0,:])Out[11]: [&lt;matplotlib.lines.Line2D at 0x1ed7b2a3630&gt;]In [12]: ax1.scatter(x_mat[:,1].flatten().A[0],mat(y_arr).T.flatten().A[0], s=2, c='red')Out[12]: &lt;matplotlib.collections.PathCollection at 0x1ed7b2be400&gt;In [13]: ax1.set_xlabel('k=0.01')Out[13]: &lt;matplotlib.text.Text at 0x1ed71765780&gt;In [14]: y_hat_2 = regression.lwlr_test(x_arr,x_arr,y_arr,1)In [15]: ax2 = fig.add_subplot(3,1,2)In [16]: ax2.plot(x_sort[:,1], y_hat_2[srt_ind][:,0,:])Out[16]: [&lt;matplotlib.lines.Line2D at 0x1ed7b44f550&gt;]In [17]: ax2.scatter(x_mat[:,1].flatten().A[0],mat(y_arr).T.flatten().A[0], s=2, c='red')Out[17]: &lt;matplotlib.collections.PathCollection at 0x1ed7b4418d0&gt;In [18]: ax2.set_xlabel('k=1')Out[18]: &lt;matplotlib.text.Text at 0x1ed71c38860&gt;In [19]: y_hat_3 = regression.lwlr_test(x_arr,x_arr,y_arr,0.003)In [20]: ax3 = fig.add_subplot(3,1,3)In [21]: ax3.scatter(x_mat[:,1].flatten().A[0],mat(y_arr).T.flatten().A[0], s=2, c='red')Out[21]: &lt;matplotlib.collections.PathCollection at 0x1ed719c5d30&gt;In [22]: ax3.plot(x_sort[:,1], y_hat_3[srt_ind][:,0,:])Out[22]: [&lt;matplotlib.lines.Line2D at 0x1ed719ffbe0&gt;]In [23]: ax3.set_xlabel('k=0.003')Out[23]: &lt;matplotlib.text.Text at 0x1ed71a59470&gt;In [27]: plt.subplots_adjust(hspace=0.5) 不同k值的局部加权回归上图中的k=0.01，中图k=1，下图k=0.003。当k=1时的模型效果和最小二乘法法相差不多，k=0.01时该模型挖出数据的潜在规则，而k=0.003时则考虑了太多噪声，导致了过拟合现象 缩减系数缩减系数适用于当特征比样本点多($n&gt;m$，即输入数据的矩阵$x$不是满秩矩阵)的情况。 岭回归岭回归是在矩阵$x^Tx$是加上一个$\lambda I$从而使得矩阵非奇异。进而能对$x^Tx+\lambda I$求逆。其中矩阵$I$是一个$m\times m$的单位矩阵，对角线上的元素全为1，其他元素全为0。$\lambda$是定义的一个数值，其限制了所有$w$之和，通过引入惩罚项，能够减少不重要的参数： \hat{w}=(x^Tx+\lambda I)^{-1}x^Ty$\lambda$通过误差最小化获得，通过选取不同的$\lambda$值来得到使预测误差最小的$\lambda$ 岭回归12345678910111213141516171819202122232425262728293031323334353637383940def ridge_regress(x_mat, y_mat, lam=0.2): """岭回归 :param x_mat: 数据集 :param y_mat: 结果集 :param lam: 缩减系数 :return: """ x_t_x = x_mat.T*x_mat denom = x_t_x+np.eye(np.shape(x_mat)[1])*lam if np.linalg.det(denom) == 0.0: print('奇异矩阵不能求逆') return ws = denom.I*(x_mat.T*y_mat) return wsdef ridge_test(x_arr, y_arr): """测试岭回归 :param x_arr: 数据集 :param y_arr: 结果集 :return: """ x_mat = np.mat(x_arr) y_mat = np.mat(y_arr).T y_mean = np.mean(y_mat, 0) y_mat = y_mat - y_mean # 对特征进行标准化处理 # 所有特征减去各自的均值并除以方差 x_mean = np.mean(x_mat, 0) x_var = np.var(x_mat, 0) x_mat = (x_mat-x_mean)/x_var num_test_pts = 30 w_mat = np.zeros((num_test_pts, np.shape(x_mat)[1])) for i in range(num_test_pts): # 在30个不同的lambda下求回归系数 # lambda以指数级变化 ws = ridge_regress(x_mat, y_mat, np.exp(i-10)) w_mat[i, :] = ws.T return w_mat lasso在增加如下约束时，普通的最小二乘法回归会得到与岭回归一样的公式: \sum_{k=1}^nw_k^2 \leq \lambdalasso对回归系数做如下限定: \sum_{k=1}^n\mid w_k\mid\leq y这个限定用绝对值取代了平方和，当$\lambda$足够小时，一些系数会因此被迫缩减到0。 前向逐步回归前向逐步回归属于一种贪心算法，即每一步都尽可能减少误差。一开始所有的权重都为1，然后每一步所做的决策是对某个权重增加或减少一个很小的值 伪代码 12345678910数据标准化，使其分布满足0均值和单位方差在每轮迭代过程中： 设置当前最小误差lowest_error为正无穷 对每个特征： 增大或缩小： 改变一个系数得到一个新的w 计算新w下的误差 如果误差error小于当前最小误差lowest_error: 设置w_best等于当前w 将w设置为新的w_best 前向逐步回归 1234567891011121314151617181920212223242526272829303132333435363738394041424344def stage_wise(x_arr, y_arr, eps=0.01, num_it=100): """逐步线性回归 :param x_arr: 数据集 :param y_arr: 结果集 :param eps: 每次迭代需要调整的步长 :param num_it: 迭代次数 :return: """ x_mat = np.mat(x_arr) y_mat = np.mat(y_arr).T # 对数据进行标准化处理 y_mean = np.mean(y_mat, 0) y_mat = y_mat - y_mean x_mean = np.mean(x_mat, 0) x_var = np.var(x_mat, 0) x_mat = (x_mat - x_mean) / x_var m, n = np.shape(x_mat) return_mat = np.zeros((num_it, n)) ws = np.zeros((n, 1)) ws_max = ws.copy() for i in range(num_it): # 开始优化 print(ws.T) # 将当前最小误差设置为正无穷 lowest_error = np.inf for j in range(n): # 对每个特征 for sign in [-1, 1]: # 增大或缩小 ws_test = ws.copy() # 改变一个系数得到新的ws # 计算预测值 # 计算误差 ws_test[j] += eps*sign y_test = x_mat*ws_test rss_err = rss_error(y_mat.A, y_test.A) if rss_err &lt; lowest_error: # 更新最小误差 lowest_error = rss_err ws_max = ws_test # 更新回归系数 ws = ws_max.copy() return_mat[i, :] = ws.T return return_mat 权衡偏差和方差偏差和方差的内容查看Coursera上的MachineLearning课程资料 示例：预测乐高玩具套装的价格因为GoogleAPI无法使用，所以解析提供的html文档得到数据 解析文档 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748def scrape_page(in_file, out_file, yr, num_pce, orig_prc): """解析下载的网页 :param in_file: 读取文件 :param out_file: 写入文件 :param yr: 年份 :param num_pce: 数量 :param orig_prc: 价格 :return: """ from bs4 import BeautifulSoup with open(in_file, 'r', encoding='utf-8') as in_f: with open(out_file, 'a', encoding='utf-8') as out_f: soup = BeautifulSoup(in_f.read(), 'lxml') i = 1 current_row = soup.findAll('table', r='%d' % i) while len(current_row) != 0: title = current_row[0].findAll('a')[1].text lwr_title = title.lower() if lwr_title.find('new') &gt; -1 or lwr_title.find('nisb') &gt; -1: new_flag = 1.0 else: new_flag = 0.0 sold_uniccde = current_row[0].findAll('td')[3].findAll('span') if len(sold_uniccde) == 0: print('item %d did not sell' % i) else: sold_price = current_row[0].findAll('td')[4] price_str = sold_price.text price_str = price_str.replace('$', '') price_str = price_str.replace(',', '') if len(sold_price) &gt; 1: price_str = price_str.replace('Free shipping','') print('%s\t%d\t%s' % (price_str, new_flag, title)) out_f.write('%d\t%d\t%d\t%f\t%s\n' % (yr, num_pce, new_flag, orig_prc, price_str)) i += 1 current_row = soup.findAll('table', r='%d' % i)def set_data_collect(): """设置数据集 :return: """ scrape_page('data/setHtml/lego8288.html', 'data/lego.txt', 2006, 800, 49.99) scrape_page('data/setHtml/lego10030.html', 'data/lego.txt', 2002, 3096, 269.99) scrape_page('data/setHtml/lego10179.html', 'data/lego.txt', 2007, 5195, 499.99) scrape_page('data/setHtml/lego10181.html', 'data/lego.txt', 2007, 3428, 199.99) scrape_page('data/setHtml/lego10189.html', 'data/lego.txt', 2008, 5922, 299.99) scrape_page('data/setHtml/lego10196.html', 'data/lego.txt', 2009, 3263, 249.99) 交叉验证测试岭回归 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def cross_validation(x_arr, y_arr, num_val=10): """交叉验证集测试岭回归 :param x_arr: 数据集 :param y_arr: 结果集 :param num_val: 交叉验证次数 :return: """ m = len(x_arr) index_list = list(range(m)) error_mat = np.zeros((num_val, 30)) for i in range(num_val): train_x, train_y = [], [] test_x, test_y = [], [] # 对元素进行混洗，实现对训练集和测试集数据点的随机选取 np.random.shuffle(index_list) for j in range(m): if j &lt; m*0.9: # 90%分隔成训练集，其余10%为测试集 train_x.append(x_arr[index_list[j]]) train_y.append(y_arr[index_list[j]]) else: test_x.append(x_arr[index_list[j]]) test_y.append(y_arr[index_list[j]]) # 保存所有回归系数 w_mat = ridge_test(train_x, train_y) for k in range(30): # 使用30个不同的lambda值创建30组不同的回归系数 mat_test_x = np.mat(test_x) mat_train_x = np.mat(train_x) # 岭回归需要使用标准化数据 # 对数据进行标准化 mean_train = np.mean(mat_train_x, 0) var_train = np.var(mat_train_x, 0) mat_test_x = (mat_test_x-mean_train)/var_train y_est = mat_test_x*np.mat(w_mat[k, :]).T + np.mean(train_y) # 计算误差 # 保存每个lambda对应的多个误差值 error_mat[i, k] = rss_error(y_est.T.A, np.array(test_y)) # 计算误差的均值 mean_errors = np.mean(error_mat, 0) min_mean = float(min(mean_errors)) best_weights = w_mat[np.nonzero(mean_errors == min_mean)] x_mat = np.mat(x_arr) y_mat = np.mat(y_arr).T mean_x = np.mean(x_mat, 0) var_x = np.var(x_mat, 0) # 数据还原 un_reg = best_weights/var_x print('岭回归最好的模型是：\n', un_reg) print('常数项是：', -1*np.sum(np.multiply(mean_x, un_reg)) + np.mean(y_mat)) 示例代码 1.$y=0.1x_1+0.2x_2$就是回归方程，$0.1$和$0.2$就是回归系数 ↩]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实战</tag>
        <tag>监督学习</tag>
        <tag>回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AdaBoost元算法]]></title>
    <url>%2F2018%2F05%2F23%2FAdaBoost%E5%85%83%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[元算法概述 将不同分类器组合使用 AdaBoost算法的优缺点 优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整 缺点：对离群点敏感 适用数据类型：数值型和标称型数据 AdaBoost的工作原理bagging自举汇聚法(bootstrap aggregating) 也称为bagging方法1，是在从原始数据集选择S次后得到S个新数据集的一种技术。新数据集和原数据集的大小相等。每个数据集都是通过在原始数据集中随机选择一个样本来进行替换而得到的(放回取样)，新数据允许有重复值同时原始数据集中的某些值在新数据集中可以不存在。在S个数据集建好后将某个学习算法分别作用于每个数据集得到S个分类器，然后在需要对新数据分类时应用这S个分类器，然后进行多数表决，分类器中投票结果最多的类别作为最终的分类结果。 boostingboosting和bagging是很类似的技术，两者使用的多个分类器的类型都是一致的。不过在boosting中不同的分类器是通过串行训练获得的，每个新分类器都根据已训练出的分类器的性能来进行训练。boosting是通过集中关注被已有分类器 错分 的那些数据来获得新的分类器的。boosting分类的结果是基于所有分类器的 加权求和 结果的，bagging中的分类器权重是相等的，boosting中的分类器权重并不相等，每个权重代表的是其对应分类器在上一轮迭代中的成功度。 AdaBoostAdaBoost是adaptive boosting(自适应boosting)的缩写，训练过程如下： 训练数据中的每个 样本，并赋予一个权重，这些权重构成一个向量D，权重都被初始化成相等值 在训练数据上训练出一个弱分类器并计算该分类器的错误率 重新调整每个样本的权重(第一次分对的样本权重将会降低，分错的样本权重将会提高)，在同一个数据集上再次训练弱分类器 为了从所有弱分类器中得到最终的分类结果，AdaBoost为每个分类器都分配了一个权重值alpha，这些alpha值是基于每个弱分类器的错误率进行计算。错误率$\epsilon$的定义为： \epsilon=\frac{未正确分类的样本数目}{所有样本数目}alpha的计算公式如下: \alpha=\frac{1}{2}ln(\frac{1-\epsilon}{\epsilon}) AdaBoost算法示意图左边的是数据集，其中直方图的不同宽度表示每个样例上的不同权重。在经过一个分类器后，加权的预测结果会通过三角形中的alpha值进行加权。每个三角形中输出的加权结果在原形中求和，从而得到最终的输出结果 计算出alpha值之后，可以对权重向量D进行更新，以使得那些正确分类的样本的权重降低而错分样本的权重升高。如果某个样本被正确分类，该样本的权重更改为: D_i^{(t+1)}=\frac{D_i^{(t)}e^{-\alpha}}{Sum(D)}如果某个样本被错分，该样本的权重更改为： D_i^{(t+1)}=\frac{D_i^{(t)}e^{\alpha}}{Sum(D)}在计算出D之后，AdaBoost进入下一轮迭代。AdaBoost算法会不断重复训练和调整权重的过程，知道训练错误率为0或者弱分类器的数目达到指定值为止 AdaBoost算法的一般流程 收集数据：可以使用任意方法 准备数据：依赖于所使用的弱分类器类型，这里使用单层决策树，这种分类器可以处理任何数据类型。可以使用任意分类器作为弱分类器，作为弱分类器，简单分类器的效果更好 分析数据：可以使用任意方法 训练算法：AdaBoost的大部分时间都用在训练上，分类器将多次在同一数据集上训练弱分类器 测试算法：计算分类的错误率 使用算法：同SVM一样，AdaBoost预测两个类别中的一个。如果想应用到多个类别的场合，就需要对AdaBoost进行修改 AdaBoost算法的实现基于单层决策树构建弱分类器单层决策树(decision stump，也称决策树桩) 是一种简单的决策树，它仅基于单个特征来做决策 创建简单数据 1234567891011def load_simple_data(): """添加一个简单数据集 :return: """ data_mat = np.matrix([[1., 2.1], [2., 1.1], [1.3, 1.], [1., 1.], [2., 1.]]) class_labels = [1.0, 1.0, -1.0, -1.0, 1.0] return data_mat, class_labels 伪代码 1234567将最小错误率min_error设置为正无穷对数据集中的每一个特征(第一层循环): 对每个步长(第二层循环): 对每个不等号(第三层循环): 建立一颗单层决策树并利用加权数据集对它进行测试 如果错误率低于min_error，则将当前单层决策树设为最佳单层决策树返回最佳单层决策树 生成单层决策树 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859def stump_classify(data_matrix, dimen, thresh_val, thresh_ineq): """通过阈值分类 :param data_matrix: 数据集 :param dimen: 特征下标 :param thresh_val: 阈值 :param thresh_ineq: 符号 :return: """ ret_array = np.ones((np.shape(data_matrix)[0], 1)) if thresh_ineq == 'lt': # 将小于阈值的置为-1类 ret_array[data_matrix[:, dimen] &lt;= thresh_val] = -1.0 else: # 将大于阈值的置为-1类 ret_array[data_matrix[:, dimen] &gt; thresh_val] = -1.0 return ret_arraydef build_stump(data_arr, class_labels, d): """生成单层决策树 :param data_arr: 数据集 :param class_labels: 类别标签 :param d: 权重 :return: """ data_mat = np.mat(data_arr) label_mat = np.mat(class_labels).T m, n = np.shape(data_mat) # 在特征的所有可能值上进行遍历 num_steps = 10.0 # 存储给定权重向量d时所得到的最佳单层决策树的相关信息 best_stump = &#123;&#125; best_class_est = np.mat(np.zeros((m, 1))) # 设为无穷大，用于寻找可能的最小错误率 min_error = np.inf for i in range(n): # 在数据的所有特征上遍历 # 通过特征的最大最小值来确定步长 range_min = data_mat[:, i].min() range_max = data_mat[:, i].max() step_size = (range_max-range_min)/num_steps for j in range(-1, int(num_steps)+1): for inequal in ['lt', 'gt']: thresh_val = (range_min + float(j) * step_size) # 计算预测的分类 predicted_vals = stump_classify(data_mat, i, thresh_val, inequal) # 设置预测分类和真实类别不同的值为1 err_arr = np.mat(np.ones((m, 1))) err_arr[predicted_vals == label_mat] = 0 # 计算错误向量的权重和 weight_error = d.T * err_arr if weight_error &lt; min_error: # 更新最小错误和最佳单层数 min_error = weight_error best_class_est = predicted_vals.copy() best_stump['dim'] = i best_stump['thresh'] = thresh_val best_stump['ineq'] = inequal return best_stump, min_error, best_class_est 完整的AdaBoost算法 伪代码 1234567对每次迭代： 利用build_stump()函数找到最佳的单层决策树 将最佳单层决策树加入到单层决策树数组 计算alpha 计算新的权重向量D 更新累计类别估计值 如果错误率等于0.0，退出循环 AdbBoost训练 123456789101112131415161718192021222324252627282930313233343536373839def adaboost_train_ds(data_arr, class_labels, num_it=40): """基于单层决策树训练AdaBoost :param data_arr: 数据集 :param class_labels: 类别标签 :param num_it: 迭代次数 :return: """ weak_class_arr = [] m = np.shape(data_arr)[0] # 为每一条数据初始化权重 d = np.mat(np.ones((m, 1))/m) # 记录每个数据点的类别估计累计值 agg_class_est = np.mat(np.zeros((m, 1))) for i in range(num_it): # 建立单层决策树 best_stump, error, class_est = build_stump(data_arr, class_labels, d) print("D:", d.T) # 计算每一个单层决策树的权重 # max(error,1e-16)保证在没有错误时不会除0异常 alpha = float(0.5*np.log((1-error)/np.longfloat(max(error, 1e-16)))) # 保存决策树权重和单层决策树 best_stump['alpha'] = alpha weak_class_arr.append(best_stump) print("class_est:", class_est.T) # 计算新的权重向量d expon = np.multiply(-1*alpha*np.mat(class_labels).T, class_est) d = np.multiply(d, np.exp(expon)) d = d/d.sum() # 类别估计值 agg_class_est += alpha*class_est print('agg_class_est', agg_class_est.T) # 获取错误率 agg_errors = np.multiply(np.sign(agg_class_est) != np.mat(class_labels).T, np.ones((m, 1))) print(agg_errors) error_rate = agg_errors.sum()/m print('total error', error_rate, '\n') if error_rate == 0.0: break return weak_class_arr 分类函数 12345678910111213141516171819def ada_classify(dat_to_class, classifier_arr): """分类函数 :param dat_to_class: 待分类的数据 :param classifier_arr: 弱分类器数组 :return: """ data_matrix = np.mat(dat_to_class) m = np.shape(data_matrix)[0] # 记录每个数据点的类别估计累计值 agg_class_est = np.mat(np.zeros((m, 1))) for i in range(len(classifier_arr)): # 计算类别估计值 class_est = stump_classify(data_matrix, classifier_arr[i]['dim'], classifier_arr[i]['thresh'], classifier_arr[i]['ineq']) agg_class_est += classifier_arr[i]['alpha']*class_est print(agg_class_est) return np.sign(agg_class_est) 非均衡分类问题大多数情况下不同类别的分类代价并不相等，所以需要分类器性能度量方法并通过图像技术来对非均衡问题下不同分类器的性能进行可视化处理 正确率、召回率及ROC曲线关于正确率和召回率的性能度量可以查看Coursera上的MachineLearning课程资料另一个用于度量分类中的非均衡性的工具是ROC曲线(ROC curve)，ROC代表接收者操作特征(receiver operating characteristic) AdaBoost马疝病监测系统的ROC曲线ROC曲线中给出了两条线，一条虚线一条实线。横轴是负阳性的比例(假阳率=负阳性/(负阳性+正阴性))，纵轴表示是正阳性的比例(真阳率=正阳性/(正阳性+假阴性))。ROC曲线给出的事当阈值变化时假阳率和真阳率的变化情况。左下角的带你对应的将所有样例判为反例的情况，而右上角的点对应将所有样例判为正例的情况。虚线是随机猜测的结果曲线。 在理想情况下，最佳的分类器应该尽可能地处于左上角。曲线下的面积(Area User the Curve,AUC) 是对不同的ROC曲线进行比较的一个指标。AUC给出的事分类器的平均性能值。一个完美的分类器AUC为1.0,而随机猜测的AUC为0.5。创建ROC曲线需按照以下步骤： 将分类样例按照其预测强度排序 从排名最低的样例开始。所有排名更低的样例被判为反例，排名更高的样例被判为正例。对应的点为$$ 将其一到排名次低的样例中去，如果该样例属于正例，修改真阳率；如果该样例属于反例，修改假阴率 ROC曲线绘制及AUC计算函数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849def plot_roc(pred_strengths, class_labels): """画出ROC曲线 :param pred_strengths: 行向量组成的矩阵，表示分类器的预测强度 :param class_labels: 类别标签 :return: """ import matplotlib.pyplot as plt # 保留绘制光标的位置 cur = (1.0, 1.0) # 用于计算AUC的值 y_sum = 0.0 # 计算正例的数目 num_pos_class = sum(np.array(class_labels) == 1.0) # 确定x轴和y轴上的步长 y_step = 1/float(num_pos_class) x_step = 1/float(len(class_labels)-num_pos_class) # 得到排序索引 # 因为索引时按照最小到最大的顺序排列 # 所以从点&lt;1.0,1.0&gt;绘到&lt;0,0&gt; sorted_indicies = pred_strengths.argsort() fig = plt.figure() fig.clf() ax = fig.add_subplot(111) for index in sorted_indicies.tolist()[0]: # 在排序值上进行循环 if class_labels[index] == 1.0: # 得到一个标签为1.0的类，在y轴上下降一个步长 # 即不断降低真阳率 del_x = 0 del_y = y_step else: # 其他类别的标签，按x轴方向上倒退一个步长 # 假阴率方向 del_x = x_step del_y = 0 # 对矩形的高度进行累加 y_sum += cur[1] # 在当前点和新点之间画一条线 ax.plot([cur[0], cur[0]-del_x], [cur[1], cur[1]-del_y], color='b') # 更新当前点的位置 cur = (cur[0]-del_x, cur[1]-del_y) ax.plot([0, 1], [0, 1], 'b--') plt.xlabel('假阳率') plt.ylabel('真阳率') plt.title('AdaBoost马疝病监测系统的ROC曲线') ax.axis([0, 1, 0, 1]) plt.show() # 乘以x_step得到总面积 print("曲线下面积为:", y_sum*x_step) 使用马疝病数据绘制ROC曲线 123456789In [13]: importlib.reload(adaboost)Out[13]: &lt;module 'adaboost' from 'D:\\machineLearning\\MachineLearningInAction\\adaboost\\adaboost.py'&gt;In [14]: data,label = adaboost.load_data_set('data/horseColicTraining2.txt')In [15]: classifier_arr,agg_class_est = adaboost.adaboost_train_ds(data, label,40)In [16]: adaboost.plot_roc(agg_class_est.T, label)曲线下面积为: 0.8918191104095092 AdaBoost马疝病监测系统的ROC曲线-40个分类器 基于代价函数的分类器决策除了调节分类器的阈值之外，还可以使用 代价敏感的学习(cost-sensitive learning) 代价矩阵第一张表的代价为 TP*0+FN*1+FP*1+TN*0 ，第二张表的代价为 TP*(-5)+FN*1+FP*50+TN*02在构建分类器时，如果知道了代价值就可以选择付出最小代价的分类器。 非均衡问题的数据抽样方法对于非均衡分类器可以对分类器的训练数据进行改造。可以通过 欠抽样(undersampling) 或者 过抽样(oversampling) 来实现。过抽样意味着复制样例，欠抽样意味着删除样例。 示例代码 1.可以使用随机森林等更先进的bagging方法 ↩2.变量的意义参考准确率和召回率的部分 ↩]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实战</tag>
        <tag>分类</tag>
        <tag>监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支持向量机(SVM)]]></title>
    <url>%2F2018%2F05%2F22%2F%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-SVM%2F</url>
    <content type="text"><![CDATA[SVM概述 SVM是一种基于最大间隔分隔数据的算法 SVM的优缺点 优点：泛化错误率低，计算开销不大，结果易解释 缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题 适用数据类型：数值型和标称型数据 SVM的工作原理基于最大间隔分隔数据 4个线性不可分的例子 线性可分和分隔实例 如上图A框的数据所示，由于数据之间分隔间距足够大，所有可以在图中画出一条直线将两组数据点分开，这种数据被称为 线性可分(linearly separable) 数据。而将数据集分隔开的直线被称为 分隔超平面(separating hyperplane)1，是分类的决策边界2。通过这种方式构建分类器是希望找到离分隔超平面最近的点，确保它们离分隔面的距离足够远。这种点到分隔面的距离被称为 间隔(margin)。为了确保分类器足够健壮，则间隔就要尽可能地大。支持向量(support vector) 就是离分隔超平面最近的点。 寻找最大间隔分隔超平面的形式可以写成$w^Tx+b$。要计算点A到分隔超平面的距离，就必须计算点到分割面的法线或垂线的长度$\frac{|w^TA+b|}{||w||}$，常数$b$类似于Logistic回归中的截距$w_0$。向量$w$和常数$b$一起描述了所给数据的分隔线或超平面。 函数间隔 分类器求解优化输入数据给分类器会输出一个类别标签。使用类似 海维赛德阶跃函数 的函数对$w^Tx+b$作用得到$f(w^Tx+b)$，当$u]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Logistic回归]]></title>
    <url>%2F2018%2F05%2F20%2FLogistic%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[Logistic回归概述 根据现有数据对分类边界线建立回归公式，以此进行分类。训练分类器就是寻找最佳拟合参数，使用的是最优化算法 Logistic回归的优缺点 优点：计算代价不高，易于理解和实现 缺点：容易欠拟合，分类精度可能不高 适用数据类型：数值型和标称型数据 Logistic回归的工作原理Sigmoid函数海维赛德阶跃函数(Heaviside step function)(或称为 单位阶跃函数)能接受所有的输入然后预测出类别(在两个类的情况下，输出0或1)。但是该函数是从跳跃点上从0瞬间跳跃到1，这个跳跃过程很难处理。另一个Sigmoid函数具有类似的性质： \sigma(z)=\frac{1}{1+e^{-z}}当$z=0$时，Sigmoid函数值为0.5，随着$z$的增大，对应的值将逼近与1；而随着$z$的减小，对应的值将逼近于0。在横坐标刻度足够大的情况下，$Sigmoid$函数看起来像一个阶跃函数。 Sigmoid函数因此，实现Logistic回归的本质就是在每个特征上乘以一个回归系数，将所有的结果相加然后带入Sigmoid函数中,得到一个范围在0~1之间的数值，然后根据分界线判断类别(这里取0.5，大于0.5分为1类，小于0.5分为0类)。 Logistic回归的一般流程 收集数据：采用任意方法收集数据 准备数据：由于需要进行距离计算，要求数据类型为数值型。结构化数据格式最佳 分析数据：采用任意方法对数据进行分析 训练算法：为了找到最佳的分类回归系数 测试算法：训练完成后，分类速度会很快 使用算法：输入一些数据将其转换为对应的结构化数值；使用训练好的回归系数拟合数据进行分类；对分好类的数据进行分析 Logistic回归的实现基于最优化方法的最佳回归系数Sigmoid 函数的输入记为$z$,可以由下面公式得出： z=w_0x_0+w_1x_1+w_2x_2+……+w_nx_n向量形式为： z=W^TX表示将两个数值向量对应元素相乘后全部加起来。其中向量$X$是分类器的输入数据，向量$W$是回归系数 梯度上升法梯度上升法 沿着某函数的梯度方向探寻找到该函数的最大值。将梯度记为$\nabla$，则函数$f(x,y)$的梯度由下式表示: \nabla f(x,y)=\begin{pmatrix}\frac{\partial f(x,y)}{\partial x}\\\frac{\partial f(x,y)}{\partial y}\end{pmatrix}这个梯度意味着沿$x$的方向移动$\frac{\partial f(x,y)}{\partial x}$，沿$y$方向移动$\frac{\partial f(x,y)}{\partial y}$。其中，函数$f(x,y)$必须在待计算的点上有定义并且可微。 梯度上升 梯度上升算法沿梯度方向移动，梯度算子总是指向函数值增长最快的方向。移动量的大小称为步长，记作$\alpha$，用向量表示梯度算法的迭代公式为: w:=w+\nabla_wf(w)该公式将一直被迭代执行，知道达到某个停止条件为止(指定值或达到允许误差范围) 使用梯度上升算法 伪代码 12345每个回归系数初始化为1重复R次： 计算整个数据集的梯度 使用alpha*gradient更新回归系数的向量 返回回归系数 Logistic回归梯度上升优化算法1 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def load_data_set(): """加载测试数据集 打开文本逐行读取，设置x_0为1，每行前两个值为x_1,x_2 第三个值对应类别标签 :return: """ data_mat = [] label_mat = [] with open('data/testSet.txt', 'r', encoding='utf-8') as fr: for line in fr.readlines(): line_arr = line.strip().split() data_mat.append([1.0, float(line_arr[0]), float(line_arr[1])]) label_mat.append(int(line_arr[2])) return data_mat, label_matdef sigmoid(in_x): """定义阶跃函数 :param in_x: 线性假设函数theta'*X(weight'*X) :return: """ return np.longfloat(1.0/(1+np.exp(-in_x)))def grad_ascent(data_mat_in, class_labels): """梯度上升函数 :param data_mat_in: 数据集m*n的矩阵，m行表示m个训练样本，n列表式n个特征值(x_0为1) :param class_labels: 类别标签,为1*m的行向量，m对应m个训练样本 :return: """ # 将数据装换为NumPy矩阵 # 将类别标签行向量转换为列向量 data_matrix = np.mat(data_mat_in) label_mat = np.mat(class_labels).transpose() m, n = np.shape(data_matrix) # 初始化梯度上升算法的一些值 # alpha为移动步长 # max_cycles为迭代次数 alpha = 0.001 max_cycles = 500 weights = np.ones((n, 1)) for k in range(max_cycles): # 计算梯度上升算法 h = sigmoid(data_matrix*weights) error = label_mat - h weights = weights + alpha*data_matrix.transpose()*error return weights 画出决策边界 1234567891011121314151617181920212223242526272829303132def plot_best_fit(weight): """ 画出分界线 :param weight: 数据训练出来的参数值 :return: """ import matplotlib.pyplot as plt # 将矩阵转换为数组 weights = weight.getA() data_mat, label_mat = load_data_set() data_arr = np.array(data_mat) n = np.shape(data_arr)[0] xcord1, ycord1 = [], [] xcord2, ycord2 = [], [] for i in range(n): # 区分数据类别 if int(label_mat[i]) == 1: xcord1.append(data_arr[i, 1]) ycord1.append(data_arr[i, 2]) else: xcord2.append(data_arr[i, 1]) ycord2.append(data_arr[i, 2]) fig = plt.figure() ax = fig.add_subplot(111) ax.scatter(xcord1, ycord1, s=30, c='red', marker='s') ax.scatter(xcord2, ycord2, s=30, c='green') # 使sigmoid函数值为0.5即weight'*X=0 x = np.arange(-3.0, 3.0, 0.1) y = (-weights[0]-weights[1]*x)/weights[2] ax.plot(x, y) plt.xlabel('X1') plt.ylabel('X2') plt.show() 最佳决策边界 随机梯度上升梯度上升算法会在每次更新回归系数时遍历整个数据集，而随机梯度上升一次仅使用一个样本点来更新回归系数。由于可以在新样本到来时对分类器进行增量式更新，所以其也是一个在线学习算法。而一次处理所有数据被称为”批处理”2。 伪代码 12345所有回归系数初始化为1对数据集中每个样本： 计算该样本的梯度 使用alpha*gradient更新回归系数值返回回归系数值 随机梯度上升 12345678910111213141516def stoc_grad_ascent0(data_matrix, class_labels): """ 随机梯度上升算法 :param data_matrix: 训练数据集 :param class_labels: 训练数据对应的分类标签 :return: """ m, n = np.shape(data_matrix) data_matrix = np.mat(data_matrix) alpha = 0.01 weights = np.ones((n,1)) for i in range(m): # 每一次只对一个样本运行梯度上升算法 h = sigmoid(data_matrix[i]*weights) error = class_labels[i] - h weights = weights + np.mat(alpha*error*data_matrix[i]).transpose() return weights 随机梯度上升决策边界 由于存在一些不能正确分类的样本点(数据集非线性可分)，在每次迭代时会引发系数的剧烈改变，从而导致最佳拟合直线并非最佳分类线。为了避免算法来回波动，可以对随机梯度上升算法做出改进。 改进的随机梯度上升算法 12345678910111213141516171819202122232425def stoc_grad_ascent1(data_matrix, class_labels, num_iter=150): """改进后的随机梯度上升算法 :param data_matrix: 训练数据集 :param class_labels: 训练数据对应的分类标签 :param num_iter: 迭代次数 :return: """ m, n = np.shape(data_matrix) # 将数据转换为矩阵 weights = np.ones((n, 1)) data_matrix = np.mat(data_matrix) class_labels = np.mat(class_labels).transpose() for j in range(num_iter): data_index = list(range(m)) for i in range(m): # 每次迭代中调整步进值 alpha = 4/(1.0+j+i)+0.01 # 随机选取样本更新回归系数 # 然后从列表中删除对应的值 rand_index = int(np.random.uniform(0, len(data_index))) h = sigmoid(data_matrix[rand_index]*weights) error = class_labels[rand_index] - h weights = weights + alpha*data_matrix[rand_index].transpose()*error del(data_index[rand_index]) return weights 改进的随机梯度上升算法决策边界 从疝气病症预测病马死亡率处理数据中的缺失值 使用可用特征的均值来填补缺失值 使用特殊值来填补缺失值，如-1 忽略有缺失值的样本 使用相似样本的均值填补缺失值 使用另外的机器学习算法预测缺失值 如果一条数据是类别标签缺失，则直接丢弃该条数据 使用Logistic回归进行分类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566"""从疝气病症预测病马的死亡率1. 收集数据：给定数据文件2. 准备数据：用Python解析文本文件并填充缺失值3. 分析数据：可视化并观察数据4. 训练算法：使用优化算法，找到最佳系数5. 测试算法：观察错误率，通过改变迭代的次数和步长等菜蔬来得到更好的回归系数6. 使用算法："""def classify_vector(in_x, weights): """分类函数 计算对应的Sigmoid值来对数据进行分类 :param in_x: 数据集 :param weights: 回归系数 :return: """ prob = sigmoid(in_x*weights) if prob &gt; 0.5: return 1.0 else: return 0.0def colic_test(): """训练函数 :return: """ train_set = [] train_labels = [] with open('data/horseColicTraining.txt', 'r', encoding='utf-8') as fr_train: # 训练训练集，训练回归系数 for line in fr_train.readlines(): curr_line = line.strip().split('\t') line_arr = [] for i in range(21): line_arr.append(float(curr_line[i])) train_set.append(line_arr) train_labels.append(float(curr_line[21])) train_weight = stoc_grad_ascent1(train_set, train_labels, 500) error_count = 0.0 num_test_vec = 0.0 with open('data/horseColicTest.txt', 'r', encoding='utf-8') as fr_test: # 使用测试集测试 for line in fr_test.readlines(): num_test_vec += 1.0 curr_line = line.strip().split('\t') line_arr = [] for i in range(21): line_arr.append(float(curr_line[i])) if int(classify_vector(np.array(line_arr), train_weight)) != int(curr_line[21]): error_count += 1 error_rate = (float(error_count)/num_test_vec) print('错误率为：%f' % error_rate) return error_ratedef multi_test(): """多次测试函数 :return: """ num_tests = 10 error_sum = 0.0 for k in range(num_tests): error_sum += colic_test() print('迭代%d次后平均误差为%f' % (num_tests, error_sum/float(num_tests))) 示例代码 1.数据有100个样本点，每个点包含两个数值型特征：X1和X2 ↩2.可以分块加载数据，这种方式叫随机批处理 ↩]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实战</tag>
        <tag>分类</tag>
        <tag>监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯]]></title>
    <url>%2F2018%2F05%2F17%2F%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%2F</url>
    <content type="text"><![CDATA[朴素贝叶斯算法概述 基于概率论的分类方法。“朴素”是因为整个形式化过程只做最原始、最简单的假设 朴素贝叶斯算法的优缺点 优点：在数据较少的情况下仍然有效，可以处理多类别问题 缺点：对于输入数据的准备方式较为敏感 适用数据类型：标称型数据 朴素贝叶斯算法的工作原理贝叶斯决策理论的核心思想是选择具有最高概率的决策。即给定某个由$x$、$y$表示的数据点，该数据点来自于类别$c_1$的概率是多少$p(c_1\mid x,y)$，数据来自于类别$c_2$的概率是多少$p(c_2\mid x,y)$，然后运用贝叶斯准则得到： p(c_i\mid x,y)=\frac{p(x,y\mid c_i)p(c_i)}{p(x,y)}使用贝叶斯准则，可以通过已知的三个概率值来计算未知的概率值。 如果$p(c_1\mid x,y)$&gt;$p(c_2\mid x,y)$,那么属于类别$c_1$ 如果$p(c_1\mid x,y)$&lt;$p(c_2\mid x,y)$,那么属于类别$c_2$ 朴素贝叶斯算法的一般过程 收集数据：可以使用任何方法 准备数据：需要数值型或者布尔型数据 分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好 训练算法：计算不同的独立特征的条件概率 测试算法：计算错误率 使用算法：常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本 实现朴素贝叶斯算法从文本中构建词向量把文本看成单词向量或者词条向量。考虑出现在所有文档中的所有单词，再决定将哪些词纳入词汇表或者所要的词汇集合，然后将每一篇文档转换为词汇表中的向量 创建实验样本数据 123456789101112def load_data_set(): """创造实验样本 :return: """ posting_list = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'], ['my', 'dalmation', 'is', 'so', 'cute', '', 'love', 'him'], ['stop', 'posting', 'stupid', 'worthless', 'garbage'], ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'], ['quit', 'buying', 'worthless', ' dog ', 'food', 'stupid']] class_vec = [0, 1, 0, 1, 0, 1] return posting_list, class_vec 创建词汇表 1234567891011def create_vocab_list(data_set): """创建一个包含在所有文档中出现的不重复词的列表 :param data_set: 数据集 :return: """ vocab_set = set([]) for document in data_set: # 将每篇文档返回的新词集合添加到该集合中 # 操作符|用于求两个集合的并集 vocab_set = vocab_set | set(document) return list(vocab_set) 创建文档向量 123456789101112131415def set_of_words_2_vec(vocab_list, input_set): """将单词转换为向量(词集模型) :param vocab_list: 词汇表 :param input_set: 某个文档 :return: """ return_vec = [0] * len(vocab_list) for word in input_set: # 遍历文档 # 如果单词在词汇表中，将词汇表向量中对应位置置为1 if word in vocab_list: return_vec[vocab_list.index(word)] = 1 else: print("该单词不在我的词汇表中:%s" % word) return return_vec 从词向量计算概率将贝叶斯准则重写为： p(c_i\mid w)=\frac{p(w\mid c_i)p(c_i)}{p(w)}其中 w替换了之前的$x$、$y$表示一个向量(即它由多个值组成)。使用这个公式对每个类别($c_i$)计算概率，然后比较概率值大小。 首先通过类别i中文档数除以总的文档数来计算概率$p(c_i)$ 接下来计算$p(w\mid c_i)$。(将w展开为一个个独立特征则其概率写作$p(w_0,w_1,w_2..w_N\mid c_i)$，假设所有词相互独立则可以使用$p(w_0\mid c_i)p(w_1\mid c_i)p(w_2\mid c_i)…p(w_N\mid c_i)$来计算上述概率) 伪代码 123456789计算每个类别中的文档数目对每篇训练文档： 对每个类别： 如果词条出现在文档中-&gt;增加该词条的计数值 增加所有词条的计数值对每个类别： 对每个词条： 将该词条的数目除以总词条数目得到条件概率返回每个类别的条件概率 朴素贝叶斯分类器训练函数 1234567891011121314151617181920212223242526def train_nb_0(train_matrix, train_category): """朴素贝叶斯分类器 :param train_matrix: 文档矩阵,由set_of_words_2_vec()转换来 :param train_category: 每篇文档类别标签构成的向量 :return: """ num_train_docs = len(train_matrix) num_words = len(train_matrix[0]) # 计算文档归属为(class=1)的概率 pa_busive = sum(train_category)/float(num_train_docs) # 初始化分子变量和分母变量 p0_num, p1_num = np.ones(num_words), np.ones(num_words) pO_denom, p1_denom = 2.0, 2.0 for i in range(num_train_docs): # 遍历所有文档 # 某个词在文档中出现该词对应的个数加1，该文档的总次数也加1 if train_category[i] == 1: p1_num += train_matrix[i] p1_denom += sum(train_matrix[i]) else: p0_num += train_matrix[i] pO_denom += sum(train_matrix[i]) # 对每个元素除以该类别中的总词数 p1_vect = np.log(p1_num/p1_denom) p0_vect = np.log(p0_num/pO_denom) return p0_vect, p1_vect, pa_busive 分类函数 1234567891011121314151617def classify_nb(vec_2_classify, p0_vec, p1_vec, p_class1): """朴素贝叶斯分类函数 :param vec_2_classify: 需要分类的向量 :param p0_vec: 分类为0的概率向量 :param p1_vec: 分类为1的概率向量 :param p_class1: 文档归属为(class=1)的概率 :return: """ # 对应元素相乘 # 将词汇表中所有词的对应值相加 # 加上类别的对数概率 p1 = sum(vec_2_classify*p1_vec)+np.log(p_class1) p0 = sum(vec_2_classify*p0_vec)+np.log(1.0-p_class1) if p1 &gt; p0: return 1 else: return 0 词袋模型词集模型(set-of-words model)将每个词的出现与否作为一个特征。词袋模型(bag-of-words model)中一个词在文档中不止出现一次，可以出现多次 词袋模型123456789101112def bag_of_words_2_vec_mn(vocab_list, input_set): """将单词转换为向量(词袋模型) :param vocab_list: 词汇表 :param input_set: 某个文档 :return: """ return_vec = [0]*len(vocab_list) for word in input_set: if word in vocab_list: # 每遇到一个单词增加词向量中的对应值 return_vec[vocab_list.index(word)] += 1 return return_vec 测试算法 测试朴素贝叶斯算法12345678910111213141516def testing_nb(): """测试朴素贝叶斯分类器 :return: """ list_o_posts, list_classes = load_data_set() my_vocab_list = create_vocab_list(list_o_posts) train_mat = [] for list_o_post in list_o_posts: train_mat.append(set_of_words_2_vec(my_vocab_list, list_o_post)) p0_v, p1_v, pa_b = train_nb_0(train_mat, list_classes) test_entry = ['love', 'my', 'dalmation'] this_doc = np.array(set_of_words_2_vec(my_vocab_list, test_entry)) print('test_entry 分类为：', classify_nb(this_doc, p0_v, p1_v, pa_b)) test_entry = ['stupid', 'garbage'] this_doc = np.array(set_of_words_2_vec(my_vocab_list, test_entry)) print('test_entry 分类为：', classify_nb(this_doc, p0_v, p1_v, pa_b)) 示例使用朴素贝叶斯过滤垃圾邮件 文件解析 12345678def text_parse(big_string): """文件解析 将大字符串解析为字符串列表 :param big_string: 大字符串 :return: """ list_of_tokens = re.split(r'\\W*', big_string) return [tok.lower()for tok in list_of_tokens if len(tok) &gt; 2] 垃圾邮件分类测试 123456789101112131415161718192021222324252627282930313233343536373839def spam_test(): """垃圾邮件分类测试 :return: """ doc_list = [] class_list = [] full_text = [] for i in range(1, 26): # 导入文件夹spam和ham下的文本文件并将它们解析为词列表 word_list = text_parse(open('data/email/spam/%d.txt' % i).read()) doc_list.append(word_list) full_text.extend(doc_list) class_list.append(1) word_list = text_parse(open('data/email/ham/%d.txt' % i).read()) doc_list.append(word_list) full_text.extend(doc_list) class_list.append(0) vocab_list = create_vocab_list(doc_list) training_set = list(range(50)) test_set = [] for i in range(10): # 随机选择10个文件作为测试集并将其从训练集中剔除 rand_index = int(np.random.uniform(0, len(training_set))) test_set.append(training_set[rand_index]) del(training_set[rand_index]) train_mat = [] train_class = [] for doc_index in training_set: # 训练所有文档 train_mat.append(set_of_words_2_vec(vocab_list, doc_list[doc_index])) train_class.append(class_list[doc_index]) p0_v, p1_v, p_spam = train_nb_0(train_mat, train_class) error_count = 0 for doc_index in test_set: # 验证文档 word_vec = set_of_words_2_vec(vocab_list, doc_list[doc_index]) if classify_nb(word_vec, p0_v, p1_v, p_spam) != class_list[doc_index]: error_count += 1 print('错误率为：', float(error_count)/len(test_set)) 示例代码]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实战</tag>
        <tag>分类</tag>
        <tag>监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树]]></title>
    <url>%2F2018%2F05%2F16%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[决策树算法概述 使用具体标签划分类别 决策树决策树算法的优缺点 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据 缺点：可能会产生过度匹配问题 适用数据类型：数值型和标称型 决策树的工作原理 伪代码123456789检测数据集中的每个子项是否属于同一分类： if so return 类标签; else 寻找划分数据集的最好特征 划分数据集 创建分支节点 for 每个划分的子集 重复前面的步骤并增加返回结果到分支节点中 return 分支节点 决策树的一般流程 收集数据：可以使用任何方法 准备数据：树构造算法只适用于标称型数据，因此数据型数据必须离散化 分析数据：可以使用任何方法，构造树完成之后，检查图形书否符合预期 训练算法：构造树的数据结构 测试算法：使用经验树计算错误率 使用算法：此步骤可以适用于任何监督学习算法，而是用决策树可以更好地理解数据的内在含义 实现决策树算法信息增益划分数据集的大原则是：将无序的数据变得更加有序。在划分数据集前后信息发生的变化称为 信息增益。可以计算每个特征值划分数据集获得的信息增益并选择使信息增益最高的特征来划分数据集。集合信息的度量方式称为 香农熵 或简称为 熵。熵(entropy) 定义为信息的期望值。如果待分类的事务可能划分在多个分类中，则符号$x_i$的信息定义为： l(x_i)=-\log_2p(x_i)其中$p(x_i)$是选择该分类的概率。为了计算熵，需要计算所有类别所有可能值包含的信息期望值： H=-\sum_{i=1}^{n}p(x_i)\log_2p(x_i)其中$n$是分类的数目 计算给定数据集的香农熵12345678910111213141516171819202122def calc_shannon_ent(data_set): """计算给定数据集的香农熵 :param data_set: :return: """ # 计算数据集大小 num_entries = len(data_set) # 创建数据字典，键值为数据集最后一列的值 # 计算每一个键值出现的次数 label_counts = &#123;&#125; for feat_vec in data_set: current_label = feat_vec[-1] if current_label not in label_counts.keys(): label_counts[current_label] = 0 label_counts[current_label] += 1 shannon_ent = 0.0 for key in label_counts: # 使用类标签的发生频率计算类别出现的概率 # 计算香农熵 prob = float(label_counts[key]) / num_entries shannon_ent -= prob * log(prob, 2) return shannon_ent 熵越高，则混合的数据越多。得到熵之后可以按照获取最大信息增益的方法划分数据集。 划分数据集分类算法除了需要测量信息熵，还需要划分数据集，度量划分数据集的熵。对每个特征划分数据集的结果计算一次信息熵，然后判断按照哪个特征划分数据集是最好的划分方式 按照给定特征划分数据集 1234567891011121314151617def split_data_set(data_set, axis, value): """按照给定特征划分数据集 :param data_set: 待划分的数据集 :param axis: 划分数据集的特征 :param value: 需要返回的特征的值 :return: """ # 创建新的list对象 ret_data_set = [] for feat_vec in data_set: # 遍历数据集中每个元素，发现符合要求的值将其添加到新的列表中 # 即按照某个特征划分数据集时将所有符合要求的元素抽取出来 if feat_vec[axis] == value: reduced_feat_vec = feat_vec[:axis] reduced_feat_vec.extend(feat_vec[axis+1:]) ret_data_set.append(reduced_feat_vec) return ret_data_set 选择最好的数据划分方式 123456789101112131415161718192021222324252627282930313233def choose_best_feature_to_split(data_set): """选取最好的数据集划分方式 循环计算香农熵和split_data_set()函数，熵计算会说明划分数据集的最好数据组织方式 1.数据必须是一种由列表元素组成的列表，所有列表元素长度相同 2.数据的最后一列或每个实例的最后一个元素是当前实例的类别标签 :param data_set: 待划分的数据集 :return: """ num_features = len(data_set[0]) - 1 # 计算整个数据集的原始香农熵，用于与划分完之后的数据集计算的熵值进行比较 base_entropy = calc_shannon_ent(data_set) best_info_gain, best_feature = 0.0, -1 for i in range(num_features): # 遍历数据集中的所有特征 # 将数据集中所有第i个特征值或者所有可能存在的值写入新的list中 feat_list = [example[i] for example in data_set] # 去重 unique_vals = set(feat_list) new_entropy = 0.0 for value in unique_vals: # 遍历所有当前特征中的所有唯一属性值 # 对每个唯一属性划分一次数据集 # 然后计算数据集的新熵值 # 对所有数据集求得的熵值求和 sub_data_set = split_data_set(data_set, i, value) prob = len(sub_data_set)/float(len(data_set)) new_entropy += prob * calc_shannon_ent(sub_data_set) info_gain = base_entropy - new_entropy if info_gain &gt; best_info_gain: # 比较信息增益，返回最好特征的索引值 best_info_gain = info_gain best_feature = i return best_feature 递归构建决策树上面完成了从数据集构造决策树算法所需要的子功能模块，工作原理如下： 得到原始数据集，然后基于最好的特征值划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分。第一次划分之后，数据将向下传递到树分支的下一个节点，然后在这个节点上再次划分数据。 递归结束的条件： 程序遍历完所有划分数据集的属性或者每个分支下的所有实例都具有相同的分类 如果所有的实例具有相同的分类，则得到一个叶子节点或者终止块。任何到达叶子节点的数据必然属于叶子节点的分类。 划分数据时的数据路径如果数据集已经处理了所有属性，但类标签依然不是唯一的，这时通常使用多数表决的方法决定该子节点的分类 多数表决 12345678910111213def majority_cnt(class_list): """多数表决 计算每个类标签出现的频率，返回出现次数最多的分类名称 :param class_list: 分类名称的列表 :return: """ class_count = &#123;&#125; for vote in class_list: if vote not in class_count.keys(): class_count[vote] = 0 class_count[vote] += 1 sorted_class_count = sorted(class_count.items(), key=operator.itemgetter(1), reverse=True) return sorted_class_count[0][0] 创建树 12345678910111213141516171819202122232425262728def create_tree(data_set, labels): """创建树 :param data_set: 数据集 :param labels: 标签列表 :return: """ class_list = [example[-1] for example in data_set] if class_list.count(class_list[0]) == len(class_list): # 所有类标签完全相同则停止划分 return class_list[0] if len(data_set[0]) == 1: # 使用完所有特征停止划分 return majority_cnt(class_list) # 存储最好特征 # 得到列表包含的所有属性值 best_feature = choose_best_feature_to_split(data_set) best_feat_label = labels[best_feature] my_tree = &#123;best_feat_label: &#123;&#125;&#125; del(labels[best_feature]) feat_values = [example[best_feature] for example in data_set] unique_vals = set(feat_values) for value in unique_vals: # 复制类标签 sub_labels = labels[:] # 在每个数据集划分上调用create_tree() # 得到的返回值被插入到my_tree中 my_tree[best_feat_label][value] = create_tree(split_data_set(data_set, best_feature, value), sub_labels) return my_tree 在Python中使用Matplotlib注解绘制树形图测试和存储分类器测试算法：使用决策树执行分类依靠训练数据构造的决策树，程序比较测试数据与决策树上的数值，递归执行该过程直到进入叶子节点，最后将测试数据定义为叶子节点所属的类型。 执行分类123456789101112131415161718192021def classify(input_tree, feat_labels, test_vec): """使用决策树分类 :param input_tree: 数据集 :param feat_labels: 特征标签 :param test_vec: 测试向量 :return: """ first_str = list(input_tree.keys())[0] second_dict = input_tree[first_str] # 找到当前列表第一个匹配first_str变量的标签 feat_index = feat_labels.index(first_str) for key in second_dict.keys(): # 遍历整棵树，比较test_vec变量中的值与树节点的值 # 如果到达叶子节点就返回节点的分类标签 if test_vec[feat_index] == key: if type(second_dict[key]).__name__ == 'dict': class_label = classify(second_dict[key], feat_labels, test_vec) else: class_label = second_dict[key] return class_label 使用算法：决策树的存储为了节省计算时间，可以使用Python模块pickle序列化构造好的决策树对象，序列化对象可以在磁盘上保存对象，并在需要的时候读取出来。 序列化1234567891011121314151617def store_tree(input_tree, filename): """存储决策树 :param input_tree: 树结构 :param filename: 文件名 :return: """ with open(filename, 'w', encoding='utf-8') as fw: pickle.dump(input_tree, fw)def grab_tree(filename): """读取决策树 :param filename: 文件名 :return: """ with open(filename, 'r', encoding='utf-8') as fr: return pickle.load(fr) 示例：使用决策树预测隐形眼镜类型 使用决策树 123456789101112131415161718192021222324In [6]: import treesIn [7]: import tree_plotterIn [8]: fr = open('data/lenses.txt')In [9]: lenses = [inst.strip().split('\t') for inst in fr.readlines()]In [10]: lenses_labels = ['age', 'prescript', 'astigmatic', 'tearRate']In [11]: lenseTree = trees.create_tree(lenses,lenses_labels)In [12]: lenseTreeOut[12]:&#123;'tearRate': &#123;'normal': &#123;'astigmatic': &#123;'no': &#123;'age': &#123;'pre': 'soft', 'presbyopic': &#123;'prescript': &#123;'hyper': 'soft', 'myope': 'no lenses'&#125;&#125;, 'young': 'soft'&#125;&#125;, 'yes': &#123;'prescript': &#123;'hyper': &#123;'age': &#123;'pre': 'no lenses', 'presbyopic': 'no lenses', 'young': 'hard'&#125;&#125;, 'myope': 'hard'&#125;&#125;&#125;&#125;, 'reduced': 'no lenses'&#125;&#125;In [13]: tree_plotter.create_plot(lenseTree) ID3算法产生的决策树 示例代码]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实战</tag>
        <tag>分类</tag>
        <tag>监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k-近邻算法]]></title>
    <url>%2F2018%2F05%2F15%2Fk-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[K-近邻算法概述 k-近邻算法采用测量不同特征值之间的距离方法进行分类 k-近邻算法优缺点 优点：精度高、对异常值不敏感、无数据输入假定 缺点：计算复杂度高、空间复杂度高 使用数据范围：数值型和标称型 工作原理 存在一个样本数据集合且样本集中每个数据存在标签。输入没有标签的新数据后将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据(最近邻)的分类标签。只选取数据前k个最相似的数据。 k-近邻算法的一般流程 收集数据：使用任何方法 准备数据：距离计算所需要的数值，最好是结构化的数据结构 分析数据：可以使用任何方法 训练算法：不适用与k-近邻算法 测试算法：计算错误率 使用算法：首先需要输入样本数据和结构化的输出结果，然后运行k-近邻算法判定输入数据所属类别，对计算出的分类执行后续处理 实现k-近邻算法k-近邻算法是使用欧式距离公式计算两个向量点$xA$和$xB$之间的距离： d=\sqrt{(xA_0-xB_0)^2+(xA_1-xB_1)^2} 例如点$(0,0)$与$(1,2)$之间的距离为： \sqrt{(1-0)^2+(2-0)^2} 如果数据集存在4个特征值，则点$(1,2,3,4)$和$(2,3,4,5)$之间的距离计算为: \sqrt{(1-2)^2+(2-3)^2+(3-4)^2+(4-5)^2} k-近邻算法 1234567891011121314151617181920212223242526def classify(in_x, data_set, labels, k): """k-近邻算法 :param in_x: 分类的输入向量 :param data_set: 输入的训练样本集 :param labels: 标签向量，元素数目和矩阵dataSet的行数相同 :param k: 用于选择最近邻居的数目 :return: """ data_set_size = data_set.shape[0] # 计算输入向量与样本的差值 diff_mat = np.tile(in_x, (data_set_size, 1)) - data_set # 计算欧式距离 sq_diff_mat = diff_mat ** 2 sq_distances = sq_diff_mat.sum(axis=1) distances = sq_distances ** 0.5 # 排序 sorted_dist_indicies = distances.argsort() class_count = &#123;&#125; for i in range(k): # 获取第i个元素的label vote_i_label = labels[sorted_dist_indicies[i]] # 计算该类别的数目 class_count[vote_i_label] = class_count.get(vote_i_label, 0) + 1 # 对类别按值进行排序 sorted_class_count = sorted(class_count.items(), key=operator.itemgetter(1), reverse=True) return sorted_class_count[0][0] k-近邻算法实战在约会网站是使用k-近邻算法 将文本记录转换到NumPy 1234567891011121314151617181920212223def file2matrix(filename): """将文本记录转换为NumPy :param filename: 文件名 :return: """ with open(filename, 'r', encoding='utf-8') as file: # 读取文本计算样本数量 array_o_lines = file.readlines() number_of_lines = len(array_o_lines) # 生成样本举证 return_mat = np.zeros((number_of_lines, 3)) class_label_vector = [] index = 0 for line in array_o_lines: # 处理每一个样本 line = line.strip() list_from_line = line.split('\t') # 获取数据 return_mat[index, :] = list_from_line[0:3] # 获取标签 class_label_vector.append(int(list_from_line[-1])) index += 1 return return_mat, class_label_vector 归一化数值 将任意取值范围的特征值转化为0~1区间内的值$new_{value} = \frac{old_{value}-min}{max-min}$ 1234567891011121314def auto_norm(data_set): """归一化特征值 :param data_set: 数据集 :return: """ # 计算最小值和最大值及两者的差值 min_vals = data_set.min(0) max_vals = data_set.max(0) ranges = max_vals - min_vals m = data_set.shape[0] # 归一化数据集 norm_data_set = data_set - np.tile(min_vals, (m, 1)) norm_data_set = norm_data_set / np.tile(ranges, (m, 1)) return norm_data_set, ranges, min_vals 测试 1234567891011121314151617def dating_class_test(): """分类器针对约会网站的测试代码 :return: """ ho_radtio = 0.10 dating_data_mat, dating_labels = file2matrix('dataSet/datingTestSet2.txt') norm_mat, ranges, min_vals = auto_norm(dating_data_mat) m = norm_mat.shape[0] num_test_vecs = int(m*ho_radtio) error_count = 0.0 for i in range(num_test_vecs): classifier_result = classify(norm_mat[i, :], norm_mat[num_test_vecs:m, :], dating_labels[num_test_vecs:m], 3) print('预测值为：%d,真实值为：%d' % (classifier_result, dating_labels[i])) if(classifier_result != dating_labels[i]): error_count += 1.0 print("错误率为：%f" % (error_count/float(num_test_vecs))) 手写系统识别 将图像转换为测试向量 把一个32x32的二进制图像矩阵转换为1x1024的向量 12345678910111213def img2vector(filename): """将图像转换为测试向量 将测试数据中32*32的二进制图像矩阵转换为1*1024的向量 :param filename: 文件名 :return: """ return_vect = np.zeros((1, 1024)) with open(filename, 'r', encoding='utf-8') as file: for i in range(32): line_str = file.readline() for j in range(32): return_vect[0, 32*i+j] = int(line_str[j]) return return_vect 识别手写数字 1234567891011121314151617181920212223242526272829303132def handwriting_class_test(): """使用k-近邻算法识别手写数字 :return: """ hw_labels = [] training_file_list = os.listdir('dataSet/digits/trainingDigits') m = len(training_file_list) training_mat = np.zeros((m, 1024)) for i in range(m): # 加载数据集并添加标签 file_name_str = training_file_list[i] file_str = file_name_str.split('.')[0] class_num_str = int(file_str.split('_')[0]) hw_labels.append(class_num_str) training_mat[i, :] = img2vector('dataSet/digits/trainingDigits/%s' % file_name_str) test_file_list = os.listdir('dataSet/digits/testDigits') error_account = 0.0 m_test = len(test_file_list) for i in range(m_test): # 预测训练集 file_name_str = test_file_list[i] file_str = file_name_str.split('.')[0] class_num_str = int(file_str.split('_')[0]) hw_labels.append(class_num_str) vector_under_test = img2vector('dataSet/digits/testDigits/%s' % file_name_str) classifier_result = classify(vector_under_test, training_mat, hw_labels, 3) print('预测值为：%d,真实值为：%d' % (classifier_result, hw_labels[i])) if(classifier_result != class_num_str): error_account += 1.0 print("预测错误个数为:%d" % error_account) print("错误率为：%f" % (error_account/float(m_test))) 示例代码]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习实战</tag>
        <tag>分类</tag>
        <tag>监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[coursera-machine-lerning-learn]]></title>
    <url>%2F2018%2F04%2F15%2Fcoursera-machine-lerning-learn%2F</url>
    <content type="text"><![CDATA[Coursera上的机器学习如果遇到中文字幕不同步的情况可以查看b站视频 监督学习第一周：机器学习分类以及概述第一周总结资料资料 第二周：线性回归第二周总结资料资料 第二周作业 生成5x5的矩阵 画出数据的图像 固定theta计算代价函数 单变量使用梯度下降算法计算theta 标准化特征变量 计算多变量的代价函数 使用梯度下降算法计算theta 使用正规方程解theta 第三周：分类(逻辑回归)第三周总结资料资料 第三周作业 画出数据集的图像 计算Sigmoid函数 计算代价函数 预测数据 正规化theta 第四周：神经网络第四周总结资料资料 第四周作业 将正规化的逻辑回归函数向量化 多类别分类 多类别预测 前向传播和预测 第五周:神经网络后向传播算法第五周总结资料资料 第五周作业 多值前向传播及代价函数 正规化代价函数 S型函数 随机初始化Theta 后向传播算法* 梯度检查 正规化梯度下降函数 使用fmincg训练算法 第六周：机器学习诊断法第六周资料资料 第六周作业 线性回归代价函数 学习曲线 添加多项式特征 验证lamda曲线 第七周：支持向量机(SVM)第七周资料资料 第七周作业 高斯核函数 单词列表 特征提取 无监督学习第八周：K-Means算法和主成分分析法(PCA)/维度约简第八周资料资料 第八周作业 簇分配 移动聚类中心 实现PCA 使用PCA降维 还原数据 第九周：异常检测和推荐系统第九周资料资料 第九周作业 使用高斯分布估计参数 选择ε 推荐系统的预测函数 推荐系统梯度下降 正规化预测函数和梯度下降参数 第十周：随机梯度下降和映射约减(Map Reduce)第十周资料资料 第十一周：照片OCR和滑动窗口习题资料]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Coursera课程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[抓取知乎图片]]></title>
    <url>%2F2018%2F03%2F31%2F%E6%8A%93%E5%8F%96%E7%9F%A5%E4%B9%8E%E5%9B%BE%E7%89%87%2F</url>
    <content type="text"><![CDATA[动机今天突然想找一点好看的壁纸，便在网上搜索“程序员都在使用什么壁纸”，好巧不巧地点进知乎一个提问，发现其中的答主提供的一些壁纸都挺符合口味的，但一看那么多如果要一张一张下太过麻烦，便决定动手写一个爬取该页图片的脚本。 过程首先打开网页调试器找到图片的元素，可以发现知乎正文的图片放在一个叫noscript的标签中，这里用BeautifulSoup提供的功能提取出链接，注意这里应该使用data-original而不是src，因为src显示的图片是压缩过的，而data-original是改图片对应的源文件:1234567noscripts = soup.findAll('noscript')imgurls = []for noscript in noscripts: img = noscript.find('img') url = img['data-original'] if img['data-original'] else '' if url != '': imgurls.append(url) 获取图片地址然后创建保存图片的路径，这里是针对Posix系统的路径处理，判断是否是从根路径开始的，如果不是则在当前路径创建，最后在路径上拼接/用来连接文件名：1234567if save_path[0] != '/': save_path = './' + save_pathif not os.path.exists(save_path): os.makedirs(save_path)if save_path[-1] != '/': save_path += '/' 最后便是通过urlretrieve方法下载图片并保存到相应的文件，文件名是使用uuid生成的防止重复：1234567891011for imgUrl in imgurls: imgnum = imgnum + 1 picname = save_path + str(uuid.uuid1())+'.jpg' print('正在下载%d张' % imgnum) try: req.urlretrieve(imgUrl, picname) except urllib.error.URLError as e: if hasattr(e, 'code') or hasattr(e, 'reason'): print('第%d张下载失败，其链接地址为%s' % (imgnum, imgUrl)) continue print('第%d张下载成功' % imgnum) 在做了以上工作之外，还添加了help方法来引导用户使用，这个提示通过调用py文件是传入-h来调用:12345678def help(): # 脚本使用帮助函数 print(""" 本脚本用于下载单页面中的图片到指定位置 -h:查看帮助文档 -p:保存的路径（可选） -u:下载的页面URL（必填） """) 源代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283#!/usr/bin/python3# -*- coding:utf-8 -*-import urllib.request as reqimport urllib.parse as parseimport osimport uuidfrom bs4 import BeautifulSoupimport urllib.errorimport sys, getoptimport redef craw(url, save_path): # 从指定单页面下载所有图片 headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) ' 'AppleWebKit/537.36 (KHTML, like Gecko) ' 'Chrome/48.0.2564.116 ' 'Safari/537.36 ' 'TheWorld 7'&#125; request = req.Request(url=url, headers=headers) html = req.urlopen(request).read(); html = str(html) soup = BeautifulSoup(html, 'lxml') noscripts = soup.findAll('noscript') imgurls = [] for noscript in noscripts: # 提取图片的URL链接 img = noscript.find('img') url = img['data-original'] if img['data-original'] else '' if url != '': imgurls.append(url) if save_path[0] != '/': save_path = './' + save_path if not os.path.exists(save_path): os.makedirs(save_path) if save_path[-1] != '/': save_path += '/' print('该页面一共有%d张图片' % len(imgurls)) imgnum = 0 for imgUrl in imgurls: imgnum = imgnum + 1 picname = save_path + str(uuid.uuid1())+'.jpg' print('正在下载%d张' % imgnum) try: req.urlretrieve(imgUrl, picname) except urllib.error.URLError as e: if hasattr(e, 'code') or hasattr(e, 'reason'): print('第%d张下载失败，其链接地址为%s' % (imgnum, imgUrl)) continue print('第%d张下载成功' % imgnum)def help(): # 脚本使用帮助函数 print(""" 本脚本用于下载单页面中的图片到指定位置 -h:查看帮助文档 -p:保存的路径（可选） -u:下载的页面URL（必填） """)if __name__ == '__main__': opts, args = getopt.getopt(sys.argv[1:], 'hp:u:') furl, savepath = '', '' for op, value in opts: if op == '-u': furl = value elif op == '-p': savepath = value elif op == '-h': help() sys.exit() if furl == '': print('请输入下载链接，脚本使用方法通过-h查看') else: craw(furl, savepath)]]></content>
      <categories>
        <category>脚本</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>脚本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy高级应用]]></title>
    <url>%2F2018%2F03%2F29%2Fnumpy%E9%AB%98%E7%BA%A7%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[ndarray对象的内部机制NumPy的ndarray提供了一种同质数据块(连续/跨越)解释为多维数组对象的方式。数据类型(dtype) 决定了数据的解释方式。ndarray所有数组对象都是数据块的一个跨度视图，ndarray由以下内容组成: 一个指向数组(一个系统内存块)的指针 数据类型或dtype 一个表示数组形状(shape)的元组，例如一个2x2的数组形状为(2,2) 一个跨度元组(stride)，其中整数指的是为了前进到当前维度的下一个元素需要跨过的字节数(C顺序的3x4x5的float64(8字节)数组跨度为(160,40,8)) ndarray的内部结构跨度可以是负数，这样会使数组在内存中后向移动，切片中obj[::-1]或obj[:,::-1]就是如此 NumPy数据类型体系dtype都有一个超类，可以跟np.issubdtype函数结合使用，调用dtype的mro方法查看其所有父类:12345678910111213141516171819In [2]: ints = np.ones(10,dtype=np.uint16)In [3]: floats = np.ones(10,dtype=np.float32)In [4]: np.issubdtype(ints.dtype,np.integer)Out[4]: TrueIn [5]: np.issubdtype(floats.dtype,np.floating)Out[5]: TrueIn [6]: np.float64.mro()#查看父类Out[6]:[numpy.float64, numpy.floating, numpy.inexact, numpy.number, numpy.generic, float, object] NumPy的dtype体系 高级数组操作数组重塑数组重塑 只需向数组的实例方法reshape传入一个表示新形状的元组，一维数组和多维数组都能被重塑，当作为参数形状的其中一维为 -1 时表示该维度的大小由数据本身推断出来；可以将数组的shape属性传入reshape方法。与reshape将一维数组转换为多维数组的运算过程相反的运算叫做 扁平化(flattening) 或 散开(raveling),ravel不会产生源数据的副本，flatten方法返回数据的副本:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455In [7]: arr = np.arange(8)In [8]: arrOut[8]: array([0, 1, 2, 3, 4, 5, 6, 7])In [9]: arr.reshape((4,2)) #转换为一个4x2的矩阵Out[9]:array([[0, 1], [2, 3], [4, 5], [6, 7]])In [10]: arr.reshape((4,2)).reshape((2,4)) #重塑多维数组Out[10]:array([[0, 1, 2, 3], [4, 5, 6, 7]])In [13]: arr = np.arange(15)In [14]: arr.reshape((3,-1)) #设置-1，让其自动推断Out[14]:array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]])In [15]: other_arr = np.ones((5,3))In [16]: other_arr.shapeOut[16]: (5, 3)In [17]: arr.reshape(other_arr.shape) #使用数组的shape属性Out[17]:array([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11], [12, 13, 14]])In [19]: arr = arr.reshape((5,3))In [21]: arrOut[21]:array([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11], [12, 13, 14]])In [22]: arr.ravel() #扁平化Out[22]: array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])In [24]: arr.flatten()Out[24]: array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]) C和Fortran顺序C顺序 是指行优先顺序，对于一个二维数组，每行中的数据项是被存放在相邻内存位置上的。 Fortran顺序 是指列优先顺序，每列中的数据项是被存放在相邻内存位置上。reshape和reval这样的函数可以接受一个表示数组数据存放顺序的order参数，一般是’C’或’F’;C和Fortran顺序的关键区别是维度的行进顺序: C/行优先顺序：先经过更高的维度(轴1优先于轴0被处理) Fortran/列优先顺序: 后经过更高的维度(轴0会先于轴1被处理) 1234567In [26]: arr = np.arange(12).reshape((3,4))In [27]: arr.ravel() #行优先Out[27]: array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])In [28]: arr.ravel('F') #列优先Out[28]: array([ 0, 4, 8, 1, 5, 9, 2, 6, 10, 3, 7, 11]) 按行或列优先进行重塑 数组的合并和拆分 数组连接/拆分函数 函数 说明 concatenate 最一般化的连接，沿一条轴连接一组数组 vstack、row_stack 以面向行的方式对数组进行堆叠(沿轴0) hstack 以面向列的方式对数组进行堆叠(沿轴1) column_stack 类似于hstack，但是会先将一维数组转换为二维列向量 dstack 以面向”深度”的方式对数组进行堆叠(沿轴2) split 沿指定轴在指定的位置拆分数组 hsplit、vsplit、dsplit split的便捷化函数，分别沿轴0、轴1、轴2 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758In [29]: #使用numpy.concatenate按指定轴将一个由数组组成的序列连接到一起In [30]: arr1 = np.array([[1,2,3],[4,5,6]])In [31]: arr2 = np.array([[7,8,9],[10,11,12]])In [32]: np.concatenate([arr1,arr2],axis=0)Out[32]:array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]])In [33]: np.concatenate([arr1,arr2],axis=1)Out[33]:array([[ 1, 2, 3, 7, 8, 9], [ 4, 5, 6, 10, 11, 12]])In [34]: #使用vstack和hstackIn [35]: np.vstack((arr1,arr2))Out[35]:array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]])In [36]: np.hstack((arr1,arr2))Out[36]:array([[ 1, 2, 3, 7, 8, 9], [ 4, 5, 6, 10, 11, 12]])In [37]: arr = np.random.randn(5,2)In [38]: arrOut[38]:array([[ 1.37094925, 0.05685231], [-0.95278187, 0.06863068], [ 0.00680745, -0.67222854], [ 0.40865965, -0.63241948], [ 1.1155423 , 2.87073277]])In [39]: #使用split将一个数组沿指定轴拆分为多个数组In [40]: first,sencond,third = np.split(arr,[1,3])In [41]: firstOut[41]: array([[ 1.37094925, 0.05685231]])In [42]: sencondOut[42]:array([[-0.95278187, 0.06863068], [ 0.00680745, -0.67222854]])In [43]: thirdOut[43]:array([[ 0.40865965, -0.63241948], [ 1.1155423 , 2.87073277]]) 堆叠辅助类：r_和c_NumPy命名空间有两个特殊的对象r_和c_，它们能简化数组的堆叠操作:12345678910111213141516171819202122232425262728293031In [44]: arr1 = np.arange(6).reshape((3,2))In [45]: arr2 = randn(3,2)In [46]: np.r_[arr1,arr2] #面向行堆叠Out[46]:array([[ 0. , 1. ], [ 2. , 3. ], [ 4. , 5. ], [ 0.76365569, 0.53019635], [ 0.10616532, -0.66130024], [-0.20727219, 0.82787646]])In [47]: np.c_[np.r_[arr1,arr2],np.arange(6)]#面向行堆叠后再面向列堆叠Out[47]:array([[ 0. , 1. , 0. ], [ 2. , 3. , 1. ], [ 4. , 5. , 2. ], [ 0.76365569, 0.53019635, 3. ], [ 0.10616532, -0.66130024, 4. ], [-0.20727219, 0.82787646, 5. ]])In [48]: #将切片翻译成数组In [49]: np.c_[1:3,-3:-1]Out[49]:array([[ 1, -3], [ 2, -2]])In [50]: np.r_[1:3,-3:-1]Out[50]: array([ 1, 2, -3, -2]) 元素的重复操作:tile和repeatrepeat会将数组中各个元素重复一定的次数，从而产生一个更大的数组，默认情况下如果传入的是一个整数，则各元素都会被重复那么多次。如果传入的是一组整数，则各元素就可以重复不同的次数。123456789101112131415161718192021222324252627282930313233343536373839In [51]: arr = np.arange(3)In [52]: arrOut[52]: array([0, 1, 2])In [53]: arr.repeat(3)Out[53]: array([0, 0, 0, 1, 1, 1, 2, 2, 2])In [55]: arr.repeat([2,3,4])Out[55]: array([0, 0, 1, 1, 1, 2, 2, 2, 2])In [56]: #指定轴重复In [57]: arr = np.random.randn(2,2)In [58]: arrOut[58]:array([[ 0.1934937 , 0.02686283], [-1.37329786, 1.04754158]])In [59]: arr.repeat(2,axis=0)Out[59]:array([[ 0.1934937 , 0.02686283], [ 0.1934937 , 0.02686283], [-1.37329786, 1.04754158], [-1.37329786, 1.04754158]])In [60]: arr.repeat([2,3],axis=0)Out[60]:array([[ 0.1934937 , 0.02686283], [ 0.1934937 , 0.02686283], [-1.37329786, 1.04754158], [-1.37329786, 1.04754158], [-1.37329786, 1.04754158]])In [61]: arr.repeat([2,3],axis=1)Out[61]:array([[ 0.1934937 , 0.1934937 , 0.02686283, 0.02686283, 0.02686283], [-1.37329786, -1.37329786, 1.04754158, 1.04754158, 1.04754158]]) tile的功能是沿指定轴向堆叠数组的副本，第二个参数是堆叠的次数，对于标量，是水平堆叠的，它还可以是一个元组12345678910111213141516171819202122232425In [62]: arrOut[62]:array([[ 0.1934937 , 0.02686283], [-1.37329786, 1.04754158]])In [63]: np.tile(arr,2)Out[63]:array([[ 0.1934937 , 0.02686283, 0.1934937 , 0.02686283], [-1.37329786, 1.04754158, -1.37329786, 1.04754158]])In [64]: np.tile(arr,(2,1))Out[64]:array([[ 0.1934937 , 0.02686283], [-1.37329786, 1.04754158], [ 0.1934937 , 0.02686283], [-1.37329786, 1.04754158]])In [65]: np.tile(arr,(3,2))Out[65]:array([[ 0.1934937 , 0.02686283, 0.1934937 , 0.02686283], [-1.37329786, 1.04754158, -1.37329786, 1.04754158], [ 0.1934937 , 0.02686283, 0.1934937 , 0.02686283], [-1.37329786, 1.04754158, -1.37329786, 1.04754158], [ 0.1934937 , 0.02686283, 0.1934937 , 0.02686283], [-1.37329786, 1.04754158, -1.37329786, 1.04754158]]) 花式索引的等价函数:take和putndarray有两个方法专门用于获取和设置单个轴向上的选区，要在其他轴上使用take需要传入axis关键字；put不接受axis参数，它只会在数组的扁平化版本(一维，C顺序)上进行索引：123456789101112131415161718192021222324252627In [66]: arr = np.arange(10)*100In [67]: inds = [4,2,5,6]In [68]: arr.take(inds)Out[68]: array([400, 200, 500, 600])In [69]: arr.put(inds,66)In [70]: arrOut[70]: array([ 0, 100, 66, 300, 66, 66, 66, 700, 800, 900])In [71]: # 其他轴上使用takeIn [72]: ins = [2,0,2,1]In [74]: arr = np.random.randn(2,4)In [75]: arrOut[75]:array([[-1.16866402, -0.38862055, -0.30056282, -0.11058824], [-2.17866244, 0.06630283, 0.47301188, -0.00937435]])In [77]: arr.take(ins,axis=1)Out[77]:array([[-0.30056282, -1.16866402, -0.30056282, -0.38862055], [ 0.47301188, -2.17866244, 0.47301188, 0.06630283]]) 广播广播(broadcasting) 指的是不同形状的数组之间的算术运算的执行方式，只要遵循一定的规则1，低纬度的值可以被广播到数组的任意维度 一维数组在轴0上的广播12345678910111213141516In [80]: arr = randn(4,3)In [81]: arr.mean(0)Out[81]: array([-0.28703998, -0.39306773, 0.41940885])In [82]: demeaned = arr-arr.mean(0)In [83]: demeanedOut[83]:array([[-0.99338257, 0.87131685, 0.65287535], [ 0.75548156, 0.13286309, 1.0316277 ], [-0.05877416, -0.41411289, -1.13737663], [ 0.29667517, -0.59006705, -0.54712642]])In [84]: demeaned.mean(0)Out[84]: array([ -1.38777878e-17, -2.77555756e-17, 0.00000000e+00]) 由于arr.mean(0)的长度为3，所以它可以在0轴(这里的行)进行广播，因为arr的后缘维度是3，所以它们是兼容的 要在1轴上做减法(各行减去行平均值)，较小的那个数组必须是(4,1)：1234567891011121314151617181920212223242526272829In [85]: arrOut[85]:array([[-1.28042256, 0.47824912, 1.0722842 ], [ 0.46844158, -0.26020464, 1.45103655], [-0.34581414, -0.80718062, -0.71796778], [ 0.00963519, -0.98313478, -0.12771757]])In [86]: row_means = arr.mean(1)In [87]: row_means.reshape((4,1))Out[87]:array([[ 0.09003692], [ 0.55309117], [-0.62365418], [-0.36707239]])In [88]: demeaned = arr-row_means.reshape((4,1))In [89]: demeanedOut[89]:array([[-1.37045948, 0.3882122 , 0.98224728], [-0.08464958, -0.8132958 , 0.89794539], [ 0.27784004, -0.18352644, -0.0943136 ], [ 0.37670757, -0.61606239, 0.23935482]])In [90]: demeaned.mean(1)Out[90]:array([ 3.70074342e-17, 0.00000000e+00, 7.40148683e-17, 0.00000000e+00]) 二维数组在轴1上的广播 三维数组再轴0上的广播 沿其他轴向广播根据广播的原则，较小数组的”广播维”必须为1，对于三维的情况，在三维的任何一维上广播其实就是将数据重塑为兼容的形状，于是变需要专门为了广播添加一个长度为1的轴。NumPy数组通过特殊的np.newaxis属性以及 “全”切片 的方式插入新轴：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970In [91]: arr = np.zeros((4,4))In [92]: arr_3d = arr[:,np.newaxis,:]In [93]: arr_3dOut[93]:array([[[ 0., 0., 0., 0.]], [[ 0., 0., 0., 0.]], [[ 0., 0., 0., 0.]], [[ 0., 0., 0., 0.]]])In [94]: arr_3d.shapeOut[94]: (4, 1, 4)In [In [95]: arr_1d = np.random.normal(size=3)In [96]: arr_1dOut[96]: array([-1.22549157, -0.3134849 , 2.53183583])In [97]: arr_1d[:,np.newaxis]Out[97]:array([[-1.22549157], [-0.3134849 ], [ 2.53183583]])In [98]: arr_1d[np.newaxis,]Out[98]: array([[-1.22549157, -0.3134849 , 2.53183583]])In [99]: #对一个三维数组的轴二进行距平化In [102]: arr = np.random.randn(3,4,5)In [103]: depth_means = arr.mean(2)In [104]: depth_meansOut[104]:array([[ 0.97772551, -0.31477537, -0.07086849, -0.00903146], [-0.05710366, -0.21994819, -0.88640694, -1.13437736], [ 0.2764665 , 0.21509737, 0.17401022, -0.02444951]])In [105]: demeaned =arr - depth_means[:,:,np.newaxis]In [106]: demeanedOut[106]:array([[[ 0.18417271, -0.6440984 , 0.33705976, -0.72700844, 0.84987438], [ 0.62215472, 0.96952659, -0.83439414, 0.53091573, -1.28820289], [-0.82589887, -1.21061523, -0.65990178, 1.04349758, 1.6529183 ], [-0.96572226, -0.17109805, 0.47527161, 0.29543314, 0.36611556]], [[-0.74199902, 1.0335094 , 0.79082135, 0.74965704, -1.83198878], [-0.13308665, -0.06519346, -1.25928735, -0.53072789, 1.98829536], [-1.29463221, 0.74406668, 0.11684248, 2.0569325 , -1.62320945], [ 0.16037305, -0.28538757, -0.52345196, 1.32871473, -0.68024825]], [[ 1.85145343, -1.72601091, -1.15556288, -0.10519721, 1.13531758], [-1.68513998, 1.01516175, 0.05503934, 1.32854393, -0.71360504], [-0.42673112, 0.17061598, -1.07571345, -0.05948793, 1.39131652], [ 0.2301913 , -1.45091258, 0.1423503 , 0.52866008, 0.54971089]]])In [107]: demeaned.mean(2)Out[107]:array([[ 4.44089210e-17, -4.44089210e-17, 0.00000000e+00, 3.33066907e-17], [ 0.00000000e+00, 0.00000000e+00, 1.33226763e-16, 1.33226763e-16], [ 0.00000000e+00, -4.44089210e-17, 0.00000000e+00, 0.00000000e+00]]) 能在三维数组上广播的二维数组的形状示例 通过广播设置数组的值算术运算所遵循的广播原则同样适用于通过索引机制设置数组值的操作123456789101112131415161718192021222324252627282930313233In [108]: arr = np.zeros((4,3))In [109]: arr[:] = 5In [110]: arrOut[110]:array([[ 5., 5., 5.], [ 5., 5., 5.], [ 5., 5., 5.], [ 5., 5., 5.]])In [111]: #用一个一维数组来设置目标数组的各列In [112]: col = np.array([1,2,3,4])In [113]: arr[:]=col[:,np.newaxis]In [114]: arrOut[114]:array([[ 1., 1., 1.], [ 2., 2., 2.], [ 3., 3., 3.], [ 4., 4., 4.]])In [115]: arr[:2] =[[5],[6]]In [116]: arrOut[116]:array([[ 5., 5., 5.], [ 6., 6., 6.], [ 3., 3., 3.], [ 4., 4., 4.]]) ufunc高级应用 ufunc的方法 方法 说明 reduce(x) 通过连续执行原始运算的方式对值进行聚合 accumulate(x) 聚合值，保留所有局部聚合结果 reduceat(x,indices) “局部”约简(groupby)。约简数据的各个切片以产生聚合型数组 outer(x,y) 对x和y中的每对元素应用原始运算。结果数组的形状为x.shape+y.shape NumPy的各个二元ufunc有一些执行特定矢量化运算的特殊方法。 reducereduce方法接受一个数组参数，并通过一系列的二元运算对其进行聚合(可指明轴向)，例如对于np.add.reduce对数组中各个元素进行求和，其起始值取决于ufunc(对于add是0)；如果设置了轴号，约简运算会沿该轴向进行： 123456789101112131415161718192021222324In [4]: arr = np.arange(10)In [5]: np.add.reduce(arr)Out[5]: 45In [6]: arr.sum()Out[6]: 45In [7]: #用np.logical_and检查数组中各行是否有序In [8]: arr = randn(5,5)In [9]: arr[::2].sort(1) #对部分进行排序In [11]: arr[:,:-1]&lt;arr[:,1:]Out[11]:array([[ True, True, True, True], [False, True, False, True], [ True, True, True, True], [ True, False, True, True], [ True, True, True, True]])In [13]: np.logical_and.reduce(arr[:,:-1]&lt;arr[:,1:],axis=1)#logical_and.reduce和all方法等价Out[13]: array([ True, False, True, False, True]) accumulateaccumulate产生一个跟原数组大小相同的中间”累计”值数组，它与reduce就像cunsun和sum一样: 1234567In [14]: arr = np.arange(15).reshape((3,5))In [15]: np.add.accumulate(arr,axis=1)Out[15]:array([[ 0, 1, 3, 6, 10], [ 5, 11, 18, 26, 35], [10, 21, 33, 46, 60]]) reduceatreduceat用于计算”局部约简”，相当于是一个对数据各切片进行聚合的groupby运算，它接受一组用于指示如何对值进行拆分和聚合的”面元边界”: 1234567891011121314151617181920In [16]: arr = np.arange(10)In [17]: np.add.reduceat(arr,[0,5,8])#在arr[0:5],arr[5:8],arr[8:0]进行约简(这里是求和)(划分的是左闭区间)Out[17]: array([10, 18, 17])In [21]: #在指定轴上进行约简In [22]: arr = np.arange(15).reshape((3,5))In [23]: arrOut[23]:array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14]])In [24]: np.add.reduceat(arr,[0,2,4],axis=1)Out[24]:array([[ 1, 5, 4], [11, 15, 9], [21, 25, 14]]) outerouter用于计算两个数组的叉积，其输出结果的维度是两个输入数据的维度之和: 12345678910111213141516171819202122232425262728293031323334In [26]: arr = np.arange(3).repeat([1,2,2])In [27]: arrOut[27]: array([0, 1, 1, 2, 2])In [28]: np.multiply.outer(arr,np.arange(5))Out[28]:array([[0, 0, 0, 0, 0], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 2, 4, 6, 8], [0, 2, 4, 6, 8]])In [33]: result = np.subtract.outer(np.arange(12).reshape((3,4)),np.arange(5))In [34]: resultOut[34]:array([[[ 0, -1, -2, -3, -4], [ 1, 0, -1, -2, -3], [ 2, 1, 0, -1, -2], [ 3, 2, 1, 0, -1]], [[ 4, 3, 2, 1, 0], [ 5, 4, 3, 2, 1], [ 6, 5, 4, 3, 2], [ 7, 6, 5, 4, 3]], [[ 8, 7, 6, 5, 4], [ 9, 8, 7, 6, 5], [10, 9, 8, 7, 6], [11, 10, 9, 8, 7]]])In [35]: result.shapeOut[35]: (3, 4, 5) 自定义ufuncnumpy.frompyfunc和numpy.vectorize两个方法可以将自定义函数像ufunc那样使用，numpy.frompyfunc接受一个Python函数以及两个分别表示输入输出参数数量的整数，用frompyfunc创建的函数总是返回Python对象数组；numpy.vectorize在类型推断方面更智能一些。(由于在计算每个元素时都执行一次Python函数调用，相对于自带的这种创建的ufunc型函数会慢很多):1234567891011121314151617181920212223242526In [36]: def add_elements(x,y): ...: # 加法的简单函数 ...: return x+y ...:In [37]: add_them = np.frompyfunc(add_elements, 2, 1)In [38]: add_them(np.arange(8),np.arange(8))Out[38]: array([0, 2, 4, 6, 8, 10, 12, 14], dtype=object)In [39]: # 使用numpy.vecorizeIn [40]: add_them = np.vectorize(add_elements,otypes=[np.float64])In [41]: add_them(np.arange(8),np.arange(8))Out[41]: array([ 0., 2., 4., 6., 8., 10., 12., 14.])In [42]: # 比较自带的ufunc和自建的ufunc性能In [43]: arr = randn(1000)In [44]: %timeit add_them(arr,arr)477 µs ± 44.4 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)In [45]: %timeit np.add(arr,arr)2.22 µs ± 110 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each) 结构化和记录式数组结构化数组 是一种特殊的ndarray，其中各个元素可以被看作C语言中的结构体或SQL表中带有各个命名字段的行，定义dtype典型的方法是元组列表，各元组的格式为 (filed_name, field_data_type)，这样数组的元素就成了元组式的对象，该对象中各个元素可以像字典那样进行访问，字段名保存在dtypes.names属性中。在访问结构化数组的某个字段时，返回的是该数据的视图，所以不会发生数据复制。结构化数组可以将单个内存块解释为带有任意复杂嵌套列的 表格型结构，数组中的每个元素都在内存中被表示为固定的字节数。所以其能提供快速高效的磁盘数据读写、网络传输等功能.123456789101112131415161718In [50]: dtype = [('e1', np.float64),('e2',np.int32)]In [51]: sarr = np.array([(1.5,6),(2,3.4)],dtype=dtype)In [52]: sarrOut[52]: array([(1.5, 6), (2. , 3)], dtype=[('e1', '&lt;f8'), ('e2', '&lt;i4')])In [53]: sarr[0]Out[53]: (1.5, 6)In [54]: sarr[1]Out[54]: (2., 3)In [55]: sarr['e1']Out[55]: array([1.5, 2. ])In [56]: sarr[1]['e1']Out[56]: 2.0 嵌套dtype和多维字段在定义结构化dtype时，可以设置一个形状( 整数或元组)，也可以嵌套dtype1234567891011121314151617181920212223242526272829303132In [58]: dtype = [('e1', np.float64,3),('e2',np.int32)] #e1字段表示的是一个长度为3的数组In [59]: arr = np.zeros(4,dtype=dtype)In [60]: arrOut[60]:array([([0., 0., 0.], 0), ([0., 0., 0.], 0), ([0., 0., 0.], 0), ([0., 0., 0.], 0)], dtype=[('e1', '&lt;f8', (3,)), ('e2', '&lt;i4')])In [61]: arr['e1'] #访问e1得到一个二维数组Out[61]:array([[0., 0., 0.], [0., 0., 0.], [0., 0., 0.], [0., 0., 0.]])In [62]: # 嵌套dtypeIn [63]: dtype = [('out1',[('in1','f8'),('in2','f4')]),('out2',np.int16)]In [64]: data = np.array([((1,2),5),((3,4,),6)],dtype=dtype)In [65]: dataOut[65]:array([((1., 2.), 5), ((3., 4.), 6)], dtype=[('out1', [('in1', '&lt;f8'), ('in2', '&lt;f4')]), ('out2', '&lt;i2')])In [66]: data['out1']Out[66]: array([(1., 2.), (3., 4.)], dtype=[('in1', '&lt;f8'), ('in2', '&lt;f4')])In [67]: data['out1']['in2']Out[67]: array([2., 4.], dtype=float32) 排序ndarray的sort实例方法是就地排序，数组内容的重新排列不会产生新数组，如果目标数组只是一个视图，则原始数组将会被修改；numpy.sort会为元素组创建一个已排序副本且接受和ndarray.sort一样的参数；两个排序方法都可以接受一个axis参数来沿指定轴向对数据块进行单独排序，这两个方法均不可以设置为降序，可以通过values[::-1]返回一个反序的列表 数组排序算法 kind 速度 稳定性 工作空间 最坏情况 ‘quicksort’ 1 否 0 O(n^2) ‘mergesort’ 2 是 n/2 O(n log n) ‘heapsort’ 3 否 0 O(n log n) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061In [71]: arr = randn(7)In [72]: arr.sort()In [73]: arrOut[73]:array([-0.89170014, -0.80979003, 0.29783243, 0.54719346, 0.70813987, 0.92117061, 1.180511 ])In [74]: arr = randn(3,5)In [75]: arrOut[75]:array([[ 0.15479476, -0.52233606, 0.43424726, 0.62303973, 0.97207718], [-0.63949636, -0.53976961, -0.82786402, 1.18934999, -0.03449638], [ 1.46898713, 1.34104614, -1.49118633, -0.80856518, 1.12933992]])In [76]: arr[:,0].sort() #对视图进行排序In [77]: arrOut[77]:array([[-0.63949636, -0.52233606, 0.43424726, 0.62303973, 0.97207718], [ 0.15479476, -0.53976961, -0.82786402, 1.18934999, -0.03449638], [ 1.46898713, 1.34104614, -1.49118633, -0.80856518, 1.12933992]])In [78]: # numpy.sort()In [79]: arr = randn(5)In [80]: arrOut[80]: array([ 0.48399707, 2.34420952, -0.2882468 , 0.67498985, -0.13933173])In [81]: np.sort(arr)Out[81]: array([-0.2882468 , -0.13933173, 0.48399707, 0.67498985, 2.34420952])In [82]: arrOut[82]: array([ 0.48399707, 2.34420952, -0.2882468 , 0.67498985, -0.13933173])In [83]: # 沿指定轴进行排序In [84]: arr = randn(3,5)In [85]: arrOut[85]:array([[ 0.56629486, -0.80691028, 0.99998769, 3.37353894, 0.28443593], [-0.32536803, -2.1990948 , 0.52076016, -2.36260719, -0.06325647], [ 0.46436748, 0.46770738, -1.22818396, -1.60317798, 1.27239436]])In [86]: arr.sort(axis=1)In [87]: arrOut[87]:array([[-0.80691028, 0.28443593, 0.56629486, 0.99998769, 3.37353894], [-2.36260719, -2.1990948 , -0.32536803, -0.06325647, 0.52076016], [-1.60317798, -1.22818396, 0.46436748, 0.46770738, 1.27239436]])In [88]: arr[:,::-1]#反序Out[88]:array([[ 3.37353894, 0.99998769, 0.56629486, 0.28443593, -0.80691028], [ 0.52076016, -0.06325647, -0.32536803, -2.1990948 , -2.36260719], [ 1.27239436, 0.46770738, 0.46436748, -1.22818396, -1.60317798]]) 间接排序：argsort和lexsort当需要根据一个或多个键对数据集进行排序时，给定一个或多个键，可以得到一个由整数组成的索引数组，其中的索引值表示在原数据的位置取相应的值填充新数据，argsort和np.lexsort实现了该功能:123456789101112131415161718192021222324252627282930In [98]: values = np.array([5,0,1,3,2])In [99]: valuesOut[99]: array([5, 0, 1, 3, 2])In [100]: indexer = values.argsort()In [101]: indexerOut[101]: array([1, 2, 4, 3, 0])In [102]: values[indexer]Out[102]: array([0, 1, 2, 3, 5])In [103]: # 根据数组的第一行进行排序In [104]: arr = randn(3,5)In [105]: arr[0] = valuesIn [106]: arrOut[106]:array([[ 5. , 0. , 1. , 3. , 2. ], [-0.9539872 , 1.03268562, 1.81014861, -1.5252794 , 0.47952654], [-0.82301317, 0.09471158, -1.64912658, 0.539759 , -0.47313276]])In [107]: arr[:,arr[0].argsort()]Out[107]:array([[ 0. , 1. , 2. , 3. , 5. ], [ 1.03268562, 1.81014861, 0.47952654, -1.5252794 , -0.9539872 ], [ 0.09471158, -1.64912658, -0.47313276, 0.539759 , -0.82301317]]) lexsort可以一次性对多个键数组执行间接排序(字典序)：1234567891011121314151617In [108]: first_name = np.array(['Jim','June','Steven','Bill','Tom','Barbara'])In [109]: last_mame = np.array(['Jones','Deng','Jones','Walters','Lucy','Jone'])In [110]: soter = np.lexsort((first_name,last_mame))In [111]: soterOut[111]: array([1, 5, 0, 2, 4, 3])In [116]: list(zip(last_mame[soter],first_name[soter]))Out[116]:[('Deng', 'June'), ('Jone', 'Barbara'), ('Jones', 'Jim'), ('Jones', 'Steven'), ('Lucy', 'Tom'), ('Walters', 'Bill')] 其他排序算法在上面的表格中罗列出了一些排序算法，稳定的(stable) 排序算法会保持等价元素的相对位置，mergsort是这些当中唯一的稳定排序，它的平均性能比默认的quicksort(快速排序)要差：12345678910111213In [117]: values = np.array(['2:first','2:sencond','1:first','1:second','1:third'])In [118]: key = np.array([2,2,1,1,1])In [119]: indexer = key.argsort(kind='mergesort')In [120]: indexerOut[120]: array([2, 3, 4, 0, 1])In [121]: values.take(indexer)Out[121]:array(['1:first', '1:second', '1:third', '2:first', '2:sencond'], dtype='&lt;U9') 在有序数组中查找元素searchsorted是一个在有序数组上执行二分查找的数组元素，只要将值插入到它返回的位置就能维持数组的有序性，而传入一组值就能得到一组索引；searchsorted默认行为时返回相等值组的左侧索引，可以通过siede=&#39;right&#39;来使用右侧索引:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152In [128]: arr = np.array([0,1,4,51,63])In [129]: arr.searchsorted(9)Out[129]: 3In [130]: arr.searchsorted([0,4,56,34])#相等的值返回左侧索引Out[130]: array([0, 2, 4, 3])In [131]: arr.searchsorted([0,4,56,34],side='right')#相等的值返回右侧索引Out[131]: array([1, 3, 4, 3])In [132]: #使用表示面元边界的数组将数据数组拆分开In [133]: data = np.floor(np.random.uniform(0,10000,size=50))In [134]: dataOut[134]:array([6492., 6539., 1800., 3197., 4684., 2820., 1159., 4291., 8972., 4913., 8830., 317., 1816., 5768., 3665., 6927., 9655., 1436., 9965., 7518., 7971., 9228., 3169., 5788., 4565., 2223., 5867., 5507., 4079., 5724., 3676., 6847., 6003., 7755., 421., 6624., 9067., 2729., 929., 3215., 340., 7163., 9407., 3527., 2332., 7660., 813., 4194., 3100., 3013.])In [135]: bins = np.array([0,100,1000,5000,10000])In [136]: #使用searchsorted获取各数据点所属区间的编号(1表示[0,100))In [137]: labels = bins.searchsorted(data)In [138]: labelsOut[138]:array([4, 4, 3, 3, 3, 3, 3, 3, 4, 3, 4, 2, 3, 4, 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, 3, 3, 4, 4, 3, 4, 3, 4, 4, 4, 2, 4, 4, 3, 2, 3, 2, 4, 4, 3, 3, 4, 2, 3, 3, 3])In [139]: #通过groupby使用该结果对原始数据集进行拆分In [140]: Series(data).groupby(labels).sum()Out[140]:2 2820.03 69603.04 171277.0dtype: float64In [141]: #digitize函数可以得到面元编号In [142]: np.digitize(data,bins)Out[142]:array([4, 4, 3, 3, 3, 3, 3, 3, 4, 3, 4, 2, 3, 4, 3, 4, 4, 3, 4, 4, 4, 4, 3, 4, 3, 3, 4, 4, 3, 4, 3, 4, 4, 4, 2, 4, 4, 3, 2, 3, 2, 4, 4, 3, 3, 4, 2, 3, 3, 3]) NumPy的matrix类NumPy提供了一个matrix类，单行或列会以二维形式返回，且使用星号(*)的乘法就是矩阵乘法，matrix还有一个特殊的属性I返回矩阵的逆。(对于个别带有大量线性代数运算的函数，可以将函数参数转换为matrix类型，然后在返回之前用np.asarray将其转换为正规的ndarray)：123456789101112131415161718192021222324252627282930313233In [146]: X = np.random.randn(16).reshape((4,4))In [147]: Xm = np.matrix(X)In [148]: ym =Xm[:,0]In [149]: XmOut[149]:matrix([[-0.15559707, -0.99331439, 0.47359971, -0.01311698], [ 0.02949084, 1.44680113, 0.52641512, 0.35424227], [-0.08636669, 0.84882811, -0.44418401, 1.87720362], [ 2.50948611, 0.71447812, 0.37073346, -0.22516639]])In [150]: ymOut[150]:matrix([[-0.15559707], [ 0.02949084], [-0.08636669], [ 2.50948611]])In [151]: ym.T*Xm*ymOut[151]: matrix([[-2.80055856]])In [152]: Xm.I*XOut[152]:matrix([[ 1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00], [ 0.00000000e+00, 1.00000000e+00, -6.93889390e-17, 0.00000000e+00], [-2.77555756e-17, 5.55111512e-17, 1.00000000e+00, 0.00000000e+00], [-2.77555756e-17, -1.11022302e-16, -2.77555756e-17, 1.00000000e+00]]) 高级数组的输入输出np.save和np.load可用于读写磁盘上以二进制格式存储的数组，而 内存映像(memory map)能处理内存中放不下的数据集。 内存映像文件内存映像文件是一种将磁盘上的非常大的二进制数据文件当做内存中的数组进行处理的方式。NumPy实现的类似于ndarray的 memmap对象允许将大文件分成小段进行读写而不是一次性将整个数组读入内存，基本上能用于ndarray的算法也能用于memmap。对memmap切片返回磁盘上数据的视图，如果对视图进行赋值，数据会先缓存在内存中，调用flush方法将其写入磁盘；只要某个内存映像超出了作用域就会被垃圾回收器回收，之前做的任何修改会被写入磁盘。当打开一个已经存在的内存映像时，需要指明 数据类型和 形状(磁盘上只是一块二进制数据，没有任何元数据)；内存映像是存放在磁盘上的ndarray，所以可以使用结构化dtype:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869In [157]: # 使用np.memmap并传入一个文件路径，数据类型，形状以及文件模式In [158]: mmap = np.memmap('mymmap',dtype='f8',mode='w+',shape=(10000,10000))In [159]: mmapOut[159]:memmap([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]])In [160]: #对memmap进行切片返回磁盘数据的视图In [162]: section = mmap[:5]In [163]: # 对其赋值并写会磁盘映像In [164]: section[:] = np.random.randn(5,10000)In [165]: mmap.flush()In [166]: mmapOut[166]:memmap([[-0.76334573, -0.63262078, 0.32267608, ..., 0.58519542, 0.33744543, 0.15489497], [-1.5176626 , -0.56939983, 0.85359754, ..., -0.0471389 , -0.69515072, 0.72046585], [ 1.35572875, 1.93748025, -0.50928442, ..., 0.49249934, -0.45443455, 1.04739763], ..., [ 0. , 0. , 0. , ..., 0. , 0. , 0. ], [ 0. , 0. , 0. , ..., 0. , 0. , 0. ], [ 0. , 0. , 0. , ..., 0. , 0. , 0. ]])In [167]: del mmap #删除mmapIn [168]: mmap---------------------------------------------------------------------------NameError Traceback (most recent call last)&lt;ipython-input-168-49f7bdb78b33&gt; in &lt;module&gt;()----&gt; 1 mmapNameError: name 'mmap' is not definedIn [169]: #读取memmap需要指明数据类型和形状In [170]: mmap = np.memmap('mymmap',dtype='f8',shape=(10000,10000))In [171]: mmapOut[171]:memmap([[-0.76334573, -0.63262078, 0.32267608, ..., 0.58519542, 0.33744543, 0.15489497], [-1.5176626 , -0.56939983, 0.85359754, ..., -0.0471389 , -0.69515072, 0.72046585], [ 1.35572875, 1.93748025, -0.50928442, ..., 0.49249934, -0.45443455, 1.04739763], ..., [ 0. , 0. , 0. , ..., 0. , 0. , 0. ], [ 0. , 0. , 0. , ..., 0. , 0. , 0. ], [ 0. , 0. , 0. , ..., 0. , 0. , 0. ]]) 性能建议 将Python循环和条件逻辑转换为数组运算和布尔数组运算 尽量使用广播 避免复制数据，尽量使用数组视图(即切片) 利用ufunc及各种方法1.广播的原则:如果两个数组的后缘维度(trailing dimension，即从末尾开始算起的维度)的轴长度相符或其中一方的长度为1，则认为它们是广播兼容的。广播会在缺失和(或)长度为1的维度上进行。 ↩]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>NumPy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[金融和经济数据应用]]></title>
    <url>%2F2018%2F03%2F29%2F%E9%87%91%E8%9E%8D%E5%92%8C%E7%BB%8F%E6%B5%8E%E6%95%B0%E6%8D%AE%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[数据规整化时间序列及截面对齐对两个相关的时间序列的索引没有对齐或两个DataFrame对象可能含有不匹配的行或列，pandas在算数运算中自动对齐及解决合并时可能带来的bug；下面对股票价格和成交量进行计算:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061In [14]: prices = pd.read_csv('stock_px.csv', parse_dates=True, index_col=0) #加载股票价格In [15]: volume = pd.read_csv('volume.csv', parse_dates=True, index_col=0) #加载成交量In [16]: pr = prices[:5]In [17]: vo = volume[:3]In [18]: prOut[18]: AA AAPL GE IBM JNJ MSFT PEP SPX XOM1990-02-01 4.98 7.86 2.87 16.79 4.27 0.51 6.04 328.79 6.121990-02-02 5.04 8.00 2.87 16.89 4.37 0.51 6.09 330.92 6.241990-02-05 5.07 8.18 2.87 17.32 4.34 0.51 6.05 331.85 6.251990-02-06 5.01 8.12 2.88 17.56 4.32 0.51 6.15 329.66 6.231990-02-07 5.04 7.77 2.91 17.93 4.38 0.51 6.17 333.75 6.33In [19]: voOut[19]: AA AAPL GE IBM JNJ \1990-02-01 2185600.0 4193200.0 14457600.0 6903600.0 5942400.01990-02-02 3103200.0 4248800.0 15302400.0 6064400.0 4732800.01990-02-05 1792800.0 3653200.0 9134400.0 5299200.0 3950400.0 MSFT PEP SPX XOM1990-02-01 89193600.0 2954400.0 154580000.0 2916400.01990-02-02 71395200.0 2424000.0 164400000.0 4250000.01990-02-05 59731200.0 2225400.0 130950000.0 5880800.0In [20]: #在加权成交量的加权平均价格时pandas会自动对齐数据，并在sum这样的函数中排除缺失数据In [21]: pr * voOut[21]: AA AAPL GE IBM JNJ \1990-02-01 10884288.0 32958552.0 41493312.0 115911444.0 25374048.01990-02-02 15640128.0 33990400.0 43917888.0 102427716.0 20682336.01990-02-05 9089496.0 29883176.0 26215728.0 91782144.0 17144736.01990-02-06 NaN NaN NaN NaN NaN1990-02-07 NaN NaN NaN NaN NaN MSFT PEP SPX XOM1990-02-01 45488736.0 17844576.0 5.082436e+10 17848368.01990-02-02 36411552.0 14762160.0 5.440325e+10 26520000.01990-02-05 30462912.0 13463670.0 4.345576e+10 36755000.01990-02-06 NaN NaN NaN NaN1990-02-07 NaN NaN NaN NaNIn [22]: vwap = (pr*vo).sum()/vo.sum()In [23]: vwapOut[23]:AA 5.029077AAPL 8.005831GE 2.870000IBM 16.976948JNJ 4.321267MSFT 0.510000PEP 6.058866SPX 330.458880XOM 6.217684dtype: float64 使用DataFrame的align方法手工对齐，它返回一个元组，含有两个对象的重索引版本:123456789101112131415In [24]: pr.align(vo, join='inner')Out[24]:( AA AAPL GE IBM JNJ MSFT PEP SPX XOM 1990-02-01 4.98 7.86 2.87 16.79 4.27 0.51 6.04 328.79 6.12 1990-02-02 5.04 8.00 2.87 16.89 4.37 0.51 6.09 330.92 6.24 1990-02-05 5.07 8.18 2.87 17.32 4.34 0.51 6.05 331.85 6.25, AA AAPL GE IBM JNJ \ 1990-02-01 2185600.0 4193200.0 14457600.0 6903600.0 5942400.0 1990-02-02 3103200.0 4248800.0 15302400.0 6064400.0 4732800.0 1990-02-05 1792800.0 3653200.0 9134400.0 5299200.0 3950400.0 MSFT PEP SPX XOM 1990-02-01 89193600.0 2954400.0 154580000.0 2916400.0 1990-02-02 71395200.0 2424000.0 164400000.0 4250000.0 1990-02-05 59731200.0 2225400.0 130950000.0 5880800.0 ) 通过一组索引可能不同的Series构建一个DataFrame，可以显式定义结果的索引(丢弃其余的数据):1234567891011121314151617181920212223In [25]: s1 = Series(range(3), index=list('abc'))In [26]: s2 = Series(range(4), index=list('dbce'))In [27]: s3 = Series(range(3),index=list('fac'))In [28]: DataFrame(&#123;'one':s1,'two':s2,'three':s3&#125;)Out[28]: one three twoa 0.0 1.0 NaNb 1.0 NaN 1.0c 2.0 2.0 2.0d NaN NaN 0.0e NaN NaN 3.0f NaN 0.0 NaNIn [29]: DataFrame(&#123;'one':s1,'two':s2,'three':s3&#125;,index=list('fabc'))#显示定义结果索引Out[29]: one three twof NaN 0.0 NaNa 0.0 1.0 NaNb 1.0 NaN 1.0c 2.0 2.0 2.0 频率不同的时间序列的运算经济学时间序列常常有着按年、季、月、日计算的或其他更特殊的频率。频率转换和重对齐的两大主要工具是resample和reindex方法。resample用于将数据转换到固定频率，而reindex则用于使数据符合一个新索引，他们都支持插值逻辑：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768In [30]: # 创建一个周型时间序列In [31]: ts1 = Series(np.random.randn(3),index=pd.date_range('2018-3-29',periods=3,freq='W-WED'))In [32]: ts1Out[32]:2018-04-04 1.5121582018-04-11 -0.6087862018-04-18 -0.851491Freq: W-WED, dtype: float64In [33]: # 将其重采样到工作日(一到五)频率，没有数据的日子就会是空In [34]: ts1.resample('B').mean()Out[34]:2018-04-04 1.5121582018-04-05 NaN2018-04-06 NaN2018-04-09 NaN2018-04-10 NaN2018-04-11 -0.6087862018-04-12 NaN2018-04-13 NaN2018-04-16 NaN2018-04-17 NaN2018-04-18 -0.851491Freq: B, dtype: float64In [35]: #调用插值方法填充空白In [36]: ts1.resample('B').mean().ffill()Out[36]:2018-04-04 1.5121582018-04-05 1.5121582018-04-06 1.5121582018-04-09 1.5121582018-04-10 1.5121582018-04-11 -0.6087862018-04-12 -0.6087862018-04-13 -0.6087862018-04-16 -0.6087862018-04-17 -0.6087862018-04-18 -0.851491Freq: B, dtype: float64In [44]: # 创建不规则样本的时间序列In [45]: dates = pd.DatetimeIndex(['2018-3-29','2018-4-18','2018-4-11','2017-5-9'])In [46]: ts2 = Series(np.random.randn(len(dates)),index=dates)In [47]: #将ts1前向填充到ts2中保存提示索引In [48]: ts1.reindex(ts2.index, method='ffill')Out[48]:2018-03-29 NaN2018-04-18 -0.8514912018-04-11 -0.6087862017-05-09 NaNdtype: float64In [49]: ts1.reindex(ts2.index, method='ffill') +ts2Out[49]:2018-03-29 NaN2018-04-18 -0.7716312018-04-11 0.7381672017-05-09 NaNdtype: float64 使用periodPeriod(表示时间区间)提供了处理那些有着特殊规范的以年或季度为频率的金融或经济序列，跟Timestamp的时间序列不同，由Period索引的不同频率的时间序列之间的运算必须进行显示转化:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849In [53]: #创建GDP和通货膨胀的宏观经济时间序列In [54]: gdp = Series([1.78,1.94,2.08,2.01,2.15,2.31,2.46],index=pd.period_range('1984Q2',periods=7,freq='Q-SEP'))In [55]: infl = Series([0.025,0.045,0.037,0.04],index=pd.period_range('1982',periods=4, freq='A-DEC'))In [57]: gdpOut[57]:1984Q2 1.781984Q3 1.941984Q4 2.081985Q1 2.011985Q2 2.151985Q3 2.311985Q4 2.46Freq: Q-SEP, dtype: float64In [58]: inflOut[58]:1982 0.0251983 0.0451984 0.0371985 0.040Freq: A-DEC, dtype: float64In [59]: # 假设infl值是年末观测，可以将其转换到Q-SEPIn [60]: infl_q = infl.asfreq('Q-SEP', how='end')In [61]: infl_qOut[61]:1983Q1 0.0251984Q1 0.0451985Q1 0.0371986Q1 0.040Freq: Q-SEP, dtype: float64In [62]: #重索引使用前向填充In [63]: infl_q.reindex(gdp.index, method='ffill')Out[63]:1984Q2 0.0451984Q3 0.0451984Q4 0.0451985Q1 0.0371985Q2 0.0371985Q3 0.0371985Q4 0.037Freq: Q-SEP, dtype: float64 时间和”最当前”数据选取处理观测值没有精确地落在期望的时间点上:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091In [64]: #生成一个交易日内的日期范围In [65]: rng = pd.date_range('3/29/2018 9:30','3/29/2018 15:59', freq='T')In [66]: #生成5天的时间点(9:30~15:59之间的值)In [67]: rng = rng.append([rng + pd.offsets.BDay(i) for i in range(1,4)])In [68]: ts = Series(np.random.randn(len(rng)), index=rng)In [69]: tsOut[69]:2018-03-29 09:30:00 -1.7439022018-03-29 09:31:00 0.7394902018-03-29 09:32:00 -0.760746 ...2018-04-03 15:57:00 -0.9245202018-04-03 15:58:00 -1.4668702018-04-03 15:59:00 1.543675Length: 1560, dtype: float64In [70]: # 利用Python的datetime.time对象进行索引抽取时间点上的值In [71]: from datetime import timeIn [72]: ts[time(10,0)]Out[72]:2018-03-29 10:00:00 -0.0637432018-03-30 10:00:00 0.5738292018-04-02 10:00:00 1.3517082018-04-03 10:00:00 -1.140183dtype: float64In [73]: # 等同于at_timeIn [74]: ts.at_time(time(10,0))Out[74]:2018-03-29 10:00:00 -0.0637432018-03-30 10:00:00 0.5738292018-04-02 10:00:00 1.3517082018-04-03 10:00:00 -1.140183dtype: float64In [75]: # between_time用于选取两个Time对象之间的值In [76]: ts.between_time(time(10,0),time(10,1))Out[76]:2018-03-29 10:00:00 -0.0637432018-03-29 10:01:00 -0.5078132018-03-30 10:00:00 0.5738292018-03-30 10:01:00 -0.6229862018-04-02 10:00:00 1.3517082018-04-02 10:01:00 -2.0148712018-04-03 10:00:00 -1.1401832018-04-03 10:01:00 -1.077669dtype: float64In [77]: # 将时间序列的大部分内容随机设置NAIn [78]: indexer = np.sort(np.random.permutation(len(ts))[700:])In [80]: irr_ts = ts.copy()In [81]: irr_ts[indexer] = np.nanIn [82]: irr_ts['2018-3-29 9:50':'2018-3-29 10:00']Out[82]:2018-03-29 09:50:00 NaN2018-03-29 09:51:00 0.9955012018-03-29 09:52:00 NaN2018-03-29 09:53:00 0.9401572018-03-29 09:54:00 0.1892362018-03-29 09:55:00 1.5753822018-03-29 09:56:00 -0.0177632018-03-29 09:57:00 -1.4789832018-03-29 09:58:00 1.0996232018-03-29 09:59:00 NaN2018-03-29 10:00:00 -0.063743dtype: float64In [83]: # 通过asof方法得到时间点处(或与之前最近)的有效值(非NA)In [84]: selection = pd.date_range('2018-3-29 10:00',periods=4, freq='B')In [85]: irr_ts.asof(selection)Out[85]:2018-03-29 10:00:00 -0.0637432018-03-30 10:00:00 0.5738292018-04-02 10:00:00 1.3517082018-04-03 10:00:00 -1.190552Freq: B, dtype: float64 拼接多个数据源在金融或经济领域中，可能出现： 在一个特定的时间点上，从一个数据源切换到另一个数据源 用一个时间序列对当前时间序列中的缺失值打补丁 将数据中的符号(国家，资产代码等)替换为实际数据123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107In [86]: data1 = DataFrame(np.ones((6,3),dtype=float), ...: columns=list('abc'), ...: index = pd.date_range('3/28/2018', periods=6)) ...:In [87]: data2 = DataFrame(np.ones((6,3),dtype=float)*2, ...: columns=list('abc'), ...: index = pd.date_range('3/31/2018', periods=6)) ...: ...:In [88]: #使用concat将两个TimeSeries或DataFrame对象合并到一起实现在特定时刻从一个时间序列切换到另一个In [89]: spliced = pd.concat([data1.loc[:'2018-3-31'],data2.loc['2018-3-31':]])In [90]: splicedOut[90]: a b c2018-03-28 1.0 1.0 1.02018-03-29 1.0 1.0 1.02018-03-30 1.0 1.0 1.02018-03-31 1.0 1.0 1.02018-03-31 2.0 2.0 2.02018-04-01 2.0 2.0 2.02018-04-02 2.0 2.0 2.02018-04-03 2.0 2.0 2.02018-04-04 2.0 2.0 2.02018-04-05 2.0 2.0 2.0In [91]: #data1缺失了data2存在的某列时间序列In [92]: data2 = DataFrame(np.ones((6,4),dtype=float)*2, ...: columns=list('abcd'), ...: index = pd.date_range('3/31/2018', periods=6)) ...:In [93]: spliced = pd.concat([data1.loc[:'2018-3-31'],data2.loc['2018-3-31':]])In [94]: splicedOut[94]: a b c d2018-03-28 1.0 1.0 1.0 NaN2018-03-29 1.0 1.0 1.0 NaN2018-03-30 1.0 1.0 1.0 NaN2018-03-31 1.0 1.0 1.0 NaN2018-03-31 2.0 2.0 2.0 2.02018-04-01 2.0 2.0 2.0 2.02018-04-02 2.0 2.0 2.0 2.02018-04-03 2.0 2.0 2.0 2.02018-04-04 2.0 2.0 2.0 2.02018-04-05 2.0 2.0 2.0 2.0In [95]: #combine_first可以引入合并点之前的数据In [96]: spliced_filled = spliced.combine_first(data2)In [97]: spliced_filledOut[97]: a b c d2018-03-28 1.0 1.0 1.0 NaN2018-03-29 1.0 1.0 1.0 NaN2018-03-30 1.0 1.0 1.0 NaN2018-03-31 1.0 1.0 1.0 2.02018-03-31 2.0 2.0 2.0 2.02018-04-01 2.0 2.0 2.0 2.02018-04-02 2.0 2.0 2.0 2.02018-04-03 2.0 2.0 2.0 2.02018-04-04 2.0 2.0 2.0 2.02018-04-05 2.0 2.0 2.0 2.0In [98]: #update方法可以实现就地更新，只想填补空值，必须传入overwrite=FalseIn [99]: spliced.update(data2, overwrite=False)In [100]: splicedOut[100]: a b c d2018-03-28 1.0 1.0 1.0 NaN2018-03-29 1.0 1.0 1.0 NaN2018-03-30 1.0 1.0 1.0 NaN2018-03-31 1.0 1.0 1.0 2.02018-03-31 2.0 2.0 2.0 2.02018-04-01 2.0 2.0 2.0 2.02018-04-02 2.0 2.0 2.0 2.02018-04-03 2.0 2.0 2.0 2.02018-04-04 2.0 2.0 2.0 2.02018-04-05 2.0 2.0 2.0 2.0In [101]: #索引机制实现数据中的符号替换为实际数据In [102]: cp_spliced = spliced.copy()In [104]: cp_spliced[['a','c']] = data1[['a','c']]In [105]: cp_splicedOut[105]: a b c d2018-03-28 1.0 1.0 1.0 NaN2018-03-29 1.0 1.0 1.0 NaN2018-03-30 1.0 1.0 1.0 NaN2018-03-31 1.0 1.0 1.0 2.02018-03-31 1.0 2.0 1.0 2.02018-04-01 1.0 2.0 1.0 2.02018-04-02 1.0 2.0 1.0 2.02018-04-03 NaN 2.0 NaN 2.02018-04-04 NaN 2.0 NaN 2.02018-04-05 NaN 2.0 NaN 2.0 分组变换和分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110In [126]: # 随机生成1000个股票代码In [127]: import random;random.seed(0)In [128]: import stringIn [129]: def rands(n): ...: choices = string.ascii_uppercase ...: return ''.join([random.choice(choices) for _ in range(n)]) ...:In [130]: tickers = np.array([rands(5) for _ in range(N)])In [131]: # 创建一个含有3列的DataFrame来承载数据，只选取部分股票In [132]: M = 500In [133]: df = DataFrame(&#123;'Momentum':np.random.randn(M)/200 +0.03, ...: 'Value':np.random.randn(M)/200 +0.08, ...: 'ShortInterest':np.random.randn(M)/200 -0.02&#125;, ...: index=tickers[:M]) ...:In [134]: # 为股票随机创建一个行业分类In [135]: ind_names = np.array(['FINANCIAL','TECH'])In [136]: sampler = np.random.randint(0, len(ind_names), N)In [137]: industries = Series(ind_names[sampler], index=tickers, name='industry')In [138]: # 根据行业分类进行分组并执行分组聚合和变换In [139]: by_industry = df.groupby(industries)In [140]: by_industry.mean()Out[140]: Momentum ShortInterest ValueindustryFINANCIAL 0.029571 -0.019912 0.079195TECH 0.030190 -0.020191 0.080153In [141]: by_industry.describe()Out[141]: Momentum \ count mean std min 25% 50%industryFINANCIAL 250.0 0.029571 0.005004 0.017440 0.026313 0.029229TECH 250.0 0.030190 0.005024 0.016312 0.026794 0.030389 ShortInterest ... \ 75% max count mean ... 75%industry ...FINANCIAL 0.033130 0.043352 250.0 -0.019912 ... -0.016613TECH 0.033411 0.049917 250.0 -0.020191 ... -0.016571 Value \ max count mean std min 25% 50%industryFINANCIAL -0.008360 250.0 0.079195 0.004889 0.063352 0.075909 0.079352TECH -0.006412 250.0 0.080153 0.005009 0.063693 0.076513 0.079783 75% maxindustryFINANCIAL 0.082551 0.095753TECH 0.083312 0.093676[2 rows x 24 columns]In [142]: def zscore(group): ...: #行业标准化处理 ...: return (group-group.mean())/group.std() ...:In [143]: df_stand = by_industry.apply(zscore)In [144]: # 处理之后各行业平均值为0，标准差为1In [145]: df_stand.groupby(industries).agg(['mean','std'])Out[145]: Momentum ShortInterest Value mean std mean std mean stdindustryFINANCIAL -1.207923e-16 1.0 -6.024070e-16 1.0 -5.688783e-15 1.0TECH -2.044143e-15 1.0 -1.899814e-15 1.0 6.292744e-15 1.0In [146]: #使用内置变换函数In [147]: ind_rank = by_industry.rank(ascending=False)In [148]: ind_rank.groupby(industries).agg(['min','max'])Out[148]: Momentum ShortInterest Value min max min max min maxindustryFINANCIAL 1.0 250.0 1.0 250.0 1.0 250.0TECH 1.0 250.0 1.0 250.0 1.0 250.0In [151]: #通过rank和zscore函数实现排名和标准化In [152]: by_industry.apply(lambda x: zscore(x.rank())) #行内排名和标准化Out[152]: Momentum ShortInterest ValueMYNBI 1.334477 1.375963 -0.117545... ... ... ...GXKFD 0.905785 -0.145202 0.919614[500 rows x 3 columns] 分组因子暴露因子分析(factor analysis) 是投资组合定量管理中的一种技术。投资组合的持有量和性能(收益和损失)可以被分解为一个或多个表示投资组合权重的因子(风险因子是其中之一)：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455In [153]: #随机生成因子In [154]: fac1,fac2,fac3 = np.random.rand(3,1000)In [155]: ticker_subset = tickers.take(np.random.permutation(N)[:1000])In [156]: # 因子加权和以及噪声In [157]: port = Series(0.7*fac1-1.2*fac2+0.3*fac3+rand(1000),index=ticker_subset)In [158]: factors = DataFrame(&#123;'f1':fac1,'f2':fac2,'f3':fac3&#125;,index=ticker_subset)In [159]: #相关性In [160]: factors.corrwith(port)Out[160]:f1 0.418068f2 -0.686129f3 0.170196dtype: float64In [161]: #使用最小二乘回归计算整个投资组合的暴露In [193]: import statsmodels.api as smIn [194]: sm.OLS(port,factors).fit().summary()Out[194]:&lt;class 'statsmodels.iolib.summary.Summary'&gt;""" OLS Regression Results==============================================================================Dep. Variable: y R-squared: 0.747Model: OLS Adj. R-squared: 0.746Method: Least Squares F-statistic: 979.6Date: Thu, 29 Mar 2018 Prob (F-statistic): 1.15e-296Time: 14:39:55 Log-Likelihood: -299.34No. Observations: 1000 AIC: 604.7Df Residuals: 997 BIC: 619.4Df Model: 3Covariance Type: nonrobust============================================================================== coef std err t P&gt;|t| [0.025 0.975]------------------------------------------------------------------------------f1 1.0206 0.029 34.643 0.000 0.963 1.078f2 -0.9222 0.030 -31.056 0.000 -0.981 -0.864f3 0.6187 0.030 20.752 0.000 0.560 0.677==============================================================================Omnibus: 82.903 Durbin-Watson: 1.811Prob(Omnibus): 0.000 Jarque-Bera (JB): 27.499Skew: 0.062 Prob(JB): 1.07e-06Kurtosis: 2.197 Cond. No. 3.16==============================================================================Warnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.""" 注：书本中大量源数据已经无法获取，所以只记录了由自己创造的那一步，而且涉及到大量金融经济相关术语，所以这一节只是作为练习熟悉部分方法]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>应用示例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[时间序列(二)]]></title>
    <url>%2F2018%2F03%2F28%2F%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[时期及其算术运算时期(period) 表示的是时间时区，比如数日、数月、数季、数年等。Period类的构造函数需要用到一个字符串或整数，以及频率；对Period对象加上或减去一个整数可根据其频率进行位移；如果两个Period对象拥有相同的频率，则它们的差就是它们之间的单位数量；period_range函数可用于创建规则的时期范围；PeriodIndex类保存了一组Period，可以在任何pandas数据结构中被用作轴索引，其构造函数还允许直接使用一组字符串:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859In [3]: # 构造一个PeriodIn [4]: p = pd.Period(2018, freq='A-DEC')In [5]: pOut[5]: Period('2018', 'A-DEC')In [6]: # 构造一个PeriodIn [7]: p = pd.Period(2018, freq='A-DEC')In [8]: p #表示2018年1月1日到2018年12月31日之间的整段时间Out[8]: Period('2018', 'A-DEC')In [9]: # 通过加减整数进行位移In [10]: p+5Out[10]: Period('2023', 'A-DEC')In [11]: p-5Out[11]: Period('2013', 'A-DEC')In [12]: # 两个相同频率的Period对象的差值In [13]: pd.Period('2021', freq='A-DEC')-pOut[13]: 3In [14]: # 使用period_range创建时期范围In [20]: g = pd.period_range('2017-8-4','3/28/2018', freq='M')In [21]: gOut[21]:PeriodIndex(['2017-08', '2017-09', '2017-10', '2017-11', '2017-12', '2018-01', '2018-02', '2018-03'], dtype='period[M]', freq='M')In [23]: #将PeriodIndex用作轴索引In [24]: Series(np.random.randn(len(g)),index=g)Out[24]:2017-08 -0.3732612017-09 -0.4771222017-10 -0.7917222017-11 -0.2701602017-12 -0.4874882018-01 -0.7944822018-02 -0.8599402018-03 -2.178295Freq: M, dtype: float64In [28]: #直接使用一组字符串构造PeriodIndexIn [29]: values = ['2018Q1','2019Q4','2020Q3']In [30]: index = pd.PeriodIndex(values, freq='Q-DEC')In [31]: indexOut[31]: PeriodIndex(['2018Q1', '2019Q4', '2020Q3'], dtype='period[Q-DEC]', freq='Q-DEC') 时期的频率转换Period和PeriodIndex对象可以通过asfreq方法转换成别的频率，在将高频率装换为低频率时，超时期(superperiod)是由子时期(subperiod)所属的位置决定的(在A-JUN频率中，月份“2018年8月”实际上是属于周期“2019年”):1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556In [35]: p = pd.Period('2018', freq='A-DEC') # 构造一个年度时期In [36]: # 转换为年初或年末的一个月度时期In [37]: p.asfreq('M', how='start') #月初Out[37]: Period('2018-01', 'M')In [38]: p.asfreq('M', how='end') #月末Out[38]: Period('2018-12', 'M')In [43]: #不以12月结束的财政年度，月度时期的归属情况不一样In [44]: p = pd.Period('2018',freq='A-JUN')In [45]: p.asfreq('M',how='start')Out[45]: Period('2017-07', 'M')In [46]: p.asfreq('D',how='end')Out[46]: Period('2018-06-30', 'D')In [47]: # 高频率转换成低频率In [48]: p = pd.Period('2017-8','M')In [49]: p.asfreq('A-JUN')Out[49]: Period('2018', 'A-JUN')In [50]: #PeriodIndex或TimeSeries的频率转换方式In [51]: rng = pd.period_range('2017','2020', freq='A-DEC')In [52]: ts = Series(np.random.randn(len(rng)), index=rng)In [53]: tsOut[53]:2017 0.2175442018 1.2158392019 0.9856242020 1.705840Freq: A-DEC, dtype: float64In [54]: ts.asfreq('M', how='start')Out[54]:2017-01 0.2175442018-01 1.2158392019-01 0.9856242020-01 1.705840Freq: M, dtype: float64In [55]: ts.asfreq('B', how='end')Out[55]:2017-12-29 0.2175442018-12-31 1.2158392019-12-31 0.9856242020-12-31 1.705840Freq: B, dtype: float64 Period频率转换示例 按季度计算的时期频率季度型数据在会计、金融等领域中很常见。许多季度型数据都会涉及”财年末”的概念，通常是一年12个月中某月的最后一个日历日或工作日。时期”2018Q4”根据财年末的不同会有不同的含义，pandas支持12种可能的季度型频率即Q-JAn到Q-DEC。例如在以1月结束的财年中，2018Q4是从11月到1月；period_range还可用于生成季度型范围12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758In [56]: #创建以1月结束的财年In [57]: p = pd.Period('2018Q4', freq='Q-JAN')In [58]: pOut[58]: Period('2018Q4', 'Q-JAN')In [59]: p.asfreq('D','start')Out[59]: Period('2017-11-01', 'D')In [60]: p.asfreq('D','e')Out[60]: Period('2018-01-31', 'D')In [67]: #该季度倒数第二个工作日下午4点的时间戳In [68]: p4pm = (p.asfreq('B', 'e')-1).asfreq('T','s')+16*60In [69]: p4pmOut[69]: Period('2018-01-30 16:00', 'T')In [70]: p4pm.to_timestamp()Out[70]: Timestamp('2018-01-30 16:00:00')In [71]: # period_range还可用于生成季度型范围InIn [72]: rng = pd.period_range('2017Q1','2018Q4', freq='Q-JAN')In [73]: ts = Series(np.random.randn(len(rng)), index=rng)In [74]: tsOut[74]:2017Q1 0.7636942017Q2 -0.1920302017Q3 -0.5148382017Q4 0.0715012018Q1 -0.7655862018Q2 0.6953122018Q3 0.7471422018Q4 -0.359449Freq: Q-JAN, dtype: float64In [75]: # 季度型范围的算术运算In [76]: new_rng = (rng.asfreq('B','e')-1).asfreq('T','s')+16*60In [77]: ts.index = new_rng.to_timestamp()In [78]: tsOut[78]:2016-04-28 16:00:00 0.7636942016-07-28 16:00:00 -0.1920302016-10-28 16:00:00 -0.5148382017-01-30 16:00:00 0.0715012017-04-27 16:00:00 -0.7655862017-07-28 16:00:00 0.6953122017-10-30 16:00:00 0.7471422018-01-30 16:00:00 -0.359449dtype: float64 将Timestamp转换为Period(及其反向过程)通过使用to_period方法，可以将由时间戳索引的Series或DataFrame对象转换成以时期索引，由于时期指的是 非重叠时间区间 ，因此对于给定的频率，一个时间戳只能属于一个时期。新PeriodIndex的频率默认是从时间戳推断出来的，也可以指定任何其他频率，结果中允许存在重复时期。要转换为时间戳可以使用to_timestamp方法:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556In [84]: # 将时间戳转换为时期In [85]: ts = Series(np.random.randn(len(rng)), index=rng)In [86]: # 将时间戳转换为时期In [87]: rng = pd.date_range('3/29/2018 8:51', periods=3, freq='M')In [88]: ts = Series(np.random.randn(len(rng)), index=rng)In [89]: pts = ts.to_period()In [90]: tsOut[90]:2018-03-31 08:51:00 0.4731232018-04-30 08:51:00 -0.2787692018-05-31 08:51:00 0.903042Freq: M, dtype: float64In [91]: ptsOut[91]:2018-03 0.4731232018-04 -0.2787692018-05 0.903042Freq: M, dtype: float64In [92]: #指定频率转换，允许存在重复时期In [93]: rng = pd.date_range('3/29/2018 8:51', periods=5, freq='D')In [94]: ts2 = Series(np.random.randn(len(rng)), index=rng)In [95]: ts2.to_period('M')Out[95]:2018-03 -0.5797082018-03 0.7937712018-03 0.3279132018-04 0.2481452018-04 1.324320Freq: M, dtype: float64In [96]: # 使用to_timestamp转换为时间戳In [97]: ptsOut[97]:2018-03 0.4731232018-04 -0.2787692018-05 0.903042Freq: M, dtype: float64In [98]: pts.to_timestamp(how='end')Out[98]:2018-03-31 0.4731232018-04-30 -0.2787692018-05-31 0.903042Freq: M, dtype: float64 通过数组创建PeriodIndex固定频率的数据集通常会将时间信息分开存放在多个列中，下面的宏观经济数据集中，年度和季度就分别放在不同的列中:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748In [112]: data = pd.read_csv('macrodata.csv')In [113]: data.yearOut[113]:0 1959.01 1959.02 1959.0 ...202 2009.0Name: year, Length: 203, dtype: float64In [114]: data.quarterOut[114]:0 1.01 2.02 3.03 4.04 1.05 2.0 ...202 3.0Name: quarter, Length: 203, dtype: float64In [115]: #将两个数组以及一个频率传入PeriodIndex，将它们合并成DataFrame的一个索引In [116]: index = pd.PeriodIndex(year=data.year,quarter=data.quarter,freq='Q-DEC')In [117]: indexOut[117]:PeriodIndex(['1959Q1', '1959Q2', '1959Q3', '1959Q4', '1960Q1', '1960Q2', '1960Q3', '1960Q4', '1961Q1', '1961Q2', ... '2007Q2', '2007Q3', '2007Q4', '2008Q1', '2008Q2', '2008Q3', '2008Q4', '2009Q1', '2009Q2', '2009Q3'], dtype='period[Q-DEC]', length=203, freq='Q-DEC')In [118]: data.index=indexIn [119]: data.inflOut[119]:1959Q1 0.001959Q2 2.341959Q3 2.74 ...2009Q1 0.942009Q2 3.372009Q3 3.56Freq: Q-DEC, Name: infl, Length: 203, dtype: float64 重采样及频率转换重采样(resampling) 指的是将时间序列从一个频率转换到另一个频率的处理过程。将高频率数据聚合到低频率称为 降采样(downsampling)；将低频率数据转换到高频率则称为 升采样(upsampling)。(将W-WED(每周三)转换为W-FRI既不是降采样也不是升采样)。pandas对象有一个resample方法能处理各种频率转换工作。resample是一个灵活高效的方法，可以处理非常大的时间序列: resample方法的参数 参数 说明 rule 表示重采样频率的字符串或DateOffset，例如’M’、’5min’或Sencond(15) axis 重采样的轴，默认为axis=0 closed=’right’ 在降采样中，各时间段的哪一端是闭合(即包含)的，’right’或’left’，默认为’right’ label=’right’ 在降采样中，如何设置聚合值的标签，’right’或’left’(面元的右边界或左边界)。例如，在9:30到9:35之间的这5分钟会被标记为9:30或9:35.默认为’right’ loffset 面元标签的时间校正值，比如’-1s/Sencond(-1)’用于将聚合标签调早1秒 convention 当重采样时期时，将低频率转换到高频率所采用的约定(‘start’或’end’)。默认为’end’ kind 聚合到时期(‘period’)或时间戳(‘timestamp’)，默认聚合到时间序列的索引类型 123456789101112131415161718192021In [133]: rng = pd.date_range('3/29/2018 8:51', periods=100, freq='D')In [134]: ts = Series(np.random.randn(len(rng)), index=rng)In [135]: ts.resample('M').mean() #聚合计算每月的平均值Out[135]:2018-03-31 0.1744322018-04-30 -0.1225222018-05-31 -0.1217702018-06-30 -0.2532772018-07-31 -0.211743Freq: M, dtype: float64In [136]: ts.resample('M',kind='period').mean() #聚合到时期计算每月的平均值Out[136]:2018-03 0.1744322018-04 -0.1225222018-05 -0.1217702018-06 -0.2532772018-07 -0.211743Freq: M, dtype: float64 降采样待聚合的数据不必拥有固定的频率，期望的频率会 自动定义 聚合的面元边界，这些面元用于将时间序列拆分为多个片段。例如，要转换到月度频率(‘M’或’BM’)，数据需要被划分到多个单月时间段中。 各时间段都是半开放的，一个数据点只能属于一个时间段，所有时间段的并集必须组成整个时间帧。在用resample对数据降采样是需要考虑： 各区间哪边是闭合的 如何标记各个聚合面元，用区间的开头还是末尾对于将”1分钟”数据通过求和的方式聚合到”5分钟”块中ts.resample(&#39;5min&#39;)，传入的频率将会以”5分钟”的增量定义面元边界。默认情况下，面元的右边界是包含的，则 00:00~00:05 区间中是包含 00:05 的。传入closed=left会让区间以左边界闭合，最终的时间序列以各面元左边界的时间戳进行标记，传入label=&#39;left&#39;使用面元的左边界进行标记；可以通过loffset设置一个字符串或日期偏移量来实现从右边界减去1秒以便更容易明白该时间戳表示哪个区间，也可以使用shifit方法实现： closed和label约定的5分钟 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162In [152]: # 定义一分钟数据In [153]: rng = pd.date_range('3/29/2018', periods=12, freq='T')In [154]: ts = Series(np.random.randn(len(rng)), index=rng)In [155]: tsOut[155]:2018-03-29 00:00:00 0.7730652018-03-29 00:01:00 -0.6309442018-03-29 00:02:00 -0.7188322018-03-29 00:03:00 -0.0760572018-03-29 00:04:00 -1.1902932018-03-29 00:05:00 -0.3201302018-03-29 00:06:00 0.7886082018-03-29 00:07:00 0.2400312018-03-29 00:08:00 0.4048832018-03-29 00:09:00 0.5620272018-03-29 00:10:00 0.5491542018-03-29 00:11:00 -0.233119Freq: T, dtype: float64In [156]: #将数据通过求和的方式聚合到5minIn [157]: ts.resample('5min').sum()Out[157]:2018-03-29 00:00:00 -1.8430602018-03-29 00:05:00 1.6754182018-03-29 00:10:00 0.316035Freq: 5T, dtype: float64In [158]: ts.resample('5min',closed='right').sum()Out[158]:2018-03-28 23:55:00 0.7730652018-03-29 00:00:00 -2.9362552018-03-29 00:05:00 2.5447032018-03-29 00:10:00 -0.233119Freq: 5T, dtype: float64In [162]: ts.resample('5min',closed='left',label='right').sum() #左边闭合 以右边的时间戳为标记Out[162]:2018-03-29 00:05:00 -1.8430602018-03-29 00:10:00 1.6754182018-03-29 00:15:00 0.316035Freq: 5T, dtype: float64In [163]: ts.resample('5min',closed='right',loffset='1s').sum() #以左边为标记，右边闭合，向右移1sOut[163]:2018-03-28 23:55:01 0.7730652018-03-29 00:00:01 -2.9362552018-03-29 00:05:01 2.5447032018-03-29 00:10:01 -0.233119Freq: 5T, dtype: float64In [170]: ts.resample('5min',closed='right').sum().shift(freq='1s') #使用shift设置偏移量Out[170]:2018-03-28 23:55:01 0.7730652018-03-29 00:00:01 -2.9362552018-03-29 00:05:01 2.5447032018-03-29 00:10:01 -0.233119Freq: 5T, dtype: float64 OHLC重采样金融领域有一种无所不在的时间序列聚合方式，即计算各面元的四个之:第一个值(open，开盘)、最后一个值(close，收盘)、最大值(high，最高)以及最小值(low，最低)，调用ohlc可以得到一个含有这四种聚合指的DataFrame:123456In [171]: ts.resample('5min').ohlc()Out[171]: open high low close2018-03-29 00:00:00 0.773065 0.773065 -1.190293 -1.1902932018-03-29 00:05:00 -0.320130 0.788608 -0.320130 0.5620272018-03-29 00:10:00 0.549154 0.549154 -0.233119 -0.233119 通过groupby进行重采样另一种降采样的方法是使用pandas的 groupby功能，如果打算根据月份或星期几进行分组，只需要传入一个能够访问时间序列的索引上的字段的函数:1234567891011121314151617181920212223In [172]: rng = pd.date_range('3/29/2018', periods=100, freq='D')In [173]: ts = Series(np.random.randn(len(rng)), index=rng)In [174]: ts.groupby(lambda x:x.month).mean()Out[174]:3 -0.7543464 0.3440395 0.1032596 0.0812907 -0.093527dtype: float64In [175]: ts.groupby(lambda x:x.weekday).mean()Out[175]:0 0.5219971 -0.1067402 -0.0163373 0.0804504 -0.0441415 0.1638686 0.336645dtype: float64 升采样和插值在将数据从低频率转换到高频率时不再需要聚合，默认会引入缺失值，可以当想要填充时，可以调用reindex一样的方法，也可以只填充指定的时期数；且新的日期索引和旧的 不必相交123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354In [182]: frame = DataFrame(np.random.randn(2,4), ...: index=pd.date_range('3/28/2018',periods=2,freq='W-WED'), ...: columns=['col1','col2','col3','col4']) ...:In [183]: frameOut[183]: col1 col2 col3 col42018-03-28 1.724639 -0.712352 -1.918885 -0.6711782018-04-04 -1.308759 -0.056425 -0.746326 0.153999In [184]: df_daily = frame.resample('D') #将其重采样到日频率，会引入缺失值In [185]: df_daily.mean()Out[185]: col1 col2 col3 col42018-03-28 1.724639 -0.712352 -1.918885 -0.6711782018-03-29 NaN NaN NaN NaN2018-03-30 NaN NaN NaN NaN2018-03-31 NaN NaN NaN NaN2018-04-01 NaN NaN NaN NaN2018-04-02 NaN NaN NaN NaN2018-04-03 NaN NaN NaN NaN2018-04-04 -1.308759 -0.056425 -0.746326 0.153999In [187]: frame.resample('D').mean().ffill() #使用ffill方法向前填充Out[187]: col1 col2 col3 col42018-03-28 1.724639 -0.712352 -1.918885 -0.6711782018-03-29 1.724639 -0.712352 -1.918885 -0.6711782018-03-30 1.724639 -0.712352 -1.918885 -0.6711782018-03-31 1.724639 -0.712352 -1.918885 -0.6711782018-04-01 1.724639 -0.712352 -1.918885 -0.6711782018-04-02 1.724639 -0.712352 -1.918885 -0.6711782018-04-03 1.724639 -0.712352 -1.918885 -0.6711782018-04-04 -1.308759 -0.056425 -0.746326 0.153999In [188]: frame.resample('D').mean().ffill(limit=2) #使用ffill方法向前填充并制定时期数Out[188]: col1 col2 col3 col42018-03-28 1.724639 -0.712352 -1.918885 -0.6711782018-03-29 1.724639 -0.712352 -1.918885 -0.6711782018-03-30 1.724639 -0.712352 -1.918885 -0.6711782018-03-31 NaN NaN NaN NaN2018-04-01 NaN NaN NaN NaN2018-04-02 NaN NaN NaN NaN2018-04-03 NaN NaN NaN NaN2018-04-04 -1.308759 -0.056425 -0.746326 0.153999In [190]: frame.resample('W-THU').mean() #频率转换为周四Out[190]: col1 col2 col3 col42018-03-29 1.724639 -0.712352 -1.918885 -0.6711782018-04-05 -1.308759 -0.056425 -0.746326 0.153999 通过时期进行重采样对使用时期的索引进行重采样是很简单的事，而升采样必须决定在新频率中各区间的哪端用于放置原来的值，convention参数默认为&#39;end&#39;，可设置为&#39;start&#39;；由于时期指的是时间区间，所以升采样和降采样的规则比较严格: 降采样中，目标频率必须是源频率的子时期(subperiod) 升采样中，目标频率必须是源频率的超时期(superperiod) 如果不满足这些条件就会引发异常，主要影响按季、年、周计算的频率，例如由Q-MAR定义的时间区间只能升采样为A-MAR、A-JUN、A-SEP、A-DEC 123456789101112131415161718192021222324252627282930In [191]: frame = DataFrame(np.random.randn(12,4), ...: index=pd.date_range('3/28/2018', '3-2019',freq='M'), ...: columns=['col1','col2','col3','col4']) ...:In [192]: frame[:5]Out[192]: col1 col2 col3 col42018-03-31 0.213243 0.555932 -1.283426 -0.5275522018-04-30 0.600141 0.312877 0.230994 2.0165972018-05-31 -0.535206 0.305608 1.046269 -1.3638082018-06-30 1.303406 0.486195 -0.230848 1.6848322018-07-31 -0.030762 -0.923983 1.971748 -0.161417In [193]: annual_frame = frame.resample('A-DEC').mean() #按A-DEC进行降采样In [194]: annual_frameOut[194]: col1 col2 col3 col42018-12-31 0.432316 0.155345 -0.072937 0.3223262019-12-31 -0.146624 0.161040 -1.046127 0.170322In [195]: annual_frame.resample('Q-DEC').ffill()#进行升采样并使用前向填充Out[195]: col1 col2 col3 col42018-12-31 0.432316 0.155345 -0.072937 0.3223262019-03-31 0.432316 0.155345 -0.072937 0.3223262019-06-30 0.432316 0.155345 -0.072937 0.3223262019-09-30 0.432316 0.155345 -0.072937 0.3223262019-12-31 -0.146624 0.161040 -1.046127 0.170322 时间序列绘图对数据的任意一列调用plot即可生成一张简单的图表: APPLE的每日价格12345678910111213141516171819In [197]: close_px_all = pd.read_csv('stock_px.csv', parse_dates=True, index_col=0)In [198]: close_px = close_px_all[['AAPL','MSFT','XOM']]In [199]: close_px = close_px.resample('B').mean().ffill()In [200]: close_pxOut[200]: AAPL MSFT XOM1990-02-01 7.86 0.51 6.121990-02-02 8.00 0.51 6.241990-02-05 8.18 0.51 6.25... ... ... ...2011-10-14 422.00 27.27 78.11[5662 rows x 3 columns]In [201]: close_px['AAPL'].plot() #选取一列绘图Out[201]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x24ce3592860&gt; 当对DataFrame调用plot时，所有时间序列都会被绘制在一个subplot上，并分别生成相应的图例12345In [202]: close_px.loc['2009'].plot() #绘制2009年的数据Out[202]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x24ce55560b8&gt;In [204]: close_px['AAPL'].loc['01-2011':'03-2011'].plot() #苹果公司在2011年1月到3月间的每日股价Out[204]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x24cf66a9710&gt; 2009年的股票价格 苹果公司在2011年1月到3月间的每日股价 季度型频率的数据会用季度标记进行格式化:1234In [211]: appl_q = close_px['AAPL'].resample('Q-DEC').mean().ffill()In [212]: appl_q['2009':].plot() #苹果公司在2009年到2011年间的每季度股价Out[212]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x24cf6593400&gt; 苹果公司在2009年到2011年间的每季度股价 移动窗口函数移动窗口函数 是指在移动窗口(可以带有指数衰减权数)上计算的各种统计函数(时间序列的数组变换)，其中还包括哪些窗口不定长的函数(如指数加权移动平均)。跟其他统计函数一样，移动窗口函数也会自动排除缺失值。使用Series.rolling或DataFrame.rolling后面加上基本数组统计方法变成类似于Series.rolling(window=250,center=False).mean() 苹果公司股价的250日均线12345In [221]: close_px.AAPL.plot()Out[221]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x24cf6ae7ac8&gt;In [222]: close_px.AAPL.rolling(250).mean().plot() #苹果公司股价的250日均线Out[222]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x24cf6ae7ac8&gt; 苹果公司250日每日回报标准差123456789101112131415In [223]: app_std250 = close_px.AAPL.rolling(250,min_periods=10).std()In [224]: app_std250[5:12]Out[224]:1990-02-08 NaN1990-02-09 NaN1990-02-12 NaN1990-02-13 NaN1990-02-14 0.1481891990-02-15 0.1410031990-02-16 0.135454Freq: B, Name: AAPL, dtype: float64In [225]: app_std250.plot() #苹果公司250日每日回报标准差Out[225]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x24cf6f95be0&gt; 要计算扩展窗口平均(expanding window mean)，可以将扩展窗口看成一个特殊的窗口，其长度和时间序列一样，但只需一期或多期即可计算一个值，然后对DataFrame调用rolling将其应用到所有列上: 各股价60日均线(对数Y轴)123456In [232]: # 定义扩展平均In [233]: expanding_mean = lambda x: x.rolling(len(x),min_periods=1)In [234]: close_px.rolling(60).mean().plot(logy=True) #各股价60日均线(对数Y轴)Out[234]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x24cf708a748&gt; 指数加权函数另一个使用固定大小窗口及相等权数观测值的办法是定义一个衰减因子(decay factor)常量，以便使近期的观测值拥有更大的权数。如果ma_t是时间t的移动平均结果，x是时间序列，结果中的各个值可用ma_t=a*ma_t-1 +(a-1)*x_-t进行计算，其中a为衰减因子。 衰减因子的定义方式有很多，比较流行的是使用时间间隔( span)，它可以使结果兼容于窗口大小等于时间间隔的简单移动窗口函数: 简单移动平均与指数加权移动平均1234567891011121314151617181920212223242526In [238]: fig, axes = plt.subplots(nrows=2,ncols=1,sharex=True,sharey=True,figsize=(12,7))In [239]: appl_px = close_px.AAPL['2005':'2009']In [240]: m60 = appl_px.rolling(60, min_periods=50).mean()In [242]: ewma60 = appl_px.ewm(span=60).mean() #指数加权移动平均In [243]: appl_px.plot(style='k-', ax=axes[0])Out[243]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x24cf793d048&gt;In [244]: m60.plot(style='k--', ax=axes[0])Out[244]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x24cf793d048&gt;In [245]: appl_px.plot(style='k-', ax=axes[1])Out[245]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x24cf7951320&gt;In [246]: ewma60.plot(style='k--',ax=axes[1])Out[246]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x24cf7951320&gt;In [247]: axes[0].set_title('Simple MA')Out[247]: &lt;matplotlib.text.Text at 0x24cf83de390&gt;In [248]: axes[1].set_title('Exponenttially-weighted MA')Out[248]: &lt;matplotlib.text.Text at 0x24cf78d28d0&gt; 二元移动窗口函数有些统计运算(如相关系数和协方差)需要在两个时间序列上执行 APPL6个月的回报与标准普尔500指数的相关系数12345678910In [249]: spx_px = close_px_all['SPX']In [250]: spx_rets = spx_px/spx_px.shift(1) -1 #计算相关系数In [251]: returns = close_px.pct_change()In [254]: corr = returns.AAPL.rolling(125,min_periods=100).corr(spx_rets) #计算百分数变化In [256]: corr.plot()Out[256]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x24cfc175dd8&gt; 想要一次性计算多只股票与标准普尔500指数的相关系数只需要传入一个TimeSeries和一个DataFrame 3只股票6个月的回报与标准普尔500指数的相关系数1234In [257]: corr = returns.rolling(125,min_periods=100).corr(spx_rets)In [258]: corr.plot()Out[258]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x24cfb760588&gt; 用户定义的移动窗口函数apply能够在移动窗口上应用自己设计的数组函数。唯一要求是 该函数要能从数组的各个片段中产生单个值(即约简) AAPL%2回报率的百分等级(一年窗口期)12345678In [11]: from scipy.stats import percentileofscoreIn [12]: score_at_2percent = lambda x:percentileofscore(x,0.02)In [13]: result = returns.AAPL.rolling(250).apply(score_at_2percent)In [14]: result.plot()Out[14]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1baac50c908&gt;]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>时间序列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[时间序列(一)]]></title>
    <url>%2F2018%2F03%2F27%2F%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[在多个时间点观察或测量到的任何事物都可以形成一段时间序列。很多时间序列是固定频率的，数据点是根据某种规律定期出现的(比如没15秒、每5分钟、每月出现一次)。时间序列也可以是不定期的。时间序列数据的意义取决于具体的应用场景，主要有几种: 时间戳(timestamp)，特定的时刻 固定时期(period)，如2018年3月或2018年全年 时间间隔(interval),由起始和结束时间戳表示。时期(period)可以被看做间隔(interval)的特例。 实验或过程时间，每个时间点都是相对于特定起始时间的一个度量。例如，从放入烤箱时起，每秒钟饼干的直径 日期和时间数据类型及工具Python标准库包含用于日期(date)和时间(time)数据的数据类型，而且有日历方面的功能。主要用到的事datetime、time、canlendar模块；datetime以毫秒形式存储日期和时间；datetime.timedelta表示两个datetime对象之间的时间差，可以给datetime对象加上(或减去)一个或多个timedelta产生一个新对象 datetime模块中的数据类型 类型 说明 date 以公历形式存储日历日期(年、月、日) time 将时间存储为时、分、秒、毫秒 datetime 存储日期和时间 timedelta 表示两个datetime值之间的差(日、秒、毫秒) 12345678910111213141516171819202122232425262728293031323334In [1]: from datetime import datetimeIn [2]: now = datetime.now()In [3]: nowOut[3]: datetime.datetime(2018, 3, 27, 9, 18, 40, 701738)In [4]: now.year, now.month, now.dayOut[4]: (2018, 3, 27)In [5]: # 使用timedeltaIn [6]: delta = datetime.now() - datetime(2017,8,4)In [7]: deltaOut[7]: datetime.timedelta(235, 33622, 69715)In [8]: delta.daysOut[8]: 235In [9]: delta.secondsOut[9]: 33622In [10]: # 加减timedeltaIn [11]: from datetime import timedeltaIn [12]: start = datetime.now()In [13]: start+timedelta(12)Out[13]: datetime.datetime(2018, 4, 8, 9, 21, 53, 280284)In [14]: start - 2*timedelta(2)Out[14]: datetime.datetime(2018, 3, 23, 9, 21, 53, 280284) 字符串和datetime的相互转换利用str或strftime方法(传入一个格式化字符串),datetime对象和pandas的Timestamp对象可以被格式化为字符串datetime.strptime可以使用相应的格式化编码将字符串转换为日期1 datetime格式定义 代码 说明 %Y 4位数的年 %y 2位数的年 %m 2位数的月[01,12] %d 2位数的日[01,31] %H 时(24小时制)[00,23] %I 是(12小时制)[01,12] %M 2位数的分[00,59] %S 秒[00,61]（秒60和61用于闰秒） %w 用整数表示的星期几[0(星期天),6] %U 每年的第几周[00,53]。星期天被认为是每周的第一天，每年第一个星期天之前的那几天被认为是”第0周” %W 每年的第几周[00,53]。星期一被认为是每周的第一天，每年第一个星期一之前的那几天被认为是”第0周” %z 以+HHMM或-HHMM表示的UTC，如果时区为naive，则返回空字符串 %F %Y-%m-%d简写形式，例如2018-03-27 %D %m%d%y简写形式，例如03/27/18 特定于当前环境的日期格式 代码 说明 %a 星期几的简写 %A 星期几的全称 %b 月份的简写 %B 月份的全称 %c 完整的日期和时间。例如”Tue 01 May 2012 04:20:57 PM” %p 不同环境中的AM或PM %x 适用于当前环境的日期格式，在美国“May 1, 2012”会产生”05/01/2012” %X 适用于当前环境的时间格式，例如”04:24:12 PM” datetime.strptime是通过已知格式进行日期解析的最佳方式，而对于一些常见的日期格式，可以使用dateutil这个三方包中的parser.parse方法来避免编写格式定义；dateutil可以解析除中文外几乎所有人类能够理解的日期表现形式212345678910111213141516171819202122232425262728293031323334In [18]: #将日期转换为字符串In [19]: stamp = datetime.now()In [20]: str(stamp)Out[20]: '2018-03-27 14:19:08.645810'In [21]: stamp.strftime('%Y-%m-%d')Out[21]: '2018-03-27'In [22]: #将字符串转换为日期In [23]: value = '2018-03-27'In [24]: datetime.strptime(value,'%Y-%m-%d')Out[24]: datetime.datetime(2018, 3, 27, 0, 0)In [25]: datestrs = ['8/4/2017','3/27/2018']In [27]: [datetime.strptime(x, '%m/%d/%Y') for x in datestrs]Out[27]: [datetime.datetime(2017, 8, 4, 0, 0), datetime.datetime(2018, 3, 27, 0, 0)]In [30]: from dateutil.parser import parseIn [31]: parse('2018/03/27')Out[31]: datetime.datetime(2018, 3, 27, 0, 0)In [32]: parse('Jan 31, 1997 10:45 PM')Out[32]: datetime.datetime(1997, 1, 31, 22, 45)In [33]: # 日出现在月的前面，传入dayfirst = TrueIn [34]: parse('4/8/2017', dayfirst=True)Out[34]: datetime.datetime(2017, 8, 4, 0, 0) pandas通常是用于处理成组日期的，不管这些日期时DataFrame的轴索引还是列。to_datetime方法可以解析多种不同的日期表示形式，并且它还可以处理缺失值(None、空字符串等)，NaT(Not a Time) 是pandas中时间戳数据的NA值12345678910111213141516In [45]: datestrsOut[45]: ['8/4/2017', '3/27/2018']In [46]: pd.to_datetime(datestrs)Out[46]: DatetimeIndex(['2017-08-04', '2018-03-27'], dtype='datetime64[ns]', freq=None)In [47]: idx = pd.to_datetime(datestrs + [None])In [48]: idxOut[48]: DatetimeIndex(['2017-08-04', '2018-03-27', 'NaT'], dtype='datetime64[ns]', freq=None)In [49]: idx[2]Out[49]: NaTIn [50]: pd.isnull(idx)Out[50]: array([False, False, True], dtype=bool) 时间序列基础pandas最基本的时间序列类型是以时间戳(以字符串或datetime对象表示)为索引的Series，datetime对象实际上是被放在DatetimeIndex中的，不同索引的时间序列之间的算数运算会自动按日期对齐；pandas使用NumPy的datetime64数据类型以纳秒形式存储时间戳，而DatetimeIndex中各个标量值是pandas的Timestamp对象，在需要的时候Timestamp可以随时自动转换为datetime对象，并且它还可以存储频率信息，且知道如何执行时区转换以及其他操作：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950In [58]: # 创建时间序列In [59]: dates = [datetime(2018,3,d) for d in range(21,27)]In [60]: ts = Series(np.random.randn(6), index=dates)In [61]: tsOut[61]:2018-03-21 0.5550442018-03-22 -0.4045482018-03-23 -1.3347042018-03-24 -0.5506042018-03-25 0.0372302018-03-26 0.298817dtype: float64In [62]: # ts的类型In [63]: type(ts)Out[63]: pandas.core.series.SeriesIn [64]: ts.indexOut[64]:DatetimeIndex(['2018-03-21', '2018-03-22', '2018-03-23', '2018-03-24', '2018-03-25', '2018-03-26'], dtype='datetime64[ns]', freq=None)In [65]: # 不同索引的时间序列之间的算数运算会自动按日期对齐In [66]: ts+ts[::2]Out[66]:2018-03-21 1.1100882018-03-22 NaN2018-03-23 -2.6694092018-03-24 NaN2018-03-25 0.0744612018-03-26 NaNdtype: float64In [67]: # 时间戳类型In [68]: ts.index.dtypeOut[68]: dtype('&lt;M8[ns]')In [69]: #DateIndex中的各个标量值是pandas的Timestamp对象In [70]: stamp =ts.index[0]In [71]: stampOut[71]: Timestamp('2018-03-21 00:00:00') 索引、选取、子集构造索引和数据选取和Series的行为相同，也可以传入一个被解释为日期的字符串；对于较长的时间序列，可以传入 “年” 或 “年月” 选取数据的切片，而通过日期切片的方式只对规则Series有效；对大部分时间序列的数据来说都是按照时间先后排序的，因此可以用不存在于该时间序列中的时间戳进行切片(即范围查找)，而这里同样可以传入字符串日期、datetime或Timestamp，和NumPy一样都是产生原时间序列的视图。可以调用truncate方法并传入before和after来实现相同的效果。而这些对DataFrame同样有效:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143In [73]: # 选取索引和数据In [74]: stamp = ts.index[2]In [75]: ts[stamp]Out[75]: -1.3347042513067129In [81]: # 通过"年"或"年月"切片In [82]: long_ts = Series(np.random.randn(1000),index=pd.date_range('1/1/2018', periods=1000))In [83]: long_tsOut[83]:2018-01-01 0.0472812018-01-02 -0.6303862018-01-03 0.630630 ...2020-09-26 0.993575Freq: D, Length: 1000, dtype: float64In [84]: long_ts['2019']Out[84]:2019-01-01 0.8903282019-01-02 -1.1311932019-01-03 -0.2254572019-01-04 -0.097514 ...2019-12-28 1.0966102019-12-29 0.5341312019-12-30 -1.8596652019-12-31 -0.408150Freq: D, Length: 365, dtype: float64In [85]: long_ts['2019-09']Out[85]:2019-09-01 0.9503792019-09-02 0.5083692019-09-03 0.631864 ...2019-09-26 1.9828012019-09-27 1.6265222019-09-28 0.5467442019-09-29 -0.9716172019-09-30 0.106587Freq: D, dtype: float64In [88]: # 通过日期进行切片In [89]: ts[datetime(2018,3,24):]Out[89]:2018-03-24 -0.5506042018-03-25 0.0372302018-03-26 0.298817dtype: float64In [95]: # 按时间戳进行切片In [96]: tsOut[96]:2018-03-21 0.5550442018-03-22 -0.4045482018-03-23 1.0000002018-03-24 1.0000002018-03-25 1.0000002018-03-26 0.298817dtype: float64In [97]: ts['23/3/2018':'25/3/2018']Out[97]:2018-03-23 1.02018-03-24 1.02018-03-25 1.0dtype: float64In [119]: #切片数据是源数据的视图In [120]: ts = Series(np.random.randn(6), index=dates)In [121]: tsOut[121]:2018-03-21 -0.2097842018-03-22 -0.0341172018-03-23 -0.3831312018-03-24 1.2077782018-03-25 -1.4194932018-03-26 0.019064dtype: float64In [122]: ts_slice = ts['23/3/2018':'25/3/2018']In [123]: ts_slice[0]=1In [124]: tsOut[124]:2018-03-21 -0.2097842018-03-22 -0.0341172018-03-23 1.0000002018-03-24 1.2077782018-03-25 -1.4194932018-03-26 0.019064dtype: float64In [125]: #通过truncate实现切片并实现复制In [126]: ts.truncate(after='24/3/2018', copy=True)Out[126]:2018-03-21 -0.2097842018-03-22 -0.0341172018-03-23 1.0000002018-03-24 1.207778dtype: float64In [127]: ts_slice = ts.truncate(after='24/3/2018', copy=True)In [128]: ts_slice[0]=1In [129]: tsOut[129]:2018-03-21 -0.2097842018-03-22 -0.0341172018-03-23 1.0000002018-03-24 1.2077782018-03-25 -1.4194932018-03-26 0.019064dtype: float64In [130]: # 对DataFrame的行进行索引In [131]: date = pd.date_range('1/3/2018', periods=100, freq='W-WED')In [132]: long_df = DataFrame(np.random.randn(100,4), ...: index=date, ...: columns=['col1','col2','col3','col4']) ...:In [133]: long_df.loc['5-2018']Out[133]: col1 col2 col3 col42018-05-02 -1.217853 -0.052510 1.854937 0.1878702018-05-09 0.555552 -0.276599 -0.145089 -0.6763292018-05-16 0.704234 0.316785 -0.057501 0.0335672018-05-23 0.451673 0.367754 0.121129 -0.8618552018-05-30 -1.057437 -0.904105 -0.025711 -2.689067 带有重复索引的时间序列当多个观测数据落在同一个时间点上的时候，可以通过检查索引的is_unique属性来确定它是否唯一，而对这个时间序列进行索引要么产生 标量值(不重复)，要么产生 切片(重复)；如果需要对具有非唯一时间戳的数据进行聚合，使用groupby并传入level=0(索引的唯一一层):123456789101112131415161718192021222324252627282930313233343536373839404142434445464748In [137]: # 生成时间索引的SeriesIn [138]: dates = pd.DatetimeIndex(['1/3/2018','1/3/2018','2/3/2018','2/3/2018','3/3/2018'])In [139]: dup_ts = Series(np.arange(5), index=dates)In [140]: dup_tsOut[140]:2018-01-03 02018-01-03 12018-02-03 22018-02-03 32018-03-03 4dtype: int32In [141]: #检查索引是否唯一In [142]: dup_ts.index.is_uniqueOut[142]: FalseIn [143]: #对时间序列进行索引In [144]: dup_ts['3/3/2018'] #不重复Out[144]: 4In [145]: dup_ts['2/3/2018'] #重复Out[145]:2018-02-03 22018-02-03 3dtype: int32In [150]: # 进行分组聚合In [151]: grouped = dup_ts.groupby(level=0)In [152]: grouped.count()Out[152]:2018-01-03 22018-02-03 22018-03-03 1dtype: int64In [153]: grouped.mean()Out[153]:2018-01-03 0.52018-02-03 2.52018-03-03 4.0dtype: float64 日期的范围、频率以及移动pandas有一整套标准时间序列频率以及用于重采样、频率推断、生成固定频率日期范围的工具。调用resample方法将时间序列转换为一个具有固定频率的时间序列:12345678910111213141516171819202122232425262728293031In [176]: dates = [datetime(2018,3,d) for d in arange(21, 31, step=2)]In [177]: ts.resample('D').sum()Out[177]:2018-03-21 -2.0840442018-03-22 NaN2018-03-23 1.0758802018-03-24 NaN2018-03-25 0.0452012018-03-26 NaN2018-03-27 -0.7540862018-03-28 NaN2018-03-29 -0.448258Freq: D, dtype: float64In [178]: dates = [datetime(2018,3,d) for d in arange(21, 31, step=2)]In [179]: ts = Series(np.random.randn(5), index=dates)In [180]: ts.resample('D').sum() #转换成固定频率(每日)Out[180]:2018-03-21 -0.3765272018-03-22 NaN2018-03-23 0.2827882018-03-24 NaN2018-03-25 -0.0149752018-03-26 NaN2018-03-27 1.2099552018-03-28 NaN2018-03-29 -1.279524Freq: D, dtype: float64 生成日期范围pandas.data_range用于生成指定长度的DatatimeIndex，默认情况下data_range会产生按天计算的时间点。如果只传入起始或结束时间，那么的传入一个表示一段时间的数字periods。起始和结束日期定义了日期索引的严格边界(如果想要生成一个由每个月最后一个工作日组成的日期索引，传入”BM”频率，这样只会包含时间间隔内(或刚好在边界上)符合频率要求的日期)。data_range默认会保留起始或结束时间戳的 时间 信息(有的话)。如果希望产生一组被规范化到午夜的时间戳，可以设置normalize=True：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556In [182]: index = pd.date_range('8/4/2017','3/27/2018')In [183]: indexOut[183]:DatetimeIndex(['2017-08-04', '2017-08-05', '2017-08-06', '2017-08-07', '2017-08-08', '2017-08-09', '2017-08-10', '2017-08-11', '2017-08-12', '2017-08-13', ... '2018-03-18', '2018-03-19', '2018-03-20', '2018-03-21', '2018-03-22', '2018-03-23', '2018-03-24', '2018-03-25', '2018-03-26', '2018-03-27'], dtype='datetime64[ns]', length=236, freq='D')In [185]: #传入起始日期或结束日期并设置一段时间In [186]: pd.date_range(start='8/4/2017',periods=20)Out[186]:DatetimeIndex(['2017-08-04', '2017-08-05', '2017-08-06', '2017-08-07', '2017-08-08', '2017-08-09', '2017-08-10', '2017-08-11', '2017-08-12', '2017-08-13', '2017-08-14', '2017-08-15', '2017-08-16', '2017-08-17', '2017-08-18', '2017-08-19', '2017-08-20', '2017-08-21', '2017-08-22', '2017-08-23'], dtype='datetime64[ns]', freq='D')In [187]: pd.date_range(end='8/4/2017',periods=20)Out[187]:DatetimeIndex(['2017-07-16', '2017-07-17', '2017-07-18', '2017-07-19', '2017-07-20', '2017-07-21', '2017-07-22', '2017-07-23', '2017-07-24', '2017-07-25', '2017-07-26', '2017-07-27', '2017-07-28', '2017-07-29', '2017-07-30', '2017-07-31', '2017-08-01', '2017-08-02', '2017-08-03', '2017-08-04'], dtype='datetime64[ns]', freq='D')In [188]: #传入频率In [189]: pd.date_range('8/4/2017','3/27/2018', freq='BM')Out[189]:DatetimeIndex(['2017-08-31', '2017-09-29', '2017-10-31', '2017-11-30', '2017-12-29', '2018-01-31', '2018-02-28'], dtype='datetime64[ns]', freq='BM')In [193]: #保留时间信息In [194]: pd.date_range('8/4/2017 12:21:56', periods=5)Out[194]:DatetimeIndex(['2017-08-04 12:21:56', '2017-08-05 12:21:56', '2017-08-06 12:21:56', '2017-08-07 12:21:56', '2017-08-08 12:21:56'], dtype='datetime64[ns]', freq='D')In [203]: #规范化时间In [204]: date_r = pd.date_range('8/4/2017 12:21:56', periods=5, normalize=True)In [205]: date_r[1]Out[205]: Timestamp('2017-08-05 00:00:00', freq='D') 频率和日期偏移量pandas中的频率由一个 基础频率 和一个 乘数 组成。基础频率通常以一个字符串别名表示，比如“M”表示每月，“H”表示每小时。对于每个基础频率，都有一个被称为 日期偏移量 的对象与之对应。按小时计算可以使用Hour类，传入一个整数即可定义偏移量的倍数。一般来说无需显式创建偏移对象，直接使用诸如”H”或”4H”这样的字符串别名即可，在基础频率前加上一个整数可创建倍数；大部分的偏移量对象可以通过加法进行连接，而传入”1h30min”这样的频率字符串也能被高效地解析为等效的表达式。有些频率描述的时间点并不是均匀分隔的。(“M”(日历月末)和”BM”(每月最后一个工作日)就取决于每月的天数，对于后者还要考虑月末是不是周末，这种称为锚点偏移量(anchored offset)3)。 时间序列的基础频率 别名 偏移量类型 说明 D Day 每日历日 B BusinessDay 每工作日 H Hour 每小时 T或min Minute 每分 S Second 每秒 L或ms Milli 每毫秒(即每千分之一秒) U Micro 每微秒(即每百万分之一秒) M MonthEnd 每月最后一个日历日 BM BusinessMonthEnd 每月最后一个工作日 MS MonthBegin 每月第一个日历日 BMS BusinessMonthBegin 每月第一个工作日 W-MON、W-TUE… Week 从指定的星期几(MON、TUE、WED、THU、FRI、SAT、SUN)开始算起，每周 WOM-1MON、WOM-2MON… WeekOfMonth 产生每月第一、第二、第三或第四周的星期几。例如，WOM-3FRI表示每月的第三个星期五 Q-JAN、Q-FEB… QuaterEnd 对于以指定月份(JAN、FEB、MAR、APR、MAY、JUN、JUL、AUG、SEP、OCT、NOV、DEC)结束的年度，每季度最后一月的最后一个日历日 BQ-JAN、BQ-FEB… BusinessQuarterEnd 对于以指定月份结束的年度，每季度最后一个月的最后一个工作日 QS-JAN、QS-FEB… QuarterBegin 对于以指定月份结束的年度，每季度最后一月的第一个日历日 BQS-JAN、BQS-FEB… BusinessQuarterBegin 对于以指定月份结束的年度，每季度最后一月的第一个工作日 A-JAN、A-FEB… YearEnd 每年指定月份(JAN、FEB、MAR、APR、MAY、JUN、JUL、AUG、SEP、OCT、NOV、DEC)的最后一个日历日 BA-JAN、BA-FEB… BusinessYearEnd 每年指定月份的最后一个工作日 AS-JAN、AS-FEB… YearBegin 每年指定月份的第一个日历日 BAS-JAN、BAS-FEB… BusinessYearBegin 每年指定月份的第一个工作日 123456789101112131415161718192021222324252627282930313233343536373839404142434445In [189]: #按小时计算的频率In [190]: from pandas.tseries.offsets import Hour, MinuteIn [191]: hour = Hour()In [192]: hourOut[192]: &lt;Hour&gt;In [193]: four_hours = Hour(4)In [194]: four_hoursOut[194]: &lt;4 * Hours&gt;In [195]: #使用“H”或“4H”字符串别名In [196]: pd.date_range('3/27/2018','3/30/2018', freq='4h')Out[196]:DatetimeIndex(['2018-03-27 00:00:00', '2018-03-27 04:00:00', '2018-03-27 08:00:00', '2018-03-27 12:00:00', '2018-03-27 16:00:00', '2018-03-27 20:00:00', '2018-03-28 00:00:00', '2018-03-28 04:00:00', '2018-03-28 08:00:00', '2018-03-28 12:00:00', '2018-03-28 16:00:00', '2018-03-28 20:00:00', '2018-03-29 00:00:00', '2018-03-29 04:00:00', '2018-03-29 08:00:00', '2018-03-29 12:00:00', '2018-03-29 16:00:00', '2018-03-29 20:00:00', '2018-03-30 00:00:00'], dtype='datetime64[ns]', freq='4H')In [197]: #通过加法连接偏移量In [198]: Hour(2)+Minute(30)Out[198]: &lt;150 * Minutes&gt;In [199]: #传入频率字符串In [200]: pd.date_range('3/27/2018',periods=10,freq='2h30min')Out[200]:DatetimeIndex(['2018-03-27 00:00:00', '2018-03-27 02:30:00', '2018-03-27 05:00:00', '2018-03-27 07:30:00', '2018-03-27 10:00:00', '2018-03-27 12:30:00', '2018-03-27 15:00:00', '2018-03-27 17:30:00', '2018-03-27 20:00:00', '2018-03-27 22:30:00'], dtype='datetime64[ns]', freq='150T') WOM日期WOM(Week Of Month)是一种非常实用的频率类，它以 WOM 开头，能够获得诸如“每个月第三个星期五”之类的日期：123456789In [202]: # 每月第三个星期五In [203]: rng = pd.date_range('3/1/2018','9/1/2018',freq='WOM-3FRI')In [204]: rngOut[204]:DatetimeIndex(['2018-03-16', '2018-04-20', '2018-05-18', '2018-06-15', '2018-07-20', '2018-08-17'], dtype='datetime64[ns]', freq='WOM-3FRI') 移动(超前和滞后)数据移动(shifting) 指的是沿着时间轴将数据前移或后移。Series和DataFrame有一个shift方法用于执行单纯的前移或后移操作，保持索引不变。shift通常用于计算一个时间序列或多个时间序列中的百分比变化(ts/ts.shift(1)-1)。单纯的移位操作不会修改索引，所以部分数据会被丢弃。如果频率已知，可以将其传给shift来实现对时间戳进行位移而不是对数据进行简单位移：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051In [205]: #使用shift方法对数据移位In [206]: ts = Series(np.random.randn(4), index=pd.date_range('27/3/2018',periods=4, freq='M'))In [207]: tsOut[207]:2018-03-31 0.5902462018-04-30 -0.4252452018-05-31 0.5055582018-06-30 -0.467728Freq: M, dtype: float64In [208]: ts.shift(2)#后移两位Out[208]:2018-03-31 NaN2018-04-30 NaN2018-05-31 0.5902462018-06-30 -0.425245Freq: M, dtype: float64In [209]: ts.shift(-2)#前移两位Out[209]:2018-03-31 0.5055582018-04-30 -0.4677282018-05-31 NaN2018-06-30 NaNFreq: M, dtype: float64In [212]: ts.shift(2,freq='M') #通过频率对时间戳进行移位，后移两个月Out[212]:2018-05-31 0.5902462018-06-30 -0.4252452018-07-31 0.5055582018-08-31 -0.467728Freq: M, dtype: float64In [213]: ts.shift(-3,freq='D') #通过频率对时间戳进行移位，前移3天Out[213]:2018-03-28 0.5902462018-04-27 -0.4252452018-05-28 0.5055582018-06-27 -0.467728dtype: float64In [214]: ts.shift(-1,freq='3D') #通过频率对时间戳进行移位，前移3天Out[214]:2018-03-28 0.5902462018-04-27 -0.4252452018-05-28 0.5055582018-06-27 -0.467728dtype: float64 通过偏移量对日期进行位移pandas的日期偏移量可以用在datetime或Timestamp对象上；如果是锚点偏移量(比如MethodEnd)，第一次增量会将原日期向前滚动到符合频率规则的下一个日期；通过锚点偏移量的rollforward和rollback方法，可显式地将日期向前或向后滚动；也可以通过groupby或者resample方法来滚动:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051In [219]: #在datetime或Timestamp对象上使用日期偏移量In [220]: from pandas.tseries.offsets import Day, MonthEndIn [221]: now = datetime.now()In [222]: nowOut[222]: datetime.datetime(2018, 3, 27, 20, 19, 34, 880551)In [223]: now + 3* Day()Out[223]: Timestamp('2018-03-30 20:19:34.880551')In [224]: #加上锚点偏移量In [225]: now + MonthEnd()Out[225]: Timestamp('2018-03-31 20:19:34.880551')In [226]: now + MonthEnd(2)Out[226]: Timestamp('2018-04-30 20:19:34.880551')In [231]: #使用rollforward和rollback方法显式向前或向后滚动日期In [232]: offset = MonthEnd()In [233]: offset.rollforward(now) #向前滚动Out[233]: Timestamp('2018-03-31 20:19:34.880551')In [234]: offset.rollback(now) #向后滚动Out[234]: Timestamp('2018-02-28 20:19:34.880551')In [238]: #使用groupby使用滚动In [239]: ts = Series(np.random.randn(20), index=pd.date_range('27/3/2018',periods=20, freq='4d'))In [240]: ts.groupby(offset.rollforward).mean()Out[240]:2018-03-31 0.8474762018-04-30 -0.1946612018-05-31 0.0923922018-06-30 -0.710720dtype: float64In [243]: # 使用resampleIn [244]: ts.resample('M').mean()Out[244]:2018-03-31 0.8474762018-04-30 -0.1946612018-05-31 0.0923922018-06-30 -0.710720Freq: M, dtype: float64 时区处理时区是以UTC偏移量的形式表示。在Python中时区信息来自第三方库pytz，它使Python可以使用Olson数据库(汇编了世界时区信息)。而pandas包装了pytz的功能，所以一般不需要单独学习，只需要记住时区名即可。时区名既可以在官方文档中查看也可以通过交互式的方式查看；要从pytz中获取时区对象，使用pytz.timezone即可，pandas方法虽然接收时区名和时区对象，但建议使用时区名：12345678910111213In [3]: # 使用交互式方式查看时区名In [4]: import pytzIn [5]: pytz.common_timezones[-5:]Out[5]: ['US/Eastern', 'US/Hawaii', 'US/Mountain', 'US/Pacific', 'UTC']In [6]: # 从pytz中获取时区对象In [7]: tz = pytz.timezone('US/Hawaii')In [8]: tzOut[8]: &lt;DstTzInfo 'US/Hawaii' LMT-1 day, 13:29:00 STD&gt; 本地化和转换默认情况下pandas中的时间序列是 单纯的(naive) 时区，其索引的tz字段为 None，在生成日期范围的时候可以加上tz=时区集；从单纯到本地化的转换可以通过tz_localize方法处理，一旦时间序列被本地化到某个特定时区，就可以用tz_convert将其转换到别的时区;tz_localize和tz_convert两个方法都是DatetimeIndex的实例方法:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495In [9]: # 生成时间序列In [10]: rng = pd.date_range('3/29/2018 8:51', periods=6, freq='D')In [11]: ts = Series(np.random.randn(len(rng)), index=rng)In [13]: print(ts.index.tz)NoneIn [16]: # 在生成日期范围时加上时区集In [17]: pd.date_range('3/28/2018 8:53',periods=10, freq='D', tz='UTC')Out[17]:DatetimeIndex(['2018-03-28 08:53:00+00:00', '2018-03-29 08:53:00+00:00', '2018-03-30 08:53:00+00:00', '2018-03-31 08:53:00+00:00', '2018-04-01 08:53:00+00:00', '2018-04-02 08:53:00+00:00', '2018-04-03 08:53:00+00:00', '2018-04-04 08:53:00+00:00', '2018-04-05 08:53:00+00:00', '2018-04-06 08:53:00+00:00'], dtype='datetime64[ns, UTC]', freq='D')In [18]: # 使用tz_localize方法将单纯转换为本地化In [19]: tsOut[19]:2018-03-29 08:51:00 -2.4962072018-03-30 08:51:00 0.6329442018-03-31 08:51:00 1.0187562018-04-01 08:51:00 -0.0179842018-04-02 08:51:00 -2.5229742018-04-03 08:51:00 1.096955Freq: D, dtype: float64In [20]: ts_utc = ts.tz_localize('UTC')In [21]: ts_utcOut[21]:2018-03-29 08:51:00+00:00 -2.4962072018-03-30 08:51:00+00:00 0.6329442018-03-31 08:51:00+00:00 1.0187562018-04-01 08:51:00+00:00 -0.0179842018-04-02 08:51:00+00:00 -2.5229742018-04-03 08:51:00+00:00 1.096955Freq: D, dtype: float64In [22]: ts_utc.indexOut[22]:DatetimeIndex(['2018-03-29 08:51:00+00:00', '2018-03-30 08:51:00+00:00', '2018-03-31 08:51:00+00:00', '2018-04-01 08:51:00+00:00', '2018-04-02 08:51:00+00:00', '2018-04-03 08:51:00+00:00'], dtype='datetime64[ns, UTC]', freq='D')In [23]: # 使用tz_covert方法转换到别的时区In [24]: ts_utc.tz_convert('US/Eastern')Out[24]:2018-03-29 04:51:00-04:00 -2.4962072018-03-30 04:51:00-04:00 0.6329442018-03-31 04:51:00-04:00 1.0187562018-04-01 04:51:00-04:00 -0.0179842018-04-02 04:51:00-04:00 -2.5229742018-04-03 04:51:00-04:00 1.096955Freq: D, dtype: float64In [25]: # 将ts本地化到EST再转换为UTC或柏林时间In [26]: ts_eastern = ts.tz_localize('US/Eastern')In [27]: ts_eastern.tz_convert('UTC')Out[27]:2018-03-29 12:51:00+00:00 -2.4962072018-03-30 12:51:00+00:00 0.6329442018-03-31 12:51:00+00:00 1.0187562018-04-01 12:51:00+00:00 -0.0179842018-04-02 12:51:00+00:00 -2.5229742018-04-03 12:51:00+00:00 1.096955Freq: D, dtype: float64In [28]: ts_eastern.tz_convert('Europe/Berlin')Out[28]:2018-03-29 14:51:00+02:00 -2.4962072018-03-30 14:51:00+02:00 0.6329442018-03-31 14:51:00+02:00 1.0187562018-04-01 14:51:00+02:00 -0.0179842018-04-02 14:51:00+02:00 -2.5229742018-04-03 14:51:00+02:00 1.096955Freq: D, dtype: float64In [29]: # tz_localize和tz_convert是DatetimeIndex的实例方法In [30]: ts.index.tz_localize('Asia/Shanghai')Out[30]:DatetimeIndex(['2018-03-29 08:51:00+08:00', '2018-03-30 08:51:00+08:00', '2018-03-31 08:51:00+08:00', '2018-04-01 08:51:00+08:00', '2018-04-02 08:51:00+08:00', '2018-04-03 08:51:00+08:00'], dtype='datetime64[ns, Asia/Shanghai]', freq='D') 操作时区意识型Timestamp对象跟时间序列和日期范围一样，Timestamp对象也能被从 单纯型(naive) 本地化为 时区意识型(time zone-aware)，并从一个时区转换到另一个时区；可以在创建Timestamp对象时，传入一个时区信息。时区意识型Timestamp对象在内部保存了一个UTC时间戳值(自UNIX纪元(1970年1月1日)算起的纳秒数)，而这个UTC值在时区转换过程中不会变化，当使用pandas的DateOffset对象执行时间算术运算时，运算过程会自动关注是否存在夏令时转变期:12345678910111213141516171819202122232425262728293031323334353637383940In [34]: # 创建Timestamp对象并本地化，转换时区In [35]: stamp = pd.Timestamp('2018-3-28 9:08')In [36]: stamp_utc = stamp.tz_localize('UTC') #本地化为UTCIn [37]: stamp_utcOut[37]: Timestamp('2018-03-28 09:08:00+0000', tz='UTC')In [38]: stamp_utc.tz_convert('US/Eastern') #转换到USE时区Out[38]: Timestamp('2018-03-28 05:08:00-0400', tz='US/Eastern')In [39]: # 创建一个带有时区信息的TimestampIn [40]: stamp_moscow = pd.Timestamp('2018-3-28 9:00', tz='Asia/Shanghai')In [41]: stamp_moscowOut[41]: Timestamp('2018-03-28 09:00:00+0800', tz='Asia/Shanghai')In [43]: stamp_moscow.valueOut[43]: 1522198800000000000In [44]: stamp_moscow.tz_convert('US/Eastern').valueOut[44]: 1522198800000000000In [52]: # DateOffset对象执行算术运算In [53]: stamp = pd.Timestamp('2012-3-12 1:30', tz='US/Eastern') #夏令时转变前30minIn [54]: stamp + Hour()Out[54]: Timestamp('2012-03-12 02:30:00-0400', tz='US/Eastern')In [55]: stamp = pd.Timestamp('2012-11-4 00:30', tz='US/Eastern') #夏令时转变前90minIn [56]: stampOut[56]: Timestamp('2012-11-04 00:30:00-0400', tz='US/Eastern')In [57]: stamp + 2*Hour()Out[57]: Timestamp('2012-11-04 01:30:00-0500', tz='US/Eastern') 不同时区之间的运算如果两个时间序列的时区不同，在将它们合并到一起时，最终结果就会是UTC；而由于时间戳是以UTC存储的，所以不需要发生任何转换:123456789101112131415161718192021222324252627282930313233343536In [78]: rng = pd.date_range('3/29/2018 8:51', periods=6, freq='B')In [79]: ts = Series(np.random.randn(len(rng)), index=rng)In [80]: # 分别设置时区In [81]: ts1 = ts[:6].tz_localize('Asia/Shanghai')In [82]: ts2 = ts1[2:].tz_convert('Europe/Moscow')In [83]: ts1Out[83]:2018-03-29 08:51:00+08:00 -0.6850112018-03-30 08:51:00+08:00 0.2579922018-04-02 08:51:00+08:00 -0.6503802018-04-03 08:51:00+08:00 0.0505112018-04-04 08:51:00+08:00 -0.4414192018-04-05 08:51:00+08:00 -1.618306Freq: B, dtype: float64In [84]: ts2Out[84]:2018-04-02 03:51:00+03:00 -0.6503802018-04-03 03:51:00+03:00 0.0505112018-04-04 03:51:00+03:00 -0.4414192018-04-05 03:51:00+03:00 -1.618306Freq: B, dtype: float64In [85]: result = ts1+ts2In [86]: result.indexOut[86]:DatetimeIndex(['2018-03-29 00:51:00+00:00', '2018-03-30 00:51:00+00:00', '2018-04-02 00:51:00+00:00', '2018-04-03 00:51:00+00:00', '2018-04-04 00:51:00+00:00', '2018-04-05 00:51:00+00:00'], dtype='datetime64[ns, UTC]', freq='B') 1.闰秒，是指为保持协调世界时接近于世界时时刻，由国际计量局统一规定在年底或年中（也可能在季末）对协调世界时增加或减少1秒的调整 ↩2.dateutil.parser并不完美，它会将一些原本不是日期的字符串认作是日期(例如将'23'解析成当月的23号，而大于日期的例如'32'会被解析为2032年的今天) ↩3.来源于书上的叫法(《利用Python进行数据分析》) ↩]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>时间序列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据聚合与分组运算示例]]></title>
    <url>%2F2018%2F03%2F26%2F%E6%95%B0%E6%8D%AE%E8%81%9A%E5%90%88%E4%B8%8E%E5%88%86%E7%BB%84%E8%BF%90%E7%AE%97%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[用特定与分组的值填充缺失值 对于缺失数据的清理工作，有时会用dropna将其滤除，而有时则可能希望用一个固定值或由数据集本身所衍生出来的值去填充NA值，这时就需要使用fillna。 1234567891011121314151617181920212223In [3]: s = Series(np.random.randn(6))In [4]: s[::2] = np.nanIn [5]: sOut[5]:0 NaN1 1.4673292 NaN3 0.0665194 NaN5 -0.324448dtype: float64In [6]: s.fillna(s.mean()) # 用平均值填充NA值Out[6]:0 0.4031331 1.4673292 0.4031333 0.0665194 0.4031335 -0.324448dtype: float64 对不同的分组填充不同的值，只需将数据分组，并使用apply和一个能够对各数据块调用fillna的函数：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859In [7]: states = ['CD', 'CQ','XJ','XM','SH','BJ','NJ','TJ']In [8]: group_key = ['West']*4 +['East']*4In [9]: data = Series(np.random.randn(8),index=states)In [10]: data[['XJ','BJ','TJ']] = np.nanIn [11]: dataOut[11]:CD 0.438333CQ 0.156206XJ NaNXM 0.309649SH -0.692189BJ NaNNJ 1.812046TJ NaNdtype: float64In [12]: data.groupby(group_key).mean()Out[12]:East 0.559928West 0.301396dtype: float64In [13]: # 用分组平均值去填充NA值In [14]: fill_mean = lambda g: g.fillna(g.mean())In [15]: data.groupby(group_key).apply(fill_mean)Out[15]:CD 0.438333CQ 0.156206XJ 0.301396XM 0.309649SH -0.692189BJ 0.559928NJ 1.812046TJ 0.559928dtype: float64In [22]: # 在代码中预定义各组的填充值，可以使用各分组的name属性In [23]: fill_values=&#123;'East':0.5,'West':1&#125;In [24]: fill_func = lambda g: g.fillna(fill_values[g.name])In [25]: data.groupby(group_key).apply(fill_func)Out[25]:CD 0.438333CQ 0.156206XJ 1.000000XM 0.309649SH -0.692189BJ 0.500000NJ 1.812046TJ 0.500000dtype: float64 随机采样和排列 从一个大数据集中随机抽取样本可以选取np.random.permutation(N)的前K个元素，其中N为完整数据的大小，K为期望的样本大小下面将构造一个扑克牌 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677In [37]: # 红桃(Hearts)、黑桃(Spades)、梅花(Clubs)、方片(Diamonds)In [38]: suits = ['H','S','C','D']In [39]: card_val = (list(range(1,11))+[10]*3)*4In [40]: base_names = ['A']+list(range(2,11))+['J','K','Q']In [41]: cards = []In [42]: for suit in suits: ...: # 构造卡牌 ...: cards.extend(str(num)+ suit for num in base_names) ...:In [43]: deck = Series(card_val,index=cards)In [44]: deck[:13] # 长度为52的Series，其索引为牌名，值则是用于记分的点数Out[44]:AH 12H 23H 34H 45H 56H 67H 78H 89H 910H 10JH 10KH 10QH 10dtype: int64In [47]: def draw(deck, n=5): ...: # 从整副牌中抽出5张 ...: return deck.take(np.random.permutation(len(deck))[:n]) ...:In [48]: draw(deck) # 抽牌Out[48]:3H 3KS 109D 910S 10JH 10dtype: int64In [53]: # 从每种花色中抽取两张牌。由于花色是最后一个字符，所以根据这个进行分组并使用applyIn [54]: get_suit = lambda card:card[-1] # 只要最后一个字母In [55]: deck.groupby(get_suit).apply(draw, n=2)Out[55]:C 10C 10 KC 10D JD 10 6D 6H 6H 6 3H 3S JS 10 7S 7dtype: int64In [56]: # 另一种方法In [57]: deck.groupby(get_suit, group_keys=False).apply(draw, n=2)Out[57]:7C 73C 310D 105D 5KH 108H 8AS 16S 6dtype: int64 分组加权平均数和相关系数 根据groupby的“拆分-应用-合并”范式，DataFrame的列与列之间或两个Series之间的运算(比如分组加权平均)称为一种标准作业 1234567891011121314151617181920212223242526272829In [60]: df = DataFrame(&#123;'category':list('aaaabbbb'), ...: 'data':np.random.randn(8), ...: 'weights':np.random.rand(8)&#125;) ...:In [61]: dfOut[61]: category data weights0 a -0.183911 0.0658501 a -0.977102 0.8974962 a 1.632742 0.0529663 a 1.820148 0.9148464 b -1.387401 0.2564625 b 0.858152 0.9776656 b 1.613297 0.5494507 b 0.365536 0.472255In [69]: # 利用catefory计算加权平均数:In [70]: grouped = df.groupby('category')In [71]: get_wavg = lambda g: np.average(g['data'], weights=g['weights'])In [72]: grouped.apply(get_wavg)Out[72]:categorya 0.446664b 0.683660dtype: float64 来自Yahoo!Finance的数据集，其中含有标准普尔500指数(SPX字段)和几只股票的收盘价123456789101112131415161718192021222324252627282930313233343536373839404142434445In [75]: # 读取数据In [76]: close_px = pd.read_csv('stock_px.csv', parse_dates=True, index_col=0)In [77]: close_px[:4]Out[77]: AA AAPL GE IBM JNJ MSFT PEP SPX XOM1990-02-01 4.98 7.86 2.87 16.79 4.27 0.51 6.04 328.79 6.121990-02-02 5.04 8.00 2.87 16.89 4.37 0.51 6.09 330.92 6.241990-02-05 5.07 8.18 2.87 17.32 4.34 0.51 6.05 331.85 6.251990-02-06 5.01 8.12 2.88 17.56 4.32 0.51 6.15 329.66 6.23In [78]: # 计算一个由日收益率(通过百分数变化计算)与SPX之间的年度相关系数组成的DataFrameIn [79]: rets = close_px.pct_change().dropna()In [80]: spx_corr = lambda x: x.corrwith(x['SPX'])In [81]: by_year = rets.groupby(lambda x: x.year)In [84]: by_year.apply(spx_corr)[:5]Out[84]: AA AAPL GE IBM JNJ MSFT PEP \1990 0.595024 0.545067 0.752187 0.738361 0.801145 0.586691 0.7831681991 0.453574 0.365315 0.759607 0.557046 0.646401 0.524225 0.6417751992 0.398180 0.498732 0.632685 0.262232 0.515740 0.492345 0.4738711993 0.259069 0.238578 0.447257 0.211269 0.451503 0.425377 0.3850891994 0.428549 0.268420 0.572996 0.385162 0.372962 0.436585 0.450516 SPX XOM1990 1.0 0.5175861991 1.0 0.5693351992 1.0 0.3184081993 1.0 0.3189521994 1.0 0.395078In [85]: # 计算列和列之间的相关系数In [88]: by_year.apply(lambda g: g['AAPL'].corr(g['MSFT']))[:4] #计算苹果和微软的年度相关系数Out[88]:1990 0.4082711991 0.2668071992 0.4505921993 0.236917dtype: float64 面向分组的线性回归 利用statsmodels库对各数据块执行普通最小二乘法回归: 安装statsmodels库 1In [89]: !pip3 install statsmodels 计算线性回归 1234567891011121314151617181920212223242526272829303132333435In [90]: import statsmodels.api as smIn [91]: def regress(data, yvar, xvars): ...: Y = data[yvar] ...: X = data[xvars] ...: X['intercept'] = 1. ...: result = sm.OLS(Y,X).fit() ...: return result.params ...:In [92]: by_year.apply(regress, 'AAPL', ['SPX']) # 按年计算AAPL对SPX收益率的线性回归Out[92]: SPX intercept1990 1.512772 0.0013951991 1.187351 0.0003961992 1.832427 0.0001641993 1.390470 -0.0026571994 1.190277 0.0016171995 0.858818 -0.0014231996 0.829389 -0.0017911997 0.749928 -0.0019011998 1.164582 0.0040751999 1.384989 0.0032732000 1.733802 -0.0025232001 1.676128 0.0031222002 1.080330 -0.0001992003 1.187770 0.0006902004 1.363463 0.0042012005 1.766415 0.0032462006 1.645496 0.0000802007 1.198761 0.0034382008 0.968016 -0.0011102009 0.879103 0.0029542010 1.052608 0.0012612011 0.806605 0.001514 2012联邦选举委员会数据库12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788In [105]: # 读取文件In [106]: fec = pd.read_csv('P00000001-ALL.csv', low_memory=False)In [107]: #fec.loc[123456]In [108]: # 读取文件In [109]: fec = pd.read_csv('P00000001-ALL.csv', low_memory=False)In [110]: fec.loc[123456] #DataFrame中记录Out[110]:cmte_id C00431445cand_id P80003338cand_nm Obama, Barackcontbr_nm ELLMAN, IRAcontbr_city TEMPEcontbr_st AZcontbr_zip 852816719contbr_employer ARIZONA STATE UNIVERSITYcontbr_occupation PROFESSORcontb_receipt_amt 50contb_receipt_dt 01-DEC-11receipt_desc NaNmemo_cd NaNmemo_text NaNform_tp SA17Afile_num 772372Name: 123456, dtype: objectIn [111]: # 数据中没有党派信息，通过unique可以获取全部候选人名单，利用字典说明党派关系In [112]: unique_cands = fec.cand_nm.unique()In [113]: unique_candsOut[113]:array(['Bachmann, Michelle', 'Romney, Mitt', 'Obama, Barack', "Roemer, Charles E. 'Buddy' III", 'Pawlenty, Timothy', 'Johnson, Gary Earl', 'Paul, Ron', 'Santorum, Rick', 'Cain, Herman', 'Gingrich, Newt', 'McCotter, Thaddeus G', 'Huntsman, Jon', 'Perry, Rick'], dtype=object)In [117]: parties=&#123;'Bachmann, Michelle':'Republican', ...: 'Romney, Mitt':'Republican', ...: "Roemer, Charles E. 'Buddy' III":'Republican', ...: 'Pawlenty, Timothy':'Republican', ...: 'Johnson, Gary Earl':'Republican', ...: 'Paul, Ron':'Republican', ...: 'Santorum, Rick':'Republican', ...: 'Cain, Herman':'Republican', ...: 'Gingrich, Newt':'Republican', ...: 'McCotter, Thaddeus G':'Republican', ...: 'Huntsman, Jon':'Republican', ...: 'Obama, Barack':'Democrat', ...: 'Perry, Rick':'Republican'&#125; ...:In [118]: # 通过这个映射以及Series对象的map方法可以根据候选人姓名得到一组党派信息In [119]: fec.cand_nm[123456:123461]Out[119]:123456 Obama, Barack123457 Obama, Barack123458 Obama, Barack123459 Obama, Barack123460 Obama, BarackName: cand_nm, dtype: objectIn [120]: fec.cand_nm[123456:123461].map(parties)Out[120]:123456 Democrat123457 Democrat123458 Democrat123459 Democrat123460 DemocratName: cand_nm, dtype: objectIn [121]: # 将其添加为一个新列In [122]: fec['party'] = fec.cand_nm.map(parties)In [123]: # 限定该数据集只有正的出资额In [124]: fec = fec[fec.contb_receipt_amt&gt;0]In [125]: # 由于Barack Obama和Mitt Romney是最主要的两名候选人，所以准备一个只包含针对两人的竞选活动的赞助信息的子集In [126]: fec_mrbo = fec[fec.cand_nm.isin(['Obama, Barack','Romney, Mitt'])] 根据职业和雇主统计赞助信息1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071In [127]: #根据职业计算出资总额In [128]: fec.contbr_occupation.value_counts()[:10]Out[128]:RETIRED 233990INFORMATION REQUESTED 35107ATTORNEY 34286HOMEMAKER 29931PHYSICIAN 23432INFORMATION REQUESTED PER BEST EFFORTS 21138ENGINEER 14334TEACHER 13990CONSULTANT 13273PROFESSOR 12555Name: contbr_occupation, dtype: int64In [129]: # 许多职业都涉及相同的基本工作类型，或者同一样东西有多重辩题。下面通过将一个职业信息映射到另一个来清理这样的数据In [130]: occ_mapping =&#123; ...: 'INFORMATION REQUESTED PER BEST EFFORTS':'NOT PROVIDED', ...: 'INFORMATION REQUESTED':'NOT PROVIDED', ...: 'INFORMATION REQUESTED (BEST EFFORTS)':'NOT PROVIDED', ...: 'C.E.O':'CEO'&#125; ...:In [131]: # 使用dict.get允许没有映射关系的职业也能通过，如果没有映射关系则返回xIn [132]: f = lambda x: occ_mapping.get(x,x)In [133]: fec.contbr_occupation = fec.contbr_occupation.map(f)In [134]: # 对雇主也进行相同的处理In [135]: emp_mapping = &#123; ...: 'INFORMATION REQUESTED PER BEST EFFORTS':'NOT PROVIDED', ...: 'INFORMATION REQUESTED':'NOT PROVIDED', ...: 'SELF':'SELF-EMPLOYED', ...: 'SELF EMPLOYED':'SELF-EMPLOYED',&#125; ...:In [136]: f = lambda x : emp_mapping.get(x,x)In [137]: fec.contbr_employer = fec.contbr_employer.map(f)In [138]: # 通过pivot_table根据党派和职业对数据进行聚合，然后过滤掉总出资额不足200万美元的数据In [139]: by_occupation = fec.pivot_table('contb_receipt_amt', index='contbr_occupation',columns='party',aggfunc='sum')In [140]: over_2mm = by_occupation[by_occupation.sum(1)&gt;2000000]In [141]: over_2mmOut[141]:party Democrat Republicancontbr_occupationATTORNEY 11141982.97 7.477194e+06C.E.O. 1690.00 2.592983e+06CEO 2074284.79 1.640758e+06CONSULTANT 2459912.71 2.544725e+06ENGINEER 951525.55 1.818374e+06EXECUTIVE 1355161.05 4.138850e+06HOMEMAKER 4248875.80 1.363428e+07INVESTOR 884133.00 2.431769e+06LAWYER 3160478.87 3.912243e+05MANAGER 762883.22 1.444532e+06NOT PROVIDED 4866973.96 2.056547e+07OWNER 1001567.36 2.408287e+06PHYSICIAN 3735124.94 3.594320e+06PRESIDENT 1878509.95 4.720924e+06PROFESSOR 2165071.08 2.967027e+05REAL ESTATE 528902.09 1.625902e+06RETIRED 25305116.38 2.356124e+07SELF-EMPLOYED 672393.40 1.640253e+06 出资总额大于两百万美元 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980In [159]: # 对Obama和Romney总出资最高的职业和企业In [160]: #先对候选人进行分组In [161]: def get_top_amounts(group, key, n=5): ...: # 求最大值 ...: totals = group.groupby(key)['contb_receipt_amt'].sum() ...: # 根据key对totals进行降序排列 ...: return totals.sort_values(ascending=False)[n:] ...:In [162]: # 根据职业和雇主进行聚合In [163]: grouped = fec_mrbo.groupby('cand_nm')In [164]: grouped.apply(get_top_amounts, 'contbr_occupation',n=7)Out[164]:cand_nm contbr_occupationObama, Barack PROFESSOR 2165071.08 CEO 2073284.79 PRESIDENT 1878509.95 NOT EMPLOYED 1709188.20 EXECUTIVE 1355161.05 TEACHER 1250969.15 WRITER 1084188.88 OWNER 1001567.36 ENGINEER 951525.55 INVESTOR 884133.00 ARTIST 763125.00 MANAGER 762883.22 SELF-EMPLOYED 672393.40 STUDENT 628099.75 REAL ESTATE 528902.09 CHAIRMAN 496547.00 ARCHITECT 483859.89 DIRECTOR 471741.73 BUSINESS OWNER 449979.30 EDUCATOR 436600.89 PSYCHOLOGIST 427299.92 SOFTWARE ENGINEER 396985.65 PARTNER 395759.50 SALES 392886.91 EXECUTIVE DIRECTOR 348180.94 MANAGING DIRECTOR 329688.25 SOCIAL WORKER 326844.43 VICE PRESIDENT 325647.15 ADMINISTRATOR 323079.26 SCIENTIST 319227.88 ...Romney, Mitt NON-PROFIT VETERANS ORG. CHAIR/ANNUITA 10.00 PARAPLANNER 10.00 APPRAISAL 10.00 SIGN CONTRACTOR 10.00 POLITICAL OPERATIVE 10.00 PORT MGT 10.00 PRESIDENT EMERITUS 10.00 CONTRACTS SPECIALIST 9.00 TEACHER &amp; FREE-LANCE JOURNALIST 9.00 FOUNDATION CONSULTANT 6.00 MAIL HANDLER 6.00 TREASURER &amp; DIRECTOR OF FINANCE 6.00 SECRETARY/BOOKKEPPER 6.00 ELAYNE WELLS HARMER 6.00 CHICKEN GRADER 5.00 DIRECTOR REISCHAUER CENTER FOR EAST A 5.00 SCOTT GREENBAUM 5.00 EDUCATION ADMIN 5.00 ENGINEER/RISK EXPERT 5.00 PLANNING AND OPERATIONS ANALYST 5.00 VILLA NOVA 5.00 FINANCIAL INSTITUTION - CEO 5.00 HORTICULTURIST 5.00 MD - UROLOGIST 5.00 DISTRICT REPRESENTATIVE 5.00 INDEPENDENT PROFESSIONAL 3.00 REMODELER &amp; SEMI RETIRED 3.00 AFFORDABLE REAL ESTATE DEVELOPER 3.00 IFC CONTRACTING SOLUTIONS 3.00 3RD GENERATION FAMILY BUSINESS OWNER 3.00Name: contb_receipt_amt, Length: 35975, dtype: float64 对出资额分组1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889In [165]: # 利用cut函数根据出资额的大小将数据离散到多个面元中In [166]: bins = np.array([0,1,10,100,1000,10000,100000,1000000,10000000])In [167]: labels = pd.cut(fec_mrbo.contb_receipt_amt,bins)In [168]: labelsOut[168]:411 (10, 100]412 (100, 1000]413 (100, 1000]414 (10, 100]415 (10, 100]416 (10, 100]417 (100, 1000]418 (10, 100]419 (100, 1000]420 (10, 100]421 (10, 100]422 (100, 1000]423 (100, 1000]424 (100, 1000]425 (100, 1000]426 (100, 1000]427 (1000, 10000]428 (100, 1000]429 (100, 1000]430 (10, 100]431 (1000, 10000]432 (100, 1000]433 (100, 1000]434 (100, 1000]435 (100, 1000]436 (100, 1000]437 (10, 100]438 (100, 1000]439 (100, 1000]440 (10, 100] ...701356 (10, 100]701357 (1, 10]701358 (10, 100]701359 (10, 100]701360 (10, 100]701361 (10, 100]701362 (100, 1000]701363 (10, 100]701364 (10, 100]701365 (10, 100]701366 (10, 100]701367 (10, 100]701368 (100, 1000]701369 (10, 100]701370 (10, 100]701371 (10, 100]701372 (10, 100]701373 (10, 100]701374 (10, 100]701375 (10, 100]701376 (1000, 10000]701377 (10, 100]701378 (10, 100]701379 (100, 1000]701380 (1000, 10000]701381 (10, 100]701382 (100, 1000]701383 (1, 10]701384 (10, 100]701385 (100, 1000]Name: contb_receipt_amt, Length: 694282, dtype: categoryCategories (8, interval[int64]): [(0, 1] &lt; (1, 10] &lt; (10, 100] &lt; (100, 1000] &lt; (1000, 10000] &lt; (10000, 100000] &lt; (100000, 1000000] &lt; (1000000, 10000000]]n [171]: # 根据候选人姓名以及面元标签对数据进行分组In [172]: grouped = fec_mrbo.groupby(['cand_nm',labels])In [173]: grouped.size().unstack(0)Out[173]:cand_nm Obama, Barack Romney, Mittcontb_receipt_amt(0, 1] 493.0 77.0(1, 10] 40070.0 3681.0(10, 100] 372280.0 31853.0(100, 1000] 153991.0 43357.0(1000, 10000] 22284.0 26186.0(10000, 100000] 2.0 1.0(100000, 1000000] 3.0 NaN(1000000, 10000000] 4.0 NaN 两位候选人收到的各种捐赠总额比例 根据州统计赞助信息1234567891011121314151617181920212223242526272829303132333435363738394041n [181]: # 根据候选人和州对数据进行聚合In [182]: grouped = fec_mrbo.groupby(['cand_nm','contbr_st'])In [183]: totals = grouped.contb_receipt_amt.sum().unstack(0).fillna(0)In [184]: totals = totals[totals.sum(1)&gt;100000]In [185]: totals[:10]Out[185]:cand_nm Obama, Barack Romney, Mittcontbr_stAK 281840.15 86204.24AL 543123.48 527303.51AR 359247.28 105556.00AZ 1506476.98 1888436.23CA 23824984.24 11237636.60CO 2132429.49 1506714.12CT 2068291.26 3499475.45DC 4373538.80 1025137.50DE 336669.14 82712.00FL 7318178.58 8338458.81In [186]: # 对各行除以总赞助额会得到各候选人在各州的总赞助额比例In [187]: percent = totals.div(totals.sum(1),axis=0)In [188]: percent[:10]Out[188]:cand_nm Obama, Barack Romney, Mittcontbr_stAK 0.765778 0.234222AL 0.507390 0.492610AR 0.772902 0.227098AZ 0.443745 0.556255CA 0.679498 0.320502CO 0.585970 0.414030CT 0.371476 0.628524DC 0.810113 0.189887DE 0.802776 0.197224FL 0.467417 0.532583]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>聚合与分组</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据聚合和分组运算]]></title>
    <url>%2F2018%2F03%2F25%2F%E6%95%B0%E6%8D%AE%E8%81%9A%E5%90%88%E5%92%8C%E5%88%86%E7%BB%84%E8%BF%90%E7%AE%97%2F</url>
    <content type="text"><![CDATA[Python和pandas可以利用任何可以接受pandas对象或NumPy数组的函数执行复杂的分组运算： 根据一个或多个键(可以是函数、数组或DataFrame列名)拆分pandas对象 计算分组摘要统计，如计数、平均数、标准差或用户自定义函数 对DataFrame的列应用各种各样的函数 应用组内转换或其他运算，如规格化、线性回归、排名或选取子集等 计算透视表或交叉表 执行分位数分析以及其他分组分析 GroupBy技术pandas对象(无论是Series、DataFrane还是其他)中的数据会根据提供的一个或多个键被拆分(split)为多组。拆分操作是在对象的特定轴上执行。然后将一个函数应用(apply)到各个分组并产生一个新值。最后这些函数的执行结果会被合并(combine)到最终结果中。结果对象的形式一般取决于数据上所执行的操作。 split-apply-combine(分组聚合) 分组键可以有多种形式，且类型不必相同(后三种只是快捷方式其最终目的是产生一组用于拆分对象的值)： 列表或数组，其长度和待分组轴一样 表示DataFrame某个列名的值 字典或Series，给出待分组轴上的值或分组名之间的对应关系 函数、用于处理轴索引或索引中的各个标签 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152In [4]: df = DataFrame(&#123;'key1':list('aabba'), ...: 'key2':['one','two','one','two','one'], ...: 'data1':np.random.randn(5), ...: 'data2':np.random.randn(5)&#125;) ...:In [5]: dfOut[5]: data1 data2 key1 key20 -0.007051 1.406114 a one1 1.136247 1.320876 a two2 0.285600 -2.665997 b one3 1.578314 0.772522 b two4 0.263382 -0.067916 a oneIn [6]: #按key1进行分组，并计算data1的平均值In [7]: #访问data1，并根据key1调用groupbyIn [8]: grouped = df['data1'].groupby(df['key1'])In [9]: grouped #变量grouped是一个GroupBy对象。它没有进行任何计算，该对象已经有了接下来对分组执行运算所需的一切信息Out[9]: &lt;pandas.core.groupby.SeriesGroupBy object at 0x11292e4e0&gt;In [10]: #调用GroupBy的mean方法计算分组平均值In [11]: grouped.mean()#数据(Series)根据分组键进行了聚合，产生新的一个Series，其索引为key1列中的唯一值Out[11]:key1a 0.464193b 0.931957Name: data1, dtype: float64In [12]: # 传入多个数组In [15]: means = df['data1'].groupby([df['key1'],df['key2']]).mean()#通过两个键对数据进行了分组，得到一个Series具有层次化索引(由唯一的建对组成)In [16]: meansOut[16]:key1 key2a one 0.128166 two 1.136247b one 0.285600 two 1.578314Name: data1, dtype: float64In [17]: means.unstack()Out[17]:key2 one twokey1a 0.128166 1.136247b 0.285600 1.578314 以上分组键均为Series，分组键可以是任何 长度适当 的数组，也可以将列名(可以是字符串、数字或其他Python对象)用作分组键:1234567891011121314151617181920212223242526272829303132In [18]: states=np.array(['Ohio','California','California','Ohio','Ohio'])In [19]: years = np.array([2005,2005,2006,2005,2006])In [20]: df['data1'].groupby([states,years]).mean()#分组键为适当长度的数组Out[20]:California 2005 1.136247 2006 0.285600Ohio 2005 0.785631 2006 0.263382Name: data1, dtype: float64In [21]: # 分组键为列名In [22]: df.groupby('key1').mean()Out[22]: data1 data2key1a 0.464193 0.886358b 0.931957 -0.946738In [23]: # 结果中没有key2列是因为这一列不是数值数据，会从结果中排除。默认情况所有数值列都会被聚合In [24]: df.groupby(['key1','key2']).mean()Out[24]: data1 data2key1 key2a one 0.128166 0.669099 two 1.136247 1.320876b one 0.285600 -2.665997 two 1.578314 0.772522 对分组进行迭代GroupBy对象支持迭代，可以产生一组二元元组(由分组名和数据块组成):123456789101112131415In [26]: for name, group in df.groupby('key1'): ...: print('分组名',name) ...: print('数据块\n',group) ...:分组名 a数据块 data1 data2 key1 key20 -0.007051 1.406114 a one1 1.136247 1.320876 a two4 0.263382 -0.067916 a one分组名 b数据块 data1 data2 key1 key22 0.285600 -2.665997 b one3 1.578314 0.772522 b two 对于多重键的情况，元组的第一个元素将会是由键值组成的元组：12345678910111213141516171819202122232425262728293031323334353637In [28]: for (k1,k2), group in df.groupby(['key1','key2']): ...: print(k1,k2) ...: print(group) ...:a one data1 data2 key1 key20 -0.007051 1.406114 a one4 0.263382 -0.067916 a onea two data1 data2 key1 key21 1.136247 1.320876 a twob one data1 data2 key1 key22 0.2856 -2.665997 b oneb two data1 data2 key1 key23 1.578314 0.772522 b twoIn [33]: # 将数据片段转化成一个字典In [34]: pieces = dict(list(df.groupby('key1')))In [35]: pieces['a']Out[35]: data1 data2 key1 key20 -0.007051 1.406114 a one1 1.136247 1.320876 a two4 0.263382 -0.067916 a oneIn [36]: piecesOut[36]:&#123;'a': data1 data2 key1 key2 0 -0.007051 1.406114 a one 1 1.136247 1.320876 a two 4 0.263382 -0.067916 a one, 'b': data1 data2 key1 key2 2 0.285600 -2.665997 b one 3 1.578314 0.772522 b two&#125; groupby默认是在axis=0(DataFrame行)上进行分组，通过设置可以在其他任何轴上进行分组：1234567891011121314151617In [42]: # 通过dtype对列进行分组In [43]: grouped = df.groupby(df.dtypes, axis=1)In [44]: dict(list(grouped))Out[44]:&#123;dtype('float64'): data1 data2 0 -0.007051 1.406114 1 1.136247 1.320876 2 0.285600 -2.665997 3 1.578314 0.772522 4 0.263382 -0.067916, dtype('O'): key1 key2 0 a one 1 a two 2 b one 3 b two 4 a one&#125; 选取一个或一组列对于由DataFrame产生的GroupBy对象，如果用一个(单个字符串)或一组(字符串数组)列名对其进行索引，就能实现选取部分列进行聚合的目的：df.groupby(&#39;key1&#39;)[&#39;data1&#39;]相当于df[&#39;data1&#39;].groupby(&#39;key1&#39;)；df.groupby(&#39;key1&#39;)[[&#39;data2&#39;]]相当于df[[&#39;data2&#39;]].groupby(df[&#39;key1&#39;])。对于大数据集很可能只需要对部分列进行聚合，这种索引操作返回的对象是一个DataFrame(如果传入的是列表或数组)或已分组的Series(如果传入的是标量形式的单个列名):123456789101112131415161718192021222324In [46]: # 计算data2列的平均值并以DataFrame形式得到结果In [47]: df.groupby(['key1','key2'])[['data2']].mean()Out[47]: data2key1 key2a one 0.669099 two 1.320876b one -2.665997 two 0.772522In [48]: s_grouped= df.groupby(['key1','key2'])['data2']In [49]: s_groupedOut[49]: &lt;pandas.core.groupby.SeriesGroupBy object at 0x1119a86a0&gt;In [50]: s_grouped.mean()Out[50]:key1 key2a one 0.669099 two 1.320876b one -2.665997 two 0.772522Name: data2, dtype: float64 通过字典或Series进行分组除了数组以外，分组信息可以是 字典和 Series。Series会被看成一个固定大小的映射，如果用Series作为分组键，则pandas会检查Series以确保其索引跟分组轴对齐：123456789101112131415161718192021222324252627282930313233343536373839404142434445In [55]: people = DataFrame(np.random.randn(5,5), ...: columns=list('abcde'), ...: index=['tom','john','jim','lancy','lucy']) ...:In [56]: people.loc[2:3,['b','c']] = np.nanIn [57]: peopleOut[57]: a b c d etom 1.522909 -1.357885 -0.262730 0.180761 -1.662128john 0.941963 -0.330136 0.520398 -0.069789 0.856472jim -0.077905 NaN NaN 0.527588 -0.162906lancy -0.141172 0.699214 -0.602441 -2.306901 -0.228982lucy -1.345708 0.931025 0.615641 -0.733455 1.321982In [58]: # 字典作为分组键In [59]: mapping = &#123;'a':'red', 'b':'red', 'c':'blue', ...: 'd':'blue', 'e':'red', 'f':'orange'&#125; ...:In [60]: by_column = people.groupby(mapping, axis=1)In [61]: by_column.sum()Out[61]: blue redtom -0.081968 -1.497105john 0.450609 1.468299jim 0.527588 -0.240812lancy -2.909342 0.329059lucy -0.117814 0.907299In [64]: # Series用作分组键In [65]: map_series = Series(mapping)In [66]: people.groupby(map_series, axis=1).count()Out[66]: blue redtom 2 3john 2 3jim 1 2lancy 2 3lucy 2 3 通过函数进行分组任何作为分组键的函数都会在各个索引值上被调用一次，其返回值就会被用作分类名称；同时将函数跟数组、列表、字典、Series混用也可以，因为任何东西都会被转换为数组：1234567891011121314151617181920In [67]: # 根据人名的长度进行分组，可以仅仅传入一个len函数In [68]: people.groupby(len).sum()Out[68]: a b c d e3 1.445004 -1.357885 -0.262730 0.708349 -1.8250354 -0.403745 0.600890 1.136039 -0.803243 2.1784545 -0.141172 0.699214 -0.602441 -2.306901 -0.228982In [72]: # 将函数、列表和数组混用In [73]: key_list =['one','one','one','two','two']In [74]: people.groupby([len,key_list]).min()Out[74]: a b c d e3 one -0.077905 -1.357885 -0.262730 0.180761 -1.6621284 one 0.941963 -0.330136 0.520398 -0.069789 0.856472 two -1.345708 0.931025 0.615641 -0.733455 1.3219825 two -0.141172 0.699214 -0.602441 -2.306901 -0.228982 根据索引级别分组通过level关键字传入级别编号或名称来根据索引级别聚合层次化索引数据集:1234567891011121314151617181920In [77]: columns = pd.MultiIndex.from_arrays([['US','US','US','CH','CH'],[1,3,5,1,3]],names=['cty','tenor'])In [78]: hier_df = DataFrame(np.random.randn(4,5), columns=columns)In [79]: hier_dfOut[79]:cty US CHtenor 1 3 5 1 30 -1.544928 0.860951 -1.021428 0.150361 0.8633321 1.500901 0.411124 0.717717 -1.186560 -0.3416702 1.728200 0.507285 -0.974570 0.856083 0.6066693 0.356776 0.399169 -0.254092 0.274927 0.169848In [80]: hier_df.groupby(level='cty',axis=1).count()Out[80]:cty CH US0 2 31 2 32 2 33 2 3 数据聚合聚合 指任何能够从数组产生标量值的数据转换过程，许多常见的聚合运算都有就地计算数据集统计信息的优化实现，也可以自己定义聚合运算，还可以调用分组对象上已经定义好的任何方法。 经过优化的groupby方法 函数名 说明 count 分组中非NA值的数量 sum 非NA值的和 mean 非NA值的平均数 median 非NA值的算术中位数 std、var 无偏(分母为n-1)标准差和方差 min、max 非NA值的最小值和最大值 prod 非NA值的积 first、last 第一个和最后一个非NA值 12345678910111213141516171819In [6]: dfOut[6]: data1 data2 key1 key20 0.303363 -1.985931 a one1 -1.510949 0.351845 a two2 1.665133 -0.527562 b one3 0.851907 -0.377448 b two4 0.141499 0.969610 a oneIn [7]: grouped = df.groupby('key1')In [8]: # 使用quantile计算Series或DataFrame列的样本分位数In [9]: grouped['data1'].quantile(0.9)Out[9]:key1a 0.27099b 1.58381Name: data1, dtype: float64 quantile是一个Series方法。GroupBy会高效地对Series进行切片，然后对各片调用quantile(0.9).要使用自己写的聚集函数，需要将其传入aggregate或agg方法：12345678910In [45]: def get_to_peak(arr): ...: return arr.max()-arr.min() ...:In [46]: grouped.agg(get_to_peak)Out[46]: data1 data2key1a 1.814312 2.955541b 0.813225 0.150115 并非严格的聚合运算(如describe)也可以在这里使用：12345678910111213141516171819In [47]: grouped.describe()Out[47]: data1 \ count mean std min 25% 50% 75%key1a 3.0 -0.355362 1.004035 -1.510949 -0.684725 0.141499 0.222431b 2.0 1.258520 0.575037 0.851907 1.055214 1.258520 1.461827 data2 \ max count mean std min 25% 50%key1a 0.303363 3.0 -0.221492 1.558956 -1.985931 -0.817043 0.351845b 1.665133 2.0 -0.452505 0.106147 -0.527562 -0.490034 -0.452505 75% maxkey1a 0.660727 0.969610b -0.414976 -0.377448 面向列的多函数应用对Series或DataFrame列的聚合运算其实是使用aggregate(使用自定义函数)或调用诸如mean、std之类的方法。如果希望对不同的列使用不同的聚合函数，或一次应用多个函数可以将函数名以 字符串 的形式传入；如果传入 一组 函数或函数名，得到的DataFrame的列就会以相应的函数命名；如果传入 (name，function) 元组组成的列表，则元组的第一个元素就会被用作DataFrame的列名(二元元组列表可以看成一个有序映射)；对于DataFrame可以定义一组应用于全部列的函数或不同的列应用不同的函数，而结果DataFrame拥有层次化的列，相当于分别对各列进行聚合然后用concat将结果组装到一起，这里也可以传入带有自定义名称的元组列表；如果要对不同的列应用不同的函数，向agg传入一个从列名映射到函数的字典；只有将多个函数应用到至少一列时，DataFrame才会拥有层次化的列，使用小费数据集:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113In [5]: # 准备数据集In [6]: tips = pd.read_csv('tips.csv')In [7]: # 添加"小费占总额百分比"的列In [8]: tips['tip_pct'] = tips['tip']/tips['total_bill']In [9]: tips[:3]Out[9]: total_bill tip smoker day time size tip_pct0 16.99 1.01 No Sun Dinner 2 0.0594471 10.34 1.66 No Sun Dinner 3 0.1605422 21.01 3.50 No Sun Dinner 3 0.166587In [20]: grouped = tips.groupby(['smoker','time']) # 根据smoker和time对tips进行分组In [21]: grouped_pct = grouped['tip_pct']In [22]: grouped_pct.agg('mean') #将函数名以字符串的形式传入Out[22]:smoker timeNo Dinner 0.158653 Lunch 0.160920Yes Dinner 0.160828 Lunch 0.170404Name: tip_pct, dtype: float64In [23]: def peak_to_peak(arr): ...: # 自定义方法 ...: return arr.max() - arr.min() ...:In [24]: grouped_pct.agg(['mean','std', peak_to_peak]) # 传入一组函数或函数名，得到的DataFrame的列名为对应的函数名Out[24]: mean std peak_to_peaksmoker timeNo Dinner 0.158653 0.040458 0.235193 Lunch 0.160920 0.038989 0.193350Yes Dinner 0.160828 0.095153 0.674707 Lunch 0.170404 0.042770 0.169300In [26]: grouped_pct.agg([('foo','mean'),('bar',np.std)]) #传入一个由(name,function)组成的二元元组列表Out[26]: foo barsmoker timeNo Dinner 0.158653 0.040458 Lunch 0.160920 0.038989Yes Dinner 0.160828 0.095153 Lunch 0.170404 0.042770In [27]: # 定义一组用于全部列的函数In [28]: functions = ['count','mean','max']In [29]: result = grouped['tip_pct','total_bill'].agg(functions)In [30]: resultOut[30]: tip_pct total_bill count mean max count mean maxsmoker timeNo Dinner 106 0.158653 0.291990 106 20.095660 48.33 Lunch 45 0.160920 0.266312 45 17.050889 41.19Yes Dinner 70 0.160828 0.710345 70 21.859429 50.81 Lunch 23 0.170404 0.259314 23 17.399130 43.11In [31]: # 结果DataFrame拥有层次化的列，相当于分别对各列进行聚合，然后用concat将结果组装到一起(列名用作keys参数)In [32]: result['tip_pct']Out[32]: count mean maxsmoker timeNo Dinner 106 0.158653 0.291990 Lunch 45 0.160920 0.266312Yes Dinner 70 0.160828 0.710345 Lunch 23 0.170404 0.259314In [33]: # 传入自定义名称的元组列表In [34]: ftuples = [('Durchschnit','mean'),('Abweichung',np.var)]In [35]: grouped['tip_pct','total_bill'].agg(ftuples)Out[35]: tip_pct total_bill Durchschnit Abweichung Durchschnit Abweichungsmoker timeNo Dinner 0.158653 0.001637 20.095660 69.604821 Lunch 0.160920 0.001520 17.050889 59.587154Yes Dinner 0.160828 0.009054 21.859429 104.148753 Lunch 0.170404 0.001829 17.399130 61.958436In [36]: # 对不同的列应用不同的函数，向agg传入一个从列名映射到函数的字典In [37]: grouped.agg(&#123;'tip':np.max,'size':'sum'&#125;)Out[37]: tip sizesmoker timeNo Dinner 9.0 290 Lunch 6.7 113Yes Dinner 10.0 173 Lunch 5.0 51In [38]: grouped.agg(&#123;'tip_pct':['min','max','mean','std'],'size':'sum'&#125;)Out[38]: tip_pct size min max mean std sumsmoker timeNo Dinner 0.056797 0.291990 0.158653 0.040458 290 Lunch 0.072961 0.266312 0.160920 0.038989 113Yes Dinner 0.035638 0.710345 0.160828 0.095153 173 Lunch 0.090014 0.259314 0.170404 0.042770 51 以”无索引”的形式返回聚合数据可以通过向groupby传入as_index=False以禁用生成唯一分组键组成的索引(层次化索引):1234567In [39]: tips.groupby(['smoker','time'], as_index=False).mean()Out[39]: smoker time total_bill tip size tip_pct0 No Dinner 20.095660 3.126887 2.735849 0.1586531 No Lunch 17.050889 2.673778 2.511111 0.1609202 Yes Dinner 21.859429 3.066000 2.471429 0.1608283 Yes Lunch 17.399130 2.834348 2.217391 0.170404 分组级运算和转换聚合是分组运算的其中一种，它是数据转换的一个特例，它接受能够将一维数组简化为标量值的函数12345678910111213141516171819202122232425262728In [43]: #使用县聚合再合并的方式为一个DataFrame添加一个用于存放各索引分组平均值的列In [44]: dfOut[44]: data1 data2 key1 key20 0.568585 -0.865028 a one1 0.191774 0.063184 a two2 1.877514 -0.445805 b one3 0.834648 -0.260108 b two4 -0.250249 -1.472969 a oneIn [45]: k1_means = df.groupby('key1').mean().add_prefix('mean_')# 利用np.mean函数对两个数据列进行转换In [46]: k1_meansOut[46]: mean_data1 mean_data2key1a 0.170037 -0.758271b 1.356081 -0.352957In [47]: pd.merge(df,k1_means, left_on='key1', right_index=True)Out[47]: data1 data2 key1 key2 mean_data1 mean_data20 0.568585 -0.865028 a one 0.170037 -0.7582711 0.191774 0.063184 a two 0.170037 -0.7582714 -0.250249 -1.472969 a one 0.170037 -0.7582712 1.877514 -0.445805 b one 1.356081 -0.3529573 0.834648 -0.260108 b two 1.356081 -0.352957 transformtransform会将一个函数应用到各个分组，然后将结果放置到适当的位置上。如果各分组产生的是一个标量值，则该值就会被广播出去；transform和aggregate一样也是一个有着严格条件的特殊函数：传入的函数只能产生两种结果，要么产生一个可以广播的标量值，要么产生一个相同大小的结果数组：12345678910111213141516171819202122232425In [57]: peopleOut[57]: a b c d etom 1.522909 -1.357885 -0.262730 0.180761 -1.662128john 0.941963 -0.330136 0.520398 -0.069789 0.856472jim -0.077905 NaN NaN 0.527588 -0.162906lancy -0.141172 0.699214 -0.602441 -2.306901 -0.228982lucy -1.345708 0.931025 0.615641 -0.733455 1.321982In [49]: key=['one','two','one','two','one']In [50]: people.groupby(key).mean()Out[50]: a b c d eone -0.761405 0.060654 0.157032 -0.791220 0.330372two -0.253093 0.201181 0.138541 0.459374 0.860159In [51]: people.groupby(key).transform(np.mean)Out[51]: a b c d etom -0.761405 0.060654 0.157032 -0.791220 0.330372john -0.253093 0.201181 0.138541 0.459374 0.860159jim -0.761405 0.060654 0.157032 -0.791220 0.330372lancy -0.253093 0.201181 0.138541 0.459374 0.860159lucy -0.761405 0.060654 0.157032 -0.791220 0.330372 从各组中减去平均值：123456789101112131415161718192021In [52]: def demean(arr): ...: # 创建一个距平化函数 ...: return arr - arr.mean() ...:In [53]: dameaned = people.groupby(key).transform(demean)In [54]: dameanedOut[54]: a b c d etom -0.529944 0.771323 -1.053227 0.098752 -0.720876john 1.022316 0.626768 -0.418873 -0.901180 -0.117146jim 0.626749 0.642272 -0.110584 0.192733 -0.023483lancy -1.022316 -0.626768 0.418873 0.901180 0.117146lucy -0.096805 -1.413596 1.163811 -0.291486 0.744359In [55]: dameaned.groupby(key).mean() #检查现在分组的平均值是否为0Out[55]: a b c d eone -7.401487e-17 0.0 0.0 3.700743e-17 -3.700743e-17two 0.000000e+00 0.0 0.0 0.000000e+00 -5.551115e-17 apply:一般性的”拆分-应用-合并”apply会将待处理的对象拆分成多个片段，然后对各片段调用传入的函数，最后尝试将各片段组合到一起；如果传给apply的函数能够接受其他参数或关键字，可以将这些内容放在函数名后面一并传入:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172In [58]: def top(df, n=5, column='tip_pct'): ...: # 在指定列找出最大值，然后把这个值所在的行选取出来 ...: return df.sort_values(by=column)[-n:] ...:In [59]: top(tips, n=3)Out[59]: total_bill tip smoker day time size tip_pct67 3.07 1.00 Yes Sat Dinner 1 0.325733178 9.60 4.00 Yes Sun Dinner 2 0.416667172 7.25 5.15 Yes Sun Dinner 2 0.710345In [60]: top(tips, n=6)Out[60]: total_bill tip smoker day time size tip_pct109 14.31 4.00 Yes Sat Dinner 2 0.279525183 23.17 6.50 Yes Sun Dinner 4 0.280535232 11.61 3.39 No Sat Dinner 2 0.29199067 3.07 1.00 Yes Sat Dinner 1 0.325733178 9.60 4.00 Yes Sun Dinner 2 0.416667172 7.25 5.15 Yes Sun Dinner 2 0.710345In [61]: def top(df, n=5, column='tip_pct'): ...: # 在指定列找出最大值，然后把这个值所在的行选取出来 ...: return df.sort_values(by=column)[-n:] ...:In [62]: top(tips, n=3)Out[62]: total_bill tip smoker day time size tip_pct67 3.07 1.00 Yes Sat Dinner 1 0.325733178 9.60 4.00 Yes Sun Dinner 2 0.416667172 7.25 5.15 Yes Sun Dinner 2 0.710345In [65]: tips.groupby('smoker').apply(top) #top函数在DataFrame的各个片段上调用，然后结果由pandas.concat组装到一起，并以分 ...: 组名称进行了标记，最后得到了一个层次化索引，内层索引值来源于原DataFrameOut[65]: total_bill tip smoker day time size tip_pctsmokerNo 88 24.71 5.85 No Thur Lunch 2 0.236746 185 20.69 5.00 No Sun Dinner 5 0.241663 51 10.29 2.60 No Sun Dinner 2 0.252672 149 7.51 2.00 No Thur Lunch 2 0.266312 232 11.61 3.39 No Sat Dinner 2 0.291990Yes 109 14.31 4.00 Yes Sat Dinner 2 0.279525 183 23.17 6.50 Yes Sun Dinner 4 0.280535 67 3.07 1.00 Yes Sat Dinner 1 0.325733 178 9.60 4.00 Yes Sun Dinner 2 0.416667 172 7.25 5.15 Yes Sun Dinner 2 0.710345In [66]: # 传给apply的函数能接受其他参数或关键字，则可以将这些内容放在函数名后面In [67]: tips.groupby(['smoker','day']).apply(top,n=2,column='total_bill')Out[67]: total_bill tip smoker day time size tip_pctsmoker dayNo Fri 91 22.49 3.50 No Fri Dinner 2 0.155625 94 22.75 3.25 No Fri Dinner 2 0.142857 Sat 59 48.27 6.73 No Sat Dinner 4 0.139424 212 48.33 9.00 No Sat Dinner 4 0.186220 Sun 112 38.07 4.00 No Sun Dinner 3 0.105070 156 48.17 5.00 No Sun Dinner 6 0.103799 Thur 85 34.83 5.17 No Thur Lunch 4 0.148435 142 41.19 5.00 No Thur Lunch 5 0.121389Yes Fri 90 28.97 3.00 Yes Fri Dinner 2 0.103555 95 40.17 4.73 Yes Fri Dinner 4 0.117750 Sat 102 44.30 2.50 Yes Sat Dinner 3 0.056433 170 50.81 10.00 Yes Sat Dinner 3 0.196812 Sun 184 40.55 3.00 Yes Sun Dinner 2 0.073983 182 45.35 3.50 Yes Sun Dinner 3 0.077178 Thur 83 32.68 5.00 Yes Thur Lunch 2 0.152999 197 43.11 5.00 Yes Thur Lunch 4 0.115982 禁止分组键分组键会跟原始对象的索引共同构成结果对象中的层次化索引,向groupby传入group_keys=False禁用:1234567In [68]: tips.groupby('smoker',group_keys=False).apply(top,n=2)Out[68]: total_bill tip smoker day time size tip_pct149 7.51 2.00 No Thur Lunch 2 0.266312232 11.61 3.39 No Sat Dinner 2 0.291990178 9.60 4.00 Yes Sun Dinner 2 0.416667172 7.25 5.15 Yes Sun Dinner 2 0.710345 分位数和桶分析pandas有一些根据指定面元或样本分位数1将数据拆分成多块的工具(比如cut和qcut)，将这些函数跟groupby结合起来，能实现对数据集的桶(bucket)2或分位数(quantile)分析。 长度相等的桶 12345678910111213141516171819202122232425262728293031323334353637In [73]: # 生成随机数据集In [74]: frame = DataFrame(&#123;'data1':np.random.randn(1000), ...: 'data2':np.random.randn(1000)&#125;) ...:In [75]: factor = pd.cut(frame.data1, 4) #使用cut将其装入长度相等的桶In [76]: factor[:5]Out[76]:0 (-0.019, 1.563]1 (-1.601, -0.019]2 (1.563, 3.146]3 (-0.019, 1.563]4 (-1.601, -0.019]Name: data1, dtype: categoryCategories (4, interval[float64]): [(-3.19, -1.601] &lt; (-1.601, -0.019] &lt; (-0.019, 1.563] &lt; (1.563, 3.146]]In [77]: # 由cut返回的Factor对象可直接用于groupbyIn [81]: def get_stats(group): ...: # 返回对应调用的方法和列名 ...: return &#123;'min':group.min(),'max':group.max(), ...: 'count':group.count(),'mean':group.mean()&#125; ...:In [82]: grouped = frame.data2.groupby(factor)In [83]: grouped.apply(get_stats).unstack()Out[83]: count max mean mindata1(-3.19, -1.601] 48.0 2.748015 0.087192 -1.809659(-1.601, -0.019] 452.0 3.046213 -0.007537 -3.326031(-0.019, 1.563] 444.0 2.372145 -0.012882 -3.489221(1.563, 3.146] 56.0 3.598193 0.165933 -2.688040 大小相等的桶 使用qcut根据样本分位数得到大小相等的桶，传入labels=False即可只获取分位数的编号 1234567891011121314In [87]: # 返回分位数编号In [88]: grouping = pd.qcut(frame.data1, 10, labels=False)In [89]: gropuped = frame.data2.groupby(grouping)In [90]: grouped.apply(get_stats).unstack()Out[90]: count max mean mindata1(-3.19, -1.601] 48.0 2.748015 0.087192 -1.809659(-1.601, -0.019] 452.0 3.046213 -0.007537 -3.326031(-0.019, 1.563] 444.0 2.372145 -0.012882 -3.489221(1.563, 3.146] 56.0 3.598193 0.165933 -2.688040 透视表和交叉表透视表透视表根据一个或多个键对数据进行聚合，并根据行和列上的分组键将数据分配到各个矩形区域中。可以通过groupby功能以及(能够使用层次化索引的)重塑运算制作透视表。DataFrame有一个pivot_table方法(默认聚合类型为分组平均数)，还有一个顶层的pandas.pivot_table函数。除了能为groupby提供便利外，pivot_table还能添加分项小计(也叫做margins)，只需要传入margins=True，这会添加标签为All的行和列，其值对应于单个等级中所有数据的分组统计；要使用其他函数通过aggfunc传入；通过使用fill_value填充NA值。 pivot_table的参数 参数名 说明 values 待聚合的列的名称。默认集合所有数值列 index 用于分组的列名或其他分组键，出现在结果透视表的行 columns 用于分组的列名或其他分组键，出现在结果透视表的列 aggfunc 聚合函数或函数列表，默认为’mean’。可以是任何对groupby有效的函数 fill_value 用于替换结果列表中的缺失值 margins 添加行/列小计和总计，默认为False 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869In [97]: #根据smoker和time计算分组平均数In [98]: tips.pivot_table(index=['smoker','time'])Out[98]: size tip tip_pct total_billsmoker timeNo Dinner 2.735849 3.126887 0.158653 20.095660 Lunch 2.511111 2.673778 0.160920 17.050889Yes Dinner 2.471429 3.066000 0.160828 21.859429 Lunch 2.217391 2.834348 0.170404 17.399130In [100]: # 聚合tip_pct和size,根据day进行分组，将smoker放在列上，day放在行上In [101]: tips.pivot_table(['tip_pct','size'],index=['time','day'],columns='smoker')Out[101]: size tip_pctsmoker No Yes No Yestime dayDinner Fri 2.000000 2.222222 0.139622 0.165347 Sat 2.555556 2.476190 0.158048 0.147906 Sun 2.929825 2.578947 0.160113 0.187250 Thur 2.000000 NaN 0.159744 NaNLunch Fri 3.000000 1.833333 0.187735 0.188937 Thur 2.500000 2.352941 0.160311 0.163863In [102]: #添加分项小计，All值为平均数In [103]: tips.pivot_table(['tip_pct','size'],index=['time','day'],columns='smoker',margins=True)Out[103]: size tip_pctsmoker No Yes All No Yes Alltime dayDinner Fri 2.000000 2.222222 2.166667 0.139622 0.165347 0.158916 Sat 2.555556 2.476190 2.517241 0.158048 0.147906 0.153152 Sun 2.929825 2.578947 2.842105 0.160113 0.187250 0.166897 Thur 2.000000 NaN 2.000000 0.159744 NaN 0.159744Lunch Fri 3.000000 1.833333 2.000000 0.187735 0.188937 0.188765 Thur 2.500000 2.352941 2.459016 0.160311 0.163863 0.161301All 2.668874 2.408602 2.569672 0.159328 0.163196 0.160803In [102]: #添加分项小计，All值为平均数In [103]: tips.pivot_table(['tip_pct','size'],index=['time','day'],columns='smoker',margins=True)Out[103]: size tip_pctsmoker No Yes All No Yes Alltime dayDinner Fri 2.000000 2.222222 2.166667 0.139622 0.165347 0.158916 Sat 2.555556 2.476190 2.517241 0.158048 0.147906 0.153152 Sun 2.929825 2.578947 2.842105 0.160113 0.187250 0.166897 Thur 2.000000 NaN 2.000000 0.159744 NaN 0.159744Lunch Fri 3.000000 1.833333 2.000000 0.187735 0.188937 0.188765 Thur 2.500000 2.352941 2.459016 0.160311 0.163863 0.161301All 2.668874 2.408602 2.569672 0.159328 0.163196 0.160803In [102]: #添加分项小计，All值为平均数In [103]: tips.pivot_table(['tip_pct','size'],index=['time','day'],columns='smoker',margins=True)Out[103]: size tip_pctsmoker No Yes All No Yes Alltime dayDinner Fri 2.000000 2.222222 2.166667 0.139622 0.165347 0.158916 Sat 2.555556 2.476190 2.517241 0.158048 0.147906 0.153152 Sun 2.929825 2.578947 2.842105 0.160113 0.187250 0.166897 Thur 2.000000 NaN 2.000000 0.159744 NaN 0.159744Lunch Fri 3.000000 1.833333 2.000000 0.187735 0.188937 0.188765 Thur 2.500000 2.352941 2.459016 0.160311 0.163863 0.161301All 2.668874 2.408602 2.569672 0.159328 0.163196 0.160803 交叉表交叉表(cross-tabulation简称crosstab)是一种用于计算分组频率的特殊透视表。crosstab前两个参数可以是数组、Series或数组列表:1234567891011In [108]: pd.crosstab([tips.time,tips.day],tips.smoker,margins=True)Out[108]:smoker No Yes Alltime dayDinner Fri 3 9 12 Sat 45 42 87 Sun 57 19 76 Thur 1 0 1Lunch Fri 1 6 7 Thur 44 17 61All 151 93 244 1.样本分位数的计算方式为loc= 1+(n-1)p;n是数据个数，p是分位点。loc介于1到n之间。把数据从小到大排列，找到loc两边的值L和R，值为(R-L) (loc-floor(loc))+L，其中floor是向下取整 ↩2.长度相等指的是区间大小相等，大小相等指的是数据点数量相等 ↩]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>聚合和分组</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[绘图和可视化]]></title>
    <url>%2F2018%2F03%2F23%2F%E7%BB%98%E5%9B%BE%E5%92%8C%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[matplotlib API入门使用matplotlib时以--pylab模式打开1ipython --pylab matplotlib API的函数都位于matplotlib.pyplot模块之下，通常通过以下语句引用:1import matplotlib.pyplot as plt Figure和Subplotmatplotlib的图像都位于Figure对象中，使用plt.figure创建一个新的Figure；plt.figure的figsize选项用于确保当图片保存到磁盘时具有一定的大小和纵横比；通过plt.gcf()得到当前Figure的引用；不能在空Figure上绘图，必须通过add_subplot创建一个或多个subplot，返回的对象是AxesSubplot对象：12345678In [7]: fig = plt.figure()In [8]: ax1 = fig.add_subplot(2,2,1) #在figure实例上调用add_subplot创建一个2x2的图像，且当前选中的事第一个(编号从1 ...: 开始)In [9]: ax2 = fig.add_subplot(2,2,2)In [10]: ax3 = fig.add_subplot(2,2,4) 带有三个subplot的Figure如果这时发出一条绘图命令，matplotlib会在最后一个用过的subplot(没有则创建一个)上进行绘制,k--是一个线型选项，告诉matplotlib绘制黑色虚线图: 1234In [16]: plt.plot(randn(50).cumsum(),'k--')Out[16]: [&lt;matplotlib.lines.Line2D at 0x26fc86694a8&gt;]In [17]: plt.savefig('Figure_2.png') 绘制一次后的图像对于add_subplot返回AxesSubplot直接调用它们的实例方法就可以在对应的实例上画图了： 123456In [14]: _ = ax1.hist(randn(100),bins=20,color='k',alpha=0.3)In [15]: ax2.scatter(np.arange(30),np.arange(30)+3*randn(30))Out[15]: &lt;matplotlib.collections.PathCollection at 0x2213001c2e8&gt;In [16]: plt.savefig('Figure_3.png') 继续绘制两次之后的图像 plt.subplots创建一个新的Figure，返回一个含有已创建的subplot对象的NumPy数组，可以通过索引获取对应的AxesSubplot对象进行绘图: pyplot.subplots的选项 参数 说明 nrows subplot的行数 ncols subplot的列数 sharex 所有subplot应该使用相同的X轴刻度(调节xlim将会影响所有subplot) sharey 所有subplot应该使用相同的Y轴刻度(调节ylim将会影响所有subplot) subplot_kw 用于创建个subplot的关键字字典 **fig_kw 创建figure的其他关键字如plt.subplots(2,2,figsize=(8,6)) 123456789101112131415161718In [17]: fig,axes = plt.subplots(2,3)In [18]: axesOut[18]:array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x000002213099BDD8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x000002213088D940&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000022136CD45F8&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000022136CE7358&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000022136B9E320&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x0000022136BBB198&gt;]], dtype=object)In [19]: figOut[19]: &lt;matplotlib.figure.Figure at 0x22130a437b8&gt;In [20]: axes[0,1].scatter(np.arange(30),np.arange(30)+3*randn(30))Out[20]: &lt;matplotlib.collections.PathCollection at 0x22136ead278&gt;In [21]: fig.savefig('Figure_4.png') 通过plt.subplots创建的Figure和AxesSubplot数组 调整subplot周围的间距matplotlib会在subplot外围留下一定的边距，并在subplot之间留下一定的间距。间距和图像的高宽有关，只要图像大小调整则间距也会自动调整利用的Figure的subplots_adjust方法可以修改间距，同时它也是个顶级函数；wspace和hspace用于控制宽度和高度的百分比，可以用作subplot之间的间距。matplotlib不会检查标签是否重叠，所以只能自己设定刻度的位置和刻度标签:123In [23]: plt.subplots_adjust(wspace=0,hspace=0)In [24]: fig.savefig('Figure_5.png') 调整间距之后的图像 颜色、标记和线型matplotlib的plot函数接受一组X和Y的坐标，还可以接受一个表示color颜色和linestyle线型的字符串缩写；常用的颜色都有一个缩写词(color=&#39;g&#39;)，如果需要使用其他颜色可以指定其RGB值(‘#CECECE’)；线形图可以加上一些标记marker来强调实际的数据点；标记和颜色都可以放在格式字符串中，但标记类型和线型必须放在颜色后面：1234567891011In [29]: fig = plt.figure()In [30]: axe1 = fig.add_subplot()In [31]: plt.plot(data,'ko--')Out[31]: [&lt;matplotlib.lines.Line2D at 0x22137639198&gt;]In [32]: plt.plot(data,color='k',linestyle='dashed',marker='o')Out[32]: [&lt;matplotlib.lines.Line2D at 0x2213a7bd588&gt;]In [33]: plt.savefig('Figure_6.png') 带有标记的线型图实例非实际数据点默认是按线性方式插值的，可以通过drawstyle选项修改: 12345678In [34]: plt.plot(data,'k--',label='Default')Out[34]: [&lt;matplotlib.lines.Line2D at 0x22134d35198&gt;]In [35]: plt.plot(data,'k--',label='step-post',drawstyle='steps-post')Out[35]: [&lt;matplotlib.lines.Line2D at 0x22134d35b70&gt;]In [37]: plt.legend(loc='best')Out[37]: &lt;matplotlib.legend.Legend at 0x22134b3a240&gt; 使用不同的drawstyle选项的线型图实例 刻度、标签和图例对于大多数图标的装饰项，有两种实现方式，一是使用过程型的pyplot接口，二是使用面向对象的原生 matplotlib API。pyplot接口是为了交互式使用，它含有xlim、xticks和xticklabels等方法。分别控制图标的范围、刻度位置、刻度标签等。使用方式有两种： 调用时不带参数，则返回当前的配置值。(plt.xlim()返回当前的X轴绘图范围) 调用时带参数，则是则会参数值。(plt.xlim([0,10])将X轴的范围设置为0到10)所有方法都是对当前或最近创建的AxesSubplot起作用的，它们各自对应subplot对象的两个方法，以xlim为例对应ax.get_xlim和ax.set_xlim 设置标题、轴标签、刻度以及刻度标签要修改X轴的刻度，使用set_xticks和set_xticklabels方法。set_xticks告诉matplotlib要将刻度放在数据范围中的哪些位置，默认情况下这些位置也就是刻度标签。set_xticklabels将任何其他值用作标签；set_xlabel为X轴设置一个名称;set_title设置一个标题：12345678910111213141516171819202122In [39]: fig = plt.figure()In [40]: ax = fig.add_subplot(1,1,1)In [41]: ax.plot(randn(1000).cumsum())Out[41]: [&lt;matplotlib.lines.Line2D at 0x2213dfb8e48&gt;]In [42]: fig.savefig('Figure_8.png')In [43]: ticks = ax.set_xticks([0,250,500,700,1000])In [44]: fig.savefig('Figure_9.png')In [45]: labels = ax.set_xticklabels(['one','two','three','four','five'], rotation=30, fontsize='small')In [46]: ax.set_title('FIRST')Out[46]: &lt;matplotlib.text.Text at 0x2213f03bb70&gt;In [47]: ax.set_xlabel('STAGES')Out[47]: &lt;matplotlib.text.Text at 0x2213ddaf320&gt;In [48]: fig.savefig('Figure_10.png') 用于显示xticks的线型图 添加图例图例(legend)是一种标识图标元素的重要工具，通过在添加 subplot 的时候传入label参数，然后调用ax.legend(loc=&#39;best&#39;)；其中legend的loc参数告诉matplotlib要将图例放在哪儿。要从图例中去除一个或多个元素，不传入label或传入label=&#39;_nolegend_&#39;: 1234567891011121314151617In [51]: fig = plt.figure()In [52]: ax = fig.add_subplot(1,1,1)In [53]: ax.plot(randn(1000).cumsum(),'k',label='one')Out[53]: [&lt;matplotlib.lines.Line2D at 0x22136e7f5c0&gt;]In [54]: ax.plot(randn(1000).cumsum(),'k--',label='two')Out[54]: [&lt;matplotlib.lines.Line2D at 0x221404f6be0&gt;]In [55]: ax.plot(randn(1000).cumsum(),'k.',label='three')Out[55]: [&lt;matplotlib.lines.Line2D at 0x2213b9aba58&gt;]In [56]: ax.legend(loc='best')Out[56]: &lt;matplotlib.legend.Legend at 0x2213dffc208&gt;In [57]: fig.savefig('Figure_11.png') 带有三条线及图例的简单线型图 注解以及在Subplot上绘图注解注解可以通过text、arrow和annotate等函数添加。text可以将文本绘制在图标的指定坐标(x,y)，还可以加上一些自定义格式ax.text(x,y,&#39;Hello world!&#39;, family=&#39;monospace&#39;, fontsize=10)，注解中可以既含有文本也哈有箭头：1234567891011121314151617181920212223242526272829303132In [67]: fig = plt.figure()In [68]: ax = fig.add_subplot(1,1,1)In [69]: data = pd.read_csv('D:\Python\ipython\spx.csv',index_col=0, parse_dates=True)In [70]: spx = data['SPX']In [71]: spx.plot(ax=ax, style='k-')Out[71]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x2213f993c50&gt;In [72]: crisis_data = [ ...: (datetime(2007,10,11),'Peak of bull market'), ...: (datetime(2008,3,12),'Bear Stearbs Fails'), ...: (datetime(2008,9,15),'Lehman Bankruptcy') ...: ] ...:In [73]: for date, label in crisis_data: ...: ax.annotate(label, xy=(date,spx.asof(date)+50), ...: xytext=(date, spx.asof(date)+200), ...: arrowprops=dict(facecolor='black'), ...: horizontalalignment='left', verticalalignment='top') ...:In [74]: # 放大到2007-2010In [76]: ax.set_xlim(['1/1/2007','1/1/2011'])Out[76]: (732677.0, 734138.0)In [76]: ax.set_xlim(['1/1/2007','1/1/2011'])Out[76]: (732677.0, 734138.0) 2008-2009年金融危机期间的重要日期1 绘图matplotlib有一些表示常见图形的对象被称为 块(patch)，完整的集合位于matplotlib.patches中，有些可以在matplotlib.pyplot中找到(如Rectangle和Circle)。要在图表中添加一个图形，需要先创建一个块对象shp，然后通过ax.add_patch(shp)将其添加到subplot中:1234567891011121314151617181920In [81]: fig = plt.figure()In [82]: ax = fig.add_subplot(1,1,1)In [83]: rect = plt.Rectangle((0.2,0.75),0.4,0.15,color='k',alpha=0.3)In [84]: circ = plt.Circle((0.7,0.2),0.15,color='b',alpha=0.3)In [85]: pgon = plt.Polygon([[0.15,0.15],[0.35,0.4],[0.2,0.6]],color='g',alpha=0.5)In [86]: ax.add_patch(rect)Out[86]: &lt;matplotlib.patches.Rectangle at 0x22142e4a908&gt;In [87]: ax.add_patch(circ)Out[87]: &lt;matplotlib.patches.Circle at 0x22140caa6d8&gt;In [88]: ax.add_patch(pgon)Out[88]: &lt;matplotlib.patches.Polygon at 0x2214117b940&gt;In [89]: fig.savefig('Figure_13.png') 由三块图形组成的图 将图表保存到文件利用plt.savefig可以将当前图表保存到文件，其相当于Figure对象的实例方法savefig。文件类型是通过文件扩展名推断出来的。可以通过dpi参数控制分辨率，通过bbox_inches剪除当前图表周围的空白部分plt.savefig(&#39;filepath.png&#39;, dpi=400,bbox_inches=&#39;tight&#39;); savepig的选项 参数 说明 fname 含有文件路径的字符串或Python的文件型对象。图像格式由文件扩展名推断得出 dpi 图像分辨率(每英寸点数),默认为100 facecolor、edgecolor 图像的背景色，默认为”w”(白色) format 显式设置文件格式(png/pdf/svg/ps….) bbox_inches 图表需要保存的部分。如果设置为”tight”则尝试剪除图表周围的空白部分 matplotlib配置matplotlib自带一些配色方案，几乎所有默认行为都能通过一组全局参数进行自定义，它们可以管理图像大小、subplot边距、配色方案、字体大小、网格类型等。操作matplotlib配置系统可以使用rc方法，rc第一个参数是希望自定义的对象，其后可以跟上一系列的关键字参数，可以将这些选项写成一个字典:123456In [100]: font_options=&#123;'family':'monospace', ...: 'weight':'bold', ...: 'size':10&#125; ...:In [101]: plt.rc('font', **font_options) pandas中的绘图函数matplotlib是一种比较低级的工具，要组装一张图表，得使用它的各种基础组件才行：数据展示(即图标类型：线型图、柱状图、盒型图、散布图、等值线图等)、图例、标题、刻度标签以及其他注解型信息。在pandas中因为已经有行标签、列标签以及分组信息，所以能够看利用DataFrame对象数据组织特点来创建标准图表的高级方法 Series.plot方法的餐素 参数 说明 label 用于图例的标签 ax 要在其上进行绘制的matplotlib subplot对象。如果没有设置，则使用当前matplotlib subplot style 将要传给matplotlib的风格字符串(如’ko—‘) alpha 图表的填充不透明度(0到1之间) kind 可以是’line’、’bar’、’barh’、’kde’ logy 在Y轴上使用对数标签 use_index 将对象的索引用作刻度标签 rot 旋转刻度标签(0到360) xticks 用作X轴刻度的值 yticks 用作Y轴刻度的值 xlim X轴的界限(例如[0,10]) ylim Y轴的界限 grid 显示轴网格线(默认打开) 专用于DataFrame的plot参数 参数 说明 subplots 将各个DataFrame列绘制到单独的subplot中 sharex 如果subplots=True，则共用同一个X轴，包括刻度和界限 sharey 如果subplots=False，则共用同一个Y轴 figsize 表示图像大小的元组 title 表示图像标题的字符串 legend 添加一个subplot图例(默认为True) sort_columns 以字母顺序绘制各列，默认使用当前列顺序 线型图Series和DataFrame都有一个生成各类图表的plot方法，默认情况下它们生成的是线型图。Series对象的索引会被传给matplotlib，并用以绘制X轴；通过use_index=False禁用该功能；X轴的刻度和界限可以通过xticks和xlim选项进行调节;Y轴用yticks和ylim：1234In [10]: s = Series(np.random.randn(10).cumsum(),index=np.arange(0,100,10))In [11]: s.plot()Out[11]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x10a781588&gt; 简单的Series图表 pandas的大部分绘图方法有一个可选的ax参数，它可以是一个matplotlib的subplot对象。这使得在网格布局中更灵活地处理subplot的位置，DataFrame的plot方法会在一个subplot中为各列绘制一条线，并自动创建图例:1234567In [16]: df = DataFrame(np.random.randn(10,4).cumsum(0), ...: columns=['A','B','C','D'], ...: index=np.arange(0,100,10)) ...:In [17]: df.plot()Out[17]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x10b8704a8&gt; 简单的DataFrame图表 柱状图在生成线型图的代码中加上kind=&#39;bar&#39;(垂直柱状图)或kind=&#39;barh&#39;(水平柱状图)即可生成柱状图。Series和DataFrame的索引会被用作X(bar)或Y(barh)刻度: 1234567891011In [26]: fig, axes = plt.subplots(2,1)In [27]: data = Series(np.random.rand(16),index=list('abcdefghijklmnop'))In [28]: data.plot(kind='bar', ax=axes[0],color='k', alpha=0.7)Out[28]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x1186e90b8&gt;In [29]: data.plot(kind='barh', ax=axes[1],color='k', alpha=0.7)Out[29]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x118bd52e8&gt;In [30]: fig.savefig('Figure_16.png') Series柱状图 对于DataFrame，柱状图会将每一行的值分为一组，使用stacked=True会使每行的值堆积在一起:123456789101112131415161718192021222324In [34]: df = DataFrame(np.random.rand(6,4), ...: index=['r1','r2','r3','r4','r5','r6'], ...: columns=pd.Index(['A','B','C','D'],name='Genus')) ...:In [35]: dfOut[35]:Genus A B C Dr1 0.820633 0.228560 0.550244 0.236735r2 0.968056 0.268256 0.539757 0.453413r3 0.827034 0.905531 0.722537 0.368674r4 0.433393 0.154539 0.223513 0.340216r5 0.037282 0.609635 0.609266 0.172542r6 0.359212 0.399398 0.044828 0.712773In [36]: fig, axes = plt.subplots(2,1)In [37]: df.plot(kind='bar',ax=axes[0])Out[37]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x11eecb198&gt;In [38]: df.plot(kind='barh',ax=axes[1],stacked=True, alpha=0.5)Out[38]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x11e691048&gt;In [39]: fig.savefig('Figure_17.png') DataFrame柱状图 直方图和密度图直方图是一种可以对值频率进行离散化显示的柱状图。数据点被拆分到离散的、间隔均匀的面元中，绘制的是各面元中数据点的数量,通过hist方法绘制“小费占消费总额的百分比”(数据集):123456In [75]: tips = pd.read_csv('/Users/dengxiaojun/Documents/pydata-book-2nd-edition/examples/tips.csv')In [76]: tips['tip_pct'] = tips['tip']/tips['total_bill']In [77]: tips['tip_pct'].hist(bins=50)Out[77]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x10ce1c240&gt; 小费百分比的直方图 与此相关的是密度图，它通过计算“可能会产生观测数据的连续概率分布的估计”而产生。一般的过程是将该分布近似为一组核(即诸如正态(高斯)分布之类的较为简单的分布),t通过调用plot的kind=&#39;kde&#39;生成:12In [91]: tips['tip_pct'].plot(kind='kde')Out[91]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x120178ba8&gt; 小费百分比密度图 这两种图表常常被画在一起。直方图以规格化形式给出(以便给出面元化密度)，然后再在其上绘制核密度估计:1234567891011In [98]: comp1 = np.random.normal(0,1,size=200)In [99]: comp2 = np.random.normal(10,2,size=200)In [100]: values = Series(np.concatenate([comp1,comp2]))In [101]: values.hist(bins=100, alpha=0.3, color='k', density=True)Out[101]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x119e863c8&gt;In [102]: values.plot(kind='kde', style='k--')Out[102]: &lt;matplotlib.axes._subplots.AxesSubplot at 0x119e863c8&gt; 带有密度估计的规格化直方图 散布图散布图是观察两个一维数据序列之间的关系的有效手段。matplotlib的scatter方法是绘制散布图的主要方法，(加载数据集选择其中几列，计算对数差:1234567891011121314151617181920In [114]: macro = pd.read_csv('/Users/dengxiaojun/Documents/pydata-book-2nd-edition/examples/macrodata.csv')In [115]: data = macro[['cpi','m1','tbilrate','unemp']]In [116]: trans_data = np.log(data).diff().dropna()In [117]: trans_data[-5:]Out[117]: cpi m1 tbilrate unemp198 -0.007904 0.045361 -0.396881 0.105361199 -0.021979 0.066753 -2.277267 0.139762200 0.002340 0.010286 0.606136 0.160343201 0.008419 0.037461 -0.200671 0.127339202 0.008894 0.012202 -0.405465 0.042560In [118]: plt.scatter(trans_data['m1'],trans_data['unemp'])Out[118]: &lt;matplotlib.collections.PathCollection at 0x11b24c780&gt;In [119]: plt.title('Changes in log %s vs .log %s' % ('m1', 'unemp'))Out[119]: Text(0.5,1,'Changes in log m1 vs .log unemp') 一张简单的散布图 在探索式数据分析工作中，同时观察一组变量的散布图很有意义，这也被称为散布矩阵。pandas.plotting提供了一个能从DataFrame创建散布图矩阵的scatter_matrix函数，它还支持在对角线上放置各变量的直方图或密度图:12345678910111213141516171819In [121]: pd.plotting.scatter_matrix(trans_data,diagonal='kde',color='k',alpha=0.4)Out[121]:array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x11b3591d0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x11b499dd8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x11b857f98&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x11b3e7eb8&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x11b383390&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x11b383208&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x11b873da0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x11d0574e0&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x119b73f98&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x119b75f28&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x119afddd8&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x119ae6128&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x119af3128&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x1261c6860&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x11ac44b70&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x126938ac8&gt;]], dtype=object) 散布图矩阵 1.数据来源 ↩]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>可视化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据规整化(二)]]></title>
    <url>%2F2018%2F03%2F22%2F%E6%95%B0%E6%8D%AE%E8%A7%84%E6%95%B4%E5%8C%96-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[重塑和轴向旋转重塑层次化索引层次化索引为DataFrame数据的重排提供了具有良好一致性的方式。主要功能包括： stack:将数据的列”旋转”为行 unstack:将数据的行”旋转”为列 stack对于一个简单的DataFrame使用stack方法会得到一个Series，stack在运算的时候默认会滤除缺失值，指定dropna=False可以保留： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172In [112]: data = DataFrame(np.arange(6).reshape((2,3)), ...: index=pd.Index(['row1','row2'],name='row'), ...: columns=pd.Index(['col1','col2','col3'],name='col')) ...:In [113]: dataOut[113]:col col1 col2 col3rowrow1 0 1 2row2 3 4 5In [114]: resulst = data.stack()In [115]: resulstOut[115]:row colrow1 col1 0 col2 1 col3 2row2 col1 3 col2 4 col3 5dtype: int32In [124]: s1 = Series(np.arange(4),index=list('abcd'))In [125]: s2 = Series([4,5,6],index=list('cde'))In [126]: data2 = pd.concat([s1,s2],keys=['one','two'])In [127]: data2Out[127]:one a 0 b 1 c 2 d 3two c 4 d 5 e 6dtype: int64In [129]: data2.unstack()Out[129]: a b c d eone 0.0 1.0 2.0 3.0 NaNtwo NaN NaN 4.0 5.0 6.0In [130]: data2.unstack().stack()Out[130]:one a 0.0 b 1.0 c 2.0 d 3.0two c 4.0 d 5.0 e 6.0dtype: float64In [132]: data2.unstack().stack(dropna=False)Out[132]:one a 0.0 b 1.0 c 2.0 d 3.0 e NaNtwo a NaN b NaN c 4.0 d 5.0 e 6.0dtype: float64 unstackunstack可以将stack方法得到的Series重排为一个DataFrame；默认情况下，unstack操作的是最内层。传入分层级别的编号或名称即可对其他级别进行unstack操作，如果不是所有级别值都能在各分组中找到的话会引入缺失值；在对DataFrame进行unstack操作时，作为旋转轴的级别将会成为结果中的最低级别: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182In [140]: resulstOut[140]:row colrow1 col1 0 col2 1 col3 2row2 col1 3 col2 4 col3 5dtype: int32In [141]: resulst.unstack()Out[141]:col col1 col2 col3rowrow1 0 1 2row2 3 4 5In [142]: resulst.unstack(0)Out[142]:row row1 row2colcol1 0 3col2 1 4col3 2 5In [143]: resulst.unstack('row')Out[143]:row row1 row2colcol1 0 3col2 1 4col3 2 5In [144]: data2Out[144]:one a 0 b 1 c 2 d 3two c 4 d 5 e 6dtype: int64In [145]: data2.unstack()Out[145]: a b c d eone 0.0 1.0 2.0 3.0 NaNtwo NaN NaN 4.0 5.0 6.0In [151]: df = DataFrame(&#123;'left':resulst,'right':resulst+5&#125;, ...: columns=pd.Index(['left','right'], name='side')) ...:In [152]: dfOut[152]:side left rightrow colrow1 col1 0 5 col2 1 6 col3 2 7row2 col1 3 8 col2 4 9 col3 5 10In [153]: df.unstack('col')Out[153]:side left rightcol col1 col2 col3 col1 col2 col3rowrow1 0 1 2 5 6 7row2 3 4 5 8 9 10In [154]: df.unstack('col').stack('side')Out[154]:col col1 col2 col3row siderow1 left 0 1 2 right 5 6 7row2 left 3 4 5 right 8 9 10 将“长格式”旋转为“宽格式”先预处理实验数据，首先加载数据，使用PeriodIndex生成新的索引，使用Index选取索引然后使用reindex方法根据新的列索引生成数据，然后修改数据的行索引为时间索引，最后生成需要的数据:123456789In [217]: periods = pd.PeriodIndex(year=data.year, quarter=data.quarter,name='date')In [218]: colums = pd.Index(['realgdp','infl','unemp'],name='item')In [219]: data = data.reindex(columns=colums)In [220]: data.index = periods.to_timestamp('D','end')In [221]: ldata = data.stack().reset_index().rename(columns=&#123;0:'value'&#125;) pivot第一个参数是行索引的列名，第二个参数是列索引的列名，最后一个参数值用于填充DataFrame的数据列的列名，如果忽略最后一个参数得到的DataFrame就会带有层次化的列:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364In [233]: ldata[:10]Out[233]: date item value0 1959-03-31 realgdp 2710.3491 1959-03-31 infl 0.0002 1959-03-31 unemp 5.8003 1959-06-30 realgdp 2778.8014 1959-06-30 infl 2.3405 1959-06-30 unemp 5.1006 1959-09-30 realgdp 2775.4887 1959-09-30 infl 2.7408 1959-09-30 unemp 5.3009 1959-12-31 realgdp 2785.204In [234]: pivoted = ldata.pivot('date','item','value')In [235]: pivoted.head()Out[235]:item infl realgdp unempdate1959-03-31 0.00 2710.349 5.81959-06-30 2.34 2778.801 5.11959-09-30 2.74 2775.488 5.31959-12-31 0.27 2785.204 5.61960-03-31 2.31 2847.699 5.2In [236]: ldata['value2'] = np.random.randn(len(ldata))In [237]: ldata[:10]Out[237]: date item value value20 1959-03-31 realgdp 2710.349 0.9445991 1959-03-31 infl 0.000 0.2441792 1959-03-31 unemp 5.800 0.0558303 1959-06-30 realgdp 2778.801 1.1825204 1959-06-30 infl 2.340 0.2663595 1959-06-30 unemp 5.100 -0.8817426 1959-09-30 realgdp 2775.488 0.0070217 1959-09-30 infl 2.740 -1.1717928 1959-09-30 unemp 5.300 0.0073569 1959-12-31 realgdp 2785.204 0.631422In [238]: pivoted = ldata.pivot('date','item')In [239]: pivoted[:5]Out[239]: value value2item infl realgdp unemp infl realgdp unempdate1959-03-31 0.00 2710.349 5.8 0.244179 0.944599 0.0558301959-06-30 2.34 2778.801 5.1 0.266359 1.182520 -0.8817421959-09-30 2.74 2775.488 5.3 -1.171792 0.007021 0.0073561959-12-31 0.27 2785.204 5.6 0.136254 0.631422 -0.8505161960-03-31 2.31 2847.699 5.2 -2.338798 0.897056 0.296124In [240]: pivoted['value2'][:5]Out[240]:item infl realgdp unempdate1959-03-31 0.244179 0.944599 0.0558301959-06-30 0.266359 1.182520 -0.8817421959-09-30 -1.171792 0.007021 0.0073561959-12-31 0.136254 0.631422 -0.8505161960-03-31 -2.338798 0.897056 0.296124 相当于使用set_index创建层次化索引，再用unstack重塑:12345678910In [243]: ldata.set_index(['date','item']).unstack('item')[:5]Out[243]: value value2item infl realgdp unemp infl realgdp unempdate1959-03-31 0.00 2710.349 5.8 0.244179 0.944599 0.0558301959-06-30 2.34 2778.801 5.1 0.266359 1.182520 -0.8817421959-09-30 2.74 2775.488 5.3 -1.171792 0.007021 0.0073561959-12-31 0.27 2785.204 5.6 0.136254 0.631422 -0.8505161960-03-31 2.31 2847.699 5.2 -2.338798 0.897056 0.296124 数据转换前面描述的均为数据重排，而另一类重要操作则是过滤、清理以及其他转换工作。 移除重复数据DataFrame的duplicated方法返回一个布尔型Series，表示各行是否有重复行；drop_duplicates方法用于返回一个移除了重复行的DataFrame。这两个方法默认会判断全部列，可以传入一个列表指定部分列作为重复项判断标准，即根据某些列过滤重复项；两个方法默认保留第一个出现的组合，传入keep=&#39;last&#39;1则会保留最后一个：1234567891011121314151617181920212223242526272829303132333435363738394041In [18]: data = DataFrame(&#123;'k1':['one']*3+['two']*2, ...: 'k2':[1,1,2,3,3,]&#125;) ...:In [19]: dataOut[19]: k1 k20 one 11 one 12 one 23 two 34 two 3In [20]: data.duplicated()Out[20]:0 False1 True2 False3 False4 Truedtype: boolIn [21]: data.drop_duplicates()Out[21]: k1 k20 one 12 one 23 two 3In [22]: data.drop_duplicates(['k1'])Out[22]: k1 k20 one 13 two 3In [23]: data.drop_duplicates(['k2','k1'],keep='last')Out[23]: k1 k21 one 12 one 24 two 3 利用函数或映射进行数据转换Series的map方法可以接受一个函数或含有映射关系的字典型对象，使用map是一种实现元素级转换以及其他数据清理工作的便捷方式。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354In [48]: data = DataFrame(&#123;'food':['bacon','pulled pork','bacon','Pastrami','corned beef', ...: 'Bacon','pastrami','honey ham','nava lox'], ...: 'ounces':[4,3,12,6,7.5,8,3,5,6]&#125;) ...:In [49]: meat_to_animal=&#123; ...: 'bacon':'pig', ...: 'pulled pork':'pig', ...: 'pastrami':'cow', ...: 'corned beef':'cow', ...: 'honey ham':'pig', ...: 'nava lox':'salmon' ...: &#125;In [50]: dataOut[50]: food ounces0 bacon 4.01 pulled pork 3.02 bacon 12.03 Pastrami 6.04 corned beef 7.55 Bacon 8.06 pastrami 3.07 honey ham 5.08 nava lox 6.0In [51]: data['animal']=data['food'].map(str.lower).map(meat_to_animal)In [52]: dataOut[52]: food ounces animal0 bacon 4.0 pig1 pulled pork 3.0 pig2 bacon 12.0 pig3 Pastrami 6.0 cow4 corned beef 7.5 cow5 Bacon 8.0 pig6 pastrami 3.0 cow7 honey ham 5.0 pig8 nava lox 6.0 salmonIn [53]: data['food'].map(lambda x: meat_to_animal[x.lower()])Out[53]:0 pig1 pig2 pig3 cow4 cow5 pig6 cow7 pig8 salmonName: food, dtype: object 替换值replace方法提供了一种实现替换功能的更简单、更灵活的方式；replace支持一次性替换多个值，只需要传入一个由待替换值组成的列表以及一个替换值；如果希望对不同的值进行不同的替换，则传入一个由替换关系组成的列表即可，也可以是字典：123456789101112131415161718192021222324252627282930313233343536373839404142434445In [54]: data = Series([1,0,2,0,3,2,0])In [55]: data.replace(0,-1)Out[55]:0 11 -12 23 -14 35 26 -1dtype: int64In [56]: data.replace([0,2],-1)Out[56]:0 11 -12 -13 -14 35 -16 -1dtype: int64In [57]: data.replace([0,2],[-1,9])Out[57]:0 11 -12 93 -14 35 96 -1dtype: int64In [58]: data.replace(&#123;0:-1,1:9&#125;)Out[58]:0 91 -12 23 -14 35 26 -1dtype: int64 重命名轴索引和Series相同，轴标签也可以通过函数或映射进行转换，从而得到一个新对象。轴还可以被就地修改而无需新建一个数据结构，这些都可以使用map方法实现：1234567891011121314151617181920In [63]: data = DataFrame(np.arange(9).reshape((3,3)), ...: index = ['row1','row2','row4'], ...: columns=['one','two','three']) ...:In [64]: dataOut[64]: one two threerow1 0 1 2row2 3 4 5row4 6 7 8In [65]: data.index = data.index.map(str.upper)In [66]: dataOut[66]: one two threeROW1 0 1 2ROW2 3 4 5ROW4 6 7 8 如果要创建数据集的转换版(而不是修改原始数据)，可以使用rename方法；rename方法可以结合字典型对象实现对部分轴标签的更新；如果希望就地修改可以传入implace=True：12345678910111213141516171819202122In [72]: data.rename(index=str.title, columns=str.upper)Out[72]: ONE TWO THREERow1 0 1 2Row2 3 4 5Row4 6 7 8In [73]: data.rename(index=&#123;'ROW1':'ROW'&#125;, columns=&#123;'three':'col3'&#125;)Out[73]: one two col3ROW 0 1 2ROW2 3 4 5ROW4 6 7 8In [74]: data.rename(index=&#123;'ROW1':'ROW'&#125;, columns=&#123;'three':'col3'&#125;,inplace=True)In [75]: dataOut[75]: one two col3ROW 0 1 2ROW2 3 4 5ROW4 6 7 8 离散化和面元划分为了便于分析，连续的数据常常被离散化或拆分为“面元”。cut函数可以实现将数据划分为面元；其返回的是一个特殊的Categorical对象，相当于一组表示面元名称的字符串。其中categories表示不同分类的名称,codes属性表示各个数据所属分组的标号。和“区间”的数学符号一样，圆括号表示开端，而方括号则被考试闭端(包括)，哪边是闭端可以使用right=True来确定。可以通过labels参数设置自己的面元名称。如果向cut传入的是面元的数量而不是确切的面元边界，则会根据最大值和最小值来计算等长面元：12345678910111213141516171819202122232425262728293031323334353637383940414243444546In [96]: ages = [20,22,25,27,21,23,47,54,35,37,32]In [97]: bins = [18,25,35,60,100]In [98]: cats = pd.cut(ages,bins)In [99]: catsOut[99]:[(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], ..., (35, 60], (35, 60], (25, 35], (35, 60], (25, 35]]Length: 11Categories (4, interval[int64]): [(18, 25] &lt; (25, 35] &lt; (35, 60] &lt; (60, 100]]In [100]: cats.categoriesOut[100]:IntervalIndex([(18, 25], (25, 35], (35, 60], (60, 100]] closed='right', dtype='interval[int64]')In [101]: cats.codesOut[101]: array([0, 0, 0, 1, 0, 0, 2, 2, 1, 2, 1], dtype=int8)In [102]: cats = pd.cut(ages,bins,right=False)In [103]: catsOut[103]:[[18, 25), [18, 25), [25, 35), [25, 35), [18, 25), ..., [35, 60), [35, 60), [35, 60), [35, 60), [25, 35)]Length: 11Categories (4, interval[int64]): [[18, 25) &lt; [25, 35) &lt; [35, 60) &lt; [60, 100)]In [104]: group_names=['Youth','YoungAdult','MiddleAged','Senior']In [105]: cats = pd.cut(ages,bins,labels=group_names,right=False)In [106]: catsOut[106]:[Youth, Youth, YoungAdult, YoungAdult, Youth, ..., MiddleAged, MiddleAged, MiddleAged, MiddleAged, YoungAdult]Length: 11Categories (4, object): [Youth &lt; YoungAdult &lt; MiddleAged &lt; Senior]In [107]: data = np.random.rand(20)In [108]: pd.cut(data,4,precision=2)Out[108]:[(0.75, 0.98], (0.75, 0.98], (0.75, 0.98], (0.52, 0.75], (0.52, 0.75], ..., (0.29, 0.52], (0.061, 0.29], (0.061, 0.29], (0.52, 0.75], (0.52, 0.75]]Length: 20Categories (4, interval[float64]): [(0.061, 0.29] &lt; (0.29, 0.52] &lt; (0.52, 0.75] &lt; (0.75, 0.98]] qcut是一个类似于cut的函数，它可以根据样本分位数对数据进行面元划分，可以得到大小基本相等的面元，和cut相同它可以自定义分位数：1234567891011121314151617181920212223In [109]: data = np.random.rand(1000)In [110]: cats = pd.qcut(data,4) #按四分位数进行切割In [111]: catsOut[111]:[(0.499, 0.749], (0.263, 0.499], (0.263, 0.499], (0.499, 0.749], (0.263, 0.499], ..., (0.263, 0.499], (0.499, 0.749], (-0.000892, 0.263], (0.749, 0.999], (0.263, 0.499]]Length: 1000Categories (4, interval[float64]): [(-0.000892, 0.263] &lt; (0.263, 0.499] &lt; (0.499, 0.749] &lt; (0.749, 0.999]]In [112]: pd.value_counts(cats)Out[112]:(0.749, 0.999] 250(0.499, 0.749] 250(0.263, 0.499] 250(-0.000892, 0.263] 250dtype: int64In [113]: pd.qcut(data,[0,0.1,0.5,0.9,1])Out[113]:[(0.499, 0.909], (0.104, 0.499], (0.104, 0.499], (0.499, 0.909], (0.104, 0.499], ..., (0.104, 0.499], (0.499, 0.909], (0.104, 0.499], (0.499, 0.909], (0.104, 0.499]]Length: 1000Categories (4, interval[float64]): [(-0.000892, 0.104] &lt; (0.104, 0.499] &lt; (0.499, 0.909] &lt; (0.909, 0.999]] 检测和过滤异常值异常值(孤立点或离群值)的过滤或变换运算在很大程度上其实就是数组运算,使用数组运算的方法来进行过滤：1234567891011121314151617181920212223242526272829303132333435363738394041In [123]: np.random.seed(12345)In [124]: data = DataFrame(np.random.randn(1000,4))In [125]: col = data[3]In [126]: col[np.abs(col)&gt;3] #选出绝对值大小超过3的值Out[126]:97 3.927528305 -3.399312400 -3.745356Name: 3, dtype: float64In [127]: data[(np.abs(data)&gt;3).any(1)]#选出超过3或-3的行Out[127]: 0 1 2 35 -0.539741 0.476985 3.248944 -1.02122897 -0.774363 0.552936 0.106061 3.927528102 -0.655054 -0.565230 3.176873 0.959533305 -2.315555 0.457246 -0.025907 -3.399312324 0.050188 1.951312 3.260383 0.963301400 0.146326 0.508391 -0.196713 -3.745356499 -0.293333 -0.242459 -3.056990 1.918403523 -3.428254 -0.296336 -0.439938 -0.867165586 0.275144 1.179227 -3.184377 1.369891808 -0.362528 -3.548824 1.553205 -2.186301900 3.366626 -2.372214 0.851010 1.332846In [128]: data[np.abs(data)&gt;3] = np.sign(data)*3#将值限制在区间-3到3，sign返回的是一个1和-1组成的数组，表示原始值的符号In [129]: data.describe()Out[129]: 0 1 2 3count 1000.000000 1000.000000 1000.000000 1000.000000mean -0.067623 0.068473 0.025153 -0.002081std 0.995485 0.990253 1.003977 0.989736min -3.000000 -3.000000 -3.000000 -3.00000025% -0.774890 -0.591841 -0.641675 -0.64414450% -0.116401 0.101143 0.002073 -0.01361175% 0.616366 0.780282 0.680391 0.654328max 3.000000 2.653656 3.000000 3.000000 排列和随机采样利用numpy.random.permutation函数实现对Series或DataFrame的列的排列工作(即随机重排序)。通过需要排列的轴的长度调用permitation，可产生一个表示新顺序的整数数组；如果不想用替换的方式选取随机子集可以使用permitation，其返回的数组中切下前k个元素，k为期望的子集大小；而要通过替换的方式产生样本，最快的方式是通过np.random.randint得到一组随机整数：12345678910111213141516171819202122232425262728293031323334353637383940In [130]: df = DataFrame(np.arange(5*4).reshape(5,4))In [131]: sample = np.random.permutation(5)In [132]: sampleOut[132]: array([1, 0, 2, 3, 4])In [133]: dfOut[133]: 0 1 2 30 0 1 2 31 4 5 6 72 8 9 10 113 12 13 14 154 16 17 18 19In [134]: df.take(sample)Out[134]: 0 1 2 31 4 5 6 70 0 1 2 32 8 9 10 113 12 13 14 154 16 17 18 19In [135]: df.take(np.random.permutation(len(df))[:3])Out[135]: 0 1 2 31 4 5 6 73 12 13 14 154 16 17 18 19In [136]: bag = np.array([5,3,25,-1,2])In [137]: sampler = np.random.randint(0,len(bag),size=10)In [138]: draws = bag.take(sampler)In [139]: drawsOut[139]: array([ 2, 2, 25, 25, 25, 5, -1, 5, 2, 3]) 计算指标/哑变量将分类变量转换为哑变量或指标矩阵是常用于统计建模或机器学习的转换方式。pandas的get_dummies函数可以实现如果DataFrame的某一列中含有k个不同的值来派生出一个k列矩阵或DataFrame(其值全为0或1)；如果需要给指标DataFrame的列加上一个前缀以便和其他数据进行合并可以使用prefix参数:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647In [152]: df = DataFrame(&#123;'key':list('bbacab'), ...: 'data':range(6)&#125;) ...:In [153]: dfOut[153]: data key0 0 b1 1 b2 2 a3 3 c4 4 a5 5 bIn [154]: pd.get_dummies(df['key'])Out[154]: a b c0 0 1 01 0 1 02 1 0 03 0 0 14 1 0 05 0 1 0In [155]: dummies = pd.get_dummies(df['key'],prefix='key')In [156]: dummiesOut[156]: key_a key_b key_c0 0 1 01 0 1 02 1 0 03 0 0 14 1 0 05 0 1 0In [157]: df_with_dummies = df[['data']].join(dummies)In [158]: df_with_dummiesOut[158]: data key_a key_b key_c0 0 0 1 01 1 0 1 02 2 1 0 03 3 0 0 14 4 1 0 05 5 0 1 0 字符串操作字符串对象方法 Python内置的字符串方法 方法 说明 count 返回子串在字符串中的出现次数(非重叠) endswith、startswith 如果字符串以某个后缀结尾(以某个前缀开头)，则返回True join 将字符串用作连接其他字符串序列的分隔符 index 如果在字符串中找到子串，则返回子串第一个字符所在的位置。如果没有找到，则引发ValueError find 如果在字符串中找到子串，则返回第一个发现的子串的第一个字符所在的位置。如果没有找到返回-1 rfind 如果在字符串中找到子串，则返回最后一个发现的子串的第一个字符所在的位置。如果没有找到，则返回-1 replace 用另一个字符串替换指定子串 strip、rstrip、lstrip 去除空白符(包括换行符)。 split 通过指定的分隔符将字符串拆分为一组子串 lower、upper 分别将字母字符转换为小写或大写 ljust、rjust 用空格(或其他字符)填充字符串的空白侧以返回符合最低宽度的字符串 12345678910111213141516171819202122232425262728293031323334In [2]: var = 'a, ,b, c'In [3]: var.split(',')Out[3]: ['a', ' ', 'b', ' c']In [5]: pieces = [x.strip() for x in var.split(',')]In [6]: piecesOut[6]: ['a', '', 'b', 'c']In [7]: first,sencond,third,fourth = piecesIn [8]: first+'::'+sencond+'::'+third+'::'+fourthOut[8]: 'a::::b::c'In [9]: '::'.join(pieces)Out[9]: 'a::::b::c'In [10]: var.index(':')---------------------------------------------------------------------------ValueError Traceback (most recent call last)&lt;ipython-input-10-d73873441320&gt; in &lt;module&gt;()----&gt; 1 var.index(':')ValueError: substring not foundIn [11]: var.count(',')Out[11]: 3In [12]: var.replace(',','::')Out[12]: 'a:: ::b:: c'In [13]: var.replace(',','')Out[13]: 'a b c' 正则表达式见正则表达式部分 pandas中矢量化的字符串函数 矢量化的字符串方法 方法 说明 cat 实现元素级的字符串连接操作，可指定分隔符 contains 返回表示各字符串是否含有指定模式的布尔型数组 count 模式的出现次数 endswith、startswith 相当于对各个元素执行x.endswith(patten)或x.startswith(pattern) findall 计算各字符串的模式列表 get 获取各元素的第i个字符 join 根据指定的分隔符将Series中各元素的字符串连接起来 len 计算各字符串的长度 lower、upper 转换大小写。相当于对各个元素执行x.lower()或x.upper() match 根据指定的正则表达式对各个元素执行re.match pad 在字符串的左边、右边或左右两边添加空白符 center 相当于pad(side=’both’) repeat 重复值。例如x.str.repeat(3)相当于对各个字符串执行x*3 slice 对Series中的各个字符串进行子串截取 split 根据分隔符或正则表达式对字符串进行拆分 strip、rstrip、lstrip 去除空白符，包括换行符。相当于对各个元素执行x.strip()、x.rstrip()、x.lstrip() 为了解决map方法应用于pandas对象时遇到NA值报错的问题，Series可以通过str属性访问跳过NA的字符串操作方法,可以执行正则表达式:1234567891011121314151617181920212223242526272829In [32]: data = &#123;'Jone':'123@qq.com','bob':'asd@163.com','jim':'jim@gmail.com','tom':np.nan&#125;In [33]: data = Series(data)In [34]: data.isnull()Out[34]:Jone Falsebob Falsejim Falsetom Truedtype: boolIn [35]: data.str.contains('gmail')Out[35]:Jone Falsebob Falsejim Truetom NaNdtype: objectIn [36]: pattern = r'([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\.([A-Z]&#123;2,4&#125;)'In [37]: data.str.findall(pattern,flags=re.I)Out[37]:Jone [(123, qq, com)]bob [(asd, 163, com)]jim [(jim, gmail, com)]tom NaNdtype: object 示例处理食品数据集 1.keep可以取‘first’、‘last’、False分别表示保留第一个，保留最后一个，全部删除。详细可以查看文档。 ↩]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>数据清洗</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据规整化(一)]]></title>
    <url>%2F2018%2F03%2F21%2F%E6%95%B0%E6%8D%AE%E8%A7%84%E6%95%B4%E5%8C%96-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[合并数据集pandas对象中的数据可以通过内置的方式进行合并: pandas.merge可根据一个或多个键将不同DataFrame中的行连接起来(数据库连接操作)1 pandas.concat可以沿着一条轴将多个对象堆叠到一起 实例方法combine_first可以将重复数据编接在一起，用一个对象中的值填充另一个对象的中的缺失值(先从第一个对象选值，不行就去第二个对象中选值) merge函数的参数 参数 说明 left 参数合并的左侧DataFrame right 参与合并的右侧DataFrame how “inner”、”outer”、”left”、”right”。默认为”inner” on 用于连接的列名。必须存在于左右两个DataFrame对象中。如果未指定，且其他连接键也未指定，则以left和right列名的交集作为连接键 left_on 左侧DataFrame中用作连接键的列 right_on 右侧DataFrame中用作连接键的列 left_index 将左侧的行索引用作其连接键 right_index 将右侧的行索引用作其连接键 sort 根据连接键合并后的数据进行排序，默认为True。处理大数据集时，禁用会获得更好的性能 suffixes 字符串值元组，用于追加到重叠列名的末尾，默认为(‘_x’,’_y’)。(如果左右两个DataFrame中都有data，则结果会出现”data_x”和”data_y”) copy 设置为False，可以在某些特殊情况下避免将数据复制到结果数据结构中。默认总是复制 数据库风格的DataFrame合并数据集的合并(merge)或链接(join)运算是通过一个或多个键将行链接起来。 多对一df1中的数据key列中有多个被标记为a,b的行，而df2中key列的每个值仅对应一行。如果没有指定要用哪个列进行链接，merge就会将重叠列的列名当做键，最好通过on显式指定；如果两个列的列名不同可以分别使用left_on和right_on指定；默认情况下merge做的是inner链接，即结果是键的交集，所以c的数据被剔除了；外链接outer求的是并集，结合了左链接left和右链接right的效果，通过how来指定链接方式:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677In [14]: df1 = DataFrame(&#123;'key':list('aaccbbc'), ...: 'data1':range(7)&#125;) ...:In [15]: df2 = DataFrame(&#123;'key':list('abc'), ...: 'data2':range(3)&#125;) ...:In [16]: df1Out[16]: data1 key0 0 a1 1 a2 2 c3 3 c4 4 b5 5 b6 6 cIn [17]: df2Out[17]: data2 key0 0 a1 1 b2 2 cIn [18]: pd.merge(df1,df2)Out[18]: data1 key data20 0 a 01 1 a 02 2 c 23 3 c 24 6 c 25 4 b 16 5 b 1In [19]: df1.merge(df2,on='key')Out[19]: data1 key data20 0 a 01 1 a 02 2 c 23 3 c 24 6 c 25 4 b 16 5 b 1In [20]: df3 = DataFrame(&#123;'lkey':list('aaccbbc'), ...: 'data1':range(7)&#125;) ...:In [21]: df4 = DataFrame(&#123;'rkey':list('abc'), ...: 'data2':range(3)&#125;) ...:In [22]: pd.merge(df3,df4,left_on='lkey',right_on='rkey')Out[22]: data1 lkey data2 rkey0 0 a 0 a1 1 a 0 a2 2 c 2 c3 3 c 2 c4 6 c 2 c5 4 b 1 b6 5 b 1 bIn [23]: df1.merge(df2,on='key',how='outer')Out[23]: data1 key data20 0 a 01 1 a 02 2 c 23 3 c 24 6 c 25 4 b 16 5 b 1 多对多多对多链接产生的是行的笛卡尔积，链接方式只影响出现在结果中的键；要根据多个键进行合并，传入一个由列名组成的列表即可(可以看成组合外键)；对于列名重复的问题可以通过设置suffixes选项指定附加到左右两个DataFrame对象列名上的字符串：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192In [29]: df1 = DataFrame(&#123;'key':list('aaccbbc'), ...: 'data1':range(7)&#125;) ...:In [30]: df2 = DataFrame(&#123;'key':list('abacd'), ...: 'data2':range(5)&#125;) ...:In [31]: df1Out[31]: data1 key0 0 a1 1 a2 2 c3 3 c4 4 b5 5 b6 6 cIn [32]: df2Out[32]: data2 key0 0 a1 1 b2 2 a3 3 c4 4 dIn [33]: pd.merge(df1,df2,on='key',how='left')Out[33]: data1 key data20 0 a 01 0 a 22 1 a 03 1 a 24 2 c 35 3 c 36 4 b 17 5 b 18 6 c 3In [42]: left = DataFrame(&#123;'key1':['foo','foo','bar'], ...: 'key2':['one','two','one'], ...: 'lval':range(3)&#125;) ...:In [43]: right = DataFrame(&#123;'key1':['bar','foo','bar','bar'], ...: 'key2':['one','two','one','two'], ...: 'lval':range(4)&#125;) ...:In [44]: leftOut[44]: key1 key2 lval0 foo one 01 foo two 12 bar one 2In [45]: rightOut[45]: key1 key2 lval0 bar one 01 foo two 12 bar one 23 bar two 3In [46]: left.merge(right,on=['key1','key2'],how='outer')Out[46]: key1 key2 lval_x lval_y0 foo one 0.0 NaN1 foo two 1.0 1.02 bar one 2.0 0.03 bar one 2.0 2.04 bar two NaN 3.0In [47]: left.merge(right,on='key1',how='outer')Out[47]: key1 key2_x lval_x key2_y lval_y0 foo one 0 two 11 foo two 1 two 12 bar one 2 one 03 bar one 2 one 24 bar one 2 two 3In [48]: left.merge(right,on='key1',how='outer',suffixes=['_left','_right'])Out[48]: key1 key2_left lval_left key2_right lval_right0 foo one 0 two 11 foo two 1 two 12 bar one 2 one 03 bar one 2 one 24 bar one 2 two 3 索引上的合并当DataFrame中的连接键位于其索引上可以通过传入left_index=True和right_index=True来说明索引应该被用作连接键，对于层次化的索引必须以列表的形式指明用作合并键的多个列，同样可以合并双方的索引：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104In [3]: left1 = DataFrame(&#123;'key':list('abaacb'), ...: 'value':range(6)&#125;) ...:In [4]: right1 = DataFrame(&#123;'group_val':[1,2]&#125;,index=['a','b'])In [5]: left1Out[5]: key value0 a 01 b 12 a 23 a 34 c 45 b 5In [6]: right1Out[6]: group_vala 1b 2In [7]: pd.merge(left1,right1,left_on='key',right_index=True)Out[7]: key value group_val0 a 0 12 a 2 13 a 3 11 b 1 25 b 5 2In [8]: pd.merge(left1,right1,left_on='key',right_index=True,how='outer')Out[8]: key value group_val0 a 0 1.02 a 2 1.03 a 3 1.01 b 1 2.05 b 5 2.04 c 4 NaNIn [24]: lefth = DataFrame(&#123;'key1':['row1','row1','row1','row2','row2'], ...: 'key2':[2001,2001,2002,2001,2002], ...: 'data':range(5)&#125;) ...:In [25]: righth = DataFrame(np.arange(12).reshape((4,3)), ...: index=[['row1','row1','row2','row2'],[2001,2002,2001,2001]], ...: columns=['col1','col2','col3']) ...:In [26]: lefthOut[26]: data key1 key20 0 row1 20011 1 row1 20012 2 row1 20023 3 row2 20014 4 row2 2002In [27]: righthOut[27]: col1 col2 col3row1 2001 0 1 2 2002 3 4 5row2 2001 6 7 8 2001 9 10 11In [28]: pd.merge(lefth,righth,left_on=['key1','key2'],right_index=True)Out[28]: data key1 key2 col1 col2 col30 0 row1 2001 0 1 21 1 row1 2001 0 1 22 2 row1 2002 3 4 53 3 row2 2001 6 7 83 3 row2 2001 9 10 11In [29]: left2 = DataFrame(np.arange(6).reshape((3,2)),index=list('ace'),columns=['col1','col2'])In [30]: right2 = DataFrame(np.arange(8).reshape((4,2)),index=list('bcde'),columns=['col_1','col_2'])In [31]: left2Out[31]: col1 col2a 0 1c 2 3e 4 5In [32]: right2Out[32]: col_1 col_2b 0 1c 2 3d 4 5e 6 7In [33]: pd.merge(left2,right2,how='outer',left_index=True,right_index=True)Out[33]: col1 col2 col_1 col_2a 0.0 1.0 NaN NaNb NaN NaN 0.0 1.0c 2.0 3.0 2.0 3.0d NaN NaN 4.0 5.0e 4.0 5.0 6.0 7.0 DataFrame有一个join方法，它能更为方便地实现按索引合并。它还可以合并多个带有相同或相似索引的DataFrame对象，而不管它们之间有没有重叠的列；同时它也支持参数DataFrame的索引跟调用者DataFrame的某个列之间的连接，对于索引的简单合并，可以向join传入一组DataFrame：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768In [42]: left1Out[42]: key value0 a 01 b 12 a 23 a 34 c 45 b 5In [43]: left2Out[43]: col1 col2a 0 1c 2 3e 4 5In [44]: left2.join(right2,how='outer')Out[44]: col1 col2 col_1 col_2a 0.0 1.0 NaN NaNb NaN NaN 0.0 1.0c 2.0 3.0 2.0 3.0d NaN NaN 4.0 5.0e 4.0 5.0 6.0 7.0In [45]: left1Out[45]: key value0 a 01 b 12 a 23 a 34 c 45 b 5In [46]: right1Out[46]: group_vala 1b 2In [47]: left1.join(right1,on='key')Out[47]: key value group_val0 a 0 1.01 b 1 2.02 a 2 1.03 a 3 1.04 c 4 NaN5 b 5 2.0In [48]: anote =DataFrame(np.arange(8).reshape((4,2)),index=['a','c','b','f'],columns=['col3','col4'])In [49]: anoteOut[49]: col3 col4a 0 1c 2 3b 4 5f 6 7In [50]: left2.join([right2,anote])Out[50]: col1 col2 col_1 col_2 col3 col4a 0 1 NaN NaN 0.0 1.0c 2 3 2.0 3.0 2.0 3.0e 4 5 6.0 7.0 NaN NaN 轴向连接另一种数据合并运算称作连接(concatenation)、绑定(binding)或堆叠(stacking)。NumPy提供了一个用于合并原始NumPy数组的concatenation函数:1234567891011121314In [54]: np.concatenate([arr,arr], axis=1)Out[54]:array([[ 0, 1, 2, 3, 0, 1, 2, 3], [ 4, 5, 6, 7, 4, 5, 6, 7], [ 8, 9, 10, 11, 8, 9, 10, 11]])In [55]: np.concatenate([arr,arr], axis=0)Out[55]:array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) 对于pandas对象(如Series和DataFrame)，带有标签的轴能进一步推广数组的连接运算: 如果个对象其他轴上的索引不同，那些轴应该做并集还是交集 结果对象中的分组需要是否各不相同 用于连接的轴是否重要 concat函数的参数 参数 说明 obj 参与连接的pandas对象的列表或字典。唯一必需的参数 axis 指明连接的轴向,默认为0 join “inner”、”outer”其中之一，默认为”outer”。指明其他轴向上的索引时按交集(inner)还是并集(outer)进行合并 join_axes 指明用于其他n-1条轴的索引，不执行并集/交集运算 keys 与连接对象有关的值，用于形成连接轴向上的层次化索引。可以是任意值的列表或数组、元组数组、列表数组(如果将levels设置成多级数组的话) levels 指定用作层次化索引各级别上的索引，如果设置了keys的话 names 用于创建分层级别的名称，如果设置了keys和levels的话 verify_integrity 检查结果对象新轴上的重复情况，如果发现则引发异常。默认(False)允许重复 ignore_index 不保留连接轴上的索引，产生一组新索引range(total_length) pandas的concat函数提供了解决以上问题的可靠方式，对没有重叠索引的Series调用concat可以将值和索引粘合在一起;默认情况是在axis=0上工作，最后产生一个新的Series。如果传入axis=1将会得到一个DataFrame，这种情况下从索引的有序并集可以看出另一条轴上没有重叠，可以传入join=&#39;inner&#39;得到交集:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455In [56]: s1 = Series([0,1],index=list('ab'))In [57]: s2 = Series([2,3,4],index=list('cde'))In [58]: s3 = Series([5,6,7],index=list('fgh'))In [59]: pd.concat([s1,s2,s3])Out[59]:a 0b 1c 2d 3e 4f 5g 6h 7dtype: int64In [60]: pd.concat([s1,s2,s3],axis=1)Out[60]: 0 1 2a 0.0 NaN NaNb 1.0 NaN NaNc NaN 2.0 NaNd NaN 3.0 NaNe NaN 4.0 NaNf NaN NaN 5.0g NaN NaN 6.0h NaN NaN 7.0In [62]: s4 = pd.concat([s1*5,s3])In [63]: s4Out[63]:a 0b 5f 5g 6h 7dtype: int64In [64]: pd.concat([s1,s4],axis=1)Out[64]: 0 1a 0.0 0b 1.0 5f NaN 5g NaN 6h NaN 7In [65]: pd.concat([s1,s4],axis=1,join='inner')Out[65]: 0 1a 0 0b 1 5 可以通过join_axes指定要在其他轴上使用的索引，使用keys可以在连接轴上创建一个层次化索引：123456789101112131415161718In [69]: pd.concat([s1,s4],axis=1,join_axes=[['a','c','b','e']])Out[69]: 0 1a 0.0 0.0c NaN NaNb 1.0 5.0e NaN NaNIn [71]: pd.concat([s1,s1,s3],keys=['one','two','three'])Out[71]:one a 0 b 1two a 0 b 1three f 5 g 6 h 7dtype: int64 如果沿着axis=1对Series进行合并，则keys就会成为DataFrame的列头，对于DataFrame效果一样；如果传入的不是列表而是一个字典，则字典的键就会被当做keys选项的值;names可以创建分层级别：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657In [73]: df1 = DataFrame(np.arange(6).reshape(3,2),index=['a','b','c'],columns=['one','two'])In [74]: df2 = DataFrame(2+np.arange(4).reshape(2,2),index=['a','c'],columns=['three','four'])In [75]: df1Out[75]: one twoa 0 1b 2 3c 4 5In [76]: df2Out[76]: three foura 2 3c 4 5In [77]: pd.concat([df1,df2],axis=1,keys=['level1','level2'])Out[77]: level1 level2 one two three foura 0 1 2.0 3.0b 2 3 NaN NaNc 4 5 4.0 5.0In [73]: df1 = DataFrame(np.arange(6).reshape(3,2),index=['a','b','c'],columns=['one','two'])In [74]: df2 = DataFrame(2+np.arange(4).reshape(2,2),index=['a','c'],columns=['three','four'])In [75]: df1Out[75]: one twoa 0 1b 2 3c 4 5In [76]: df2Out[76]: three foura 2 3c 4 5In [77]: pd.concat([df1,df2],axis=1,keys=['level1','level2'])Out[77]: level1 level2 one two three foura 0 1 2.0 3.0b 2 3 NaN NaNc 4 5 4.0 5.0In [79]: pd.concat(&#123;'level1':df1,'level2':df2&#125;,axis=1,names=['upper','down'])Out[79]:upper level1 level2down one two three foura 0 1 2.0 3.0b 2 3 NaN NaNc 4 5 4.0 5.0 需要排除与分析无关的行索引，传入ignore_index=True：123456789101112131415161718192021In [83]: df1Out[83]: one twoa 0 1b 2 3c 4 5In [84]: df2Out[84]: three foura 2 3c 4 5In [85]: pd.concat([df1,df2],ignore_index=True)Out[85]: four one three two0 NaN 0.0 NaN 1.01 NaN 2.0 NaN 3.02 NaN 4.0 NaN 5.03 3.0 NaN 2.0 NaN4 5.0 NaN 4.0 NaN 合并重叠数据如果数据集的索引全部或部分重叠就不能用简单的合并(merge)或连接(concatenation)运算来处理了。combine_first实现了相同索引择一选择的功能,满足条件则选1否则选2，类似于np.where(pd.isnull(a),b,a)：1234567891011121314151617181920212223242526272829303132333435363738In [91]: a = Series([np.nan,2,np.nan,3,4,np.nan],index=list('fedcba'))In [92]: b = Series(np.arange(len(a)),dtype=np.float64,index=list('fedcba'))In [93]: b[2:3] = np.nanIn [94]: aOut[94]:f NaNe 2.0d NaNc 3.0b 4.0a NaNdtype: float64In [95]: bOut[95]:f 0.0e 1.0d NaNc 3.0b 4.0a 5.0dtype: float64In [96]: np.where(pd.isnull(a),b,a)Out[96]: array([ 0., 2., nan, 3., 4., 5.])In [97]: b[:-2].combine_first(a[2:])Out[97]:a NaNb 4.0c 3.0d NaNe 1.0f 0.0dtype: float64 对于DataFrame，combine_first会在列上做同样的事，可以看做参数对象中的数据为调用者对象的缺失数据”打补丁”:12345678910111213141516171819202122232425262728293031323334In [107]: df1 = DataFrame(&#123;'a':[1,np.nan,5,np.nan], ...: 'b':[np.nan,2,np.nan,6], ...: 'c':range(2,18,4)&#125;) ...:In [108]: df2 = DataFrame(&#123;'a':[5,4,np.nan,3,7], ...: 'b':[np.nan,3,4,5,9]&#125;) ...:In [109]: df1Out[109]: a b c0 1.0 NaN 21 NaN 2.0 62 5.0 NaN 103 NaN 6.0 14In [110]: df2Out[110]: a b0 5.0 NaN1 4.0 3.02 NaN 4.03 3.0 5.04 7.0 9.0In [111]: df1.combine_first(df2)Out[111]: a b c0 1.0 NaN 2.01 4.0 2.0 6.02 5.0 4.0 10.03 3.0 6.0 14.04 7.0 9.0 NaN 1.可用做实例方法df1.merge(df2),df1想当于left，df2相当于right ↩]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>数据清洗</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据加载、存储与文件格式]]></title>
    <url>%2F2018%2F03%2F21%2F%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E3%80%81%E5%AD%98%E5%82%A8%E4%B8%8E%E6%96%87%E4%BB%B6%E6%A0%BC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[读写文本格式的数据pandas提供了一些用于将表格型数据读取为DataFrame对象的函数 pandas中的解析函数 函数 说明 read_csv 从文件、URL、文件型对象中加载带分隔符的数据。默认分隔符为逗号 read_table 从文件、URL、文件型对象中加载带分隔符的数据。默认分隔符为制表符(“\t”) read_fwf 读取定宽列格式数据(没有分隔符) read_clipboard 读取剪贴板中的数据，可以看做read_table的剪贴板版。将网页转换为表格时很有用 这些函数的选项可以划分为几个大类： 索引：将一个或多个列当做返回的DataFrame处理，以及是否从文件、用户获取列名 类型推断和数据转换：包括用户定义值的转换、缺失值标记列表等 日期解析： 包括组合功能，比如将分散在多个列中的日期时间信息组合成结果中的单个列 迭代：支持对大文件进行逐块迭代 不规整数据问题：跳过一些行、页脚、注释或其他一些不重要的东西 read_csv/read_table函数的参数 参数 说明 filepath_or_buffer 表示文件系统位置、URL、文件型对象的字符串或任何有read()函数的对象(file handle或StringIO) sep/delimiter 用于对行中各字段进行拆分的字符序列或正则表达式 header 用作列名的行号。默认为0(第一行)，如果没有header行就应该设置为None index_col 用作行索引的列编号或列名。可以是单个名称/数字或由多个名称/数字组成的列表(层次化索引) names 用于结果的列名列表，结合header=None skiprows 需要忽略的行数(从文件开始处算起)，或需要跳过的行号列表(从0开始) na_values 一组用于替换NA的值 comment 用于将注释信息从行尾拆分出去的字符(一个或多个) parse_dates 尝试将数据解析为日期，默认为False。如果为True，则尝试解析所有列。此外，还可以指定需要解析的一组列号或列名。如果列表的元素为列表或元组，就会将多个列组合到一起再进行日期解析工作(日期/时间分别位于两个列中) keep_data_col 如果连接多列解析日期，则保持参加连接的列。默认为False dayfirst 当解析有歧义的日期时，将其看做国际格式(7/6/2018 -&gt; June 7,2018)。默认为False date_parser 用于解析日期的函数 nrows 需要读取的行数(从文件开始处算起) iterator 返回一个TextParser以便逐块读取文件 chunksize 文件快的大小(用于迭代) skip_footer 需要忽略的行数(从文件末尾处算起) verbose 打印各种解析器输出信息，比如“非数值列中缺失值的数量” encoding 用于unicode的文本编码格式。“utf-8”表示用UTF-8编码的文本 squeeze 如果数据经解析后仅含一列，则返回Series thousands 千分位分隔符，如“，”或“.” 可以使用read_csv和read_table读取一个以逗号分隔的(CSV)文本文件，使用read_table时指定分隔符sep=&#39;,&#39;；当未指定列名时，会使用第一行数据当做列名，可以通过设置header=None使用默认的列名，也可以使用names=[]自己定义列名:123456789101112131415161718192021222324252627282930313233343536373839404142In [12]: !type ex1.csva,b,c,d,message1,2,3,4,hello5,6,7,8,world9,10,11,12,fooIn [13]: pd.read_csv('ex1.csv')Out[13]: a b c d message0 1 2 3 4 hello1 5 6 7 8 world2 9 10 11 12 fooIn [14]: pd.read_table('ex1.csv',sep=',')Out[14]: a b c d message0 1 2 3 4 hello1 5 6 7 8 world2 9 10 11 12 fooIn [15]: pd.read_csv('ex1.csv',header=None)Out[15]: 0 1 2 3 40 a b c d message1 1 2 3 4 hello2 5 6 7 8 world3 9 10 11 12 fooIn [16]: pd.read_csv('ex1.csv',names=['col1','col2','col3','col4'])Out[16]: col1 col2 col3 col4a b c d message1 2 3 4 hello5 6 7 8 world9 10 11 12 fooIn [17]: pd.read_csv('ex1.csv',names=['col1','col2','col3','col4','col5'])Out[17]: col1 col2 col3 col4 col50 a b c d message1 1 2 3 4 hello2 5 6 7 8 world3 9 10 11 12 foo 如果需要将数据指定为索引列，可以通过设置index_col参数指定索引列，而希望将多个列做成一个层次化索引，只需要传入列编号或列名组成的列表即可：123456789101112131415161718192021222324252627282930313233343536In [21]: !type csv_mindex.csvkey1,key2,value1,value2one,a,1,2one,b,3,4one,c,5,6one,d,7,8two,a,9,10two,b,11,12two,c,13,14two,d,15,16In [22]: pd.read_csv('csv_mindex.csv',index_col='key1')Out[22]: key2 value1 value2key1one a 1 2one b 3 4one c 5 6one d 7 8two a 9 10two b 11 12two c 13 14two d 15 16In [23]: pd.read_csv('csv_mindex.csv',index_col=['key1','key2'])Out[23]: value1 value2key1 key2one a 1 2 b 3 4 c 5 6 d 7 8two a 9 10 b 11 12 c 13 14 d 15 16 有些表格可能不是固定的分隔符去分隔字段的，对此可以编写一个正则表达式来作为read_table的分隔符:123456789101112131415In [26]: list(open('ex3.txt'))Out[26]:[' A B C\n', 'aaa -0.264438 -1.026059 -0.619500\n', 'bbb 0.927272 0.302904 -0.032399\n', 'ccc -0.264273 -0.386314 -0.217601\n', 'ddd -0.871858 -0.348382 1.100491\n']In [27]: pd.read_table('ex3.txt',sep='\s+')Out[27]: A B Caaa -0.264438 -1.026059 -0.619500bbb 0.927272 0.302904 -0.032399ccc -0.264273 -0.386314 -0.217601ddd -0.871858 -0.348382 1.100491 同时可以使用skiprows跳过指定的行：1234567891011121314In [28]: !type ex4.csv# hey!a,b,c,d,message# just wanted to make things more difficult for you# who reads CSV files with computers, anyway?1,2,3,4,hello5,6,7,8,world9,10,11,12,fooIn [29]: pd.read_csv('ex4.csv',skiprows=[0,2,3])Out[29]: a b c d message0 1 2 3 4 hello1 5 6 7 8 world2 9 10 11 12 foo 默认情况pandas会用一组经常出现的标记值识别缺失值，如NA、-1.#IND以及NULL，可以使用na_values指定一组用于表示缺失值的字符串，可以使用一个字典为各列指定不同的NA标记值:12345678910111213141516171819202122232425In [30]: !type ex5.csvsomething,a,b,c,d,messageone,1,2,3,4,NAtwo,5,6,,8,worldthree,9,10,11,12,fooIn [31]: pd.read_csv('ex5.csv')Out[31]: something a b c d message0 one 1 2 3.0 4 NaN1 two 5 6 NaN 8 world2 three 9 10 11.0 12 fooIn [32]: pd.read_csv('ex5.csv',na_values=['NULL'])Out[32]: something a b c d message0 one 1 2 3.0 4 NaN1 two 5 6 NaN 8 world2 three 9 10 11.0 12 fooIn [33]: pd.read_csv('ex5.csv',na_values=&#123;'message':['foo','NA'],'something':['two']&#125;)Out[33]: something a b c d message0 one 1 2 3.0 4 NaN1 NaN 5 6 NaN 8 world2 three 9 10 11.0 12 NaN 逐块读取文本文件在读取大文件中的参数时，只想读取文件的一小部分或逐块对文件进行迭代;nrows用于指定读取几行;chunksize用于逐块读取文件时设置行数，read_csv返回的TextParse对象可以根据chunksize对文件进行逐块迭代：1234567891011121314151617181920212223242526272829303132333435In [46]: pd.read_csv('ex6.csv')Out[46]: one two three four key0 0.467976 -0.038649 -0.295344 -1.824726 L1 -0.358893 1.404453 0.704965 -0.200638 B2 -0.501840 0.659254 -0.421691 -0.057688 G... ... ... ... ... ..9998 -0.362559 0.598894 -1.843201 0.887292 G9999 -0.096376 -1.012999 -0.657431 -0.573315 0[10000 rows x 5 columns]In [47]: chunk = pd.read_csv('ex6.csv',chunksize=1000)In [48]: tot = Series([])In [49]: for piece in chunk: ...: tot = tot.add(piece['key'].value_counts(),fill_value=0) ...:In [50]: tot = tot.sort_values(ascending=False)In [51]: tot[:10]Out[51]:E 368.0X 364.0L 346.0O 343.0Q 340.0M 338.0J 337.0F 335.0K 334.0H 330.0dtype: float64 将数据写出到文本格式利用DataFrame的to_csv方法可以将数据写到一个以逗号分隔的文件中，可以是sep参数指定其他的分隔符；缺失值在输出结果空会被表示为空字符串，可以使用na_rep设置别的标记值；如果没有设置其他选项，则会写出行和列的标签，可以通过index=False和header=False设置禁用；可以通过设置columns来指定顺序排列：123456789101112131415161718192021222324252627282930313233343536373839In [52]: data = pd.read_csv('ex5.csv')In [53]: dataOut[53]: something a b c d message0 one 1 2 3.0 4 NaN1 two 5 6 NaN 8 world2 three 9 10 11.0 12 fooIn [54]: data.to_csv('out.csv')In [55]: !type out.csv,something,a,b,c,d,message0,one,1,2,3.0,4,1,two,5,6,,8,world2,three,9,10,11.0,12,fooIn [56]: data.to_csv(sys.stdout,sep='|')|something|a|b|c|d|message0|one|1|2|3.0|4|1|two|5|6||8|world2|three|9|10|11.0|12|fooIn [57]: data.to_csv(sys.stdout,na_rep='NULL'),something,a,b,c,d,message0,one,1,2,3.0,4,NULL1,two,5,6,NULL,8,world2,three,9,10,11.0,12,fooIn [58]: data.to_csv(sys.stdout,index=False,header=False)one,1,2,3.0,4,two,5,6,,8,worldthree,9,10,11.0,12,fooIn [60]: data.to_csv(sys.stdout,index=False,columns=['a','b','c'])a,b,c1,2,3.05,6,9,10,11.0 Series也有to_csv方法，同时Series可以使用from_csv读取数据12345678910In [61]: Series.from_csv('tseries.csv',parse_dates=True)Out[61]:2000-01-01 02000-01-02 12000-01-03 22000-01-04 32000-01-05 42000-01-06 52000-01-07 6dtype: int64 手工处理分隔符格式 CSV语支选项 参数 说明 delimiter 用于分隔字段的单字符字符串。默认为”,” lineterminator 用于写操作的行结束符，默认为”\r\n”。读操作将忽略此选项，它能认出跨平台的行结束符 quotechar 用于带有特殊字符(如分隔符)的字段的引用符号。默认为“”” quoting 引用约定。可选值包括csv.QUOTE_ALL(引用所有字段)、csv.QUOTE_MINIMAL(只应用带有诸如分隔符之类的特殊字符的字段)、csv.QUOTE_NONNUMERIC以及csv.QUOTE_NON(不引用)。默认为QUOTE_MINIMAL skipinitialspace 忽略分隔符后面的空白符。默认为False doublequote 如何处理字段内的引用符号。如果为True，则双写。 escapechar 用于调分隔符进行转义的字符串(如果quoting被设置为csv.QUOIE_NONE)。默认禁用 对于单字符分隔符文件，可以使用Python内置csv模块，将任意已打开的文件或文件型对象传给csv.reader,对这个reader进行迭代将会为每一行产生去除引号的列表，为了是数据合乎要求，可以做一些整理:1234567891011121314151617In [66]: !type ex7.csv"a","b","c""1","2","3""1","2","3"In [67]: import csvIn [68]: f = open('ex7.csv')In [69]: reader=csv.reader(f)In [70]: for line in reader: ...: print(line) ...:['a', 'b', 'c']['1', '2', '3']['1', '2', '3'] csv的文件有很多，可以定义csv.Dialect的子类定义出新格式(专门的分隔符、字符串引用约定、行结束符等)，CSV语支的参数也可以以关键字的形式提供给csv.reader：12345678910In [84]: class my_dialect(csv.Dialect): ...: lineterminator = '\n' ...: delimiter = ';' ...: quotechar = '"' ...: quoting = csv.QUOTE_MINIMAL ...:In [85]: reader = csv.reader(f, dialect=my_dialect)In [86]: reader = csv.reader(f,delimiter='|') JSON、XML和HTML关于此类文件解析可查看Python文本处理 二进制数据格式可以使用Python内置的pickle序列化来实现数据的二进制存储可以使用pandas.read_pickle函数将数据读回到Python：12345678In [91]: frame.to_pickle('frame')In [92]: pd.read_pickle('frame')Out[92]: a b c d message0 1 2 3 4 hello1 5 6 7 8 world2 9 10 11 12 foo 使用HDF5格式HDF5能实现高效读取磁盘上以二进制格式存储的科学依据。HDF5中HDF指的是层次性数据格式。每个HDF5文件都含有一个文件系统式的节点结构，能够存储多个数据集并支持元数据。HDF5支持多种压缩器的及时压缩，还能更高效地存储重复模式数据。对于那些非常大的无法直接放入内存的数据集，它可以高效地分块读写。Python的HDF5库有两个接口(PyTables和h5py)。 PyTables PyTables抽象了HDF5的许多细节以提供多种灵活的数据容器、表索引、查询功能以及对核外计算技术的支持。 h5py h5py提供了一种直接而高级的HDF5 API访问接口。 使用SDFStore类需要先下载tables：1pip3 install tables 然后通过PyTables存储pandas对象，HDF5文件中的对象可以通过与字典一样的方式进行获取:1234567891011121314In [101]: store['obj1'] = frameIn [102]: storeOut[102]:&lt;class 'pandas.io.pytables.HDFStore'&gt;File path: mydata.h5/obj1 frame (shape-&gt;[3,5])In [103]: store['obj1']Out[103]: a b c d message0 1 2 3 4 hello1 5 6 7 8 world2 9 10 11 12 foo 读取Microsoft Excel文件pandas的ExcelFile类支持读取存储在Excel 2003(或更高版本)中的表格型数据。由于ExcelFile用到了xlrd和openpyxl包，所以需要安装它们:12In [104]: !pip3 install xlrdIn [105]: !pip3 install openpyxl 通过传入一个xls或xlsx的路径创建一个ExcelFile实例然后将存放在工作表中的数据读取到DataFrame中：12345678910In [112]: xls_file = pd.ExcelFile('ex1.xlsx')In [113]: table = xls_file.parse('Sheet1')In [114]: tableOut[114]: a b c d message0 1 2 3 4 hello1 5 6 7 8 world2 9 10 11 12 foo HTML和Web APIHTML和Web API相关内容查看Python-We客户端和服务器 数据库数据库相关内容查看Python数据库编程]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>输入输出</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas入门(四)]]></title>
    <url>%2F2018%2F03%2F20%2Fpandas%E5%85%A5%E9%97%A8-%E5%9B%9B%2F</url>
    <content type="text"><![CDATA[层次化索引层次化索引能在一个轴上拥有多个(两个以上)索引级别，能以低纬度形式处理高纬度数据。在创建Series时，可以使用一个由列表或数组组成的列表作为索引。对于一个层次化索引的对象，选取数据子集的操作同样很简单，有时可以在”内层”中进行选取：12345678910111213141516171819202122232425262728293031In [206]: data = Series(np.random.randn(10),index=[list('aaabbbvvdd'), ...: ['in1','in2','in3','in1','in2','in3','in1','in2','in2','in3']]) ...:In [207]: dataOut[207]:a in1 0.837994 in2 0.360445 in3 -0.657047b in1 0.017681 in2 -0.577803 in3 0.080992v in1 -0.158913 in2 -0.011517d in2 0.632189 in3 -1.181628dtype: float64In [208]: data['a']Out[208]:in1 0.837994in2 0.360445in3 -0.657047dtype: float64In [209]: data[:,'in1']Out[209]:a 0.837994b 0.017681v -0.158913dtype: float64 层次化索引在数据重塑和基于分组的操作中非常重要，使用unstack方法可以将Series多层索引安排到一个DataFrame中,statck是其逆运算:123456789101112131415161718192021In [210]: data.unstack()Out[210]: in1 in2 in3a 0.837994 0.360445 -0.657047b 0.017681 -0.577803 0.080992d NaN 0.632189 -1.181628v -0.158913 -0.011517 NaNIn [211]: data.unstack().stack()Out[211]:a in1 0.837994 in2 0.360445 in3 -0.657047b in1 0.017681 in2 -0.577803 in3 0.080992d in2 0.632189 in3 -1.181628v in1 -0.158913 in2 -0.011517dtype: float64 对于一个DataFrame，每条轴都可以有分层索引，各层都可以有名字；有了列索引后可以通过其选取列分组：123456789101112131415161718192021222324252627282930313233343536In [213]: df = DataFrame(np.arange(16).reshape(4,4), ...: index = [['row1','row1','row2','row2'],[1,2,1,2]], ...: columns=[['col1','col1','col2','col2'],['red','blue','red','blue']]) ...:In [214]: dfOut[214]: col1 col2 red blue red bluerow1 1 0 1 2 3 2 4 5 6 7row2 1 8 9 10 11 2 12 13 14 15In [215]: df.index.names=['rowname1','rowname2']In [216]: df.columns.names=['colname1','colname2']In [217]: dfOut[217]:colname1 col1 col2colname2 red blue red bluerowname1 rowname2row1 1 0 1 2 3 2 4 5 6 7row2 1 8 9 10 11 2 12 13 14 15In [218]: df['col1']Out[218]:colname2 red bluerowname1 rowname2row1 1 0 1 2 4 5row2 1 8 9 2 12 13 重排分级顺序 swaplevelswaplevel接收两个级别编号或名称，并返回一个互换了级别的新对象： 12345678910111213141516171819In [219]: dfOut[219]:colname1 col1 col2colname2 red blue red bluerowname1 rowname2row1 1 0 1 2 3 2 4 5 6 7row2 1 8 9 10 11 2 12 13 14 15In [220]: df.swaplevel('rowname1','rowname2')Out[220]:colname1 col1 col2colname2 red blue red bluerowname2 rowname11 row1 0 1 2 32 row1 4 5 6 71 row2 8 9 10 112 row2 12 13 14 15 sort_index(level=)sort_index(level=)根据单个级别中的值对数据进行排序(稳定的): 12345678910111213141516171819In [225]: df.sort_index(level=1)Out[225]:colname1 col1 col2colname2 red blue red bluerowname1 rowname2row1 1 0 1 2 3row2 1 8 9 10 11row1 2 4 5 6 7row2 2 12 13 14 15In [226]: df.swaplevel(0,1).sort_index(level=0)Out[226]:colname1 col1 col2colname2 red blue red bluerowname2 rowname11 row1 0 1 2 3 row2 8 9 10 112 row1 4 5 6 7 row2 12 13 14 15 根据级别汇总统计许多对于DataFrame和Series的描述和汇总统计都有一个level选项，用于指定在某条轴上求和的级别：12345678910111213141516In [25]: df.sum(level='rowname2')Out[25]:colname1 col1 col2colname2 red blue red bluerowname21 8 10 12 142 16 18 20 22In [26]: df.sum(level='colname1',axis=1)Out[26]:colname1 col1 col2rowname1 rowname2row1 1 1 5 2 9 13row2 1 17 21 2 25 29 使用DataFrame的列 set_indexset_index函数将一个或多个列转换为行索引，并创建一个新的DataFrame，默认情况下用于创建索引的列会被移除，可以通过设置drop=False保留： 123456789101112131415161718192021222324252627282930313233343536373839In [231]: frame = DataFrame(&#123;'a':range(7),'b':range(7,0,-1), ...: 'c':['one','one','one','two','two','two','two'], ...: 'd':[0,1,2,0,1,2,3]&#125;) ...:In [232]: frameOut[232]: a b c d0 0 7 one 01 1 6 one 12 2 5 one 23 3 4 two 04 4 3 two 15 5 2 two 26 6 1 two 3In [233]: frame.set_index(['c','d'])Out[233]: a bc done 0 0 7 1 1 6 2 2 5two 0 3 4 1 4 3 2 5 2 3 6 1In [234]: frame.set_index(['c','d'],drop=False)Out[234]: a b c dc done 0 0 7 one 0 1 1 6 one 1 2 2 5 one 2two 0 3 4 two 0 1 4 3 two 1 2 5 2 two 2 3 6 1 two 3 reset_indexreset_index将层次化索引的级别转移到列里面，和set_index相反: 12345678910111213141516171819202122In [236]: frame2Out[236]: a bc done 0 0 7 1 1 6 2 2 5two 0 3 4 1 4 3 2 5 2 3 6 1In [237]: frame2.reset_index()Out[237]: c d a b0 one 0 0 71 one 1 1 62 one 2 2 53 two 0 3 44 two 1 4 35 two 2 5 26 two 3 6 1 整数索引当一个pandas对象含有类似0、1、2的索引时，很难推断出需要的是基于标签或位置的索引，为了保证良好的一致性，如果轴索引含有索引器，那么根据整数进行数据选取的操作将总是面向标签的；如果需要可靠地、不考虑索引类型的、基于位置的索引，可以使用loc:1234567891011121314151617181920212223In [271]: obj = Series(np.arange(3))In [272]: obj.loc[:1]Out[272]:0 01 1dtype: int32In [273]: frame = DataFrame(np.arange(9).reshape(3,3),index=[2,0,1])In [274]: frame.loc[0,:]Out[274]:0 31 42 5Name: 0, dtype: int32In [275]: frame.loc[:,0]Out[275]:2 00 31 6Name: 0, dtype: int32]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas入门(三)]]></title>
    <url>%2F2018%2F03%2F20%2Fpandas%E5%85%A5%E9%97%A8-%E4%B8%89%2F</url>
    <content type="text"><![CDATA[汇总和计算描述统计pandas对象拥有一组常用的数学和统计方法。她们大部分属于约简和汇总统计，用于从Series中提取单个值(如sum或mean)或从DataFrame的行或列中提取一个Series，他们都是基于没有缺失数据的假设构建的。 约简方法的选项 选项 说明 axis 约简的轴。DataFrame的行用0，列用1 skipna 排除缺失值，默认值为True level 如果轴是层次化索引的(即MultiIndex)，则根据level分组约简 描述和汇总统计 方法 说明 count 非NA值的数量 describe 针对Series或各DataFrame列计算汇总统计 min、max 计算最小值和最大值 argmin、argmax 计算能够获取到最小值和最大值的索引位置(整数) idxmin、idmax 计算能够获取到最小值和最大值的索引值 quantile 计算样本的分位数(0到1) sum 值的总和 mean 值的平均值 median 指的算术中位数 mad 根据平均值计算平均绝对离差 var 样本值的方差 std 样本值的标准差 skew 样本值的偏度(三阶矩) kurt 样本值的峰度(四阶矩) cumsum 样本值的累计和 cummin、cummax 样本值的累计最小值和累计最大值 cumprod 样本值的累计积 diff 计算一阶差分(对时间序列有用)_ pct_change 计算百分数变化 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748In [33]: df = DataFrame([[1,np.nan],[2,3],[np.nan,np.nan],[4,5]], ...: index=list('abcd'), ...: columns=['one','two']) ...:In [34]: dfOut[34]: one twoa 1.0 NaNb 2.0 3.0c NaN NaNd 4.0 5.0In [35]: df.sum()Out[35]:one 7.0two 8.0dtype: float64In [36]: df.sum(axis=1)Out[36]:a 1.0b 5.0c NaNd 9.0dtype: float64In [37]: df.mean(axis=1,skipna=False)Out[37]:a NaNb 2.5c NaNd 4.5dtype: float64In [38]: df.idxmax()Out[38]:one dtwo ddtype: objectIn [39]: df.cumsum()Out[39]: one twoa 1.0 NaNb 3.0 3.0c NaN NaNd 7.0 8.0 describe用于一次性产生多个汇总统计，对于非数值类型会产生另外一种汇总统计：123456789101112131415161718192021In [40]: df.describe()Out[40]: one twocount 3.000000 2.000000mean 2.333333 4.000000std 1.527525 1.414214min 1.000000 3.00000025% 1.500000 3.50000050% 2.000000 4.00000075% 3.000000 4.500000max 4.000000 5.000000In [41]: obj =Series(list('aabc')*4)In [42]: obj.describe()Out[42]:count 16unique 3top afreq 8dtype: object 相关系数和协方差Series的corr方法用于计算两个Series中重叠的、非NA的、按索引对齐的相关系数；使用cov计算协方差：123456789In [46]: obj = Series([1,2,3,4],index=list('abcd'))In [47]: obj2 = Series([1,np.nan,5,6,7],index=list('acdse'))In [48]: obj.corr(obj2)Out[48]: 1.0In [49]: obj.cov(obj2)Out[49]: 6.0 DataFrame的corr和cov方法将以DataFrame的形式返回完整的相关系数或协方差矩阵：12345678910111213141516171819202122232425262728293031323334353637383940414243In [60]: df = DataFrame(np.arange(16).reshape(4,4), ...: index=list('abcd'), ...: columns=['col1','col2','col3','col4']) ...:In [61]: df2 = DataFrame(np.arange(25).reshape(5,5), ...: index=list('abcde'), ...: columns=['col1','col2','col3','col4','col5']) ...: ...:In [62]: dfOut[62]: col1 col2 col3 col4a 0 1 2 3b 4 5 6 7c 8 9 10 11d 12 13 14 15In [63]: df2Out[63]: col1 col2 col3 col4 col5a 0 1 2 3 4b 5 6 7 8 9c 10 11 12 13 14d 15 16 17 18 19e 20 21 22 23 24In [64]: df.corr()Out[64]: col1 col2 col3 col4col1 1.0 1.0 1.0 1.0col2 1.0 1.0 1.0 1.0col3 1.0 1.0 1.0 1.0col4 1.0 1.0 1.0 1.0In [65]: df.cov()Out[65]: col1 col2 col3 col4col1 26.666667 26.666667 26.666667 26.666667col2 26.666667 26.666667 26.666667 26.666667col3 26.666667 26.666667 26.666667 26.666667col4 26.666667 26.666667 26.666667 26.666667 利用DataFrame的corrwith方法可以计算其列或行跟另一个Series或DataFrame之间的相关系数；传入一个Series将会返回一个相关系数值Series，传入一个DataFrame则会计算按列名配对的相关系数(传入axis=1按列计算)：12345678910111213141516In [66]: df.corrwith(df2)Out[66]:col1 1.0col2 1.0col3 1.0col4 1.0col5 NaNdtype: float64In [69]: df.corrwith(df2.col1)Out[69]:col1 1.0col2 1.0col3 1.0col4 1.0dtype: float64 唯一值、值计数以及成员资格 唯一值、值计数、成员资格方法 方法 说明 isin 计算一个表示“Series各值是否包含于传入的值序列中”的布尔型数组 unique 计算Series中的唯一值数组，按发现顺序返回 value_counts 返回一个Series，其索引为唯一值，其值为频率，按计数值降序排列 unique可以从Series中获取唯一值数组，返回的唯一值是未排序的，可以对结果进行排序(unique().sort())。value_counts用于计算一个Series中各值出现的频率，结果Series是按值频率降序排列的。value_counts是一个顶级pandas方法，可以用于任何数组或序列；isin用于判断矢量化集合的成员资格，可用于选取Series中或DataFrame列中数据的子集：123456789101112131415161718192021222324252627282930313233343536373839404142434445In [78]: obj = Series(list('abbddc'))In [79]: sor = obj.unique()In [80]: sorOut[80]: array(['a', 'b', 'd', 'c'], dtype=object)In [81]: sor.sort()In [82]: sorOut[82]: array(['a', 'b', 'c', 'd'], dtype=object)In [83]: obj.value_counts()Out[83]:b 2d 2c 1a 1dtype: int64In [84]: pd.value_counts(obj.values, sort=False)Out[84]:d 2a 1b 2c 1dtype: int64In [85]: mask = obj.isin(['a','c'])In [86]: maskOut[86]:0 True1 False2 False3 False4 False5 Truedtype: boolIn [87]: obj[mask]Out[87]:0 a5 cdtype: object 可以将pandas.value_counts传递给DataFrame的aplly函数得到DataFrame中多个相关列的柱状图：12345678910111213141516171819202122232425In [89]: data = DataFrame(&#123;'Q1':[1,3,4,4,5], ...: 'Q2':[2,3,4,2,1], ...: 'Q3':[4,1,4,5,6]&#125;) ...:In [90]: dataOut[90]: Q1 Q2 Q30 1 2 41 3 3 12 4 4 43 4 2 54 5 1 6In [91]: result = data.apply(pd.value_counts).fillna(0)In [92]: resultOut[92]: Q1 Q2 Q31 1.0 1.0 1.02 0.0 2.0 0.03 1.0 1.0 0.04 2.0 1.0 2.05 1.0 0.0 1.06 0.0 0.0 1.0 处理缺失数据缺失数据在大部分数据分析应用中都很常见。pandas使用浮点值NaN(Not a Number)表示浮点和非浮点数组中的缺失数据，它只是一个便于检测的标记。Python内置的None值也会被当做NA处理 NA处理方法 方法 说明 dropna 根据各标签中是否存在缺失数据对轴标签进行过滤，可通过阈值调节对缺失值的容忍度 fillna 用指定值或插值方法(如ffill或bfill)填充缺失数据 isnull 返回一个含有布尔值的对象，这些布尔值表示哪些值是缺失值/NA，该对象的类型与源类型一样 notnull isnull的否定式 123456789101112131415161718192021In [99]: obj = Series([1,np.nan,2,np.nan,4])In [100]: obj.isnull()Out[100]:0 False1 True2 False3 True4 Falsedtype: boolIn [101]: obj[0]=NoneIn [102]: obj.isnull()Out[102]:0 True1 True2 False3 True4 Falsedtype: bool 滤除缺失数据对于Series，dropna返回一个仅含有非空数据和索引值的Series(通过布尔型索引达到一样的效果)：1234567891011121314151617181920In [104]: objOut[104]:0 NaN1 NaN2 2.03 NaN4 4.0dtype: float64In [105]: obj.dropna()Out[105]:2 2.04 4.0dtype: float64In [106]: obj[obj.notnull()]Out[106]:2 2.04 4.0dtype: float64 对于DataFrame对象，dropna默认丢弃任何含有缺失值的行，传入how=&#39;all&#39;将只丢弃全为NA的那些行，要丢弃列需要传入axis=1123456789101112131415161718192021222324252627282930313233343536373839In [108]: data = DataFrame([[1,4,5],[1,np.nan,np.nan],[np.nan,np.nan,np.nan],[np.nan,2,3]])In [109]: dataOut[109]: 0 1 20 1.0 4.0 5.01 1.0 NaN NaN2 NaN NaN NaN3 NaN 2.0 3.0In [110]: data.dropna()Out[110]: 0 1 20 1.0 4.0 5.0In [111]: data.dropna(how='all')Out[111]: 0 1 20 1.0 4.0 5.01 1.0 NaN NaN3 NaN 2.0 3.0In [112]: data[3]=np.nanIn [113]: dataOut[113]: 0 1 2 30 1.0 4.0 5.0 NaN1 1.0 NaN NaN NaN2 NaN NaN NaN NaN3 NaN 2.0 3.0 NaNIn [114]: data.dropna(axis=1,how='all')Out[114]: 0 1 20 1.0 4.0 5.01 1.0 NaN NaN2 NaN NaN NaN3 NaN 2.0 3.0 thresh参数移除非NA个数小于设定值的行：12345678910111213In [123]: df = DataFrame(np.random.randn(7,3))In [124]: df.loc[:3,1] = np.nanIn [125]: df.loc[:2,2] = np.nanIn [126]: df.dropna(thresh=2)Out[126]: 0 1 23 0.620445 NaN -0.3796384 -0.642811 0.033634 0.7000095 0.510774 1.458027 1.2476876 0.614596 -1.986715 -0.378179 填充缺失数据fillna方法是填充缺失数据的主要函数。通过一个常数调用fillna将会将缺失值替换为那个常数值；通过字典调用fillna可以实现对不同的列填充不同的值；fillna默认会返回新对象，通过设置inplace=True可以对现有对象进行就地修改，对reindex有效的插值方法也可用于fillna: fillna函数的参数 参数 说明 value 用于填充缺失值的标量值或字典对象 method 插值方式。如果函数调用时未指定其他参数的话，默认为“ffill” axis 待填充的轴，默认axis=0 inplace 修改调用者对象而不产生副本 limit (对于前向和后向填充)可以连续填充的最大数量 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273In [127]: dfOut[127]: 0 1 20 -0.293799 NaN NaN1 0.728953 NaN NaN2 0.573023 NaN NaN3 0.620445 NaN -0.3796384 -0.642811 0.033634 0.7000095 0.510774 1.458027 1.2476876 0.614596 -1.986715 -0.378179In [128]: df.fillna(0)Out[128]: 0 1 20 -0.293799 0.000000 0.0000001 0.728953 0.000000 0.0000002 0.573023 0.000000 0.0000003 0.620445 0.000000 -0.3796384 -0.642811 0.033634 0.7000095 0.510774 1.458027 1.2476876 0.614596 -1.986715 -0.378179In [129]: df.fillna(&#123;1:0.5, 3:-1&#125;)Out[129]: 0 1 20 -0.293799 0.500000 NaN1 0.728953 0.500000 NaN2 0.573023 0.500000 NaN3 0.620445 0.500000 -0.3796384 -0.642811 0.033634 0.7000095 0.510774 1.458027 1.2476876 0.614596 -1.986715 -0.378179In [130]: _ = df.fillna(0,inplace=True)In [131]: dfOut[131]: 0 1 20 -0.293799 0.000000 0.0000001 0.728953 0.000000 0.0000002 0.573023 0.000000 0.0000003 0.620445 0.000000 -0.3796384 -0.642811 0.033634 0.7000095 0.510774 1.458027 1.2476876 0.614596 -1.986715 -0.378179In [138]: df = DataFrame(np.random.randn(7,3))In [139]: df.loc[3:,1] = np.nanIn [140]: df.loc[2:,2] = np.nanIn [141]: dfOut[141]: 0 1 20 -1.741073 -0.993316 -1.0300551 0.139948 -1.446029 0.7978562 -0.373251 0.505183 NaN3 1.179879 NaN NaN4 0.764752 NaN NaN5 1.405856 NaN NaN6 -1.053222 NaN NaNIn [142]: df.fillna(method='ffill')Out[142]: 0 1 20 -1.741073 -0.993316 -1.0300551 0.139948 -1.446029 0.7978562 -0.373251 0.505183 0.7978563 1.179879 0.505183 0.7978564 0.764752 0.505183 0.7978565 1.405856 0.505183 0.7978566 -1.053222 0.505183 0.797856]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas入门(二)]]></title>
    <url>%2F2018%2F03%2F19%2Fpandas%E5%85%A5%E9%97%A8-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[基本功能重新索引 reindex的(插值)method选项 参数 说明 fffill或pad 前向填充(或搬运)值 bfill或backfill 后向填充(或搬运)值 reindex函数的参数 参数 说明 index 用作索引的新序列。既可以是Index实例，也可以是其他序列型的Python数据结构。Index会被完全使用，就像没有任何复制一样 method 插值(填充)方式 fill_value 再重新索引的过程中，需要引入缺失值时使用的替代值 limit 前向或后向填充时的最大填充量 level 在MultiIndex的指定级别上匹配简单索引，否则选取其子集 copy 默认为True，无论如何都复制；如果为False，则新旧相等就不复制 pandas对象的reindex方法用于创建一个适应新索引的新对象，reindex将会根据新索引进行重排。如果某个索引值当前不存在，就引入缺失值。method选项可以在重新索引时做一些插值处理：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253In [86]: obj = Series([1,2,3,4],index=['a','b','c','d'])In [87]: objOut[87]:a 1b 2c 3d 4dtype: int64In [88]: obj2 = obj.reindex(['q','w','e','r'])In [89]: obj2Out[89]:q NaNw NaNe NaNr NaNdtype: float64In [90]: obj2 = obj.reindex(['a','b','c','d','e'])In [91]: obj2Out[91]:a 1.0b 2.0c 3.0d 4.0e NaNdtype: float64In [94]: obj2 = obj.reindex(['a','b','c','d','e'],fill_value=0)In [95]: obj2Out[95]:a 1b 2c 3d 4e 0dtype: int64In [98]: obj3 = obj.reindex(['a','b','e','f','c','d'],method='ffill')In [99]: obj3Out[99]:a 1b 2e 4f 4c 3d 4dtype: int64 对于DataFrame,reindex可以修改(行)索引、列、或两个都修改。如果仅传入一个序列，则会重新索引行，使用columns关键字可以重新索引列,也可以同时对行和列进行重新索引，但插值只能按行应用:1234567891011121314151617181920212223242526In [105]: frame = DataFrame(np.arange(9).reshape((3,3)),index=['a','b','c'],columns=['col1','col2','col3'])In [106]: frame2 = frame.reindex(['a','b','c','d'])In [107]: frame2Out[107]: col1 col2 col3a 0.0 1.0 2.0b 3.0 4.0 5.0c 6.0 7.0 8.0d NaN NaN NaNIn [108]: frame.reindex(columns=['col_a','col1','col2','col3'])Out[108]: col_a col1 col2 col3a NaN 0 1 2b NaN 3 4 5c NaN 6 7 8In [109]: frame.reindex(index=['a','b','c','d'],method='ffill',columns=['col_a','col1','col2','col3'])Out[109]: col_a col1 col2 col3a 2 0 1 2b 5 3 4 5c 8 6 7 8d 8 6 7 8 利用loc的标签索引功能重新索引：1234567In [111]: frame.loc[['a','b','c','d'],['col_a','col1','col2','col3']]Out[111]: col_a col1 col2 col3a NaN 0.0 1.0 2.0b NaN 3.0 4.0 5.0c NaN 6.0 7.0 8.0d NaN NaN NaN NaN 丢弃指定轴上的项使用drop方法删除指定轴上的项，只需要传入一个索引数组或列表，对于DataFrame可以传入指定的轴(axis)来进行删除,返回的都是删除轴之后的新对象:12345678910111213141516171819202122232425262728In [112]: obj = Series([1,2,3,4],index=['a','b','c','d'])In [113]: obj.drop('a')Out[113]:b 2c 3d 4dtype: int64In [114]: obj.drop(['a','b'])Out[114]:c 3d 4dtype: int64In [115]: frame = DataFrame(np.arange(9).reshape((3,3)),index=['a','b','c'],columns=['col1','col2','col3'])In [116]: frame.drop(['a','b'])Out[116]: col1 col2 col3c 6 7 8In [117]: frame.drop(['col1','col2'],axis=1)Out[117]: col3a 2b 5c 8 索引、选取和过滤Series索引(obj[……])的工作方式类似于NumPy数组的索引，并且可以使用非整数；而利用切片运算其 末端是包含的(封闭)：12345678910111213141516171819202122232425262728293031323334353637383940In [3]: obj = Series(np.arange(4), index=['a','b','c','d'])In [4]: objOut[4]:a 0b 1c 2d 3dtype: int64In [5]: obj['a']Out[5]: 0In [6]: obj[2:4]Out[6]:c 2d 3dtype: int64In [7]: obj['c':'d']Out[7]:c 2d 3dtype: int64In [8]: obj[['a','d']]Out[8]:a 0d 3dtype: int64In [9]: obj['b':'c']=5In [10]: objOut[10]:a 0b 5c 5d 3dtype: int64 对DataFrame进行索引是获取一个或多个列，可以通过切片或布尔型数组选取行，也可以使用布尔型DataFrame进行索引：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859In [15]: data = DataFrame(np.arange(16).reshape(4,4), ...: index=['a','b','c','d'], ...: columns=['col1','col2','col3','col4']) ...:In [16]: dataOut[16]: col1 col2 col3 col4a 0 1 2 3b 4 5 6 7c 8 9 10 11d 12 13 14 15In [17]: data['col1']Out[17]:a 0b 4c 8d 12Name: col1, dtype: int64In [18]: data[['col1','col4']]Out[18]: col1 col4a 0 3b 4 7c 8 11d 12 15In [19]: data[:2]Out[19]: col1 col2 col3 col4a 0 1 2 3b 4 5 6 7In [20]: data[data['col3']&gt;5]Out[20]: col1 col2 col3 col4b 4 5 6 7c 8 9 10 11d 12 13 14 15In [21]: data&lt;5Out[21]: col1 col2 col3 col4a True True True Trueb True False False Falsec False False False Falsed False False False FalseIn [22]: data[data&lt;5] = -5In [23]: dataOut[23]: col1 col2 col3 col4a -5 -5 -5 -5b -5 5 6 7c 8 9 10 11d 12 13 14 15 为了在DataFrame的行上进行标签索引，可以通过loc进行：123456789101112131415161718In [48]: data.loc['a',['col1','col2']]Out[48]:col1 -5col2 -5Name: a, dtype: int64In [49]: data.loc[['a','d'],['col1','col3']]Out[49]: col1 col3a -5 -5d 12 14In [50]: data.loc[data.col3&gt;5,:'col3']Out[50]: col1 col2 col3b -5 5 6c 8 9 10d 12 13 14 算术运算和数据对齐pandas可以对不同索引的对象进行算数运算。在将对象相加时，如果存在不同的索引对，则结果的索引就是对该索引对的并集，自动的数据对齐操作在不重叠的索引处引入NA值，缺失值会在算术运算过程中传播:123456789101112131415161718192021222324252627282930In [55]: s1 = Series(np.arange(3),index=['a','b','c'])In [56]: s2 = Series(np.arange(3,9),index=['a','b','c','d','e','f'])In [57]: s1Out[57]:a 0b 1c 2dtype: int64In [58]: s2Out[58]:a 3b 4c 5d 6e 7f 8dtype: int64In [59]: s1+s2Out[59]:a 3.0b 5.0c 7.0d NaNe NaNf NaNdtype: float64 对于DataFrame，对齐操作会同时发生在行和列上，它们相加后会返回一个新的DataFrame，其索引和列为原来两个DataFrame的并集：123456789101112131415161718192021222324252627282930In [65]: df1 = DataFrame(np.arange(9).reshape(3,3),columns=list('abc'), ...: index=['row1','row2','row3']) ...:In [66]: df2 = DataFrame(np.arange(16).reshape(4,4),columns=list('abcd'), ...: index=['row1','row2','row3','row4']) ...:In [67]: df1Out[67]: a b crow1 0 1 2row2 3 4 5row3 6 7 8In [68]: df2Out[68]: a b c drow1 0 1 2 3row2 4 5 6 7row3 8 9 10 11row4 12 13 14 15In [69]: df1+df2Out[69]: a b c drow1 0.0 2.0 4.0 NaNrow2 7.0 9.0 11.0 NaNrow3 14.0 16.0 18.0 NaNrow4 NaN NaN NaN NaN 在算术方法中填充值 灵活的算术方法 方法 说明 add 用于加法(+)的方法 sub 用于减法(-)的方法 div 用于除法(/)的方法 mul 用于乘法(*)的方法 对于不同索引的对象进行算术运算时，当一个对象中某个轴标签在另一个对象中找不到时填充一个特殊值,在对Series或DataFrame重新索引时也可以指定一个填充值：1234567891011121314In [76]: df2.add(df1,fill_value=0)Out[76]: a b c drow1 0.0 2.0 4.0 3.0row2 7.0 9.0 11.0 7.0row3 14.0 16.0 18.0 11.0row4 12.0 13.0 14.0 15.0In [77]: df1.reindex(columns=df2.columns,fill_value=0)Out[77]: a b c drow1 0 1 2 0row2 3 4 5 0row3 6 7 8 0 DataFrame和Series之间的运算默认情况下DataFrame和Series之间的算术运算会将Series的索引匹配到DataFrame的列，然后沿着行一直向下广播；如果某个索引值在DataFrame的列或Series的索引中找不到，则参与运算的两个对象就会被重新索引形成并集；如果希望匹配行且在列上广播则必须使用算术运算方法：1234567891011121314151617181920212223242526272829303132333435363738394041424344In [94]: s1 = df2.loc['row1']In [95]: df2Out[95]: a b c drow1 0 1 2 3row2 4 5 6 7row3 8 9 10 11row4 12 13 14 15In [96]: s1Out[96]:a 0b 1c 2d 3Name: row1, dtype: int64In [97]: df2-s1Out[97]: a b c drow1 0 0 0 0row2 4 4 4 4row3 8 8 8 8row4 12 12 12 12In [98]: s2 = Series(range(3),index=list('abf'))In [99]: df2-s2Out[99]: a b c d frow1 0.0 0.0 NaN NaN NaNrow2 4.0 4.0 NaN NaN NaNrow3 8.0 8.0 NaN NaN NaNrow4 12.0 12.0 NaN NaN NaNIn [100]: s3 = df2['a']Out[101]: a b c drow1 0 1 2 3row2 0 1 2 3row3 0 1 2 3row4 0 1 2 3 函数应用和映射NumPy的ufuncs(元素级数组方法)也可用于操作pandas对象:12345678910111213141516171819In [102]: frame = DataFrame(np.random.randn(4,3),columns=list('abc'), ...: index=['row1','row2','row3','row4']) ...:In [103]: frameOut[103]: a b crow1 0.755289 0.886977 -0.984527row2 0.460170 -0.514393 0.180462row3 0.828386 -0.545317 -1.176786row4 0.860822 -1.659938 0.952070In [104]: np.abs(frame)Out[104]: a b crow1 0.755289 0.886977 0.984527row2 0.460170 0.514393 0.180462row3 0.828386 0.545317 1.176786row4 0.860822 1.659938 0.952070 apply方法可以将函数应用到各列或行所形成的一维数组上，许多常见的数组统计功能都被实现成DataFrame方法(如sum和mean)，因此无需使用apply方法；除标量外，传递给apply的函数还可以返回多个值组成的Series；元素级的Python函数也是可以使用的，可以使用applymap得到frame中各个浮点值的格式化字符串:123456789101112131415161718192021222324252627282930313233343536In [112]: f = lambda x:x.max() -x.min()In [113]: frame.apply(f)Out[113]:a 0.400653b 2.546915c 2.128856dtype: float64In [114]: def f(x): ...: return Series([x.min(),x.max()],index=['min','max']) ...:In [115]: frame.apply(f)Out[115]: a b cmin 0.460170 -1.659938 -1.176786max 0.860822 0.886977 0.952070In [116]: format = lambda x: '%.2f' % xIn [117]: frame.applymap(format)Out[117]: a b crow1 0.76 0.89 -0.98row2 0.46 -0.51 0.18row3 0.83 -0.55 -1.18row4 0.86 -1.66 0.95In [118]: frame['a'].map(format)Out[118]:row1 0.76row2 0.46row3 0.83row4 0.86Name: a, dtype: object 排序和排名排序使用sort_index方法对行或列索引进行排序(按字典顺序)，它将返回一个已排序的对象；对于DataFrame则可以根据任意一个轴上的索引进行排序；数据默认时按升序进行排序的，可以设置ascending=False来降序排序：12345678910111213141516171819202122232425262728293031In [134]: obj = Series(range(4), index=list('dabc'))In [135]: obj.sort_index()Out[135]:a 1b 2c 3d 0dtype: int64In [136]: frame = DataFrame(np.arange(8).reshape((2,4)),index=['col2','col1'], ...: columns=list('badc')) ...:In [137]: frame.sort_index()Out[137]: b a d ccol1 4 5 6 7col2 0 1 2 3In [138]: frame.sort_index(axis=1)Out[138]: a b c dcol2 1 0 3 2col1 5 4 7 6In [139]: frame.sort_index(axis=1, ascending=False)Out[139]: d c b acol2 2 3 0 1col1 6 7 4 5 sort_values方法用于按值进行排序，在排序时，任何的缺失值默认都会放到Series的末尾：123456789In [144]: obj.sort_values()Out[144]:4 -3.05 2.00 4.02 7.01 NaN3 NaNdtype: float64 在DataFrame中，可以将一个或多个列的名字传递给by选项来根据一个或多个列中的值进行排序，要根据多个列进行排序，可以传入名称的列表：12345678910111213141516171819202122232425In [150]: frame = DataFrame(&#123;'b':[2,5,0,1],'a':[0,1,0,1]&#125;)In [151]: frameOut[151]: a b0 0 21 1 52 0 03 1 1In [152]: frame.sort_values(by='b')Out[152]: a b2 0 03 1 10 0 21 1 5In [153]: frame.sort_values(by=['a','b'])Out[153]: a b2 0 00 0 23 1 11 1 5 排名排名会增设一个排名值(从1开始，一直到数组中有效的数据的数量)，它可以根据某种规则破坏平级关系；rank是通过“为各组分配一个平均排名”的方式破坏平级关系1。 排名用于破坏平级关系的method的选项 method 说明 ‘average’ 默认：在相等分组中，为各个值分配平均排名 ‘min’ 使用整个分组的最小排名 ‘max’ 使用整个分组的最大排名 ‘first’ 按值在原始数据中的出现顺序分配排名 按降序进行排名使用ascending=False，其他的相似:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091In [9]: obj = Series([7,6,7,5,4,4,3])In [10]: obj.rank()Out[10]:0 6.51 5.02 6.53 4.04 2.55 2.56 1.0dtype: float64In [11]: obj.rank(method='min')Out[11]:0 6.01 5.02 6.03 4.04 2.05 2.06 1.0dtype: float64In [12]: obj.rank(method='max')Out[12]:0 7.01 5.02 7.03 4.04 3.05 3.06 1.0dtype: float64In [13]: obj.rank(method='first')Out[13]:0 6.01 5.02 7.03 4.04 2.05 3.06 1.0dtype: float64In [9]: obj = Series([7,6,7,5,4,4,3])In [10]: obj.rank()Out[10]:0 6.51 5.02 6.53 4.04 2.55 2.56 1.0dtype: float64In [11]: obj.rank(method='min')Out[11]:0 6.01 5.02 6.03 4.04 2.05 2.06 1.0dtype: float64In [12]: obj.rank(method='max')Out[12]:0 7.01 5.02 7.03 4.04 3.05 3.06 1.0dtype: float64In [13]: obj.rank(method='first')Out[13]:0 6.01 5.02 7.03 4.04 2.05 3.06 1.0dtype: float64 DataFrame可以在行或列上计算排名:12345678910111213141516171819202122In [15]: frame = DataFrame(&#123;'b':[1,3,-1],'a':[2,-1,-2],'c':[1,2,3]&#125;)In [16]: frameOut[16]: a b c0 2 1 11 -1 3 22 -2 -1 3In [17]: frame.rank(axis=0)Out[17]: a b c0 3.0 2.0 1.01 2.0 3.0 2.02 1.0 1.0 3.0In [18]: frame.rank(axis=1)Out[18]: a b c0 3.0 1.5 1.51 1.0 3.0 2.02 1.0 2.0 3.0 带有重复值的轴索引带有重复索引值的Series和DataFrame可以使用is_unique属性确认它是否唯一；对于带有重复值的索引，如果某个值对应多个值，则会返回一个Series(或DataFrame)；而对应单个值则返回一个标量(Series)：123456789101112131415161718192021222324252627282930313233343536373839404142434445In [19]: obj = Series(range(5),index=list('abbvd'))In [20]: objOut[20]:a 0b 1b 2v 3d 4dtype: int32In [21]: obj.index.is_uniqueOut[21]: FalseIn [22]: obj['a']Out[22]: 0In [23]: obj['b']Out[23]:b 1b 2dtype: int32In [24]: df = DataFrame(np.random.randn(4,3),index=['a','a','b','c'])In [26]: dfOut[26]: 0 1 2a 2.139973 0.102242 0.366141a -0.999559 0.324575 -0.808672b 1.121435 1.508694 1.151597c 0.610592 1.623871 -1.331131In [27]: df.loc['c']Out[27]:0 0.6105921 1.6238712 -1.331131Name: c, dtype: float64In [28]: df.loc['a']Out[28]: 0 1 2a 2.139973 0.102242 0.366141a -0.999559 0.324575 -0.808672 1.破坏平级关系是指在两个相同的数之间确认先后顺序。使用average表示如果在数组中7排在第五位和第六位，则其排名为5.5。min则为min(5,6)为5；max则为max(5,6)为7；first则表示在原数据中先出现排序靠前，紧邻的+1，依次递增。 ↩]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pandas入门(一)]]></title>
    <url>%2F2018%2F03%2F19%2Fpandas%E5%85%A5%E9%97%A8-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[SeriesSeries1 是一种类似于一维数组的对象，它由一组数据(各种NumPy数据类型)以及一组与之相关的数据标签(即索引)组成。Series的字符串表现形式为：索引在左边，值在右边。如果没有为数据指定索引，会自动创建一个0到n-1的整数型索引。可以通过index参数指定索引来代替自动生成的索引:12345678910111213141516171819In [4]: ser1 = Series([1,2,2,3])In [5]: ser1Out[5]:0 11 22 23 3dtype: int64In [6]: ser2 = Series([1,2,2,3],index=['a','b','c','d'])In [7]: ser2Out[7]:a 1b 2c 2d 3dtype: int64 可以通过索引的方式选取Series中的单个或一组值；数组运算(布尔型数组进行过滤，标量乘法，应用数学函数)都会保留索引和值之间的连接；Series可以看成是一个定长的有序字典，可以用在原本需要字典参数的函数中:1234567891011121314151617181920212223242526272829In [8]: ser2['a']Out[8]: 1In [9]: ser2[['a','b']]Out[9]:a 1b 2dtype: int64In [10]: ser2*2Out[10]:a 2b 4c 4d 6dtype: int64In [11]: ser2[ser2&gt;=2]Out[11]:b 2c 2d 3dtype: int64In [12]: 'a' in ser2Out[12]: TrueIn [13]: 'g' in ser2Out[13]: False 可以直接通过字典来创建Series，则Series中的索引就是原字典的键(有序列表)，如果键对应的值找不到，将会是使用NA表示缺失数据,pandas的isnull和notnull函数可用于检测缺失数据：123456789101112131415161718192021In [14]: dic = &#123;'a':1,'b':2,'c':3&#125;In [15]: dics = Series(dic)In [16]: dicsOut[16]:a 1b 2c 3dtype: int64In [17]: states = ['a','b','c','d']In [18]: dicstates = Series(dic,index=states)In [19]: dicstatesOut[19]:a 1.0b 2.0c 3.0d NaN Series在算数运算中会自动对齐不同索引的数据：12345678910111213141516171819202122In [20]: dicsOut[20]:a 1b 2c 3dtype: int64In [21]: dicstatesOut[21]:a 1.0b 2.0c 3.0d NaNdtype: float64In [22]: dics+dicstatesOut[22]:a 2.0b 4.0c 6.0d NaNdtype: float64 Series本身及其索引有一个name属性，同时Series的索引可以通过赋值的方式就地修改:1234567891011121314151617181920In [23]: dics.name='dics'In [24]: dics.index.name='letter'In [25]: dicsOut[25]:lettera 1b 2c 3Name: dics, dtype: int64In [26]: dics.index=['z','x','y']In [27]: dicsOut[27]:z 1x 2y 3Name: dics, dtype: int64 DataFrame构造DataFrame 可以输入给DataFrame构造器的数据 类型 说明 二维ndarray 数据矩阵，还可以传入行标和列标 由数组、列标或元组组成的字典 每个序列会变成DataFrame的一列，所有序列的长度必须相同 NumPy的结构化/记录数组 类似于“由数组组成的字典” 由Series组成的字典 每个Series会成为一列。如果没有显示指定索引，则个Series的索引会被合并成结果的行索引 由字典组成的字典 各内层字典会成为一列。键会被合并成结果的行索引，跟“由Series组成的字典”情况一样 字典或Series的列表 各项将会成为DataFrame的一行。字典键或Series索引的并集将会成为DataFrame的列标 另一个DataFrame 该DataFrame的索引将会被沿用，除非显式指定了其他索引 NumPy的MaskedArray 类似于“二维ndarray”的情况，只是掩码值在结果DataFrame会编程NA/缺失值 DataFrame 是一个表格型的数据结构。它含有一组有序的列，每列可以是不同的值类型(数值、字符串、布尔值等)。DataFrame既有行索引也有列索引，它可以被看做由Series组成的字典(共同用一个索引)，DataFrame面向行和面向列的操作基本上是平衡的。构建DataFrame可以通过直接传入一个由等长列表或NumPy数组组成的字典，和Series一样DataFrame也会自动加上索引且全部列会被有序排列，如果指定了列索引，则DataFrame的列会按照指定顺序进行排列。如果传入的列在数据中找不到，会产生NA值：123456789101112131415161718192021222324In [30]: data =&#123;'state':['a','b','c','d'], ...: 'year':[2000,2001,2002,2003], ...: 'pop':[1,2,3,4]&#125;In [31]: frame = DataFrame(data)In [32]: frameOut[32]: pop state year0 1 a 20001 2 b 20012 3 c 20023 4 d 2003In [34]: DataFrame(data,columns=['year','pop','state','debt'],index=['i1','i2','i3','i4'])Out[34]: year pop state debti1 2000 1 a NaNi2 2001 2 b NaNi3 2002 3 c NaNi4 2003 4 d NaNIn [35]: frame.columnsOut[35]: Index(['pop', 'state', 'year'], dtype='object') 可以通过字典标记的方式或属性的方式将DataFrame的列获取为一个Series，返回的Series拥有原DataFrame相同的索引，且其name属性已经被相应地设置好了。行也可以通过位置或名称的方式进行获取，比如用索引字段loc:12345678910111213141516171819202122In [40]: frame.stateOut[40]:0 a1 b2 c3 dName: state, dtype: objectIn [41]: frame['year']Out[41]:0 20001 20012 20023 2003Name: year, dtype: int64In [42]: frame.loc[1]Out[42]:pop 2state byear 2001Name: 1, dtype: object 列可以通过赋值的方式进行修改，将列表或数组给某个列时，其长度必须跟DataFrame的长度相匹配。如果赋值的是一个Series就会精确匹配DataFrame的索引，所有的空位都将被填上缺失值，为不存在的列赋值会创建出一个新列，关键字del可以删除列:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061In [49]: frame2=DataFrame(data,columns=['year','pop','state','debt'],index=['i1','i2','i3','i4'])In [50]: frame2Out[50]: year pop state debti1 2000 1 a NaNi2 2001 2 b NaNi3 2002 3 c NaNi4 2003 4 d NaNIn [51]: frame2['debt']=np.arange(4.)In [52]: frame2Out[52]: year pop state debti1 2000 1 a 0.0i2 2001 2 b 1.0i3 2002 3 c 2.0i4 2003 4 d 3.0In [53]: frame2=DataFrame(data,columns=['year','pop','state','debt'],index=['i1','i2','i3','i4'])In [54]: frame2Out[54]: year pop state debti1 2000 1 a NaNi2 2001 2 b NaNi3 2002 3 c NaNi4 2003 4 d NaNIn [55]: val = Series([-1,-2,-3],index=['i1','i3','i4'])In [56]: frame2['debt']=valIn [57]: frame2Out[57]: year pop state debti1 2000 1 a -1.0i2 2001 2 b NaNi3 2002 3 c -2.0i4 2003 4 d -3.0In [58]: frame2['big']= frame2['pop']&gt;=3In [59]: frame2Out[59]: year pop state debt bigi1 2000 1 a -1.0 Falsei2 2001 2 b NaN Falsei3 2002 3 c -2.0 Truei4 2003 4 d -3.0 TrueIn [60]: del frame2['big']In [61]: frame2Out[61]: year pop state debti1 2000 1 a -1.0i2 2001 2 b NaNi3 2002 3 c -2.0i4 2003 4 d -3.0 嵌套字典被传给DataFrame后会被解释为：外层字典的键作为列，内层字典键作为行索引，可以通过T进行转置。内层字典的键会被合并，排序以形成最终的索引。如果显式指定了索引，就不会如此。同理，Series组成的字典也是一样的用法:123456789101112131415161718192021222324252627282930313233In [63]: pop = &#123;'out1':&#123;2002:1.1,2001:1.2&#125;, ...: 'out2':&#123;2001:1.3,2004:1.4&#125;&#125;In [64]: frame3 = DataFrame(pop)In [65]: frame3Out[65]: out1 out22001 1.2 1.32002 1.1 NaN2004 NaN 1.4In [66]: frame3.TOut[66]: 2001 2002 2004out1 1.2 1.1 NaNout2 1.3 NaN 1.4In [67]: DataFrame(pop,index=[2002,2001,2004])Out[67]: out1 out22002 1.1 NaN2001 1.2 1.32004 NaN 1.4In [68]: sData = &#123;'out1':frame3['out1'][:-1], ...: 'out2':frame3['out2'][:-1]&#125;In [69]: DataFrame(sData)Out[69]: out1 out22001 1.2 1.32002 1.1 NaN 设置了DataFrame的index和columns的name属性，这些信息将会被显示出来，values属性会以二维ndarray的形式返回DataFrame中的数据，如果DataFrame各列的数据类型不同，则值数组的数据类型就会选用能兼容所有列的数据类型：123456789101112131415161718192021222324In [70]: frame3.index.name='year'In [71]: frame3.columns.name='state'In [72]: frame3Out[72]:state out1 out2year2001 1.2 1.32002 1.1 NaN2004 NaN 1.4In [73]: frame3.valuesOut[73]:array([[ 1.2, 1.3], [ 1.1, nan], [ nan, 1.4]])In [74]: frame2.valuesOut[74]:array([[2000, 1, 'a', -1.0], [2001, 2, 'b', nan], [2002, 3, 'c', -2.0], [2003, 4, 'd', -3.0]], dtype=object) 索引对象pandas的索引对象负责管理轴标签和其他元数据。 pandas中主要的Index对象 类 说明 Index 最泛化的Index对象，将轴标签表示为一个由Python对象组成的NumPy数组 Int64Index 针对整数的特殊Index MultiIndex “层次化”索引对象，表示单个轴上的多层索引。可以看做由元组组成的数组 DatetimeIndex 存储纳秒级时间戳(用NumPy的datetime64类型表示) PeriodIndex 针对Period数据(时间间隔)的特殊Index Index的方法和属性 方法 说明 append 连接另一个Index对象，产生一个新的Index diff 计算差集，并得到一个Index intersection 计算交集 union 计算并集 isin 计算一个指示各值是否都包含在参数集合中的布尔型数组 delete 删除索引i处的元素，并得到新的Index drop 删除传入的值，并得到新的Index insert 将元素插入到索引i处，并得到新的Index is_monotonic 当各元素均大于等于前一个元素时，返回True is_unique 当Index没有重复值时，返回True unique 计算Index中唯一值的数组 构建Series或DataFrame时，所得到的任何数组或其他序列的标签都会被转换成一个Index，Index对象是 不可修改的，这使得Index对象在多个数据结构之间安全共享。除了长得像数组，Index的功能也类似与一个固定大小的集合，每个索引都有一些方法和属性，它们用于设置逻辑并回答有关索引所包含数据的常见问题:123456789101112131415161718192021222324252627282930In [76]: obj = Series(range(3),index=['a','b','c'])In [77]: index =obj.indexIn [78]: indexOut[78]: Index(['a', 'b', 'c'], dtype='object')In [79]: index[:-1]Out[79]: Index(['a', 'b'], dtype='object')In [80]: inde=pd.Index(np.arange(3))In [81]: obj2=Series(['a','b','c'],index=inde)In [82]: obj2.index is indeOut[82]: TrueIn [83]: frame3Out[83]:state out1 out2year2001 1.2 1.32002 1.1 NaN2004 NaN 1.4In [84]: 'out1' in frame3.columnsOut[84]: TrueIn [85]: 2005 in frame3.indexOut[85]: False 1.使用 from pandas import Series, DataFrame和 import pandas as pd引入相关的包 ↩]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy基础(二)]]></title>
    <url>%2F2018%2F03%2F17%2Fnumpy%E5%9F%BA%E7%A1%80-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[通用函数通用函数(即ufunc) 是一种对ndarray中的数据执行元素级运算的函数。它是简单函数(接受一个或多个标量值，并产生一个或多个标量值)的矢量化包装器。 一元ufunc 函数 说明 abs、fabs 计算整数、浮点数或复数的绝对值。对于非复数值，可以使fabs sqrt 计算各元素的平方根。相当于arr**0.5 square 计算各元素的平方。相当于arr**2 exp 计算各元素的指数e^x log、log10、log2、log1p 分别对自然对数(底为e)、底为10的log、底为2的log、log(1+x) sign 计算各元素的正负号:1(正数)、0(零)、-1(负数) ceil 计算各元素的ceiling值，即大于等于该值的最小整数 floor 计算各元素的floor值，即小于等于该值的最大整数 rint 将各元素四舍五入到最接近的整数，保留dtype modf 将数组的小数和整数部分以独立数组的形式返回 isnan 返回一个表示“哪些值是NaN(这不是一个数字)”的布尔型数组 isfinite、isinf 分别返回一个表示“哪些元素是有穷的(非inf、非NaN)”或“哪些元素是无穷的”布尔型数组 cos、cosh、sin、sinh、tan、tanh 普通型和双曲型三角函数 arccos、arccosh、arcsin、arcsinh、arctan、arctanh 反三角函数 logical_not 计算各元素not x的真值。相当于-arr 二元ufunc 函数 说明 add 将数组中对应的元素相加 subtract 从第一个数组中减去第二个数组中的元素 multiply 数组元素相乘 divide、floor_divide 除法或向下圆整除法(丢弃余数) power 对第一个数组中的元素A，根据第二个数组中的相应元素B，计算A^B maximum、fmax 元素级的最大值计算。fmax将忽略NaN minimum、fmin 元素级的最小值计算。fmin将忽略NaN mod 元素级的求模计算(除法的余数) copysign 将第二个数组中的值的符号复制给第一个数组中的值 greater、greater_equal、less、less_equal、equal、not_equal 执行元素级的比较运算，最终产生布尔型数组。相当于中缀运算符&gt;、&gt;=、&lt;、&lt;=、==、!= logical_and、logical_or、logical_xor 执行元素级的真值逻辑运算。相当于中缀运算符&amp;、&#124;、^ 许多ufunc都是简单的元素级变体，一元的ufunc接受一个数组，二元的接受两个并返回一个结果数组：123456789101112131415161718192021222324252627282930In [2]: arr = np.arange(10)In [3]: arrOut[3]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])In [4]: np.sqrt(arr)Out[4]:array([0. , 1. , 1.41421356, 1.73205081, 2. , 2.23606798, 2.44948974, 2.64575131, 2.82842712, 3. ])In [5]: np.exp(arr)Out[5]:array([1.00000000e+00, 2.71828183e+00, 7.38905610e+00, 2.00855369e+01, 5.45981500e+01, 1.48413159e+02, 4.03428793e+02, 1.09663316e+03, 2.98095799e+03, 8.10308393e+03])In [6]: x = randn(5)In [7]: y = randn(5)In [8]: xOut[8]:array([-9.27415622e-01, -1.67964296e-03, -1.91023663e+00, -1.31307986e-01, -1.17927352e+00])In [9]: yOut[9]: array([ 0.82239493, 0.43695129, -0.00905311, 0.31991891, -0.34529735])In [10]: np.maximum(x,y)Out[10]: array([ 0.82239493, 0.43695129, -0.00905311, 0.31991891, -0.34529735]) 利用数组进行数据处理矢量化 将数据处理任务表述为简洁的数组表达式，用数组表达式代替循环。例如计算函数sqrt(x^2+y^2)。np.meshgrid接受两个一维数组，并产生两个二维矩阵(对应两个数组中所有(x,y)对):12345678910111213141516171819202122232425262728293031In [11]: points = np.arange(-5, 5, 0.01)In [12]: xs, ys = np.meshgrid(points,points)In [13]: xsOut[13]:array([[-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99], [-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99], [-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99], ..., [-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99], [-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99], [-5. , -4.99, -4.98, ..., 4.97, 4.98, 4.99]])In [14]: z = np.sqrt(np.square(xs)+np.square(ys))In [15]: zOut[15]:array([[7.07106781, 7.06400028, 7.05693985, ..., 7.04988652, 7.05693985, 7.06400028], [7.06400028, 7.05692568, 7.04985815, ..., 7.04279774, 7.04985815, 7.05692568], [7.05693985, 7.04985815, 7.04278354, ..., 7.03571603, 7.04278354, 7.04985815], ..., [7.04988652, 7.04279774, 7.03571603, ..., 7.0286414 , 7.03571603, 7.04279774], [7.05693985, 7.04985815, 7.04278354, ..., 7.03571603, 7.04278354, 7.04985815], [7.06400028, 7.05692568, 7.04985815, ..., 7.04279774, 7.04985815, 7.05692568]]) 将条件逻辑表述为数组运算np.where是三元表达式x if condition else y的矢量化版本。np.wehere接受三个参数cond、xarr、yarr,当判定cond为true时选择执行xarr否则执行yarr。因此np.where也可以嵌套使用。np.where的第二个和第三个参数不必是数组，可以是标量值。where通常用于根据另一个数组产生新的数组。1234567891011121314151617181920212223242526272829303132333435363738In [23]: xarr = np.array([0.1,0.2,0.3])In [24]: yarr = np.array([1.1,1.2,1.3])In [25]: cond = np.array([True,False,True])In [26]: result = np.where(cond,xarr,yarr)In [27]: resultOut[27]: array([0.1, 1.2, 0.3])In [28]: arr = randn(4,4)In [29]: arrOut[29]:array([[ 0.17276546, -1.27954884, -0.07326268, -2.40995669], [-0.15796552, -1.85102248, 0.53499154, -0.1332765 ], [ 0.81853502, 1.35768841, 1.55543773, 1.01407613], [-1.05967769, 0.39289449, 0.13509303, -0.68143339]])In [30]: np.where(arr&gt;0,1,-1)Out[30]:array([[ 1, -1, -1, -1], [-1, -1, 1, -1], [ 1, 1, 1, 1], [-1, 1, 1, -1]])In [31]: np.where(arr&gt;0,1,arr)Out[31]:array([[ 1. , -1.27954884, -0.07326268, -2.40995669], [-0.15796552, -1.85102248, 1. , -0.1332765 ], [ 1. , 1. , 1. , 1. ], [-1.05967769, 1. , 1. , -0.68143339]])In [32]: cond2 = np.array([False,False,True])In [33]: np.where(cond&amp;cond2,0,np.where(cond,1,np.where(cond,2,3)))Out[33]: array([1, 3, 0]) 数学和统计方法可以通过数组上的一组数学函数对整个数组或某个轴向的数据进行统计。sum、mean以及标准差std等聚合计算(aggregation)既可以当做数组的实例方法调用，也可以当做顶级NumPy函数使用。 基本数组统计方法 方法 说明 sum 对数组中全部或某轴向的元素求和。零长度的数组的sum为0 mean 算数平均数。零长度的数组的mean为NaN std、var 分别为标准差和方差，自由度可调(默认为n) argmin、argmax 分别为最大和最小元素的索引 cumsum 所有元素的累计和 cumprod 所有元素的累计积 mean和sum这类的函数接受一个axis参数(用于计算该轴向上的统计值)，最终结果是一个少一维的数组:123456789101112131415161718In [36]: arr = np.random.randn(5,4)In [37]: arrOut[37]:array([[-0.00502805, 0.23545272, 0.04886622, -0.46971953], [-1.08918278, 1.19958904, -0.54808552, -1.60148873], [-0.60059372, -0.9743709 , 1.39660621, -0.97132217], [-0.11917925, 1.99922758, -1.58943388, 1.60237969], [-0.28361465, -2.57463163, -0.96786527, -0.00376026]])In [38]: arr.mean()Out[38]: -0.26580774446749256In [39]: np.mean(arr)Out[39]: -0.26580774446749256In [40]: arr.mean(axis=1)Out[40]: array([-0.04760716, -0.509792 , -0.28742014, 0.47324854, -0.95746796]) cumsum和cumprod之类的方法不聚合，而是产生一个由中间结果组成的数组：12345678910111213141516171819202122232425In [47]: arr = np.array([[0,1,2],[3,4,5],[6,7,8]])In [48]: arr.cumsum(0)Out[48]:array([[ 0, 1, 2], [ 3, 5, 7], [ 9, 12, 15]])In [49]: arr.cumsum(1)Out[49]:array([[ 0, 1, 3], [ 3, 7, 12], [ 6, 13, 21]])In [50]: arr.cumprod(0)Out[50]:array([[ 0, 1, 2], [ 0, 4, 10], [ 0, 28, 80]])In [51]: arr.cumprod(1)Out[51]:array([[ 0, 0, 0], [ 3, 12, 60], [ 6, 42, 336]]) 用于布尔型数组的方法上面罗列的方法中，布尔值会被强制转换为1(True)和0(False)。所以sum可以用来对布尔型数组中的True值计数。而any方法用来测试数组中是否存在一个或多个True，all用来检查数组中所有值是否都是True：123456789101112In [52]: arr = randn(20)In [53]: (arr &gt; 0).sum()Out[53]: 11In [54]: bools=np.array([False,True,False])In [55]: bools.any()Out[55]: TrueIn [56]: np.all(bools)Out[56]: False 排序NumPy数组通过sort方法就地排序，多维数组可以在任何一个轴向上进行排序，只需将轴编号传给sort。顶级方法np.sort返回的是数组的已排序副本，而就地排序则会修改数组本身：123456789101112131415161718192021222324252627282930313233343536373839In [57]: arr = randn(6)In [58]: arrOut[58]:array([ 0.20563118, -0.6733116 , -1.44713961, 0.49352122, 0.73564391, 1.71627219])In [59]: arr.sort()In [60]: arrOut[60]:array([-1.44713961, -0.6733116 , 0.20563118, 0.49352122, 0.73564391, 1.71627219])In [61]: arr = rand(5,3)In [62]: arrOut[62]:array([[0.18125744, 0.10766187, 0.46160903], [0.34363544, 0.28353683, 0.06096776], [0.52424935, 0.13756835, 0.78614215], [0.12944147, 0.95273729, 0.09078996], [0.85118943, 0.18928544, 0.23857278]])In [63]: np.sort(arr,1)Out[63]:array([[0.10766187, 0.18125744, 0.46160903], [0.06096776, 0.28353683, 0.34363544], [0.13756835, 0.52424935, 0.78614215], [0.09078996, 0.12944147, 0.95273729], [0.18928544, 0.23857278, 0.85118943]])In [64]: arrOut[64]:array([[0.18125744, 0.10766187, 0.46160903], [0.34363544, 0.28353683, 0.06096776], [0.52424935, 0.13756835, 0.78614215], [0.12944147, 0.95273729, 0.09078996], [0.85118943, 0.18928544, 0.23857278]]) 唯一化及其他集合逻辑NumPy提供了一些针对一维ndarray的基本集合运算。 数组的集合运算 方法 说明 unique(x) 计算x中的唯一元素，并返回有序结果 intersect1d(x,y) 计算x和y中的公共元素，并返回有序结果 union1d(x,y) 计算x和y的并集，并返回有序结果 in1d(x,y) 得到一个表示“x的元素是否包含于y”的布尔型数组 setdiff1d(x,y) 集合的差，即元素在x中且不在y中 setxor1d(x,y) 集合的对称差，即存在于一个数组中单不同时存在于两个数组中的元素(异或) 12In [66]: np.unique(names)Out[66]: array(['Jim', 'Tom', 'bob'], dtype='&lt;U3') 用于数组的文件输入输出将数组以二进制格式保存到磁盘np.save和np.load是读写磁盘数组数据的两个主要函数。默认情况下数组以未压缩的原始二进制格式保存在扩展名为 .npy 的文件中。np.savez可以将多个数组保存到一个压缩文件中，将数组以关键字参数的形式传入即可。加载 .npz 文件时，将会得到一个类似字典的对象，该对象对各个数组进行延迟加载:12345678910111213In [68]: arr = np.arange(10)In [69]: np.save('arr',arr)In [70]: np.load('arr.npy')Out[70]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])In [71]: np.savez('arr_more',a=arr,b=arr)In [72]: arr_more = np.load('arr_more.npz')In [73]: arr_more['a']Out[73]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) 线性代数 numpy.linalg函数 函数 说明 diag 以一维数组的形式返回方阵的对角线(或非对角线)元素，或将一维数组转换为方阵(非对角线元素为0) dot 矩阵乘法 trace 计算对角线的和 det 计算矩阵行列式 eig 计算方阵的特征值和特征向量 inv 计算方阵的逆 pinv 计算矩阵的Moore-Penrose伪逆 qr 计算QR分解 svd 计算奇异值分解(SVD) solve 解线性方程组Ax=b，其中A为一个方阵 lstsq 计算Ax=b的最小二乘解 1234567891011121314151617181920212223242526272829303132333435363738394041In [83]: x = randn(3,3)In [84]: xOut[84]:array([[ 0.45880764, -0.90269718, -1.62963467], [-0.76727739, 2.24799683, 0.65118256], [ 0.43217346, -2.47319723, -0.48442667]])In [85]: mat = x.T.dot(x)In [86]: matOut[86]:array([[ 0.98599295, -3.20785172, -1.45668284], [-3.20785172, 11.9850565 , 4.13300564], [-1.45668284, 4.13300564, 3.31441709]])In [87]: inv(mat)Out[87]:array([[16.03043111, 3.2650856 , 2.97386734], [ 3.2650856 , 0.8114194 , 0.42317945], [ 2.97386734, 0.42317945, 1.08102824]])In [88]: matOut[88]:array([[ 0.98599295, -3.20785172, -1.45668284], [-3.20785172, 11.9850565 , 4.13300564], [-1.45668284, 4.13300564, 3.31441709]])In [89]: mat.dot(inv(mat))Out[89]:array([[ 1.00000000e+00, -1.71619808e-16, 2.22044605e-16], [ 1.06801676e-14, 1.00000000e+00, 0.00000000e+00], [ 3.55271368e-15, 8.88178420e-16, 1.00000000e+00]])In [90]: q,r = qr(mat)In [91]: rOut[91]:array([[-3.65847231, 13.01900838, 5.33621718], [ 0. , -1.23249681, 1.27109691], [ 0. , 0. , 0.31324131]]) 随机数生成 部分numpy.random函数 函数 说明 seed 确定随机数生成器的种子 permutation 返回一个序列的随机排列或返回一个随机排列的范围 shuffle 对一个序列就地随机排序 rand 产生均匀分布的样本值 randint 从给定的上下限范围内随机选取整数 randn 产生正态分布(平均值为0，标准差为1)的样本值 normal 产生正态(高斯)分布的样本值 binomial 产生二项分布的样本值 beta 产生Beta分布的样本值 chisquare 产生卡方分布的样本值 gamma 产生Gamma分布的样本值 uniform 产生在[0,1）中均匀分布的样本值 随机漫步使用np.random模块一次性产生1000个“掷硬币”结果(即两个数中任选一个),将其分别设置为1或-1，然后计算累计和,然后可以做求取最大值最小值的简单统计：12345678910111213In [96]: nsteps = 1000In [97]: draws = np.random.randint(0, 2, size=nsteps)In [98]: steps = np.where(draws&gt;0,1,-1)In [99]: walk = steps.cumsum()In [101]: walk.min()Out[101]: -24In [102]: walk.max()Out[102]: 9 然后可以计算首次穿越时间，即随机漫步过程中第一次到达某个特定值的时间。使用np.abs(walk)&gt;=10得到一个布尔型数组，它表示的距离是否达到或超过10，使用argmax(并不高效，会对数组进行完全扫描)找到布尔型数组第一个最大值索引(True就是最大值):12In [103]: (np.abs(walk)&gt;=10).argmax()Out[103]: 107 一次模拟多个随机漫步通过给numpy.random函数传入一个二元元组产生一个二维数组,然后使用累计和创建随机漫步过程(一行一个)，接着计算最大值和最小值。得到这些数据后来计算30或-30的最小穿越时间。因为不是5000个都达到了30，所以使用any进行检查，然后利用检查后的布尔数组选出哪些穿越了30(绝对值)的随机漫步(行)，并调用argmax在轴1上获取穿越时间1234567891011121314151617181920212223242526272829303132333435363738In [104]: nwalks = 5000In [105]: nsteps = 5000In [106]: draws = np.random.randint(0,2,size=(nwalks,nsteps))In [107]: steps = np.where(draws&gt;0,1,-1)In [108]: walks = steps.cumsum(1)In [109]: walksOut[109]:array([[ -1, -2, -1, ..., 20, 19, 18], [ -1, 0, 1, ..., 16, 17, 18], [ -1, 0, 1, ..., 80, 79, 78], ..., [ 1, 2, 3, ..., 6, 5, 6], [ 1, 0, -1, ..., -130, -131, -132], [ 1, 0, 1, ..., -16, -17, -16]])In [110]: walks.max()Out[110]: 255In [111]: walks.min()Out[111]: -235In [112]: hits30 = (np.abs(walks)&gt;=30).any(1)In [113]: hits30Out[113]: array([ True, True, True, ..., True, True, True])In [115]: hits30.sum()Out[115]: 4992In [117]: crossing_time =(np.abs(walks[hits30])&gt;=30).argmax(1)In [118]: crossing_time.mean()Out[118]: 883.0564903846154]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>NumPy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[numpy基础(一)]]></title>
    <url>%2F2018%2F03%2F16%2Fnumpy%E5%9F%BA%E7%A1%80-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[NumPy1的部分功能如下： ndarray，一个具有矢量运算和复杂广播能力的快熟且节省空间的多维数组 用于对数组数据进行快速运算的标准数学函数 线性代数、随机数生成及傅里叶变换功能 用于集成C、C++、Fortran等语言编写的代码的工具 对于大部分数据分析应用，关注的功能集中在： 用于数据整理和清理、子集构造和过滤、转换等快速的矢量化数组运算 常用的数组算法，如排序、唯一化、集合运算等 高效的描述统计和数据聚合/摘要运算 用于异构数据集的合并/连接运算的数据对齐和关系型数据运算 将条件逻辑表述为数组表达式(而不是带有if-ekif-else分支的循环) 数据的分组运算(聚合、转换、函数应用等) 多维数组对象(ndarray)N维数组对象(即ndarray)是一个快速而灵活的大数据集容器。可以利用这种数组执行一些数学运算，语法和标量元素之间的运算一样：12345678910111213141516In [4]: data = np.array([[1,2,3],[3,4,5]])In [5]: dataOut[5]:array([[1, 2, 3], [3, 4, 5]])In [6]: data*10Out[6]:array([[10, 20, 30], [30, 40, 50]])In [7]: data+dataOut[7]:array([[ 2, 4, 6], [ 6, 8, 10]]) 创建ndarrayarray函数是创建数组最简单的方法，它接受一切序列型的对象(包括其他数组)，然后产生一个新的含有传入数据的NumPy数组。 数组创建函数 函数 说明 array 将输入数据(列表、元组或其他序列类型)转换为ndarray。要么推断出dtype，要么显示指定dtype。默认直接复制输入数据 asarray 将输入转换为ndarray，如果输入本身就是一个ndarray就不进行复制 arange 类似于内置的range，但返回的是一个ndarray而不是列表 ones、ones_like 根据指定的形状和dtype创建一个全1数组。ones_like以另一个数组为参数，并根据其形状和dtype创建一个全1数组 zeros、zeros_like 类似于ones、ones_like，产生全0数组 empty、empty_like 创建新数组，只分配内存空间但不填充任何值 eye、identity 创建一个正方的NXN单位矩阵(对角线为1，其余为0) 列表转换 1234In [8]: arr1 = np.array([1,2,3])In [9]: arr1Out[9]: array([1, 2, 3]) 嵌套序列将会被转换为一个多维数组: 123456In [10]: arr2 = np.array([[1,2,3],[1,2,3]])In [11]: arr2Out[11]:array([[1, 2, 3], [1, 2, 3]]) 除非显式说明，np.array会尝试为新建的数组推断一个合适的数据类型。数据类型保存在一个特殊的dtype对象中:12In [12]: arr1.dtypeOut[12]: dtype('int32') 除了np.array之外，zeros和ones可以创建指定长度或形状的全0或全1数组。empty可以创建一个没有任何具体值的数组。使用这些方法创建数组，只需传入一个表示形状的元组:12345678910111213141516171819In [15]: np.zeros(10)Out[15]: array([ 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])In [16]: np.zeros((3,2))Out[16]:array([[ 0., 0.], [ 0., 0.], [ 0., 0.]])In [17]: np.empty((3,2,3))Out[17]:array([[[ 6.23042070e-307, 4.67296746e-307, 1.69121096e-306], [ 1.33511290e-306, 1.15711989e-306, 1.42418987e-306]], [[ 1.37961641e-306, 1.60220528e-306, 1.24611266e-306], [ 9.34598925e-307, 1.24612081e-306, 1.11260755e-306]], [[ 1.60220393e-306, 1.51320640e-306, 9.34609790e-307], [ 1.86921279e-306, 1.24610723e-306, 0.00000000e+000]]]) 使用empty方法创建的数组返回的是一些未初始化的垃圾值，而不是0arange是Python内置函数range的数组版:12In [18]: np.arange(5)Out[18]: array([0, 1, 2, 3, 4]) ndarray的数据类型 NumPy的数据类型 类型 类型代码 说明 int8、uint8 i1、u1 有符号和无符号的8位(1个字节)整型 int16、uint16 i2、u2 有符号和无符号的18位(2个字节)整型 int32、uint32 i4、u4 有符号和无符号的32位(4个字节)整型 int64、uint64 i8、u8 有符号和无符号的64位(8个字节)整型 float16 f2 半精度浮点数 float32 f4或f 标准的单精度浮点数。与C的float兼容 float64 f8或d 标准的双精度浮点数。与C的double和Python的float对象兼容 float128 f16或g 扩展精度浮点数 complex64、complex128、complex256 c8、c16、c32 复数 bool ? 存储True和False的布尔类型 object O Python对象类型 string_ S 固定长度的字符串类型(每个字符1个字节)。例如要创建一个长度为10的字符串，应使用S10 unicode_ U 固定长度的unicode类型(字节数由平台决定)。跟字符串的定义方式一样(如U10) 可以再创建array对象时使用dtype参数设定数据类型，也可以通过astype方法显式转换其dtype,如果将浮点数转换成整数，则小数部分将会被截断；如果字符串数组全是数字，也可以用astype将其转换为数值形式：123456789101112In [19]: arr = np.array([1.2,-3.4,5.6], dtype='f8')In [20]: arr.dtypeOut[20]: dtype('float64')In [21]: arr.astype(np.int32)Out[21]: array([ 1, -3, 5])In [22]: numeric_string = np.array(['1.5','2.5','1.1'],dtype=np.string_)In [23]: numeric_string.astype(np.float64)Out[23]: array([ 1.5, 2.5, 1.1]) 数组和标量之间的运算矢量化 是指数组不用编写任何循环即可对数据执行批量运算。大小相等的数组之间的任何算术运算都会将运算应用到元素级：1234567891011In [24]: arr = np.array([[1,1,1],[2,2,2]])In [25]: arr*arrOut[25]:array([[1, 1, 1], [4, 4, 4]])In [26]: arr+arrOut[26]:array([[2, 2, 2], [4, 4, 4]]) 数组和标量的运算会将标量值传播到各个元素:123456789In [27]: 1/arrOut[27]:array([[ 1. , 1. , 1. ], [ 0.5, 0.5, 0.5]])In [28]: arr ** 0.5Out[28]:array([[ 1. , 1. , 1. ], [ 1.41421356, 1.41421356, 1.41421356]]) 基本的索引和切片将一个标量赋值给一个切片时。该值会自动传播到整个选区。跟列表最重要的区别在于数组切片是原始数组的视图。这说明数据不会被复制，视图上的任何修改都会直接反映到原数组上:2 1234567891011121314151617In [35]: arr = np.arange(10)In [36]: arrOut[36]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])In [37]: arr_slice = arr[2:5]In [38]: arr_sliceOut[38]: array([2, 3, 4])In [39]: arr_slice[1:2] = 1000In [40]: arrOut[40]: array([ 0, 1, 2, 1000, 4, 5, 6, 7, 8, 9])In [41]: arr_sliceOut[41]: array([ 2, 1000, 4]) 对于高维数组，各索引的位置上的元素不在是标量，而是降维数组，索引对各个元素进行递归访问。对二维数组而言，一级索引对应的是一维数组，二级索引对应的是一维数组下的元素索引(这里一维数组的元素是标量),索引有两种方式:123456789101112131415In [42]: arr = np.array([[1,2,3],[4,5,6]])In [43]: arrOut[43]:array([[1, 2, 3], [4, 5, 6]])In [44]: arr[0]Out[44]: array([1, 2, 3])In [45]: arr[0][1]Out[45]: 2In [46]: arr[0,2]Out[46]: 3 对于高维数组而言，不添加索引返回整个数组，添加一级索引，返回一个降维数组(这里是2x3的数组)，添加二级索引则在一级索引的基础上添加索引返回(这里是一维数组):1234567891011121314151617In [47]: arr = np.array([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]])In [48]: arrOut[48]:array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]])In [49]: arr[0]Out[49]:array([[1, 2, 3], [4, 5, 6]])In [50]: arr[0,1]Out[50]: array([4, 5, 6]) 标量和数组都可以赋值给原数组:123456789101112131415161718192021222324252627282930313233343536In [67]: arr = np.array([[[1,2,3],[4,5,6]],[[7,8,9],[10,11,12]]])In [68]: arrOut[68]:array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]])In [69]: old_arr = arr[0].copy()In [70]: old_arrOut[70]:array([[1, 2, 3], [4, 5, 6]])In [71]: arr[0]= 1In [72]: arrOut[72]:array([[[ 1, 1, 1], [ 1, 1, 1]], [[ 7, 8, 9], [10, 11, 12]]])In [73]: arr[0] = old_arrIn [74]: arrOut[74]:array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]]) 切片索引ndarray的切片语法跟Python列表的一维对象差不多，但是高纬度对象的花样很多，可以在一个或多个轴上进行切片，也可以跟整数索引混合使用,传入多个切片和索引相同都是递归切片:312345678910111213141516In [75]: arr2d = np.array([[1,2,3],[3,4,5]])In [76]: arr2dOut[76]:array([[1, 2, 3], [3, 4, 5]])In [77]: arr2d[:2]Out[77]:array([[1, 2, 3], [3, 4, 5]])In [78]: arr2d[:2,:1]Out[78]:array([[1], [3]]) 同时传入索引和切片可以得到低纬度的切片:12In [80]: arr2d[1,:1]Out[80]: array([3]) 只有:表示选取整个轴，可以通过这个对高纬度的进行切片：1234In [81]: arr2d[:,:1]Out[81]:array([[1], [3]]) 同时对切片表达式的赋值也会扩展到整个选区：123456In [82]: arr2d[:,:1] = 0In [83]: arr2dOut[83]:array([[0, 2, 3], [0, 4, 5]]) 布尔型索引可以对数组使用比较运算，其返回一个布尔型数组；可以使用布尔型数组进行数组索引，会返回True对应的数组，布尔型数组的长度必须跟被索引的轴长度一致:12345678910111213141516171819202122232425In [84]: names = np.array(['bob','john','tom'])In [85]: data = randn(3,6)In [86]: namesOut[86]:array(['bob', 'john', 'tom'], dtype='&lt;U4')In [87]: dataOut[87]:array([[-1.38783828, 1.53823048, -0.83396793, 2.53149852, -0.55033656, 0.13621489], [-1.92912846, 1.45011928, 0.76228734, 1.37168505, 0.71817348, -0.48010419], [-0.27052654, 0.72243318, -0.53976533, -0.55488584, -0.18700473, -0.06341261]])In [88]: names == 'bob'Out[88]: array([ True, False, False], dtype=bool)In [90]: data[names == 'bob']Out[90]:array([[-1.38783828, 1.53823048, -0.83396793, 2.53149852, -0.55033656, 0.13621489]]) 布尔型数组还可以和切片、整数(或整数序列)混合使用:12345In [91]: data[names == 'bob',2:]Out[91]: array([[-0.83396793, 2.53149852, -0.55033656, 0.13621489]])In [92]: data[names == 'bob',2]Out[92]: array([-0.83396793]) 要选取除某个元素以外的值，可以使用不等号(!=)，也可以通过~对条件进行否定；同时也可以使用&amp;(和)、|(或)之类的布尔运算符：1234567891011121314151617181920212223In [93]: data[names != 'bob']Out[93]:array([[-1.92912846, 1.45011928, 0.76228734, 1.37168505, 0.71817348, -0.48010419], [-0.27052654, 0.72243318, -0.53976533, -0.55488584, -0.18700473, -0.06341261]])In [95]: data[~(names == 'bob')]Out[95]:array([[-1.92912846, 1.45011928, 0.76228734, 1.37168505, 0.71817348, -0.48010419], [-0.27052654, 0.72243318, -0.53976533, -0.55488584, -0.18700473, -0.06341261]])In [97]: data[(names == 'bob')| (names=='tom')]Out[97]:array([[-1.38783828, 1.53823048, -0.83396793, 2.53149852, -0.55033656, 0.13621489], [-0.27052654, 0.72243318, -0.53976533, -0.55488584, -0.18700473, -0.06341261]])In [98]: data[(names == 'bob')&amp; (names=='tom')]Out[98]: array([], shape=(0, 6), dtype=float64) 通过布尔索引选取数组中的数据，总是创建数据的副本，即使返回一模一样的数组。 花式索引花式索引(Fancy indexing)指的是利用整数数组进行索引。为了以特定顺序选取行子集，只需传入一个指定顺序的整数列表或ndarray即可：12345678910111213141516171819202122In [102]: arr = np.empty((7,4))In [103]: for i in range(7): ...: arr[i]=i ...:In [104]: arrOut[104]:array([[ 0., 0., 0., 0.], [ 1., 1., 1., 1.], [ 2., 2., 2., 2.], [ 3., 3., 3., 3.], [ 4., 4., 4., 4.], [ 5., 5., 5., 5.], [ 6., 6., 6., 6.]])In [105]: arr[[4,3,1,2]]Out[105]:array([[ 4., 4., 4., 4.], [ 3., 3., 3., 3.], [ 1., 1., 1., 1.], [ 2., 2., 2., 2.]]) 使用负数索引将会从末尾开始选取行:123456In [106]: arr[[-4,-3,-1,-2]]Out[106]:array([[ 3., 3., 3., 3.], [ 4., 4., 4., 4.], [ 6., 6., 6., 6.], [ 5., 5., 5., 5.]]) 一次性传入多个索引数组返回的是一个一维数组，其中的元素对应各个索引元组：1234567891011121314151617In [107]: arr = np.arange(32).reshape((8,4))In [108]: arrOut[108]:array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23], [24, 25, 26, 27], [28, 29, 30, 31]])In [109]: arr[[1,2,3,4],[1,2,3,4]]In [110]: arr[[1,2,3,4],[0,1,2,3]]Out[110]: array([ 4, 9, 14, 19]) 其中选出的元素是(1,0),(2,1),(3,2),(4,3)。而选取矩阵的行列子集的方法如下：123456In [111]: arr[[1,2,3,4]][:,[0,1,2,3]]Out[111]:array([[ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19]]) 选取矩阵的另一个方法是使用np.ix_函数，它可以将两个一维整数数组转换为一个用于选取方形区域的索引器：123456In [112]: arr[np.ix_([1,2,3,4],[0,1,2,3])]Out[112]:array([[ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19]]) 花式索引总是将数据复制到新数组中。 数组转置和轴对换转置(transpose)是重塑的一种特殊形式，它返回的是源数据的视图。数组不仅有transpose方法，还有一个特殊的T属性，在计算内积的时候经常需要用到：12345678910111213In [114]: arr.TOut[114]:array([[ 0, 4, 8, 12, 16, 20, 24, 28], [ 1, 5, 9, 13, 17, 21, 25, 29], [ 2, 6, 10, 14, 18, 22, 26, 30], [ 3, 7, 11, 15, 19, 23, 27, 31]])In [115]: np.dot(arr.T, arr)Out[115]:array([[2240, 2352, 2464, 2576], [2352, 2472, 2592, 2712], [2464, 2592, 2720, 2848], [2576, 2712, 2848, 2984]]) transpose需要得到一个由轴编号组成的元组才能对这些轴进行转置:12345678910111213141516171819202122232425262728In [126]: arrOut[126]:array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]])In [127]: arr.shapeOut[127]: (2, 2, 3)In [128]: arr.transpose((1,2,0))Out[128]:array([[[ 0, 6], [ 1, 7], [ 2, 8]], [[ 3, 9], [ 4, 10], [ 5, 11]]])In [129]: arr.transpose((1,0,2))Out[129]:array([[[ 0, 1, 2], [ 6, 7, 8]], [[ 3, 4, 5], [ 9, 10, 11]]]) 上面arr.shape返回的结果是(2,2,3) 表明这是一个三维数组，形状为2x2x3,执行arr.transpose((1,2,0))对三个轴进行了重新排列形状变成了2x3x2。在原数组中元素对应的下标为：12345[[[(0,0,0), (0,0,1), (0,0,2)], [(0,1,0), (0,1,1), (0,1,2)]], [[(1,0,0), (1,0,1), (1,0,2)], [(1,1,0), (1,1,1),(1,1,2)]]] 进行轴变换之后的原下标变为:123456789101112131415(0,0,0)-&gt;(0,0,0)(0,0,1)-&gt;(0,1,0)(0,0,2)-&gt;(0,2,0)(0,1,0)-&gt;(1,0,0)(0,1,1)-&gt;(1,1,0)(0,1,2)-&gt;(1,2,0)(1,0,0)-&gt;(0,0,1)(1,0,1)-&gt;(0,1,1)(1,0,2)-&gt;(0,2,1)(1,1,0)-&gt;(1,0,1)(1,1,1)-&gt;(1,1,1)(1,1,2)-&gt;(1,2,1) 将将上面重新编号后的索引对应轴排列并将原数组对应的数字填入其中：123456[[[(0,0,0), (0,0,1)], -&gt; (0,0,0) ,(1,0,0) -&gt; 0,6 [(0,1,0), (0,1,1)], -&gt; (0,0,1) ,(1,0,1) -&gt; 1,7 [(0,2,0), (0,2,1)]], -&gt; (0,0,2) ,(1,0,2) -&gt; 2,8 [[(1,0,0), (1,0,1)], -&gt; (0,1,0) ,(1,1,0) -&gt; 3,9 [(1,1,0), (1,1,1)], -&gt; (0,1,1) ,(1,1,1) -&gt; 4,10 [(1,2,0), (1,2,1)]] -&gt; (0,1,2) ,(1,1,2) -&gt; 5, 11 所以最后的数组变为：1234567array([[[ 0, 6], [ 1, 7], [ 2, 8]], [[ 3, 9], [ 4, 10], [ 5, 11]]]) swapaxes方法需要接受一对轴编号,其返回的事源数据的视图:12345678910111213array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]]])In [138]: arr.swapaxes(0,1)Out[138]:array([[[ 0, 1, 2], [ 6, 7, 8]], [[ 3, 4, 5], [ 9, 10, 11]]]) 1.NumPy将通过语句import numpy as np导入 ↩2.使用副本需要显式地进行复制操作，arr[1:4].copy() ↩3.轴根据shape返回元组的大小确认，如果arr.shape返回(2,2,3)则表示这是一个三维数组，0就是对应第一个2的数轴，指的第一维，1对应第二个2的数轴，指的第二维，2对应3的数轴，指的第三维 ↩]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>NumPy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IPython入门]]></title>
    <url>%2F2018%2F03%2F15%2FIPython%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[IPython基础IPython的环境需要自行安装。如果已经安装了Python，可以通过执行pip install ipython安装。然后只需要在命令行输入ipython就能启动：12345Python 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:54:40) [MSC v.1900 64 bit (AMD64)]Type 'copyright', 'credits' or 'license' for more informationIPython 6.2.1 -- An enhanced Interactive Python. Type '?' for help.In [1]: 可以在IPython中执行任何Python语句，和使用Python解释器一样：1234567891011121314151617181920In [1]: import numpy as npIn [2]: from numpy.random import randnIn [3]: data = &#123;i:randn() for i in range(10)&#125;In [4]: dataOut[4]:&#123;0: -0.24193324837938815, 1: 0.22563840475528563, 2: 0.14465306885873513, 3: 0.5076262433687561, 4: 0.9067731627966235, 5: 0.23827518072962814, 6: 0.3233586627456586, 7: 0.0327013232275763, 8: -0.357340429464286, 9: -1.4105691657079547&#125;In [5]: 许多Python对象都被格式化为可读性更好的形式 Tab键自动完成在shell中输入表达式时，只要按下Tab键，当前命名空间中任何与已输入的字符串相匹配的变量(对象、函数等)就会被找出来：12345678In [5]: an_example1 = 15In [6]: an_example2 = 20In [7]: an&lt;TAB&gt; an_example1 AnalogCommonProxyStub.dll an_example2 and any() 也可以在任何对象后面输入一个句点以便自动完成方法和属性的输入：123456In [7]: a = [1, 2, 3]In [8]: a.&lt;TAB&gt; append() count() insert() reverse() clear() extend() pop() sort() copy() index() remove() 应用在模块上:123456In [8]: import datetimeIn [9]: datetime. date() MAXYEAR timedelta datetime MINYEAR timezone datetime_CAPI time() tzinfo() IPython默认会隐藏那些以下划线开头的方法和属性。如果需要应Tab键自动完成，可以先输入一个下划线。也可以直接修改IPython配置文件中的相关设置。Tab键还可以找出电脑文件系统中与之匹配的东西：1234In [6]: ca&lt;TAB&gt; callable() %%capture catchLink/ 其中 catchLibk/ 为当前目录下的一个子目录。在使用补全目录的时候需要使用正斜杠(/)，文件夹或文件名中间不能有空格。 内省在变量前面或者后面加上一个问号(?)就可以将有关该对象的一些通用信息显示:123456789In [2]: b = []In [3]: b?Type: listString form: []Length: 0Docstring:list() -&gt; new empty listlist(iterable) -&gt; new list initialized from iterable's items 如果该对象是一个函数或实例方法，则其docstring也会被显示出来：1234567891011121314151617181920In [4]: def add_number(a,b): ...: """ ...: Add two numbers together ...: Returns ...: ----------------------- ...: the sum: type of arguments ...: """ ...: return a+b ...: ...:In [5]: add_number?Signature: add_number(a, b)Docstring:Add two numbers togetherReturns-----------------------the sum: type of argumentsFile: d:\python\&lt;ipython-input-4-7144b04645ed&gt;Type: function 使用??还将显示源代码:12345678910111213In [6]: add_number??Signature: add_number(a, b)Source:def add_number(a,b): """ Add two numbers together Returns ----------------------- the sum: type of arguments """ return a+bFile: d:\python\&lt;ipython-input-4-7144b04645ed&gt;Type: function ?还可以搜索IPython的命名空间，一些字符再配以通配符(*)即可显示出所有与该通配符表达式相匹配的名称:12345678In [7]: import numpy as npIn [8]: np.*load*?np.__loader__np.loadnp.loadsnp.loadtxtnp.pkgload %run命令在IPython会话环境中，所有文件都可以通过%run命令当做Python程序来运行。现在在目录下有一个叫做ipython_script_test.py的脚本：1234567891011#!/usr/bin/python3# -*- coding:utf-8 -*-def f(x, y, z): return (x+y) /za = 1b = 2c = 3result = f(a, b, c) 然后运行，并且运行成功后该文件中所定义的全部变量(import、函数和全局变量)都可以在IPython shell中访问:1234567In [9]: %run ipython_script_test.pyIn [10]: resultOut[10]: 1.0In [11]: aOut[11]: 1 中断正在执行的代码任何代码在执行时只要按下“Ctrl-C/control-C”,就会引发一个KeyboardInterrupt，除非Python代码已经调用某个已编译的扩展模块需要等待Python解释器重新获取控制权外，绝大部分Python程序将立即停止执行。 执行剪切板中的代码使用%paste和%cpaste两个魔术函数粘贴代码在shell中以整体执行： %paste 123456789In [12]: %pastedef f(x, y, z): return (x+y) /za = 1b = 2c = 3result = f(a, b, c)## -- End pasted text -- %cpaste 相比于%paste，%cpaste多出了一个用于粘贴代码的特殊提示符,若果发现粘贴的代码有错，只需按下“Ctrl-C/control-C”即可终止%cpaste提示符：12345678910In [16]: %cpastePasting code; enter '--' alone on the line to stop or use Ctrl-D.:def f(x, y, z):: return (x+y) /z::a = 1:b = 2:c = 3:result = f(a, b, c):-- 键盘快捷键IPython提供了许多用于提示符导航和查阅历史shell命令的键盘快捷键(C指代Ctrl或control)： 命令 说明 C-P或上箭头 后向搜索命令历史中以当前输入的文本开头的命令 C-N或下箭头 前向搜索命令历史中以当前输入的文本开头的命令 C-R 按行读取的反向历史搜索(部分匹配) C-Shift-V/Command-Shift-V 从剪切板粘贴文本 C-C 终止当前正在执行的代码 C-A 将光标移动到行首 C-E 将光标移动到行尾 C-K 删除从光标开始至行尾的文本 C-U 清楚当前行的所有文本(只是和C-K相反，即删除从光标开始至行首的文本) C-F 将光标向前移动一个字符 C-b 将光标向后移动一个字符 C-L 清屏 异常和跟踪如果%run某段脚本或执行某条语句是发生异常，IPython会默认输出整个调用栈跟踪，其中还会附上调用栈各点附近的几行代码作为上下文参考:12345678910111213141516In [17]: %run ipython_bug.py---------------------------------------------------------------------------ZeroDivisionError Traceback (most recent call last)D:\Python\ipython\ipython_bug.py in &lt;module&gt;() 5 b = 2 6 c = 0----&gt; 7 result = f(a, b, c)D:\Python\ipython\ipython_bug.py in f(x, y, z) 1 def f(x, y, z):----&gt; 2 return (x+y) /z 3 4 a = 1 5 b = 2ZeroDivisionError: division by zero 魔术命令IPython有一些特殊命令，它们有的为常见任务提供便利，有的则使控制IPython系统的行为更轻松。魔术命令以百分号 % 为前缀的命令。例如通过%timeit检测任何Python语句的执行时间:1234In [41]: a = np.random.randn(100,100)In [42]: %timeit np.dot(a,a)237 µs ± 40 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) 魔术命令可以看做运行于IPython系统中的命令行程序，使用?即可查看其选项:123456789101112131415161718192021222324252627282930313233343536373839In [44]: %reset?Docstring:Resets the namespace by removing all names defined by the user, ifcalled without arguments, or by removing some types of objects, suchas everything currently in IPython's In[] and Out[] containers (seethe parameters for details).Parameters-----------f : force reset without asking for confirmation.-s : 'Soft' reset: Only clears your namespace, leaving history intact. References to objects may be kept. By default (without this option), we do a 'hard' reset, giving you a new session and removing all references to objects from the current session.in : reset input historyout : reset output historydhist : reset directory historyarray : reset only variables that are NumPy arraysSee Also--------reset_selective : invoked as ``%reset_selective``Examples--------:: In [6]: a = 1 In [7]: a Out[7]: 1 In [8]: 'a' in _ip.user_ns Out[8]: True 魔术命令可以不带百分号使用，只要没有定义与其同名的变量。 常用的魔术命令 命令 说明 %quickref 显示Python的快速参考 %magic 显示所有魔术命令的详细文档 %debug 从最新的异常跟踪的底部进入交互式调试器 %hist 打印命令的输入(可选输出)历史 %pdb 在异常发生后自动进入调试器 %paste 执行剪切板中的Python代码 %cpaste 打开一个特殊提示符以便手工粘贴待执行的Python代码 %reset 删除interactive命名空间的全部变量/名称 %page OBJECT 通过分页器打印输出OBJECT %run script.py 在IPython中执行一个Python脚本文件 %prun statement 通过cProfile执行statement，并打印分析器的输出结果 %time statement 报告statement的执行时间 %timeit statement 多次执行statement以计算系统平均执行时间。对那些执行时间非常小的代码很有用 %who、%who_is、%whos 显示interactive命名空间中定义的变量，信息级别/冗余度可变 %xdel variable 删除variable，并参加过时清除其在IPython中的对象上的一切引用 matplotlib集成与pylab模式启动IPython时加上--pylab标记来集成matplotlibipython --pylab。这样IPython会默认GUI后台集成，就可以创建matplotlib绘图了。并且NumPy和matplotlib的大部分功能会被引入到最顶层的interactive命名空间以产生一个交互式的计算环境。也可以通过%gui对此进行手工设置。123456Python 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:54:40) [MSC v.1900 64 bit (AMD64)]Type 'copyright', 'credits' or 'license' for more informationIPython 6.2.1 -- An enhanced Interactive Python. Type '?' for help.Using matplotlib backend: TkAggIn [1]: 使用命令历史IPython维护着一个位于硬盘上的小型数据库，其中含有执行过的每条命令的文本： 只需很少的按键次数即可搜索、自动完成并执行之前已经执行过的命令 在会话间持久化命令历史 将输入/输出历史记录到日志文件 搜索并重用命令历史如果需要输入之前执行过的相同的命令，只需要按照上面的快捷键表操作，就可以搜索出命令历史中第一个与输入的字符相匹配的命令。既可以后向搜索也可以前向搜索。 输入和输出变量IPython会将输入(输入的文本)和输出(返回的对象)的引用保存在一些特殊变量中。最近的两个输出结果分别保存在 _(一个下划线)和 __(两个下划线)变量中：1234567891011121314In [6]: 1+1Out[6]: 2In [7]: _Out[7]: 2In [8]: _+1Out[8]: 3In [9]: 3+1Out[9]: 4In [10]: __Out[10]: 3 输入的文本保存在名为_ix的变量中，其中 X 是输入行的行号。每个输入变量都有一个对应的输出变量_x:12345In [11]: _i6Out[11]: '1+1'In [12]: _6Out[12]: 2 由于输入变量是字符串，因此可以用Python的exec()方法重新执行:1234In [18]: exec(_i6)In [19]: _Out[19]: '1+1' 有几个魔术命令可以用于控制输入和输出历史。%hist用于打印全部或部分输入历史，可以选择是否带行号。%reset用于清空interactive命名空间，并可选择是否清空输入和输出缓存。%xdel用于从IPython系统中移除特定对象的一切引用。 记录输入和输出IPython能够记录整个控制台会话，包括输入和输出。执行%logstart即可开始记录日志：12345678In [20]: %logstartActivating auto-logging. Current session state plus future input saved.Filename : ipython_log.pyMode : rotateOutput logging : FalseRaw input log : FalseTimestamping : FalseState : active IPython的日志功能可以在任何时刻开启。还有与%logstart配套的%logoff、%logon、%logstate和%logstop，可以参考其文档。 与操作系统交互可以在IPython中实现标准的Windows或UNIX命令行活动，将命令的执行结果保存在Python对象中 跟系统相关的IPython魔术命令 命令 说明 !cmd 在系统shell中执行cmd output=!cmd args 执行cmd，并将stout存放在output中 %alias alias_name cmd 为系统shell命令定义别名 %bookmark 使用IPython的目录书签系统 %cd directory 将系统工作目录更改为directory %pwd 返回系统的当前工作目录 %pushd directory 将当前目录入栈，并转向目标目录 %popd 弹出栈顶目录，并转向该目录 %dirs 返回一个含有当前目录栈的列表 %dhist 打印目录访问历史 %env 以dict形式返回系统环境变量 shell命令和别名在IPython中，以感叹号(!)开头的命令行表示其后的所有内容需要在系统shell中执行:1234In [23]: !pythonPython 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:54:40) [MSC v.1900 64 bit (AMD64)] on win32Type "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; 还可以将shell命令的控制台输出存放到变量中，只需将 ! 开头的表达式赋值给变量:1234In [152]: ip_info = !lsIn [153]: ip_infoOut[153]: ['experiment.py', 'ipython_bug.py', 'ipython_script_test.py'] 软件开发工具IPython集成并加强了Python内置的pdb调试器，同时提供了一些简单易用的代码运行时间及性能分析工具。 交互式调试器IPython的调试器增强了pdb，如Tab键自动完成、语法高亮、为异常跟踪的每条信息添加上下文参考。%debug命令(在发生异常之后马上输入)将会调用那个“事后”调试器，并直接跳转到引发异常的那个栈帧：123456789101112131415161718192021222324In [45]: %run ipython_bug.py---------------------------------------------------------------------------ZeroDivisionError Traceback (most recent call last)D:\Python\ipython\ipython_bug.py in &lt;module&gt;() 5 b = 2 6 c = 0----&gt; 7 result = f(a, b, c)D:\Python\ipython\ipython_bug.py in f(x, y, z) 1 def f(x, y, z):----&gt; 2 return (x+y) /z 3 4 a = 1 5 b = 2ZeroDivisionError: division by zeroIn [46]: %debug&gt; d:\python\ipython\ipython_bug.py(2)f() 1 def f(x, y, z):----&gt; 2 return (x+y) /z 3 4 a = 1 5 b = 2 在这个调试器中，可以执行任意Python代码并查看各个栈帧中的一切对象和数据。默认是从最低级开始(即错误发生的地方)。输入u(或up)和d(或down)即可在栈跟踪的各级别之间切换:123456789101112131415ipdb&gt; u&gt; d:\python\ipython\ipython_bug.py(7)&lt;module&gt;() 3 4 a = 1 5 b = 2 6 c = 0----&gt; 7 result = f(a, b, c)ipdb&gt; d&gt; d:\python\ipython\ipython_bug.py(2)f() 1 def f(x, y, z):----&gt; 2 return (x+y) /z 3 4 a = 1 5 b = 2 执行%pdp命令可以让IPython在出现异常之后自动调用调试器。如果需要设置断点或对函数/脚本进行单步调试以查看各条语句的执行情况时，可以使用带有-d选项的%run命令，这会在执行脚本文件中的代码之前打开调试器，然后输入s(或step)步进才能进入脚本:123456789101112131415161718192021222324252627282930313233In [50]: %run -d ipython_bug.pyBreakpoint 1 at d:\python\ipython\ipython_bug.py:1NOTE: Enter 'c' at the ipdb&gt; prompt to continue execution.&gt; d:\python\ipython\ipython_bug.py(1)&lt;module&gt;()1---&gt; 1 def f(x, y, z): 2 return (x+y) /z 3 4 a = 1 5 b = 2ipdb&gt; s&gt; d:\python\ipython\ipython_bug.py(4)&lt;module&gt;() 2 return (x+y) /z 3----&gt; 4 a = 1 5 b = 2 6 c = 0ipdb&gt; s&gt; d:\python\ipython\ipython_bug.py(5)&lt;module&gt;() 3 4 a = 1----&gt; 5 b = 2 6 c = 0 7 result = f(a, b, c)ipdb&gt; s&gt; d:\python\ipython\ipython_bug.py(6)&lt;module&gt;() 3 4 a = 1 5 b = 2----&gt; 6 c = 0 7 result = f(a, b, c) 通过b num在num行出设置断点，输入c(或continue)使脚本一直运行下去直到该断点时为止,然后输入n(或next)直到执行下一行(即step over):1234567891011121314151617181920212223242526272829303132333435363738In [53]: %run -d ipython_bug.pyBreakpoint 1 at d:\python\ipython\ipython_bug.py:1NOTE: Enter 'c' at the ipdb&gt; prompt to continue execution.&gt; d:\python\ipython\ipython_bug.py(1)&lt;module&gt;()1---&gt; 1 def f(x, y, z): 2 return (x+y) /z 3 4 a = 1 5 b = 2ipdb&gt; b 7Breakpoint 2 at d:\python\ipython\ipython_bug.py:7ipdb&gt; c&gt; d:\python\ipython\ipython_bug.py(7)&lt;module&gt;() 3 4 a = 1 5 b = 2 6 c = 02---&gt; 7 result = f(a, b, c)ipdb&gt; nZeroDivisionError: division by zero&gt; d:\python\ipython\ipython_bug.py(7)&lt;module&gt;() 3 4 a = 1 5 b = 2 6 c = 02---&gt; 7 result = f(a, b, c)ipdb&gt; n--Return--None&gt; d:\python\ipython\ipython_bug.py(7)&lt;module&gt;() 3 4 a = 1 5 b = 2 6 c = 02---&gt; 7 result = f(a, b, c) IPython调试器命令 命令 功能 h(elp) 显示命令列表 help command 显示command的文档 c(ontinue) 恢复程序的执行 q(uit) 退出调试器，不再执行任何代码 b(readk) number 在当前文件的第number行设置一个断点 b path/to/file.py:number 在指定文件的第number行设置一个断点 s(tep) 单步进入函数调用 n(ext) 执行当前行，并前进到当前级别的下一行 u(p)/d(own) 在函数调用栈中向上或向下移动 a(rgs) 显示当前函数的参数 debug statement 在新的(递归)调试器中调用语句statement l(ist) statement 显示当前行，以及当前栈级别的上下文参考代码 w(here) 打印当前位置的完整栈跟踪(包括上下文参考代码) 测试代码的执行时间:%time和%timeit%time一次执行一条语句，然后报告总体执行时间1234567In [56]: strings = ['foo','bar','abc','foobar','python','Guide Peple']*100000In [57]: %time method1 = [x for x in strings if x.startswith('foo')]Wall time: 102 msIn [58]: %time method2 = [x for x in strings if x[:3] == 'foo']Wall time: 59.2 ms %timeit对于任意语句，它会自动多次执行以产生一个非常精确的平均执行时间12345In [59]: %timeit method1 = [x for x in strings if x.startswith('foo')]100 ms ± 5.73 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)In [60]: %timeit method2 = [x for x in strings if x[:3] == 'foo']57 ms ± 7.12 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) 基本性能分析：%prun和%run -p代码的性能分析跟代码执行时间密切相关，只不过它关注的事耗费时间的位置，主要的Python性能分析工具是cProfile模块。CProfile在执行一个程序或代码块时，会记录各函数所耗费的时间。CProfile一般在命令行上使用，它将执行整个程序然后输出各函数的执行时间。%prun分析的是Python语句而不是整个.py文件：123456789101112131415161718192021222324252627In [141]: %cpastePasting code; enter '--' alone on the line to stop or use Ctrl-D.:def run_experiment(niter=100): k = 100 results = [] for _ in range(niter): mat = np.random.randn(k, k) max_eigenvalue = np.abs(eigvals(mat)).max() results.append(max_eigenvalue) return results:::::::::--In [142]: %prun -l 7 -s cumulative run_experiment() 3804 function calls in 0.901 seconds Ordered by: cumulative time List reduced from 31 to 7 due to restriction &lt;7&gt; ncalls tottime percall cumtime percall filename:lineno(function) 1 0.000 0.000 0.901 0.901 &#123;built-in method builtins.exec&#125; 1 0.000 0.000 0.901 0.901 &lt;string&gt;:1(&lt;module&gt;) 1 0.002 0.002 0.901 0.901 &lt;ipython-input-141-78ef833ef08b&gt;:1(run_experiment) 100 0.814 0.008 0.838 0.008 linalg.py:834(eigvals) 100 0.060 0.001 0.060 0.001 &#123;method 'randn' of 'mtrand.RandomState' objects&#125; 100 0.012 0.000 0.018 0.000 linalg.py:213(_assertFinite) 300 0.008 0.000 0.008 0.000 &#123;method 'reduce' of 'numpy.ufunc' objects&#125; 执行%run -p -s cumulative experiment.py也能达到以上的效果，无需退出IPython:123456789101112131415In [75]: %run -p -l 7 -s cumulative experiment.pyLargest one we saw:11.9165340849 3888 function calls (3887 primitive calls) in 0.467 seconds Ordered by: cumulative time List reduced from 77 to 7 due to restriction &lt;7&gt; ncalls tottime percall cumtime percall filename:lineno(function) 2/1 0.000 0.000 0.467 0.467 &#123;built-in method builtins.exec&#125; 1 0.000 0.000 0.467 0.467 &lt;string&gt;:1(&lt;module&gt;) 1 0.000 0.000 0.467 0.467 interactiveshell.py:2445(safe_execfile) 1 0.000 0.000 0.467 0.467 py3compat.py:182(execfile) 1 0.000 0.000 0.467 0.467 experiment.py:1(&lt;module&gt;) 1 0.001 0.001 0.466 0.466 experiment.py:5(run_experiment) 100 0.431 0.004 0.436 0.004 linalg.py:819(eigvals) ipython html notebook需要安装 jupyter 来使用该功能:1pip3 install jupyter 这是一个基于Web的交互式计算文档格式。它有一种基于JSON的文档格式.upynb，可以轻松分享代码、输出结果以及图片等内容。执行如下命令启动：1jupyter notebook 这是运行于命令行上的轻量级服务器进程，Web浏览器会自动打开Notebook的仪表盘。]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>IPython</tag>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python文本处理]]></title>
    <url>%2F2018%2F03%2F13%2FPython%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[逗号分割值(CSV)CSV简介逗号分割值(Comma-Spearated Value, CSV) 通常用于在电子表格软件和纯文本之间交互数据。CSV文件内容仅仅是一些用逗号分隔的原始字符串值。CSV格式的文件需要专门用于解析和生成CSV的库，不能使用str.splt(&#39;,&#39;)来解析，因为会处理单个字段中含有逗号的情形。123456789101112131415161718192021222324252627282930#!/usr/bin/python3# -*- coding:utf-8 -*-import csv# 创建需要导入的数据源DATA = ( (1, 'Web Clients and Servers', 'base64,urllib'), (2, 'Web program：CGI &amp; WSGI', 'cgi, time, wsgiref'), (3, 'Web Services', 'urllib,twython'))print('*** WRITTING CSV DATA')# 打开一个csv文件，使用utf-8编码，同时为了防止写入时附加多的空白行设置newline为空with open('bookdata.csv', 'w', encoding='utf-8', newline='') as w: # csv.writer笑一个打开的文件(或类文件)对象，返回一个writer对象 # 可以用来在打开的文件中逐行写入逗号分隔的数据。 writer = csv.writer(w) for record in DATA: writer.writerow(record)# writer对象提供一个writerow()方法print('****REVIEW OF SAVED DATA')with open('bookdata.csv', 'r', encoding='utf-8') as r: # csv.reader()用于返回一个可迭代对象，可以读取该对象并解析为CSV数据的每一行 # csv.reader()使用一个已打开文件的句柄，返回一个reader对象 # 当逐行迭代数据时，CSV数据会自动解析并返回给用户 reader = csv.reader(r) for chap, title, modpkgs in reader: print('Chapter %s: %r (featureing %s)' % (chap, title, modpkgs)) 输出结果 12345*** WRITTING CSV DATA****REVIEW OF SAVED DATAChapter 1: &apos;Web Clients and Servers&apos; (featureing base64,urllib)Chapter 2: &apos;Web program：CGI &amp; WSGI&apos; (featureing cgi, time, wsgiref)Chapter 3: &apos;Web Services&apos; (featureing urllib,twython) csv模块还提供csv.DictReader类和csv.DictWriter类，用于将CSV数据读进字典中(首先查找是否使用给定字段名，如果没有，就是用第一行作为键)，接着将字典字段写入CSV文件中。 JSONJSON是JavaScript的子集，专门用于指定结构化的数据。JSON是以人类更易读的方式传输结构化数据。 JSON和Python类型之间的区别 JSON Python3 object dict array list tuple string str number(int) int number(real) float true True false False null None json提供了dump()/load()和dumps()/loads()。除了基本参数外，这些函数还包含许多仅用于JSON的选项。模块还包括encoder类和decoder类，用户既可以继承也可以直接使用。Python字典可以转化为JSON对象，Python列表和元组也可以转成对应的JSON数组。12345678910111213141516171819202122232425262728293031323334353637383940#!/usr/bin/python3# -*- coding:UTF-8 -*-# 返回一个表示Python对象的字符串# 用来美观地输出Python对象from json import dumpsfrom pprint import pprint# Python字典，使用字典是因为其可以构建具有结构化层次的属性。# 在等价的JSON表示方法中，会移除所有额外的逗号Books = &#123; '0000001': &#123; 'title': 'Core', 'edition': 2, 'year': 2007, &#125;, '0000002': &#123; 'title': 'Python Programming', 'edition': 3, 'authors': ['Jack', 'Bob', 'Jerry'], 'year': 2009, &#125;, '0000003': &#123; 'title': 'Programming', 'year': 2009, &#125;&#125;# 显示转储的Python字典print('***RAW DICT***')print(Books)# 使用更美观的方法输出print('***PRETTY_PRINTED DICT***')pprint(Books)# 使用json.dumps()内置的美观的输出方式，传递缩进级别print('***PRETTY_PRINTED JSON***')print(dumps(Books, indent=4)) 输出结果 12345678910111213141516171819202122232425262728293031***RAW DICT***&#123;&apos;0000001&apos;: &#123;&apos;title&apos;: &apos;Core&apos;, &apos;edition&apos;: 2, &apos;year&apos;: 2007&#125;, &apos;0000002&apos;: &#123;&apos;title&apos;: &apos;Python Programming&apos;, &apos;edition&apos;: 3, &apos;authors&apos;: [&apos;Jack&apos;, &apos;Bob&apos;, &apos;Jerry&apos;], &apos;year&apos;: 2009&#125;, &apos;0000003&apos;: &#123;&apos;title&apos;: &apos;Programming&apos;, &apos;year&apos;: 2009&#125;&#125;***PRETTY_PRINTED DICT***&#123;&apos;0000001&apos;: &#123;&apos;edition&apos;: 2, &apos;title&apos;: &apos;Core&apos;, &apos;year&apos;: 2007&#125;, &apos;0000002&apos;: &#123;&apos;authors&apos;: [&apos;Jack&apos;, &apos;Bob&apos;, &apos;Jerry&apos;], &apos;edition&apos;: 3, &apos;title&apos;: &apos;Python Programming&apos;, &apos;year&apos;: 2009&#125;, &apos;0000003&apos;: &#123;&apos;title&apos;: &apos;Programming&apos;, &apos;year&apos;: 2009&#125;&#125;***PRETTY_PRINTED JSON***&#123; &quot;0000001&quot;: &#123; &quot;title&quot;: &quot;Core&quot;, &quot;edition&quot;: 2, &quot;year&quot;: 2007 &#125;, &quot;0000002&quot;: &#123; &quot;title&quot;: &quot;Python Programming&quot;, &quot;edition&quot;: 3, &quot;authors&quot;: [ &quot;Jack&quot;, &quot;Bob&quot;, &quot;Jerry&quot; ], &quot;year&quot;: 2009 &#125;, &quot;0000003&quot;: &#123; &quot;title&quot;: &quot;Programming&quot;, &quot;year&quot;: 2009 &#125;&#125; XMLXML数据是纯文本数据，但是其可读性不高，所以需要使用解析器进行解析。 将字典转化为XML 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#!/usr/bin/python3# -*- coding:UTF-8 -*-#from xml.etree.ElementTree import Element, SubElement, tostringfrom xml.dom.minidom import parseString# Python字典，使用字典是因为其可以构建具有结构化层次的属性。# 在等价的JSON表示方法中，会移除所有额外的逗号Books = &#123; '0000001': &#123; 'title': 'Core', 'edition': 2, 'year': 2007, &#125;, '0000002': &#123; 'title': 'Python Programming', 'edition': 3, 'authors': 'Jack:Bob:Jerry', 'year': 2009, &#125;, '0000003': &#123; 'title': 'Programming', 'year': 2009, &#125;&#125;# 创建顶层对象# 将所有其他内容添加到该节点下books = Element('books')for isbn, info in Books.items(): # 对于每一本书添加一个book子节点 # 如果原字典没有提供作者和版本，则使用提供的默认值。 book = SubElement(books, 'book') info.setdefault('authors', 'Bob') info.setdefault('edition', 1) for key, val in info.items(): # 遍历所有键值对，将这些内容作为其他子节点添加到每个book中 SubElement(book, key).text = ', '.join(str(val).split(':'))xml = tostring(books)print('*** RAW XML***')print(xml)print('***PRETTY-PRINTED XML***')dom = parseString(xml)print(dom.toprettyxml(' '))print('***FLAT STRUCTURE***')for elmt in books.iter(): print(elmt.tag, '-', elmt.text)print('\n***TITLE ONLY***')for book in books.findall('.//title'): print(book.text) 输出结果 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647*** RAW XML***b&apos;&lt;books&gt;&lt;book&gt;&lt;title&gt;Core&lt;/title&gt;&lt;edition&gt;2&lt;/edition&gt;&lt;year&gt;2007&lt;/year&gt;&lt;authors&gt;Bob&lt;/authors&gt;&lt;/book&gt;&lt;book&gt;&lt;title&gt;Python Programming&lt;/title&gt;&lt;edition&gt;3&lt;/edition&gt;&lt;authors&gt;Jack, Bob, Jerry&lt;/authors&gt;&lt;year&gt;2009&lt;/year&gt;&lt;/book&gt;&lt;book&gt;&lt;title&gt;Programming&lt;/title&gt;&lt;year&gt;2009&lt;/year&gt;&lt;authors&gt;Bob&lt;/authors&gt;&lt;edition&gt;1&lt;/edition&gt;&lt;/book&gt;&lt;/books&gt;&apos;***PRETTY-PRINTED XML***&lt;?xml version=&quot;1.0&quot; ?&gt;&lt;books&gt; &lt;book&gt; &lt;title&gt;Core&lt;/title&gt; &lt;edition&gt;2&lt;/edition&gt; &lt;year&gt;2007&lt;/year&gt; &lt;authors&gt;Bob&lt;/authors&gt; &lt;/book&gt; &lt;book&gt; &lt;title&gt;Python Programming&lt;/title&gt; &lt;edition&gt;3&lt;/edition&gt; &lt;authors&gt;Jack, Bob, Jerry&lt;/authors&gt; &lt;year&gt;2009&lt;/year&gt; &lt;/book&gt; &lt;book&gt; &lt;title&gt;Programming&lt;/title&gt; &lt;year&gt;2009&lt;/year&gt; &lt;authors&gt;Bob&lt;/authors&gt; &lt;edition&gt;1&lt;/edition&gt; &lt;/book&gt;&lt;/books&gt;***FLAT STRUCTURE***books - Nonebook - Nonetitle - Coreedition - 2year - 2007authors - Bobbook - Nonetitle - Python Programmingedition - 3authors - Jack, Bob, Jerryyear - 2009book - Nonetitle - Programmingyear - 2009authors - Bobedition - 1***TITLE ONLY***CorePython ProgrammingProgramming]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>文本处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Web框架:Django]]></title>
    <url>%2F2018%2F03%2F12%2FWeb%E6%A1%86%E6%9E%B6-Django%2F</url>
    <content type="text"><![CDATA[Django简介 安装在使用Django开发之前，必须安装必需的组件，包括依赖组件和Django本身 1pip3 install django 项目和应用项目 是指的一系列文件，用来创建并运行一个完整的Web站点。在项目文件夹下，有一个或多个子文件夹，每个文件夹有特定的功能，称为 应用。应用不一定要位于项目文件夹中。应用可以专注于项目某一方面的功能，或可以作为通用组件，用于不同的项目。应用是一个具有特定功能的子模块，这些子模块组合起来就能完成Web站点的功能。 在Django中创建项目Django自带有一个名为django-admin.py/django-admin.exe的工具，它可以简.化任务。在POSIX平台上，一般在/usr/local/bin、/usr/bin这样的目录中。使用Windows系统会安装在Python包下的Scripts目录下，如E:\Python\Python36\Scripts。两种系统都应该确保文件位于PATH环境变量中。在项目文件加下执行命令创建项目: 1django-admin.py startproject mysite Django项目文件 文件名 描述/用途 init.py 告诉Python这是一个软件包 urls.py 全局URL配置(“URLconf”) setting.py 项目相关的配置 manage.py 应用的命令行接口 运行开发服务器Django内置Web服务器，该服务器运行在本地，专门用于开发阶段，仅用于开发用途。使用开发服务器有以下几个优点： 可以直接运行与测试项目和应用，无需完整的生产环境 当改动Python源码文件并重新载入模块时，开发服务器会自动检测，无须每次编辑代码后手动重启 开发服务器知道如何为Django管理应用程序寻找和显示静态媒体文件，所以无须立即了解管理方面的内容 启动服务器 1python manage.py runserver 应用创建应用在项目目录下使用如下命令创建一个应用：1python3 ./manage.py startapp blog 这样就建立了一个blog目录，其中有如下内容： 文件名 描述/目的 __init.py 告诉Python这是一个包 urls.py 应用的URL配置文件(“URLconf”)，这个文件并不像项目的URLconf那样自动创建 models.py 数据模型 views.py 视图函数(即MVC中的控制器) tests.py 单元测试 与项目类似，应用也是一个Python包。本地应用的URLconf需要手动创建，接着使用URLconf里的include()指令将请求分配给应用的URLconf。为了让Django知道这个新应用是项目的一部分，需要编辑 settings.py，将应用名称(blog)添加到元组的末尾。Django使用 INSTALLED_APPS 来配置系统的不同部分，包括自动管理应用程序和测试框架。123456789INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'blog'] 创建模型添加数据库服务创建模型models.py 将定义博客的数据结构，首先创建一个基本类型。数据模型表示将会存储在数据库每条记录的数据类型。Django提供了许多字段类型，用来将数据映射到应用中。1234567891011121314from django.db import models# Create your models here.class BlogPost(models.Model): """ django.db.models.Model的子类Model是Django中用于数据模型的标准基类。 BlogPost中的字段像普通类属性那样定义， 每个都是特定字段类的实例，每个实例对应数据库中的一条记录。 """ title = models.CharField(max_length=150) body = models.TextField() timestamp = models.DateTimeField() 创建数据库在项目的setting.py文件中设置数据库。关于数据库，有6个相关设置(有时只需要两个):ENGINE、NAME、HOST、PORT、USER、PASSWORD。只需要在相关设置选项后面添上需要让Django使用的数据库服务器中合适的值即可。 使用MySQL 1234567891011DATABASES = &#123; # 使用mysql 'default': &#123; 'ENGINE': 'django.db.backends.mysql', 'NAME': 'django_test', 'USER': 'root', 'PASSWORD': '', 'HOST': 'localhost', 'PORT': '3306', &#125;&#125; 使用SQLiteSQLite一般用于测试，它没有主机、端口、用户、密码信息。因为其使用本地文件存储信息，本地文件系统的访问权限就是数据库的访问控制。SQLite不仅可以使用本地文件，还可以使用纯内存数据库。使用实际的Web服务器(如Apache)来使用SQLite时，需要确保拥有Web服务器进程的账户同时拥有数据库文件本身和含有数据库文件目录的写入权限。 1234567DATABASES = &#123; # 使用sqlite 'default': &#123; 'ENGINE': 'django.db.backends.sqlite3', 'NAME': os.path.join(BASE_DIR, 'db.sqlite3'), &#125;&#125; 创建表使用 makemigrations 参数创建映射文件，当执行命令时Django会查找INSTALLED_APPS中列出的应用的models.py文件。对于每个找到的模型，都会创建一个映射表。1python3 ./manage.py makemigrations 使用 migrate 映射到数据库1python3 ./manage.py migrate Python应用Shell在Django中使用Python shell即使没有模版(view)或视图(controller)，也可以通过添加一些BlogPost项来测试数据模型。如果应用由RDBMS支持，则可以为每个blog项的表添加一个数据记录。如果使用的是NoSQL数据库，则需要向数据库中添加其他对象、文档或实体。通过以下命令启动shell(使用对应版本)：123456python3 ./manage.py shellPython 3.6.4 (default, Jan 6 2018, 11:51:59)Type &apos;copyright&apos;, &apos;credits&apos; or &apos;license&apos; for more informationIPython 6.2.1 -- An enhanced Interactive Python. Type &apos;?&apos; for help.In [1]: Django shell和标准的shell相比更专注于Django项目的环境，可以与视图函数和数据模型交互，这个shell会自动设置环境变量，包括sys.path，它可以访问Django与自己项目中的模块和包，否则需要手动配置。除了标准shell之外，还有其他的交互式解释器可供选择。Django更倾向于使用功能丰富的shell，如IPython和bpython，这些shell在普通的解释器基础上提供及其强大的功能。运行shell命令时，Django首先查找含有扩展功能的shell，如果没有回返回标准解释器。这里使用的是IPython。也可以使用 -i 来强制使用普通解释器。1234567python3 ./manage.py shell -i pythonPython 3.6.4 (default, Jan 6 2018, 11:51:59)[GCC 4.2.1 Compatible Apple LLVM 9.0.0 (clang-900.0.39.2)] on darwinType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.(InteractiveConsole)&gt;&gt;&gt; 测试数据模型在启动Python shell之后输入一些Python或IPython命令来测试应用及其数据模型。1234567891011121314151617181920212223242526272829303132333435In [1]: from datetime import datetimeIn [2]: from blog.models import BlogPostIn [3]: BlogPost.objects.all()Out[3]: &lt;QuerySet [&lt;BlogPost: BlogPost object (1)&gt;, &lt;BlogPost: BlogPost object (2)&gt;, &lt;BlogPost: BlogPost object (3)&gt;]&gt;In [4]: bp = BlogPost(title=&apos;my blog&apos;, body=&apos;&apos;&apos; ...: my 1st blog... ...: yoooo!&apos;&apos;&apos;, ...: timestamp=datetime.now())In [5]: bpOut[5]: &lt;BlogPost: BlogPost object (None)&gt;In [6]: bp.save()In [7]: BlogPost.objects.count()Out[7]: 4In [9]: bp = BlogPost.objects.all()[0]In [11]: print(bp.title)test shellIn [13]: print(bp.body)my 1st blog post...yo!In [14]: bp.timestamp.ctime()Out[14]: &apos;Sun Mar 11 08:13:31 2018&apos; 前两行命令导入相应的对象，第3步查询数据库中BlogPost对象，第4步是实例化一个BlogPost对象来向数据库中添加BlogPost对象，向其中传入对应属性的值(title、body和timestamp)。创建完对象后，需要通过BlogPost.save()方法将其写入到数据库中。完成创建和写入后，使用BlogPost.objects.count()方法确认数据库中对象的个数。然后获取BlogPost对象列表的第一个元素并获取对应属性的值。设置时区:123456789LANGUAGE_CODE = 'zh-hans'TIME_ZONE = 'Asia/Shanghai'USE_I18N = TrueUSE_L10N = TrueUSE_TZ = False Django管理应用admin应用让开发者在完成完整的UI之前验证处理数据的代码。 设置admin在 setting.py 的INSTALLED_APP中添加&#39;django.contrib.admin&#39;,，然后运行python3 ./manage.py makemigrations和python3 ./manage.py migrate两条命令来创建其对应的表。在admin设置完之后于 urls.py 中设置url路径：123456from django.contrib import adminfrom django.urls import pathurlpatterns = [ path('admin/', admin.site.urls),] 最后应用程序需要告诉Django哪个模型需要在admin页面中显示并编辑，这时候就需要在应用的 admin.py 中注册BlogPost：1234from django.contrib import adminfrom blog import models# Register your models here.admin.site.register(models.BlogPost) 使用admin使用命令python3 ./manage.py runserver启动服务，然后在浏览器中输入 http://localhost:8000/admin 访问admin页面。在访问之前使用python3 manage.py createsuperuser创建的超级用户的用户名和密码用于登录管理页面。（账号：root，密码：Aa123456）为了更好地显示博文列表，更新blog/admin.py文件，使用新的BlogPostAdmin类：12345678910from django.contrib import adminfrom blog import models# Register your models here.class BlogPostAdmin(admin.ModelAdmin): list_display = ('title', 'timestamp')admin.site.register(models.BlogPost, BlogPostAdmin) 创建博客的用户界面Django shell和admin是针对于开发者的工具，而现在需要构建用户的界面。Web页面应该有以下几个经典组建： 模板，用于显示通过Python类字典对象传入的信息 视图函数，用于执行针对请求的核心逻辑。视图会从数据库中获取信息，并格式化显示结果 模式，将传入的请求映射到对应的视图中，同时也可以将参数传递给视图 Django是自底向上处理请求，它首先查找匹配的URL模式，接着调用对应的视图函数，最后将渲染好的数据通过模板展现给用户。构建应用可以按照如下顺序： 因为需要一些可观察对象，所以先创建基本的模板 设计一个简单的URL模式，让Django可以立刻访问应用 开发出一个视图函数原型，然后在此基础上迭代开发在构建应用过程中模板和URL模式不会发生太大的变化，而应用的核心是视图。这非常符合 测试驱动模型(TDD) 的开发模式。 创建模板 变量标签变量标签 是由 花括号() 括起来的内容，花括号内用于显示对象的内容。在变量标签中，可以使用Python风格的 点分割标识 访问这些变量的属性。这些值可以是纯数据，也可以是可调用对象，如果是后者，会自动调用这些对象而无需添加圆括号”()”来表示这个函数或方法可调用。 过滤器过滤器 是在变量标签中使用的特殊函数，它能在标签中立即对变量进行处理。方法是在变量右边插入一个 管道符号(“|”)，接着跟上过滤器名称。&lt;h2&gt; { { post.title | title } } &lt;/h2&gt; 上下文上下文 是一种特殊的Python字典，是传递给模板的变量。假设通过上下文传入的BlogPost对象称为”post”。通过上下文传入所有的博文，这样可以通过循环显示所有文章。 块标签块标签 通过花括号和百分号来表示：&#123;%…%&#125;，它们用于向HTML模版中插入如循环或判断这样的逻辑。 将HTML模版代码保存到一个简单的模版文件中，命名为archive.html，放置在应用文件夹下的 templates 目录下，模版名称任取，但模版目录一定是 templates12345&#123;%for post in posts%&#125; &lt;h2&gt;&#123;&#123;post.title&#125;&#125;&lt;/h2&gt; &lt;h2&gt;&#123;&#123;post.timestamp&#125;&#125;&lt;/h2&gt; &lt;h2&gt;&#123;&#123;post.body&#125;&#125;&lt;/h2&gt;&#123;% endfor%&#125; 创建URL模式 项目的URLconf服务器通过WSGI的功能，最终会将请求传递给Django。接受请求的类型(GET、POST等)和路径(URL中除了协议、主机、端口之外的内容)并传递到项目的URLconf文件(mysite/urls.py)。为了符合代码重用、DRY、在一处调试相同的代码等准则，需要应用能负责自己的URL。在项目的urls.py(这里时mysite/urls.py)中添加url配置项，让其指向应用的URLconf。123456789from django.contrib import adminfrom django.urls import pathfrom django.urls import includeurlpatterns = [ path('admin/', admin.site.urls), # include函数将动作推迟到其他URLconf # 这里将以blog/开通的请求缓存起来，并传递给mysite/blog/urls.py path('blog/', include('blog.urls'))] include() 会移除当前的URL路径头，路径中剩下的部分传递给下游URLconf中的path()函数。（当输入’http://localhost:8080/blog/foo/bar‘ 这个URL时，项目的URLconf接收到的是blog/foo/bar，匹配blog找到一个include()函数，然后将foo/bar传递给mysite/blog/urls.py）。上述代码中使用include()和未使用include()的区别在于使用include()传递的是 字符串，未使用include传递的是 对象。 应用的URLconf在项目的URLconf中通过include()包含blog.urls，让匹配blog应用的URL将剩余的部分传递到blog应用中处理。在mysite/blog/urls.py(没有就创建),添加以下代码：123456from django.urls import *import blog.viewsurlpatterns = [ # 第一个参数是路径，第二个参数是视图函数，在调用到这个URL时用于处理信息 path('', blog.views.archive)] 请求URL的头部分(blog/)匹配到的是根URLconf已经被去除。添加新的视图在列表中添加一行代码即可。 创建视图函数一个简单的视图函数会从数据库获取所有博文，并使用模板显示给用户： 向数据库查询所有博客条目 载入模板文件 为模板创建上下文字典 将模板渲染到HTML中 通过HTTP响应返回HTML在应用的views.py中添加如下代码:12345678910from django.shortcuts import renderfrom blog.models import BlogPostfrom django.template import loader, Contextfrom django.shortcuts import render_to_response# Create your views here.def archive(request): posts = BlogPost.objects.all() return render_to_response('archive.html', &#123;'posts': posts&#125;) 改进输出现在得到了一个可以工作的应用，有了可以工作的简单博客，可以响应客户端的请求，从数据库提取信息，向用户显示博文。现在更改查询方式，让博文按时间逆序显示，并且限制每页显示的数目。 BlogPOST是数据模型类。Objects属性是模型的Manager类，其中含有all()方法来获取QuerySet。QuerySet执行“惰性迭代”，在求值时才会真正查询数据库。 实现排序只需调用order_by()方法时提供一个排序参数即可(views.py)：1234def archive(request): # 在timestamp前面加上减号(-)指定按时间逆序排列。正常的升序只需要移除减号 posts = BlogPost.objects.all().order_by('-timestamp') return render_to_response('archive.html', &#123;'posts': posts&#125;) 为了测试限制显示数目，先启动Django shell添加数据：12345678910python ./manage.py shellPython 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:54:40) [MSC v.1900 64 bit (AMD64)] on win32Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.(InteractiveConsole)&gt;&gt;&gt; from datetime import datetime&gt;&gt;&gt; from blog.models import BlogPost&gt;&gt;&gt; for i in range(10):... bp = BlogPost(title=&apos;post $%d&apos; % i ,body=&apos;body of post $%d&apos; %d, timestamp=datetime.now())... bp.save()... 然后使用切片的方式获取最新的10篇(views.py)：1234def archive(request): # 在timestamp前面加上减号(-)指定按时间逆序排列。正常的升序只需要移除减号 posts = BlogPost.objects.all().order_by('-timestamp')[:10] return render_to_response('archive.html', &#123;'posts': posts&#125;) 设置模型的默认排序方式 如果在模型中设置首选的排序方式，其他基于Django的应用或访问这个数据的项目也会使用这个顺序。为了给模型设置默认顺序，需要创建一个名为 Meta 的内部类，在其中设置一个名为 ordering 的属性(models.py):123456789101112class BlogPost(models.Model): """ django.db.models.Model的子类Model是Django中用于数据模型的标准基类。 BlogPost中的字段像普通类属性那样定义， 每个都是特定字段类的实例，每个实例对应数据库中的一条记录。 """ title = models.CharField(max_length=150) body = models.TextField() timestamp = models.DateTimeField() class Meta: ordering = ('-timestamp',) 取消视图函数中的排序(views.py):1234def archive(request): # 在timestamp前面加上减号(-)指定按时间逆序排列。正常的升序只需要移除减号 posts = BlogPost.objects.all()[:10] return render_to_response('archive.html', &#123;'posts': posts&#125;) 处理用户输入 添加一个HTML表单，让用户可以输入数据(archive.html),为了防止 1234567891011121314&lt;form action="/blog/create/" method="post"&gt; Title: &lt;input type="text" name="title"&gt;&lt;br&gt; Body: &lt;textarea name="body" rows="3" cols="60"&gt;&lt;/textarea&gt;&lt;br&gt; &lt;input type="submit"&gt;&lt;/form&gt;&lt;hr&gt;&#123;%for post in posts%&#125; &lt;h2&gt;&#123;&#123;post.title&#125;&#125;&lt;/h2&gt; &lt;p&gt;&#123;&#123;post.timestamp&#125;&#125;&lt;/p&gt; &lt;p&gt;&#123;&#123;post.body&#125;&#125;&lt;/p&gt;&lt;hr&gt;&#123;% endfor %&#125; 插入(URL，视图)这样的URLConf项使用前面的HTML，需要用到/blog/create/的路径，所以需要将其关联到一个视图函数中，该函数用于把内容保存到数据库中，这个函数命名为create_blogpost()，在应用的urls.py中添加： 1234567from django.urls import *import blog.viewsurlpatterns = [ # 第一个参数是路径，第二个参数是视图函数，在调用到这个URL时用于处理信息 path('', blog.views.archive), path(r'create/', blog.views.create_blogpost)] 创建视图来处理用户输入在应用的views.py中添加上面定义的处理方法 1234567891011def create_blogpost(request): if request.method == 'POST': # 检查POST请求 # 创建新的BlogPost项，获取表单数据，并用当前时间建立时间戳。 BlogPost( title=request.POST.get('title'), body=request.POST.get('body'), timestamp=datetime.now() ).save() # 重定向会/blog return HttpResponseRedirect('/blog') 在完成上面的步骤之后，会发现创建表单的调用会被拦截报403的错误。这是因为Django有数据保留特性，不允许不安全的POST通过 跨站点请求伪造（Cross-site Request Forgery,CSRF） 来进行攻击。需要在HTML表单添加CSRF标记(&#123;% csrf_token %&#125;):123456789101112131415&lt;form action="/blog/create/" method="post"&gt;&#123;%csrf_token%&#125; Title: &lt;input type="text" name="title"&gt;&lt;br&gt; Body: &lt;textarea name="body" rows="3" cols="60"&gt;&lt;/textarea&gt;&lt;br&gt; &lt;input type="submit"&gt;&lt;/form&gt;&lt;hr&gt; &#123;%for post in posts%&#125; &lt;h2&gt;&#123;&#123;post.title&#125;&#125;&lt;/h2&gt; &lt;p&gt;&#123;&#123;post.timestamp&#125;&#125;&lt;/p&gt; &lt;p&gt;&#123;&#123;post.body&#125;&#125;&lt;/p&gt;&lt;hr&gt;&#123;% endfor %&#125; 通过模板发送向这些标记请求的上下文实例，这里将archive()方法调用的render_to_response()改为render:1234def archive(request): # 在timestamp前面加上减号(-)指定按时间逆序排列。正常的升序只需要移除减号 posts = BlogPost.objects.all()[:10] return render(request, 'archive.html', &#123;'posts': posts&#125;) 表单和模型表单 如果表单字段完全匹配一个数据模型，则通过Django ModelForm能更好的完成任务(models.py): 123456class BlogPostForm(forms.ModelForm): class Meta: # 定义一个Meta类，他表示表单基于哪个数据模型。当生成HTML表单时，会含有对应数据模型中的所有属性字段。 # 不信赖用户输入正确的时间戳可以通过添加exclude属性来设置。 model = BlogPost exclude = ('timestamp',) 使用ModelForm来生成HTML表单(archive.html): 123456789101112&lt;form action="/blog/create/" method="post"&gt;&#123;%csrf_token%&#125; &lt;table&gt;&#123;&#123;form&#125;&#125;&lt;/table&gt; &lt;input type="submit"&gt;&lt;/form&gt;&lt;hr&gt; &#123;%for post in posts%&#125; &lt;h2&gt;&#123;&#123;post.title&#125;&#125;&lt;/h2&gt; &lt;p&gt;&#123;&#123;post.timestamp&#125;&#125;&lt;/p&gt; &lt;p&gt;&#123;&#123;post.body&#125;&#125;&lt;/p&gt;&lt;hr&gt;&#123;% endfor %&#125; 因为数据已经存在于数据模型中，便不用去通过请求获取单个字段，而由于timestamp不能从表单获取，所以修改后的views.py中create_blogpost()方法如下: 12345678910111213141516def create_blogpost(request): if request.method == 'POST': # 检查POST请求 # 创建新的BlogPost项，获取表单数据，并用当前时间建立时间戳。 # BlogPost( # title=request.POST.get('title'), # body=request.POST.get('body'), # timestamp=datetime.now() # ).save() form = BlogPostForm(request.POST) if form.is_valid(): post = form.save(commit=False) post.timestamp = datetime.now() post.save() # 重定向会/blog return HttpResponseRedirect('/blog') 添加测试Django通过扩展Python自带的单元测试模块来提供测试功能。Django还可以测试文档字符串(即docstring)，这称为 文档测试(doctest) 应用的tests.py 12345678910111213141516171819202122232425262728293031323334353637383940414243from django.test import TestCasefrom datetime import datetimefrom django.test.client import Clientfrom blog.models import BlogPost# Create your tests here.class BlogPostTest(TestCase): # 测试方法必须以“test_”开头，方法名后面的部分随意。 def test_obj_create(self): # 这里仅仅通过测试确保对象成功创建，并验证标题内容 BlogPost.objects.create( title='raw title', body='raw body', timestamp=datetime.now()) # 如果两个参数相等则测试成功，否则该测试失败 # 这里验证对象的数目和标题 self.assertEqual(1, BlogPost.objects.count()) self.assertEqual('raw title', BlogPost.objects.get(id=1).title) def test_home(self): # 在'/blog/'中调用应用的主页面，确保收到200这个HTTP返回码 response = self.client.get('/blog/') self.assertIn(response.status_code, (200, )) def test_slash(self): # 测试确认重定向 response = self.client.get('/') self.assertIn(response.status_code, (301, 302)) def test_empty_create(self): # 测试'/blog/create/'生成的视图，测试在没有任何数据就错误地生成GET请求， # 代码应该忽略掉这个请求，然后重定向到'/blog' response = self.client.get('/blog/create/') self.assertIn(response.status_code, (301, 302)) def test_post_create(self): # 模拟真实用户请求通过POST发送真实数据，创建博客项，让后将用户重定向到"/blog" response = self.client.post('/blog/create/', &#123; 'title': 'post title', 'body': 'post body' &#125;) self.assertIn(response.status_code, (301, 302)) self.assertEqual(1, BlogPost.objects.count()) self.assertEqual('post title', BlogPost.objects.get(id=1).title) 源代码]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Web框架</tag>
        <tag>Django</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CGI和WSGI]]></title>
    <url>%2F2018%2F03%2F08%2FCGI%E5%92%8CWSGI%2F</url>
    <content type="text"><![CDATA[CGI这里将会主要介绍CGI的含义、与Web服务器的工作方式，使用Python创建CGI应用 CGI简介 通用网关接口(Common Gateway Interface CGI) 在Web服务器和应用之间充当了交互作用 Web服务器从客户端接收到请求(GET或POST)，并调用相应的应用程序 Web服务器和客户端等待HTML页面 应用程序处理完成后将会生成动态的HTML页面返回服务器，服务器将这个结果返回给用户 表单处理过程，服务器与外部应用程序交互，收到并生成的HTML页面通过CGI返回客户端含有需要用户输入项(文本框、单选按钮等)、Submit按钮、图片的Web页面，都会涉及某种CGI活动。创建HTML的CGI应用程序通常是高级语言来实现的，可以接受、处理用户数据，向服务器端返回HTML页面。CGI有明显的局限性，以及限制Web服务器同时处理客户端的数量。(CGI被抛弃的原因) CGI应用程序和和相关模块 CGI应用程序CGI 应用程序和典型的应用程序主要区别在于输入、输出以及用户和程序的交互方面。当一个CGI脚本启动后，需要获得用户提供的表单数据，但这些数据必须从Web客户端才可以获得，这就是 请求(request)。与标准输出不同，这些输出将会发送回连接的Web客户端，而不是发送到屏幕、GUI窗口或者硬盘上。这些返回的数据必须是具有一系列有效头文件的HTML标签数据。用户和脚本之间没有任何交互，所有交互都发生在Web客户端(基于用户的行为)、Web服务器端和CGI应用程序间。 cgi模块cgi模块有一个主要类 FieldStorage 完成了所有的工作。Python CGI脚本启动会实例化这个类，通过Web服务器从Web客户端读出相关的用户信息。在实例化完成后，其中会包含一个类似字典的对象，它具有一系列键值对。键就是通过表单传入的表单条目的名字，而值则包含响应的数据。这些值有三个对象：FieldStorage 对象；MiniFieldStorage 对象用在没有文件上传或mulitple-part格式数据的情况下，MiniFieldStorage 实例只包含名称和数据的键值对；当表单中的某个字段有多个输入值时，还可以是这些对象的列表。 cgitb模块cgitb模块用于在浏览器中看到Web应用程序的回溯信息，而不是“内部服务器错误”。 CGI应用程序 再启动服务器的目录下创建一个cgi-bin目录，放入Python CGI脚本。将一些HTML文件放到启动服务器的目录中。确保启动服务器目录中有个cgi-bin目录，同时确保其中有相应的.py文件。否则服务器将会把Python文件作为静态文本返回而不是执行它们 CGI服务器 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232#!/usr/bin/python3# -*- coding:UTF-8 -*-from cgi import FieldStoragefrom os import environfrom io import StringIOfrom urllib.parse import quote, unquoteclass AdvCGI(object): # 创建header和url静态类变量，在显示不同页面的方法中会用到这些变量 header = 'Content-Type:text/html\n\n' url = '/cgi-bin/advcgi.py' # HTML静态文本表单，其中含有程序语言设置和每种语言的HTML元素 formhtml = ''' &lt;HTML&gt; &lt;HEAD&gt; &lt;TITLE&gt;Advanced CGI Demo&lt;/TITLE&gt; &lt;/HEAD&gt; &lt;BODY&gt; &lt;H2&gt;Advanced CGI Demo&lt;/H2&gt; &lt;FORM METHOD=post ACTION='%s' ENCTYPE='multipart/form-data'&gt; &lt;H3&gt;My Cookie Setting&lt;/H3&gt; &lt;LI&gt; &lt;CODE&gt;&lt;B&gt;CPPuser = %s&lt;/B&gt;&lt;/CODE&gt; &lt;H3&gt;Enter cookie value&lt;BR&gt; &lt;INPUT NAME=cookie value='%s'/&gt;(&lt;I&gt;optional&lt;/I&gt;) &lt;/H3&gt; &lt;H3&gt;Enter your name&lt;BR&gt; &lt;INPUT NAME=person VALUE='%s'/&gt;(&lt;I&gt;required&lt;/I&gt;) &lt;/H3&gt; &lt;H3&gt;What languages can you program in ? (&lt;I&gt;at least one required&lt;/I&gt;) &lt;/H3&gt; %s &lt;H3&gt;Enter file to upload&lt;SMALL&gt;(max size 4k)&lt;/SMALL&gt;&lt;/H3&gt; &lt;INPUT TYPE=file NAME=upfile VALUE='%s' SIZE=45&gt; &lt;P&gt;&lt;INPUT TYPE=submit /&gt; &lt;/LI&gt; &lt;/FORM&gt; &lt;/BODY&gt; &lt;/HTML&gt; ''' langset = ('Python', 'Java', 'C++', 'C', 'JavaScript') langItem = '&lt;INPUT TYPE=checkbox NAME=lang VALUE="%s"%s&gt; %s\n' def get_cpp_cookies(self): """ 当浏览器对应用进行连续调用时，将相同的cookie通过HTTP头发送回服务器 :return: """ # 通过HTTP_COOKIE访问这些值 if 'HTTP_COOKIE' in environ: cookies = [x.strip() for x in environ['HTTP_COOKIE'].split(';')] for eachCookie in cookies: # 寻找以CPP开头的字符串 # 只查找，名为“CPPuser”和“CPPinfo”的cookie值 if len(eachCookie) &gt; 6 and eachCookie[:3] == 'CPP': # 去除索引8处的值进行计算，计算结果保存到Python对象中 tag = eachCookie[3:7] try: # 查看cookie负载，对于非法的Python对象，仅仅保存相应的字符串值。 self.cookies[tag] = eval(unquote(eachCookie[8:])) except (NameError, SyntaxError): self.cookies[tag] = unquote(eachCookie[8:]) # 如果这个cookie丢失，就给他指定一个空字符串 if 'info' not in self.cookies: self.cookies['info'] = '' if 'user' not in self.cookies: self.cookies['user'] = '' else: self.cookies['info'] = self.cookies['user'] = '' if self.cookies['info'] != '': self.who, langstr, self.fn = self.cookies['info'].split(';') self.langs = langstr.split(',') else: self.who = self.fn = '' self.langs = ['Python'] def show_form(self): """ 将表单显示给用户 :return: """ # 从之前的请求中(如果有)获取cookie，并适当地调整表单的格式 self.get_cpp_cookies() langstr = [] for eachLang in AdvCGI.langset: langstr.append(AdvCGI.langItem % ( eachLang, ' CHECKED' if eachLang in self.langs else '', eachLang)) if not ('user' in self.cookies and self.cookies['user']): cookstatus = '&lt;I&gt;(cookie has not been set yet)&lt;/I&gt;' usercook = '' else: usercook = cookstatus = self.cookies['user'] print('%s%s' % (AdvCGI.header, AdvCGI.formhtml % ( AdvCGI.url, cookstatus, usercook, self.who, ''.join(langstr), self.fn))) errhtml = ''' &lt;HTML&gt; &lt;HEAD&gt; &lt;TITLE&gt;Advanced CGI Demo&lt;/TITLE&gt; &lt;/HEAD&gt; &lt;BODY&gt; &lt;H3&gt;ERROR&lt;/H3&gt; &lt;B&gt;%s&lt;/B&gt; &lt;P&gt; &lt;FORM&gt; &lt;INPUT TYPE= button VALUE=Back ONCLICK="window.history.back()"&gt;&lt;/INPUT&gt; &lt;/FORM&gt; &lt;/BODY&gt; &lt;/HTML&gt; ''' def show_error(self): """ 生成错误页面 :return: """ print('%s%s' % (AdvCGI.header, AdvCGI.errhtml % (self.error))) reshtml = ''' &lt;HTML&gt; &lt;HEAD&gt; &lt;TITLE&gt;Advanced CGI Demo&lt;/TITLE&gt; &lt;/HEAD&gt; &lt;BODY&gt; &lt;H2&gt;Your Uploaded Data&lt;/H2&gt; &lt;H3&gt;Your cookie value is: &lt;B&gt;%s&lt;/B&gt;&lt;/H3&gt; &lt;H3&gt;Your name is: &lt;B&gt;%s&lt;/B&gt;&lt;/H3&gt; &lt;H3&gt;You can program in the following languages:&lt;/H3&gt; &lt;UL&gt;%s&lt;/UL&gt; &lt;H3&gt;Your uploaded file...&lt;BR&gt; Name: &lt;I&gt;%s&lt;/I&gt;&lt;BR&gt; Contents: &lt;/H3&gt; &lt;PRE&gt;%s&lt;/PRE&gt; Click &lt;A HREF="%s"&gt;&lt;B&gt;here&lt;/B&gt;&lt;/A&gt; to return to form. &lt;/BODY&gt; &lt;/HTML&gt;''' def set_cpp_cookies(self): """ 应用程序调用这个方法来发送cookie（从Web服务器）到浏览器，并存储在浏览器中 :return: """ for eachCookie in self.cookies: print('Set-Cookie: CPP%s=%s; path=/' % ( eachCookie, quote(self.cookies[eachCookie]))) def doResult(self): """ 生成结果页面 :return: """ MAXBYTES = 4096 langlist = ''.join('&lt;LI&gt;%s&lt;BR&gt;' % eachLang for eachLang in self.langs) filedata = self.fp.read(MAXBYTES) if len(filedata) == MAXBYTES and f.read(): filedata = '%s%s' % (filedata, '...&lt;B&gt;&lt;I&gt;(file truncated due to size)&lt;/I&gt;&lt;/B&gt;') self.fp.close() if filedata == '': filedata = '&lt;B&gt;&lt;I&gt;(file not give or upload error)&lt;/I&gt;&lt;/B&gt;' filename = self.fn if not ('user' in self.cookies and self.cookies['user']): cookstatus = '&lt;I&gt;(cookie has not been set yet)&lt;/I&gt;' usercook = '' else: usercook = cookstatus = self.cookies['user'] self.cookies['info'] = ':'.join((self.who, ','.join(self.langs), filename)) self.set_cpp_cookies() print('%s%s' % ( AdvCGI.header, AdvCGI.reshtml % (cookstatus, self.who, langlist, filename, filedata, AdvCGI.url))) def go(self): self.cookies = &#123;&#125; self.error = '' form = FieldStorage() if not list(form.keys()): self.show_form() return if 'person' in form: print(form.keys()) self.who = form['person'].value.strip().title() if self.who == '': self.error = 'Your name is required.(blank)' else: self.error = 'Your name is required.(missing)' self.cookies['user'] = unquote(form['cookie'].value.strip()) if 'cookie' in form else '' if 'lang' in form: lang_data = form['lang'] if isinstance(lang_data, list): self.langs = [eachLang.value for eachLang in lang_data] else: self.langs = [lang_data.value] else: self.error = 'At least one language required' if 'upfile' in form: upfile = form['upfile'] self.fn = upfile.filename or '' if upfile.file: self.fp = upfile.file else: self.fp = StringIO('(no data)') else: self.fp = StringIO('(no file)') self.fn = '' if not self.error: self.doResult() else: self.show_error()if __name__ == '__main__': page = AdvCGI() page.go() 启动程序 将启动程序放在启动目录中，然后执行。 1234567#!/usr/bin/python# -*- coding:UTF-8 -*-from http.server import CGIHTTPRequestHandler, testif __name__ == '__main__': test(CGIHTTPRequestHandler) 源代码 WSGIWSGI1 是为了替代CGI而出现的。 服务器集成和外部进程 服务器集成服务器集成也叫 服务器API，其针对CGI性能的解决方案是将网关集成进服务器，不是讲服务器切分成多个语言解释器来分别处理请求，而是生成函数调用，运行应用程序代码，在运行过程中进行响应。服务器根据对应的API通过一组预先创建的进程或线程处理工作。服务器API的会使含有bug的代码影响服务器执行效率，不同语言的实现无法兼容，应用程序必须线程安全。 外部进程外部进程让CGI应用在服务器外部运行。当有请求进入时，服务器将这个请求传递到外部进程中。外部进程存在时间长，不是处理完单个请求后就终止，所以其扩展性比纯CGI好。因为使用了不同的调用机制，所以造成开发者的负担，不仅要开发应用本省，还要决定于服务器的集成。 WSGI简介WSGI只是定义的一个接口，其目标是在Web服务器和Web框架层之间提供一个通用的API标准，减少之间的会操作性并形成统一的调用方式。根据WSGI定义，其应用是可调用对象，其参数固定为：含有服务器环境变量的字典；可调用对象，该对象使用HTTP状态码和返回给客户端的HTTP头来初始化响应。 WSGI服务器在服务器端，必须调用应用，传入环境变量和start_response()这个可调用对象，接着等待应用执行完毕。在执行完成后，必须获得返回的可迭代对象，将这些数据返回给客户端。 1.WSGI只是做一个简单的了解，可以结合框架一起看。 ↩]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Web编程</tag>
        <tag>CGI</tag>
        <tag>WSGI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Web客户端和服务器]]></title>
    <url>%2F2018%2F03%2F06%2FPython-Web%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%92%8C%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[Python Web客户端工具浏览器只是Web客户端的一种。任何一个向Web服务器端发送请求来获取数据的应用程序都是“客户端”。使用urllib模块下载或者访问Web上信息的应用程序就是简单的Web客户端。 统一资源定位符 URL(统一资源定位符)适用于网页浏览的一个地址，这个地址用来在Web上定位一个文档，或者调用一个CGI程序来为客户端生成一个文档。URL是多种统一资源标识符(Uniform Resource Identifier, URI)的一部分。一个URL是一个简单的URI，它使用已有的协议或方案(http/ftp等)。非URL的URI有时称为统一资源名称(Uniform Resource Name, URN)，现在唯一使用的URI只有URL。 URL使用以下格式：post_sch://net_loc/path;parans?query#frag Web地址的各个组件 URL组件 描述 post_sch 网络协议或下载方案 net_loc 服务器所在地(也许含有用户信息) path 使用斜杠(/)分割的文件或CGI应用的路径 params 可选参数 query 连接符(&amp;)分割的一系列键值对 frag 指定文档内特定锚的部分 net_loc可以拆分为多个组件，一些可选一些必备：user:passwd@host:port 网络地址的各个组件 组件 描述 user 用户名或登录(FTP) passwd 用户密码(FTP) host 运行Web服务器的计算机名称或地址(必需的) port 端口号(如果不是默认的80) Python3 使用urllib.parse和urllib.request两种不同的模块分别以不同的功能和兼容性来处理URL urllib.parse模块 urllib.parse核心函数 urllib.parse函数 描述 urllib.parse.urlparse(urlstring, scheme=’’,allow_fragments=True) 将urlstring解析成各个组件，如果在urlstring中没有给定协议或者方法，使用scheme；allow_fragments决定是否允许URL片段 urllib.parse.urlunparse(parts) 将URL数据的一个元组拼成URL字符串 urllib.parse.urljoin(base,url,allow_fragments=True) 将URL的根域名和url拼合成一个完整的URL；allow_fragments的决定是否允许URL片段 urllib.parse.quote(string,safe=’/‘,encoding=None,errors=None) 对string在URL里无法使用的字符进行编码，safe中的字符无需编码 urllib.parse.quote_plus(string,safe=’’,encoding,errors) 除了将空格编译成加(+)号(而非20%)之外，其他功能与quote()相似 urllib.parse.unquote(string,encoding=’utf-8’,errors=’replace’) 将string编译过的字符解码 urllib.parse.unquote_plus(string,encoding=’utf-8’,errors=’replace’) 除了将加好转换为空格，其他功能与unquote()相同 urllib.parse.urlencode(query,doseq=False,safe=’’,encoding=None,errors=None,quote_via=quote_plus) 将query通过quote_plus()编译成有效的CGI查询自妇产，用quote_plus()对这个字符串进行编码 下面将对每个方法进行演示,首先导入urllib.parse下面的所有方法from urllib.parse import * urllib.parse.urlparse(urlstring, scheme=’’,allow_fragments=True) 123urlparse('http://coldjune.com/categories/')# 输出结果ParseResult(scheme='http', netloc='coldjune.com', path='/categories/', params='', query='', fragment='') urllib.parse.urlunparse(parts) 123urlunparse(('http', 'coldjune.com', '/categories/', '', '', ''))# 输出结果'http://coldjune.com/categories/' urllib.parse.urljoin(base,url,allow_fragments=True) 1234567891011121314151617# 如果是绝对路径将整个替换除根域名以外的所有内容urljoin('http://coldjune.com/categories/1.html','/tags/2.html')# 输出结果'http://coldjune.com/tags/2.html'# 如果是相对路径将会将末端文件去掉与心得url连接urljoin('http://coldjune.com/categories/1.html','tags/2.html')# 输出结果'http://coldjune.com/categories/tags/2.html'``* *urllib.parse.quote(string,safe='/',encoding=None,errors=None)*&gt; 逗号、下划线、句号、斜线和字母数字这类符号不需要转换，其他均需转换。URL不能使用的字符前面会被加上百分号(%)同时转换为十六进制(%xx,xx表示这个字母的十六进制) ```Python quote('http://www.~coldjune.com/tag categoriese?name=coold&amp;search=6') # 输出结果 'http%3A//www.%7Ecoldjune.com/tag%20categoriese%3Fname%3Dcoold%26search%3D6' urllib.parse.unquote(string,encoding=’utf-8’,errors=’replace’) 123unquote('http%3A//www.%7Ecoldjune.com/tag%20categoriese%3Fname%3Dcoold%26search%3D6')# 输出结果'http://www.~coldjune.com/tag categoriese?name=coold&amp;search=6' urllib.parse.quote_plus(string,safe=’’,encoding,errors) 123quote_plus('http://www.~coldjune.com/tag categoriese?name=coold&amp;search=6')# 输出结果'http%3A%2F%2Fwww.%7Ecoldjune.com%2Ftag+categoriese%3Fname%3Dcoold%26search%3D6' urllib.parse.unquote_plus(string,encoding=’utf-8’,errors=’replace’) 123unquote_plus('http%3A%2F%2Fwww.%7Ecoldjune.com%2Ftag+categoriese%3Fname%3Dcoold%26search%3D6')# 输出结果'http://www.~coldjune.com/tag categoriese?name=coold&amp;search=6' urllib.parse.urlencode(query,doseq=False,safe=’’,encoding=None,errors=None,quote_via=quote_plus) 1234query=&#123;'name':'coldjune','search':'6'&#125;urlencode(query)# 输出结果'name=coldjune&amp;search=6' urllib.request模块/包 urllib.request模块核心函数 urllib.request函数 描述 urllib.request.urlopen(url, data=None, [timeout,]*,cafile=None, capath=None,cadefault=False,context=None) 打开url(string或者Request对象)，data为发送给服务器的数据，timeout为超时属性， cafile,capath,cadefault为调用HTTPS请求时证书认证 urllib.request.urlretrieve(url,filename=None,reporthook=None,data=None) 将url中的文件下载到filename或临时文件中(如果没有指定filename)；如果函数正在执行，reporthook将会获得下载的统计信息 urllib.request.urlopen(url, data=None, [timeout,],cafile=None, capath=None,cadefault=False,context=None) urlopen()打开url所指向的URL；如果没有给定协议或者下载方案，或者传入”file”方案，urlopen()会打开一个本地文件。对于所有的HTTP请求，使用”GET”请求，向Web服务器发送的请求字符串应该是url的一部分；使用”POST”请求，请求的字符串应该放到data变量中。连接成功后返回的是一个文件类型对象 urlopen()文件类型对象的方法 方法 描述 f.read([bytes]) 从f中读出所有或bytes个字节 f.readline() 从f中读取一行 f.readlines() 从f中读取所有行，作为列表返回 f.close() 关闭f的URL连接 f.fileno() 返回f的文件句柄 f.info() 获取f的MIME头文件 f.geturl() 返回f的真正URL urllib.request.urlretrieve(url,filename=None,reporthook=None,data=None) urlretrieve（）用于下载完整的HTML 如果提供了reporthook函数，则在每块数据下载或传输完成后调用这个函数。调用使用目前读入的块数、块的字节数和文件的总字节数三个参数。urlretrieve()返回一个二元组(local_filename, headers)，local_filename是含有下载数据的本地文件名，headers是Web服务器响应后返回的一系列MIME文件头。 HTTP验证示例 需要先启动本地的tomcat并访问tomcat地址 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#!/usr/bin/python3# -*- coding:UTF-8 -*-import urllib.requestimport urllib.errorimport urllib.parse# 初始化过程# 后续脚本使用的常量LOGIN = 'wesly'PASSWD = "you'llNeverGuess"URL = 'http://localhost:8080/docs/setup.html'REALM = 'Secure Archive'def handler_version(url): # 分配了一个基本处理程序类，添加了验证信息。 # 用该处理程序建立一个URL开启器 # 安装该开启器以便所有已打开的URL都能用到这些验证信息 hdlr = urllib.request.HTTPBasicAuthHandler() hdlr.add_password(REALM, urllib.parse.urlparse(url)[1], LOGIN, PASSWD) opener = urllib.request.build_opener(hdlr) urllib.request.install_opener(opener=opener) return urldef request_version(url): # 创建了一个Request对象，在HTTP请求中添加了简单的base64编码的验证头 # 该请求用来替换其中的URL字符串 from base64 import encodebytes req = urllib.request.Request(url) b64str = encodebytes(bytes('%s %s' % (LOGIN, PASSWD), 'utf-8'))[:-1] req.add_header("Authorization", 'Basic %s' % b64str) return reqfor funcType in ('handler', 'request'): # 用两种技术分别打开给定的URL，并显示服务器返回的HTML页面的第一行 print('***Using %s:' % funcType.upper()) url = eval('%s_version' % funcType)(URL) f = urllib.request.urlopen(url) print(str(f.readline(), 'utf-8')) f.close() 输出结果 12345***Using HANDLER:&lt;!DOCTYPE html SYSTEM &quot;about:legacy-compat&quot;&gt;***Using REQUEST:&lt;!DOCTYPE html SYSTEM &quot;about:legacy-compat&quot;&gt; Web客户端一个稍微复杂的Web客户端例子就是 网络爬虫。这些程序可以为了不同目的在因特网上探索和下载页面。 通过起始地址(URL)，下载该页面和其他后续连接页面，但是仅限于那些与开始页面有相同域名的页面。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181#!/usr/bin/python3# -*- coding:UTF-8 -*-# 导入相关的包，其中bs4中的BeautifulSoup负责解析html文档import osimport sysimport urllib.requestimport urllib.parsefrom bs4 import BeautifulSoupclass Retriever(object): """ 从Web下载页面，解析每个文档中的连接并在必要的时候把它们加入"to-do"队列。 __slots__变量表示实例只能拥有self.url和self.file属性 """ __slots__ = ('url', 'file') def __init__(self, url): """ 创建Retriever对象时调用，将get_file()返回的URL字符串和对 应的文件名作为实例属性存储起来 :param url: 需要抓取的连接 """ self.url, self.file = self.get_file(url) def get_file(self, url, default='index.html'): """ 把指定的URL转换成本地存储的更加安全的文件，即从Web上下载这个文件 :param url: 指定URL获取页面 :param default: 默认的文件名 :return: 返回url和对应的文件名 """ # 将URL的http://前缀移除，丢掉任何为获取主机名 # 而附加的额外信息，如用户名、密码和端口号 parsed = urllib.parse.urlparse(url) host = parsed.netloc.split('@')[-1].split(':')[0] # 将字符进行解码，连接域名创建文件名 filepath = '%s%s' % (host, urllib.parse.unquote(parsed.path)) if not os.path.splitext(parsed.path)[1]: # 如果URL没有文件扩展名后这将default文件加上 filepath = os.path.join(filepath, default) # 获取文件路径 linkdir = os.path.dirname(filepath) if not os.path.isdir(linkdir): # 如果linkdir不是一个目录 if os.path.exists(linkdir): # 如果linkdir存在则删除 os.unlink(linkdir) # 创建同名目录 os.makedirs(linkdir) return url, filepath def download(self): """ 通过给定的连接下载对应的页面，并将url作为参数调用urllib.urlretrieve() 将其另存为文件名。如果出错返回一个以'*'开头的错误提示串 :return: 文件名 """ try: retval = urllib.request.urlretrieve(self.url, filename=self.file) except IOError as e: retval = (('***ERROR: bad URL "%s": %s' % (self.url, e)),) return retval def parse_links(self): """ 通过BeautifulSoup解析文件，查看文件包含的额外连接。 :return: 文件中包含连接的集合 """ with open(self.file, 'r', encoding='utf-8') as f: data = f.read() soup = BeautifulSoup(data, 'html.parser') parse_links = [] for x in soup.find_all('a'): if 'href' in x.attrs: parse_links.append(x['href']) return parse_linksclass Crawler(object): """ 管理Web站点的完整抓取过程。添加线程则可以为每个待抓取的站点分别创建实例 """ # 用于保持追踪从因特网上下载下来的对象数目。没成功一个递增1 count = 0 def __init__(self, url): """ self.q 是待下载的连接队列，这个队列在页面处理完毕时缩短，每个页面中发现新的连接则增长 self.seen 是已下载连接的集合 self.dom 用于存储主链接的域名，并用这个值判定后续连接的域名与主域名是否一致 :param url: 抓取的url """ self.q = [url] self.seen = set() parsed = urllib.parse.urlparse(url) host = parsed.netloc.split('@')[-1].split(':')[0] self.dom = '.'.join(host.split('.')[-2:]) def get_page(self, url, media=False): """ 用于下载页面并记录连接信息 :param url: :param media: :return: """ # 实例化Retriever类并传入需要抓取的连接 # 下在对应连接并取到文件名 r = Retriever(url) fname = r.download()[0] if fname[0] == '*': print(fname, '....skipping parse') return Crawler.count += 1 print('\n(', Crawler.count, ')') print('URL:', url) print('FILE:', fname) self.seen.add(url) # 跳过所有非Web页面 ftype = os.path.splitext(fname)[1] if ftype not in ('.htm', '.html'): return for link in r.parse_links(): if link.startswith('mailto:'): print('...discarded , mailto link') continue if not media: ftype = os.path.splitext(link)[1] if ftype in ('.mp3', '.mp4', '.m4av', '.wav'): print('... discarded, media file') continue if not link.startswith('http://') and ':' not in link: link = urllib.parse.quote(link, safe='#') link = urllib.parse.urljoin(url, link) print('*', link) if link not in self.seen: if self.dom not in link: print('... discarded, not in domain') else: # 如果没有下载过并且是属于该网站就加入待下载列表 if link not in self.q: self.q.append(link) print('...New, added to Q') else: print('...discarded, already in Q') else: print('...discarded, already processed') def go(self, media=False): """ 处理所有待下载连接 :param media: :return: """ while self.q: url = self.q.pop() self.get_page(url, media)def main(): if len(sys.argv) &gt; 1: url = sys.argv[1] else: try: url = input('Enter starting URL:') except (KeyboardInterrupt, EOFError): url = '' if not url: return if not url.startswith('http://') and not url.startswith('ftp://') and not url.startswith('https://'): url = 'http://%s' % url robot = Crawler(url) robot.go()if __name__ == '__main__': main() 解析Web页面BeautifulSoup是解析页面的常用库，这个库不是标准库，需要单独下载。其使用可以参照上例中的代码。 可编程的Web浏览可以使用MechanicalSoup用来模拟浏览器。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Web客户端和服务器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[全双工聊天室]]></title>
    <url>%2F2018%2F03%2F04%2F%E5%85%A8%E5%8F%8C%E5%B7%A5%E8%81%8A%E5%A4%A9%E5%AE%A4%2F</url>
    <content type="text"><![CDATA[在前面的学习中，学习了正则表达式、多线程、网络编程、数据库等相关知识点。这里结合多线程、网络编程、GUI等相关内容实现了一个全双工的聊天室。 设计思路 GUI部分框架的搭建，并编写通用部分代码，完成显示部分的基类 客户端和服务器对GUI基类进行扩展，用于显示各自特有的内容 编程线程的通用类，使所有线程的实现都通过该类，便于统一管理 完成客户端和服务器端的代码并进行整合调试 实现代码GUI的基类 chat_base.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#!/usr/bin/python3# -*- coding:UTF-8 -*-import tkinter as tkclass ChatWindowBase(object): # 窗口的基类，创建通用的窗口布局 def __init__(self): # 初始化方法 # 创建tkinter.TK()顶层窗口 # 所有主要控件都是构建在顶层窗口对象之上 # 通过tkinter.TK()创建 self.top = tk.Tk() # 在顶层窗口上添加Label控件 self.label = tk.Label(self.top, text='聊天室') # 通过Packer来管理和显示控件 # 调用pack()方法显示布局 self.label.pack() # 通过Frame控件创建子容器，用于存放其他控件 # 该对象将作为单个子对象代替父对象 self.chatfm = tk.Frame(self.top) # Scrollbar可以让显示的数据在超过Listbox的大小时能够移动列表 self.chatsb = tk.Scrollbar(self.chatfm) # 将Scrollbar放置在子容器的右侧，并且是针对y轴 self.chatsb.pack(side='right', fill='y') # 在子容器中创建高为15宽为50的Listbox # 将Listbox和Scrollbar关联起来 # 显示列表 # 显示子容器 # 控件的显示应该内部控件先显示，再显示外部控件 self.chats = tk.Listbox(self.chatfm, height=15, width=50, yscrollcommand=self.chatsb.set) self.chatsb.config(command=self.chats.yview()) self.chats.pack(side='left', fill='both') self.chatfm.pack() # 创建发送消息的子容器 self.sendfm = tk.Frame(self.top, width=50) # 创建输入框 # 绑定回车键，并且绑定send方法 # 绑定一个方法是指在触发一个事件时会去调用的方法 self.chatn = tk.Entry(self.sendfm, width=40) self.chatn.bind('&lt;Return&gt;', self.send) self.chatn.pack(side='left') # 添加按钮控件、绑定方法 self.sendchat = tk.Button(self.sendfm, text='发送', command=self.send) self.sendchat.pack(side='right', fill='both') self.sendfm.pack() def send(self, ev=None): # 创建发送消息的方法 # 空实现是为了继承时扩展 pass def receive(self): # 创建接受消息的方法 # 空实现是为了继承时扩展 pass 线程的通用类 chat_thread.py 123456789101112131415161718#!/usr/bin/python3# -*- coding:UTF-8 -*-import threadingclass ChatThread(threading.Thread): # 继承自threading.Thread，用于创建聊天室的通用线程 def __init__(self, func, args): # func: 方法 # args：方法所需要的参数 threading.Thread.__init__(self) self.func = func self.args = args def run(self): # 实现run方法，将参数传给相应的方法 self.func(*self.args) 服务端 chat_s.py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586#!/usr/bin/python3# -*- coding:UTF-8 -*-from chat_base import ChatWindowBasefrom chat_thread import ChatThreadfrom socket import *from time import ctimeimport tkinterHOST = ''PORT = 12345ADDR = (HOST, PORT)BUFSIZ = 1024class ChatS(ChatWindowBase): # 服务器的实现类，继承自ChatWindowBase def __init__(self): # 调用父类的__init__()方法 super(ChatS, self).__init__() self.label.configure(text='服务器') # 设置属性 # 用于保存客户端链接对象 # 用于保存客户端链接地址 self.send_sock = None self.addr = '' # 在服务器窗口创建时调用 self.receive() def send(self, ev=None): # 获取输入框信息 message = self.chatn.get() # 启动线程 ChatThread(self.send_s, (message,)).start() # 将输入框信息按照格式显示在Listbox self.chats.insert('end', '[%s]:to %s\n' % (ctime(), self.addr)) self.chats.insert('end', '%s' % message) # 删除输入框内容 self.chatn.delete(first=0, last=len(message)+1) def receive(self): # 创建socket链接 # 绑定地址 # 设置监听 # 阻塞直到有链接调用，然后保存链接的客户端对象和地址 sock = socket(AF_INET, SOCK_STREAM) sock.bind(ADDR) sock.listen(5) cli_sock, addr = sock.accept() self.addr = addr self.send_sock = cli_sock print('addr', addr) # 有链接接入时在Listbox中显示消息 self.chats.insert('end', '%s 上线' % str(addr)) # 更新顶层窗口 self.top.update() # 启动接受消息的线程 ChatThread(self.receive_s, (cli_sock, addr)).start() def send_s(self, message): # 向客户端发送消息 self.send_sock.send(bytes(message, 'utf-8')) def receive_s(self, cli_sock, addr): # 接受消息 # cli_sock: 客户端sock # addr: 客户端地址 while True: # 进入无限循环接受消息，并在Listbox显示消息 receiveData = cli_sock.recv(BUFSIZ) print('接受到消息', receiveData.decode('utf-8')) self.chats.insert('end', '[%s]:from %s' % (ctime(), addr)) self.chats.insert('end', '%s' % receiveData.decode('utf-8')) self.top.update()def main(): # 创建服务器窗口 s = ChatS() # 调用mainloop()运行整个GUI tkinter.mainloop()if __name__ == '__main__': main() 客户端 chat_c.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#!/usr/bin/python3# -*- coding:UTF-8 -*-from chat_base import ChatWindowBasefrom chat_thread import ChatThreadfrom socket import *from time import ctimeimport tkinterHOST = '127.0.0.1'PORT = 12345ADDR = (HOST, PORT)BUFSIZ = 1024class ChatC(ChatWindowBase): # 客户端的实现类，继承子ChatWindowBase方法 def __init__(self): # 初始化方法 # 在子类中必须调用父类的__init__()方法 super(ChatC, self).__init__() # 设置label的标题 self.label.configure(text='客户端') # 设置属性，用于保存sock对象用于发送和接受消息 self.sock = None # 在创建窗口时链接服务器， # 客户端需要比服务器后创建 # 否则链接会创建失败 self.receive() def send(self, ev=None): # 继承自父类，为控件调用的方法 # 获取输入框的值 message = self.chatn.get() # 创建发送消息的线程 # 将方法和方法需要的参数用作线程初始化，并启动线程 ChatThread(self.send_c, (message,)).start() # 在Listbox中按格式显示消息 self.chats.insert('end', '[%s]:to %s' % (ctime(), ADDR)) self.chats.insert('end', '%s' % message) # 删除输入框中的消息 self.chatn.delete(first=0, last=len(message)+1) # 通过更新顶层窗口显示消息 self.top.update() def receive(self): # 继承自父类 # 创建socket链接 self.sock = socket(AF_INET, SOCK_STREAM) self.sock.connect(ADDR) # 启动线程 # 将方法和方法需要的参数用作线程初始化，并启动线程 ChatThread(self.receive_c, (self.sock,)).start() def send_c(self, message): # 调用sock的send方法，向服务器发送消息 self.sock.send(bytes(message, 'utf-8')) def receive_c(self, sock): # 接受服务器数据的方法 while True: # 进入循环，等待服务器发送的消息 data = sock.recv(BUFSIZ) # 将消息按照格式显示到Listbox中 self.chats.insert('end', '[%s]:from %s' % (ctime(), ADDR)) self.chats.insert('end', '%s' % data.decode('utf-8')) # 更新控件 self.top.update()def main(): # 实例化客户端窗口 c = ChatC() # 调用mainloop方法运行整个GUI tkinter.mainloop()if __name__ == '__main__': main() 源代码]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>多线程</tag>
        <tag>网络编程</tag>
        <tag>GUI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python数据库编程(二)]]></title>
    <url>%2F2018%2F02%2F28%2FPython%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BC%96%E7%A8%8B-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[上一篇中主要对直接操作数据库做了一个比较详细的总结，这里将会对使用ORM框架进行简要的描述。 ORMORM系统的作者将纯SQL语句进行了抽象化处理，将其实现为Python中的对象，这样只操作这些对象就能完成与生成SQL语句相同的任务。 python与ORMSQLAlchemy和SQLObject是两种不同的Python ORM。这两种ORM并不在Python标准库中，所以需要安装。 安装SQLAlchemypip3 install sqlalchemy 安装SQLObjectpip3 install -U SQLObject 在这里将会通过两种ORM移植上一篇的数据库适配器示例应用 SQLAlchemy SQLAlchemy相比于SQLObject的接口更加接近于SQL语句。SQLAlchemy中对象的抽象化十分完成，还可以以更好的灵活性提交原生的SQL语句 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159#!/usr/bin/python3# -*- coding:UTF-8 -*-# 首先导入标准库中的模块(os.path、random)# 然后是第三方或外部模块(sqlalchemy)# 最后是应用的本地模块(ushuffleDB)from os.path import dirnamefrom random import randrange as randfrom sqlalchemy import Column, Integer, \ String, create_engine, exc, ormfrom sqlalchemy.ext.declarative \ import declarative_basefrom ushuffleDB import DBNAME, NAMELEN, \ randName, FIELDS, tformat, cformat, setup# 数据库类型+数据库驱动名称://用户名:密码@地址:端口号/数据库名称DSNs = &#123; 'mysql': 'mysql+pymysql://root:root@localhost:3306/%s' % DBNAME, 'sqlite': 'sqlite:///:memory:',&#125;# 使用SQLAlchemy的声明层# 使用导入的sqlalchemy.ext.declarative.declarative_base# 创建一个Base类Base = declarative_base()class Users(Base): # 数据子类 # __tablename__定义了映射的数据库表名 __tablename__ = 'users' # 列的属性，可以查阅文档来获取所有支持的数据类型 login = Column(String(NAMELEN)) userid = Column(Integer, primary_key=True) projid = Column(Integer) def __str__(self): # 用于返回易于阅读的数据行的字符串格式 return ''.join(map(tformat, (self.login, self.userid, self.projid)))class SQLAlchemyTest(object): def __init__(self, dsn): # 类的初始化执行了所有可能的操作以便得到一个可用的数据库，然后保存其连接 # 通过设置echo参数查看ORM生成的SQL语句 # create_engine('sqlite:///:memory:', echo=True) try: eng = create_engine(dsn) except ImportError: raise RuntimeError() try: eng.connect() except exc.OperationalError: # 此处连接失败是因为数据库不存在造成的 # 使用dirname()来截取掉数据库名，并保留DSN中的剩余部分 # 使数据库的连接可以正常运行 # 这是一个典型的操作任务而不是面向应用的任务，所以使用原生SQL eng = create_engine(dirname(dsn)) eng.execute('CREATE DATABASE %s' % DBNAME).close() eng = create_engine(dsn) # 创建一个会话对象，用于管理单独的事务对象 # 当涉及一个或多个数据库操作时，可以保证所有要写入的数据都必须提交 # 然后将这个会话对象保存，并将用户的表和引擎作为实例属性一同保存下来 # 引擎和表的元数据进行了额外的绑定，使这张表的所有操作都会绑定到这个指定的引擎中 Session = orm.sessionmaker(bind=eng) self.ses = Session() self.users = Users.__table__ self.eng = self.users.metadata.bind = eng def insert(self): # session.add_all()使用迭代的方式产生一系列的插入操作 self.ses.add_all( Users(login=who, userid=userid, projid=rand(1, 5)) for who, userid in randName() ) # 决定是提交还是回滚 self.ses.commit() def update(self): fr = rand(1, 5) to = rand(1, 5) i = -1 # 会话查询的功能，使用query.filter_by()方法进行查找 users = self.ses.query(Users).filter_by(projid=fr).all() for i, user in enumerate(users): user.projid = to self.ses.commit() return fr, to, i+1 def delete(self): rm = rand(1, 5) i = -1 users = self.ses.query(Users).filter_by(projid=rm).all() for i, user in enumerate(users): self.ses.delete(user) self.ses.commit() return rm, i+1 def dbDump(self): # 在屏幕上显示正确的输出 print('\n%s' % ''.join(map(cformat, FIELDS))) users = self.ses.query(Users).all() for user in users: print(user) self.ses.commit() def __getattr__(self, attr): # __getattr__()可以避开创建drop()和create()方法 # __getattr__()只有在属性查找失败时才会被调用 # 当调用orm.drop()并发现没有这个方法时，就会调用getattr(orm, 'drop') # 此时调用__getattr__()，并且将属性名委托给self.users。结束期会发现 # slef.users存在一个drop属性，然后传递这个方法调用到self.users.drop()中 return getattr(self.users, attr) def finish(self): # 关闭连接 self.ses.connection().close()def main(): # 入口函数 print('\n***Connnect to %r database' % DBNAME) db = setup() if db not in DSNs: print('ERROR: %r not supported, exit' % db) return try: orm = SQLAlchemyTest(DSNs[db]) except RuntimeError: print('ERROR: %r not supported, exit' % db) return print('\n*** Create users table(drop old one if appl.') orm.drop(checkfirst=True) orm.create() print('\n***Insert namse into table') orm.insert() orm.dbDump() print('\n***Move users to a random group') fr, to, num = orm.update() print('\t(%d users moved) from (%d) to (%d))' % (num, fr, to)) orm.dbDump() print('\n***Randomly delete group') rm, num = orm.delete() print('\t(group #%d; %d users removed)' % (rm, num)) orm.dbDump() print('\n***Drop users table') orm.drop() print('***Close cxns') orm.finish()if __name__ == '__main__': main() mysql输出结果 123456789101112131415161718192021222324252627282930313233343536***Connnect to &apos;test&apos; databaseChoose a database system: (M)ySQL (S)QLiteEnter choice:M*** Create users table(drop old one if appl.***Insert namse into tableLOGIN USERID PROJID Bob 1234 1 Dave 4523 1 Angela 4567 3 ***Move users to a random group (2 users moved) from (1) to (4))LOGIN USERID PROJID Bob 1234 4 Dave 4523 4 Angela 4567 3 ***Randomly delete group (group #2; 0 users removed)LOGIN USERID PROJID Bob 1234 4 Dave 4523 4 Angela 4567 3 ***Drop users table***Close cxns SQLite输出结果 1234567891011121314151617181920212223242526272829303132333435***Connnect to &apos;test&apos; databaseChoose a database system: (M)ySQL (S)QLiteEnter choice:S*** Create users table(drop old one if appl.***Insert namse into tableLOGIN USERID PROJID Bob 1234 2 Dave 4523 1 Angela 4567 2 ***Move users to a random group (2 users moved) from (2) to (2))LOGIN USERID PROJID Bob 1234 2 Dave 4523 1 Angela 4567 2 ***Randomly delete group (group #1; 1 users removed)LOGIN USERID PROJID Bob 1234 2 Angela 4567 2 ***Drop users table***Close cxns SQLObjectSQLObject需要mysqldb支持，但是由于mysqldb不再支持python3，所以根据提示安装替代方案Mysqlclient，选择对应的版本进行下载后执行相应的命令：pip3 install mysqlclient-1.3.12-cp36-cp36m-win_amd64.whl 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123#!/usr/bin/python3# -*- coding:UTF-8 -*-# 使用SQLObject代替SQLAlchemy# 其余和使用SQLAlchemy的相同from os.path import dirnamefrom random import randrange as randfrom sqlobject import *from ushuffleDB import DBNAME, NAMELEN, \ randName, FIELDS, tformat, cformat, setupDSNs = &#123; 'mysql': 'mysql://root:root@127.0.0.1:3306/%s' % DBNAME, 'sqlite': 'sqlite:///:memory:',&#125;class Users(SQLObject): # 扩展了SQLObject.SQLObject类 # 定义列 login = StringCol(length=NAMELEN) userid = IntCol() projid = IntCol() def __str__(self): # 提供用于显示输出的方法 return ''.join(map(tformat, ( self.login, self.userid, self.projid)))class SQLObjectTest(object): def __init__(self, dsn): # 确保得到一个可用的数据库，然后返回连接 try: cxn = connectionForURI(dsn) except ImportError: raise RuntimeError() try: # 尝试对已存在的表建立连接 # 规避RMBMS适配器不可用，服务器不在线及数据库不存在等异常 cxn.releaseConnection(cxn.getConnection()) except dberrors.OperationalError: # 出现异常则创建表 cxn = connectionForURI(dirname(dsn)) cxn.query('CREATE DATABASE %s' % DBNAME) cxn = connectionForURI(dsn) # 成功后在self.cxn中保存连接对象 self.cxn = sqlhub.processConnection = cxn def insert(self): # 插入 for who, userid in randName(): Users(login=who, userid=userid, projid=rand(1, 5)) def update(self): # 更新 fr = rand(1, 5) to = rand(1, 5) i = -1 users = Users.selectBy(projid=fr) for i, user in enumerate(users): user.projid = to return fr, to, i+1 def delete(self): # 删除 rm = rand(1, 5) users = Users.selectBy(projid=rm) i = -1 for i, user in enumerate(users): user.destroySelf() return rm, i+1 def dbDump(self): print('\n%s' % ''.join(map(cformat, FIELDS))) for user in Users.select(): print(user) def finish(self): # 关闭连接 self.cxn.close()def main(): print('***Connect to %r database' % DBNAME) db = setup() if db not in DSNs: print('\nError: %r not support' % db) return try: orm = SQLObjectTest(DSNs[db]) except RuntimeError: print('\nError: %r not support' % db) return print('\n***Create users table(drop old one if appl.)') Users.dropTable(True) Users.createTable() print('\n*** Insert names into table') orm.insert() orm.dbDump() print('\n*** Move users to a random group') fr, to, num = orm.update() print('\t(%d users moved) from (%d) to (%d)' % (num, fr, to)) orm.dbDump() print('\n*** Randomly delete group') rm, num = orm.delete() print('\t(group #%d;%d users removed)' % (rm, num)) orm.dbDump() print('\n*** Drop users table') # 使用dropTable()方法 Users.dropTable() print('\n***Close cxns') orm.finish()if __name__ == '__main__': main() MySQL输出结果 123456789101112131415161718192021222324252627282930313233Choose a database system:(M)ySQL(S)QLiteEnter choice:M***Create users table(drop old one if appl.)*** Insert names into tableLOGIN USERID PROJID Bob 1234 4 Dave 4523 3 Angela 4567 1 *** Move users to a random group(0 users moved) from (2) to (4)LOGIN USERID PROJID Bob 1234 4 Dave 4523 3 Angela 4567 1 *** Randomly delete group(group #3;1 users removed)LOGIN USERID PROJID Bob 1234 4 Angela 4567 1 *** Drop users table***Close cxns SQLite输出结果 123456789101112131415161718192021222324252627282930313233Choose a database system:(M)ySQL(S)QLiteEnter choice:S***Create users table(drop old one if appl.)*** Insert names into tableLOGIN USERID PROJID Bob 1234 2 Angela 4567 4 Dave 4523 3 *** Move users to a random group(1 users moved) from (3) to (1)LOGIN USERID PROJID Bob 1234 2 Angela 4567 4 Dave 4523 1 *** Randomly delete group(group #2;1 users removed)LOGIN USERID PROJID Angela 4567 4 Dave 4523 1 *** Drop users table***Close cxns 非关系型数据库Web和社交服务会产生大量的数据，并且数据的产生速率可能要比关系型数据库能够处理得更快。非关系数据库有对象数据库、键-值对存储、文档存储（或数据存储）、图形数据库、表格数据库、列/可扩展记录/宽列数据库、多值数据库等很多种类。 MongoDBMongoDB是非常流行的文档存储非关系数据库。 文档存储(MongoDB、CouchDB/Amazon SimpleDB)与其他非关系数据库的区别在于它介于简单的键-值对存储(Redis、Voldemort)与列存储(HBase、Google Bigtable)之间。比基于列的存储更简单、约束更少。比普通的键-值对存储更加灵活。一般情况下其数据会另存为JSON对象、并且允许诸如字符串、数值、列表甚至嵌套等数据类型 MongoDB(以及NoSQL)要讨论的事文档、集合而不是关系数据库中的行和列。MongoDB将数据存储于特殊的JSON串(文档)中，由于它是一个二进制编码的序列化，通常也称其为BSON格式。它和JSON或者Python字典都很相似。 PyMongo:MongoDB和PythonPyMongo是Python MongoDB驱动程序中最正式的一个。使用之前需要安装MongoDB数据库和PyMongo：pip3 install pymongo在windows下需要运行mongo.exe启动MongoDB，进入cmd到MongoDB的bin目录下，执行如下命令.mongod --dbpath E:\MongoDB\data 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798#!/usr/bin/python3# -*- coding:UTF-8 -*-# 主要导入的是MongoClient对象和及其包异常errorsfrom random import randrange as randfrom pymongo import MongoClient, errorsfrom ushuffleDB import DBNAME, randName, FIELDS, tformat, cformat# 设置了集合(“表”)名COLLECTION = 'users'class MongoTest(object): def __init__(self): # 创建一个连接，如果服务器不可达，则抛出异常 try: cxn = MongoClient() except errors.AutoReconnect: raise RuntimeError # 创建并复用数据库及“users”集合 # 关系数据库中的表会对列的格式进行定义， # 然后使遵循这个列定义的每条记录成为一行 # 非关系数据库中集合没有任何模式的需求， # 每条记录都有其特定的文档 # 每条记录都定义了自己的模式，所以保存的任何记录都会写入集合中 self.db = cxn[DBNAME] self.users = self.db[COLLECTION] def insert(self): # 向MongoDB的集合中添加值 # 使用dict()工厂函数为每条记录创建一个文档 # 然后将所有文档通过生成器表达式的方式传递给集合的insert()方法 self.users.insert( dict(login=who, userid=uid, projid=rand(1, 5) )for who, uid in randName() ) def update(self): # 集合的update()方法可以给开发者相比于典型的数据库系统更多的选项 fr = rand(1, 5) to = rand(1, 5) i = -1 # 在更新前，首先查询系统中的项目ID(projid)与要更新的项目组相匹配的所有用户 # 使用find()方法，并将查询条件传进去(类似SQL的SELECT语句) for i, user in enumerate(self.users.find(&#123;'projid': fr&#125;)): # 使用$set指令可以显式地修改已存在的值 # 每条MongoDB指令都代表一个修改操作，使得修改操作更加高效、有用和便捷 # 除了$set还有一些操作可以用于递增字段值、删除字段(键-值对)、对数组添加/删除值 # update()方法可以用来修改多个文档(将multi标志设为True) self.users.update(user, &#123; '$set': &#123;'projid': to&#125; &#125;) return fr, to, i+1 def delete(self): # 当得到所有匹配查询的用户后，一次性对其执行remove()操作进行删除 # 然后返回结果 rm = rand(1, 5) i = -1 for i, user in enumerate(self.users.find(&#123;'projid': rm&#125;)): self.users.remove(user) return rm, i+1 def dbDump(self): # 没有天剑会返回集合中所有用户并对数据进行字符串格式化向用户显示 print('%s' % ''.join(map(cformat, FIELDS))) for user in self.users.find(): print(''.join(map(tformat, ( user[k] for k in FIELDS))))def main(): print('***Connect to %r database' % DBNAME) try: mongo = MongoTest() except RuntimeError: print('\nERROR: MongoDB server unreadable, exit') return print('\n***Insert names into table') mongo.insert() mongo.dbDump() print('\n***Move users to a random group') fr, to, num = mongo.update() print('\t(%d users moved) from (%d) to (%d)' % (num, fr, to)) mongo.dbDump() print('\n*** Randomly delete group') rm, num = mongo.delete() print('\tgroup #%d; %d users removed' % (rm, num)) mongo.dbDump() print('\n***Drop users table') mongo.db.drop_collection(COLLECTION)if __name__ == '__main__': main() 执行结果 12345678910111213141516171819202122***Connect to &apos;test&apos; database***Insert names into tableLOGIN USERID PROJID Dave 4523 4 Bob 1234 4 Angela 4567 2 ***Move users to a random group (0 users moved) from (1) to (2)LOGIN USERID PROJID Dave 4523 4 Bob 1234 4 Angela 4567 2 *** Randomly delete group group #2; 1 users removedLOGIN USERID PROJID Dave 4523 4 Bob 1234 4 ***Drop users table]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>数据库编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python数据库编程(一)]]></title>
    <url>%2F2018%2F02%2F28%2FPython%E6%95%B0%E6%8D%AE%E5%BA%93%E7%BC%96%E7%A8%8B-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[Python和大多数语言一样，访问数据库包括直接通过数据库接口访问和使用ORM访问两种方式。其中ORM访问的方式不需要显式地给出SQL命令。在Python中数据库是通过适配器的方式进行访问的。适配器是一个Python模块，使用它可以与关系型数据库的客户端库接口相连。 Python的DB-API DB-API是阐明一系列所需对象和数据库访问机制的标准，它可以为不同的数据库适配器和底层数据库系统提供一致性访问 模块属性DB-API模块属性 属性 描述 apilevel 需要适配器兼容的DB-API版本 threadsafety 本模块的线程安全级别 paramstyle 本模块的SQL语句参数风格 connect() Connect()函数 (多种异常) 数据属性 apilevel 该字符串指明了模块需要兼容的DB-API最高版本，默认值为1.0 threadsafety 0: 不支持线程安全。线程间不能共享模块1: 最小化线程安全支持：线程间可以共享模块，但是不能共享连接2: 适度的线程安全支持：线程间可以共享模块和连接，但是不能共享游标3: 完整的线程安全支持：线程间可以共享模块、连接和游标 如果有资源需要共享，那么就需要诸如自旋锁、信号量等同步原语达到原子锁定的目的 参数风格 paramstyle 参数风格 描述 示例 numeric 数值位置风格 WHERE name=:1 named 命名风格 WHERE name=:name pyformat Python字典printf()格式转换 WHERE name=%(name)s qmark 问号风格 WHERE name=? format ANSIC的printf()格式转换 WHERE name=%s 函数属性 connect()函数通过Connection对象访问数据库。兼容模块必须实现connect()函数。该函数创建并放回一个Connection对象 connect()函数使用例子：connect(dsn=&#39;myhost:MYDB&#39;, user=&#39;root&#39;, password=&#39;root&#39;) connect()函数属性 参数 描述 user 用户名 password 面 host 主机名 database 数据库名 dsn 数据源名 使用ODBC或JDBC的API需要使用DSN；直接使用数据库，更倾向于使用独立的登录参数。 异常 异常 描述 Warning 警告异常基类 Error 错误异常基类 InterfaceError 数据库接口(非数据库)错误 DatabaseError 数据库错误 DataError 处理数据时出现错误 OperationError 数据库操作执行期间出现的错误 IntegrityError 数据库关系完整性错误 InternalError 数据库内部错误 ProgrammingError SQL命令执行失败 NotSupportedError 出现不支持的操作 Connection对象 只有通过数据连接才能把命令传递到服务器，并得到返回的结果。当一个连接(或一个连接池)建立后，可以创建一个游标，向数据库发送请求，然后从数据库接收回应 Connection对象方法 方法名 描述 close() 关闭数据库连接 commit() 提交当前事务 rollback() 取消当前事务 cursor() 使用该连接创建(并返回)一个游标或类游标的对象 errorhandler(cxn,cur,errcls,errval) 作为给定连接的游标的处理程序 当使用close()时，这个连接将不能再使用，否则会进入到异常处理中 如果数据库不支持事务处理或启用了自动提交功能，commit()方法都无法使用 rollback()只能在支持事务处理的数据库中使用。发生异常时，rollback()会将数据库的状态恢复到事务处理开始时。 如果RDBMS(关系数据库管理系统)不支持游标，cursor()会返回一个尽可能模仿真实游标的对象 Cursor对象 游标可以让用户提交数据库命令，并获得查询的结果行。 对象属性 描述 arraysize 使用fetchmany()方法时，一次取出的结果行数，默认为1 connection 创建此游标的连接(可选) description 返回游标活动状态(7项元组):(name,type_code,display_size,internal_size,precision,scale,null-ok)，只有name和type_code是必需的 lastrowid 上次修改行的行ID(可选，如果不支持行ID，则返回None) rowcount 上次execute*()方法处理或影响的行数 callproc(func[,args]) 调用存储过程 close() 关闭游标 execute(op[,args]) 执行数据库查询或命令 executemany(op,args) 类似execute()和map()的结合，为给定的所有参数准备并执行数据库查询或命令 fetchone() 获取查询结果的下一行 fetchmany([size=cursor,arraysize]) 获取查询结果的下面size行 fetchall() 获取查询结果的所有(剩余)行 iter() 为游标创建迭代器对象(可选，参考nexi()) messages 游标执行后从数据库中获得的消息列表(元组集合，可选) next() 被迭代器用于获取查询结果的下一行(可选，类似fetchone(),参考iter()) nextset() 移动到下一个结果集合(如果支持) rownumber 当前结果集中游标的索引(以行为单位，从0开始，可选) setinputsizes(sizes) 设置允许的最大输入大小(必须有，但是实现是可选的) setoutputsize(size[,col]) 设置大列获取的最大缓冲区大小(必须有，但是实现是可选的) 游标对象最重要的属性是execute()和fetch()方法，所有针对数据库的服务请求都通过它们执行。当不需要是关闭游标 类型对象和构造函数 创建构造函数，从而构建可以简单地转换成适当数据库对象的特殊对象 类型对象 描述 Date(yr,mo,dy) 日期值对象 Time(hr,min,sec) 时间值对象 Timestamp(yr,mo,dy,hr,min,sec) 时间戳值对象 DateFromTicks(ticks) 日期对象，给出从新纪元时间（1970 年1 月1 日00:00:00 UTC）以来的秒数 TimeFromTicks(ticks) 时间对象，给出从新纪元时间（1970 年1 月1 日00:00:00 UTC）以来的秒数 TimestampFromTicks(ticks) 时间戳对象，给出从新纪元时间（1970 年1 月1 日00:00:00 UTC）以来的秒数 Binary(string) 对应二进制(长)字符串对象 STRING 表示基于字符串列的对象，比如VARCHAR BINARY 表示(长)二进制列的对象，比如RAW、BLOB NUMBER 表示数值列的对象 DATETIME 表示日期/时间列的对象 ROWID 表示“行ID”列的对象 SQL的NULL值对应于Python的NULL对象None 数据库适配器示例应用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201#!/usr/bin/python3# -*- coding:UTF-8 -*-# 导入必需的模块import osfrom random import randrange as rand# 创建了全局变量# 用于显示列的大小，以及支持的数据库种类COLSIZ = 10FIELDS = ('login', 'userid', 'projid')RDBMSs = &#123; 's': 'sqlite', 'm': 'mysql',&#125;DBNAME = 'test'DBUSER = 'root'# 数据库异常变量，根据用户选择运行的数据库系统的不同来制定数据库异常模块DB_EXC = NoneNAMELEN = 16# 格式化字符串以显示标题# 全大写格式化函数，接收每个列名并使用str.upper()方法把它转换为头部的全大写形式# 两个函数都将其输出左对齐，并限制为10个字符的宽度ljust(COLSIZ)tformat = lambda s: str(s).title().ljust(COLSIZ)cformat = lambda s: s.upper().ljust(COLSIZ)def setup(): return RDBMSs[input(''' Choose a database system: (M)ySQL (S)QLite Enter choice: ''').strip().lower()[0]]def connect(db): # 数据库一致性访问的核心 # 在每部分的开始出尝试加载对应的数据库模块，如果没有找到合适的模块 # 就返回None，表示无法支持数据库系统 global DB_EXC dbDir = '%s_%s' % (db, DBNAME) if db == 'sqlite': try: # 尝试加载sqlite3模块 import sqlite3 except ImportError: return None DB_EXC = sqlite3 # 当对SQLite调用connect()时，会使用已存在的目录 # 如果没有，则创建一个新目录 if not os.path.isdir(dbDir): os.mkdir(dbDir) cxn = sqlite3.connect(os.path.join(dbDir, DBNAME)) elif db == 'mysql': try: # 由于MySQLdb不支持python3.6，所以导入pymysql import pymysql import pymysql.err as DB_EXC try: cxn = pymysql.connect(host="localhost", user="root", password="root", port=3306, db=DBNAME) except DB_EXC.InternalError: try: cxn = pymysql.connect(host="localhost", user="root", password="root", port=3306) cxn.query('CREATE DATABASE %s' % DBNAME) cxn.commit() cxn.close() cxn = pymysql.connect(host="localhost", user="root", password="root", port=3306, db=DBNAME) except DB_EXC.InternalError: return None except ImportError: return None else: return None return cxndef create(cur): # 创建一个新表users try: cur.execute(''' CREATE TABLE users( login VARCHAR(%d), userid INTEGER, projid INTEGER ) ''' % NAMELEN) except DB_EXC.InternalError as e: # 如果发生错误，几乎总是这个表已经存在了 # 删除该表，重新创建 drop(cur) create(cur)# 删除数据库表的函数drop = lambda cur: cur.execute('DROP TABLE users')# 由用户名和用户ID组成的常量NAMES = ( ('bob', 1234), ('angela', 4567), ('dave', 4523))def randName(): # 生成器 pick = set(NAMES) while pick: yield pick.pop()def insert(cur, db): # 插入函数 # SQLite风格是qmark参数风格，而MySQL使用的是format参数风格 # 对于每个用户名-用户ID对，都会被分配到一个项目卒中。 # 项目ID从四个不同的组中随机选出的 if db == 'sqlite': cur.executemany("INSERT INTO users VALUES(?,?,?)", [(who, uid, rand(1, 5)) for who, uid in randName()]) elif db == 'mysql': cur.executemany("INSERT INTO users VALUES(%s, %s, %s)", [(who, uid, rand(1, 5)) for who, uid in randName()])# 返回最后一次操作后影响的行数，如果游标对象不支持该属性，则返回-1getRC = lambda cur: cur.rowcount if hasattr(cur, 'rowcount') else -1# update()和delete()函数会随机选择项目组中的成员# 更新操作会将其从当前组移动到另一个随机选择的组中# 删除操作会将该组的成员全部删除def update(cur): fr = rand(1, 5) to = rand(1, 5) cur.execute('UPDATE users SET projid=%d WHERE projid=%d' % (to, fr)) return fr, to, getRC(cur)def delete(cur): rm = rand(1, 5) cur.execute('DELETE FROM users WHERE projid=%d' % rm) return rm, getRC(cur)def dbDump(cur): # 来去所有行，将其按照打印格式进行格式化，然后显示 cur.execute('SELECT * FROM users') # 格式化标题 print('%s' % ''.join(map(cformat, FIELDS))) for data in cur.fetchall(): # 将数据(login,userid,projid)通过map()传递给tformat()， # 是数据转化为字符串，将其格式化为标题风格 # 字符串按照COLSIZ的列宽度进行左对齐 print(''.join(map(tformat, data)))def main(): # 主函数 db = setup() print('*** Connect to %r database' % db) cxn = connect(db) if not cxn: print('ERROR: %r not supported or unreadable, exit' % db) return cur = cxn.cursor() print('***Creating users table') create(cur=cur) print('***Inserting names into table') insert(cur, db) dbDump(cur) print('\n***Randomly moving folks') fr, to, num = update(cur) print('(%d users moved) from (%d) to (%d)' % (num, fr, to)) dbDump(cur) print('***Randomly choosing group') rm, num = delete(cur) print('\t(group #%d; %d users removed)' % (rm, num)) dbDump(cur) print('\n***Droping users table') drop(cur) print('\n*** Close cxns') cur.close() cxn.commit() cxn.close()if __name__ == '__main__': main() MySQL数据库访问结果 123456789101112131415161718192021222324252627Choose a database system: (M)ySQL (S)QLiteEnter choice:M*** Connect to &apos;mysql&apos; database***Creating users table***Inserting names into tableLOGIN USERID PROJID Dave 4523 2 Bob 1234 3 Angela 4567 3 ***Randomly moving folks(2 users moved) from (3) to (1)LOGIN USERID PROJID Dave 4523 2 Bob 1234 1 Angela 4567 1 ***Randomly choosing group (group #1; 2 users removed)LOGIN USERID PROJID Dave 4523 2 ***Droping users table*** Close cxns SQLite数据库访问结果 12345678910111213141516171819202122232425262728Choose a database system:(M)ySQL(S)QLiteEnter choice:S*** Connect to &apos;sqlite&apos; database***Creating users table***Inserting names into tableLOGIN USERID PROJID Dave 4523 1 Bob 1234 2 Angela 4567 3 ***Randomly moving folks(1 users moved) from (1) to (1)LOGIN USERID PROJID Dave 4523 1 Bob 1234 2 Angela 4567 3 ***Randomly choosing group(group #3; 1 users removed)LOGIN USERID PROJID Dave 4523 1 Bob 1234 2 ***Droping users table*** Close cxns]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>数据库编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python多线程(二)]]></title>
    <url>%2F2018%2F02%2F26%2FPython%E5%A4%9A%E7%BA%BF%E7%A8%8B-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[在上篇主要对线程的概念做了一个简要的介绍，同时介绍了_thread模块和threading模块的使用方法，通过几个简短的程序实现了线程的调用。这篇将会记录一些多线程简单的应用以及相关生产者和消费者的问题。 多线程实践Python虚拟机是单线程（GIL）的原因，只有线程在执行I/O密集型的应用时才会更好地发挥Python的并发性。下面的例子是通过多线程下载图书排名信息的调用 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#!/usr/bin/python3# -*- coding:UTF-8 -*-from atexit import registerimport reimport threadingimport timeimport urllib.request# 匹配排名的正则表达式# 亚马逊的网站REGEX = re.compile(b'#([\d,]+) in Books')AMZN = 'https://www.amazon.com/dp/'# ISBN编号和书名ISBNs = &#123; '0132269937': 'Core Python Programming', '0132356139': 'Python Web Development with Django', '0137143419': 'Python Fundamentals'&#125;# 请求头# 因为亚马逊会检测爬虫,所以需要加上请求头伪装成浏览器访问headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 ' '(KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36 TheWorld 7'&#125;def get_ranking(isbn): # 爬取网页,获取数据 # 使用str.format()格式化数据 url = '&#123;0&#125;&#123;1&#125;'.format(AMZN, isbn) # 爬取网页并解析 req = urllib.request.Request(url, headers=headers) page = urllib.request.urlopen(req) data = page.read() page.close() return str(REGEX.findall(data)[0], 'utf-8')def _show_ranking(isbn): # 显示结果 print('- %r ranked %s' % (ISBNs[isbn], get_ranking(isbn)))def _main(): print('At', time.ctime(), 'on Amazon...') for isbn in ISBNs: (threading.Thread(target=_show_ranking, args=(isbn,))).start() #_show_ranking(isbn)@registerdef _atexit(): # 注册一个退出函数，在脚本退出先请求调用这个函数 print('all DONE at:', time.ctime())if __name__ == '__main__': _main() 输出结果 12345At Tue Feb 27 10:40:51 2018 on Amazon...- &apos;Python Fundamentals&apos; ranked 4,358,513- &apos;Python Web Development with Django&apos; ranked 1,354,091- &apos;Core Python Programming&apos; ranked 458,510all DONE at: Tue Feb 27 10:42:39 2018 锁示例锁有两种状态:锁定 和 未锁定。同时它也支持两个函数：获得锁 和 释放锁。当多线程争夺锁时，允许第一个获得锁的线程进入临界区，并执行。之后到达的线程被阻塞，直到第一个线程执行结束，退出临界区，并释放锁。其他等待的线程随机获得锁并进入临界区。 锁和更多的随机性 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#!/usr/bin/python3# -*- coding:UTF-8 -*-from __future__ import with_statementfrom atexit import registerfrom random import randrangefrom threading import Thread, Lock, current_threadfrom time import sleep, ctimeclass CleanOutputSet(set): # 集合的子类，将默认输出改变为将其所有元素 # 按照逗号分隔的字符串 def __str__(self): return ', '.join(x for x in self)# 锁# 随机数量的线程(3~6)，每个线程暂停或睡眠2~4秒lock = Lock()loops = (randrange(2, 5) for x in range(randrange(3, 7)))remaining = CleanOutputSet()def loop(sec): # 获取当前执行的线程名，然后获取锁并保存线程名 myname = current_thread().name lock.acquire() remaining.add(myname) print('[%s] Started %s' % (ctime(), myname)) # 释放锁并睡眠随机秒 lock.release() sleep(sec) # 重新获取锁，输出后再释放锁 lock.acquire() remaining.remove(myname) print('[%s] Completed %s (%d sec)' % (ctime(), myname, sec)) print(' (remaining: %s)' % (remaining or 'NONE')) lock.release()def loop_with(sec): myname = current_thread().name with lock: remaining.add(myname) print('[%s] Started %s' % (ctime(), myname)) sleep(sec) with lock: remaining.remove(myname) print('[%s] Completed %s (%d sec)' % (ctime(), myname, sec)) print(' (remaining: %s)' % (remaining or 'NONE'))def _main(): for pause in loops: # Thread(target=loop, args=(pause,)).start() Thread(target=loop_with, args=(pause,)).start()@registerdef _atexit(): print('all DONE at:', ctime())if __name__ == '__main__': _main() 输出结果 loop方法12345678910111213141516171819[Tue Feb 27 11:26:13 2018] Started Thread-1[Tue Feb 27 11:26:13 2018] Started Thread-2[Tue Feb 27 11:26:13 2018] Started Thread-3[Tue Feb 27 11:26:13 2018] Started Thread-4[Tue Feb 27 11:26:13 2018] Started Thread-5[Tue Feb 27 11:26:13 2018] Started Thread-6[Tue Feb 27 11:26:15 2018] Completed Thread-2 (2 sec) (remaining: Thread-3, Thread-4, Thread-1, Thread-5, Thread-6)[Tue Feb 27 11:26:15 2018] Completed Thread-6 (2 sec) (remaining: Thread-3, Thread-4, Thread-1, Thread-5)[Tue Feb 27 11:26:16 2018] Completed Thread-3 (3 sec) (remaining: Thread-4, Thread-1, Thread-5)[Tue Feb 27 11:26:16 2018] Completed Thread-4 (3 sec) (remaining: Thread-1, Thread-5)[Tue Feb 27 11:26:16 2018] Completed Thread-5 (3 sec) (remaining: Thread-1)[Tue Feb 27 11:26:17 2018] Completed Thread-1 (4 sec) (remaining: NONE)all DONE at: Tue Feb 27 11:26:17 2018 loop_with方法12345678910111213141516171819[Tue Feb 27 11:43:15 2018] Started Thread-1[Tue Feb 27 11:43:15 2018] Started Thread-2[Tue Feb 27 11:43:15 2018] Started Thread-3[Tue Feb 27 11:43:15 2018] Started Thread-4[Tue Feb 27 11:43:15 2018] Started Thread-5[Tue Feb 27 11:43:15 2018] Started Thread-6[Tue Feb 27 11:43:17 2018] Completed Thread-3 (2 sec) (remaining: Thread-1, Thread-5, Thread-4, Thread-6, Thread-2)[Tue Feb 27 11:43:17 2018] Completed Thread-6 (2 sec) (remaining: Thread-1, Thread-5, Thread-4, Thread-2)[Tue Feb 27 11:43:17 2018] Completed Thread-5 (2 sec) (remaining: Thread-1, Thread-4, Thread-2)[Tue Feb 27 11:43:18 2018] Completed Thread-1 (3 sec) (remaining: Thread-4, Thread-2)[Tue Feb 27 11:43:18 2018] Completed Thread-4 (3 sec) (remaining: Thread-2)[Tue Feb 27 11:43:18 2018] Completed Thread-2 (3 sec) (remaining: NONE)all DONE at: Tue Feb 27 11:43:18 2018 信号量示例对于拥有有限资源的应用来说，可以使用信号量的方式来代替锁。信号量 是一个计数器，当资源消耗时递减，当资源释放时递增。信号量比锁更加灵活，因为可以有多个线程，每个线程拥有有限资源的一个实例。消耗资源使计数器递减的操作成为P()，当一个线程对一个资源完成操作时，该资源返回资源池的操作称为V()。 糖果机和信号量 这个特制的机器只有5个可用的槽来保持库存。如果所有槽都满了，糖果不能再加入这个机器中；如果每个槽都空了，想要购买的消费者无法买到糖果。使用信号量来跟踪这些有限的资源 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#!/usr/bin/python3# -*- coding:UTF-8 -*-# 导入相应的模块和信号量类# BoundedSemaphore的额外功能是这个计数器的值永远不会超过它的初始值# 它可以防范其中信号量释放次数多余获得次数的异常用例from atexit import registerfrom random import randrangefrom threading import BoundedSemaphore, Lock, Threadfrom time import sleep, ctime# 全局变量# 锁# 库存商品最大值的常量# 糖果托盘lock = Lock()MAX = 5candytray = BoundedSemaphore(MAX)def refill(): # 当虚构的糖果机所有者向库存中添加糖果时执行 # 代码会输出用户的行动，并在某人添加的糖果超过最大库存是给予警告 lock.acquire() print('Refilling candy...') try: candytray.release() except ValueError: print('full, skipping') else: print('OK') lock.release()def buy(): # 允许消费者获取一个单位的库存 lock.acquire() print('Buying candy....') # 检测是否所有资源都已经消费完了 # 通过传入非阻塞的标志False，让调用不再阻塞，而在应当阻塞的时候返回一个False # 指明没有更多资源 if candytray.acquire(False): print('OK') else: print('Empty, skipping') lock.release()def producer(loops): for i in range(loops): refill() sleep(randrange(3))def consumer(loops): for i in range(loops): buy() sleep(randrange(3))def _main(): print('starting at:', ctime()) nloops = randrange(2, 6) print('THE CANDY MACHINE (full with %d bars)' % MAX) Thread(target=consumer, args=(randrange(nloops, nloops+MAX+2),)).start() Thread(target=producer, args=(nloops,)).start()@registerdef _atexit(): print('all DONE at:', ctime())if __name__ == '__main__': _main() 输出结果 12345678910111213141516171819202122232425262728293031starting at: Tue Feb 27 14:48:31 2018THE CANDY MACHINE (full with 5 bars)Buying candy....OKRefilling candy...OKRefilling candy...full, skippingBuying candy....OKRefilling candy...OKBuying candy....OKRefilling candy...OKRefilling candy...full, skippingBuying candy....OKBuying candy....OKBuying candy....OKBuying candy....OKBuying candy....OKBuying candy....Empty, skippingall DONE at: Tue Feb 27 14:48:42 2018 生产者-消费者问题和queue模块生产商品的时间是不确定的，消费生产者生产的商品的时间也是不确定的。在这个场景下将其放在类似队列的数据结构中。queue模块来提供线程间通信的机制，从而让线程之间可以互相分享数据。具体而言就是创建一个队列，让生产者在其中放入新的商品，而消费者消费这些商品 queue模块常用属性 属性 描述 Queue(maxsize=0) 创建一个先入先出队列。如果给定最大值，则在队列没有空间时阻塞，否则(没有指定最大值),为无限队列 LifoQueue(maxsize=0) 创建一个后入先出队列。如果给定最大值，则在队列没有空间时阻塞，否则(没有指定最大值),为无限队列 PriorityQueue(maxsize) 创建一个优先级队列。如果给定最大值，则在队列没有空间时阻塞，否则(没有指定最大值),为无限队列 queue异常 Empty 当对空队列调用get*()方法时抛出异常 Full 当对已满的队列调用put*()方法时抛出异常 queue对象方法 qsize() 返回队列大小(由于返回时队列大小可能被其他线程修改，所以改值为近似值) empty() 如果队列为空，则返回True；否则，返回False full() 如果队列已满，则返回True；否则，返回False put(item,block=True,timeout=None) 将item放入队列。如果block为True(默认)且timeout为None，则在有可用空间之前阻塞；如果timeout为正值，则最多阻塞timeout秒；如果block为False，则抛出Empty异常 put_nowait() 和put(item,False)相同 get(block=True,timeout=None) 从队列中取得元素，如果给定了block(非0)，则一直阻塞到有可用的元素为止 get_nowait() 和get(False)相同 task_done() 用于标识队列中的某个元素已执行完成，该方法会被下面的join()使用 join() 在队列中所有元素执行完毕并调用上面的task_done()信号之前，保持阻塞 生产者消费者问题使用了Queue对象，以及随机生产(消费)的商品的数量。生产者和消费者独立且并发地执行线程 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#!/usr/bin/python3# -*- coding:UTF-8 -*-# 使用queue.Queue对象和之前的myThread.MyThread线程类from random import randintfrom time import sleepfrom queue import Queuefrom myThread import MyThreaddef writeQ(queue): # 将一个对象放入队列中 print('producing object for Q...') queue.put('xxx', 1) print('size now', queue.qsize())def readQ(queue): # 消费队列中的一个对象 val = queue.get(1) print('consumed object from Q... size now', queue.qsize())def writer(queue, loops): # 作为单个线程运行 # 向队列中放入一个对象，等待片刻，然后重复上述步骤 # 直至达到脚本执行时随机生成的次数没值 for i in range(loops): writeQ(queue) # 睡眠的随机秒数比reader短是为了阻碍reader从空队列中获取对象 sleep(randint(1, 3))def reader(queue, loops): # 作为单个线程运行 # 消耗队列中一个对象，等待片刻，然后重复上述步骤 # 直至达到脚本执行时随机生成的次数没值 for i in range(loops): readQ(queue) sleep(randint(2, 5))# 设置派生和执行的线程总数funcs = [writer, reader]nfuncs = range(len(funcs))def main(): nloops = randint(2, 5) q = Queue(32) threads = [] for i in nfuncs: t = MyThread(funcs[i], (q, nloops), funcs[i].__name__) threads.append(t) for i in nfuncs: threads[i].start() for i in nfuncs: threads[i].join() print('all DONE')if __name__ == '__main__': main() 输出结果 1234567891011121314starting at: Tue Feb 27 15:17:16 2018producing object for Q...size now 1starting at: Tue Feb 27 15:17:16 2018consumed object from Q... size now 0producing object for Q...size now 1producing object for Q...size now 2done at: Tue Feb 27 15:17:20 2018consumed object from Q... size now 1consumed object from Q... size now 0done at: Tue Feb 27 15:17:26 2018all DONE 线程的替代方案subprocess模块multiprocessing模块concurrent.futures模块]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python多线程(一)]]></title>
    <url>%2F2018%2F02%2F24%2FPython%E5%A4%9A%E7%BA%BF%E7%A8%8B-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[多线程编程对于以下编程任务是非常理想的： 本质上是异步的 需要多个并发活动 每个活动的处理顺序可能是不确定的(随机、不可预测的) 使用多线程或者类似Queue的共享数据结构可以将一个串行程序规划成几个执行特定任务的线程 UserRequestThread: 负责读取客户端输入。程序将创建多个线程，每个客户端一个，客户端的请求将会被放入队列中 RequestProcessor: 该线程负责从队列中获取请求并进行处理，为第三个线程提供输出 ReplyThread: 负责向用户输出，将结果传回给用户，或者把数据写到本地文件系统或者数据库中 线程和进程 进程 计算机程序是储存在磁盘上的可执行二进制(或其他类型)的文件。进程 （有时称为 重量级进程）则是一个执行中的程序。每一个进程都拥有自己的地址空间、内存、数据栈以及其他用于跟踪执行的辅助数据。操作系统管理其上的所有进程的执行，并为它们合理地分配时间。进程可以通过 派生(fork或spawn)新的进程来执行任务,而进程之间的通信只能通过 进程间通信(IPC) 的方式共享信息 线程 线程（有时称为 轻量级进程）共享相同的上下文。相当于在主进程中并行运行的一些“迷你进程”。当其他线程运行是，它可以被抢占（中断）和临时挂起（睡眠），这种做法叫 让步(yielding)。早单核CPU系统中，线程的实际规划是：每个线程运行一小会儿，然后让步给其他线程（再次排队等待更多的CPU时间）。在整个进程的执行当中，每个线程执行它自己特定的任务，在必要时和其他线程进行结果通信。 线程与Python全局解释锁 对Python虚拟机的访问是由全局解释锁(GIL) 控制的。这个锁用来保证同时只能有一个线程运行。在多线程环境中，Python虚拟机将按照下面的方式执行。 设置GIL 切换进一个线程去运行 执行下面操作之一 a. 指定数量的字节码指令 b. 线程主动让出控制权(可以调用time.sleep(0)来完成) 把线程设置回睡眠状态(切换出线程) 解锁GIL 重复上述步骤 当调用外部代码(即，任意C/C++扩展的内置函数)时，GIL会保持锁定，直至函数执行结束。 退出线程 当一个线程完成函数的执行时，就会退出。还可以通过调用thread.exit()或者sys.exit()退出进程，或者抛出SystemExit异常，是线程退出。 _thread模块 _thread模块提供了派生线程、基本的同步数据结构(锁对象(lock object),也叫 原语锁、简单锁、互斥锁、互斥 和 二进制信号量) _thread模和锁对象 函数/方法 描述 _thread模块的函数 start_new_thread(function, args, kwargs = None) 派生一个新的线程，使用给定的args和可选的kwargs来执行function allocate_lock() 分配LockType锁对象 exit() 给线程退出命令 LockType锁对象的方法 acquire(wait = None) 尝试获取锁对象 locked() 如果获取了锁对象则返回True，否则，返回False release() 释放锁 使用线程一般方式 程序 123456789101112131415161718192021222324252627282930313233#!usr/bin/python3# -*- coding:UTF-8 -*-import _threadfrom time import ctime, sleepdef loop_0(): print('start loop_0 at:', ctime()) sleep(4) print('loop_0 done at:', ctime())def loop_1(): print('start loop_1 at:', ctime()) sleep(2) print('loop_1 done at:', ctime())def main(): print('starting at:', ctime()) # start_new_thread 方法即使要执行的 # 函数不需要参数，也需要传递一个空元组 _thread.start_new_thread(loop_0, ()) _thread.start_new_thread(loop_1, ()) # 阻止主线程的执行，保证其最后执行， # 后续去掉这种方式，引入锁的方式 sleep(6) print('all done at', ctime())if __name__ == '__main__': main() 执行结果 在主线程中同时开启了两个线程，loop_1()由于只睡眠了2s，所以先执行完，其实执行完loo_0()，线程执行的总时间是最慢的那个线程(loop_0() )的运行时间 123456starting at: Mon Feb 26 08:52:10 2018start loop_0 at: Mon Feb 26 08:52:10 2018start loop_1 at: Mon Feb 26 08:52:10 2018loop_1 done at: Mon Feb 26 08:52:12 2018loop_0 done at: Mon Feb 26 08:52:14 2018all done at Mon Feb 26 08:52:16 2018 使用锁对象 程序 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#!usr/bin/python3# -*- coding:UTF-8 -*-import _threadfrom time import ctime, sleeploops = [4, 2]def loop(nloop, sec, lock): # nloop: 第几个线程 # sec: 时间 # lock: 分配的锁 print('start loop', nloop, 'at:', ctime()) sleep(sec) print('loop', nloop, 'done at:', ctime()) # 当时间到了的时候释放锁 lock.release()def main(): print('starting at:', ctime()) locks = [] nloops = range(len(loops)) for i in nloops: # 生成锁对象 # 通过allocate_lock()函数得到锁对象 # 通过acquire()取到每个锁 # 添加进locks列表 lock = _thread.allocate_lock() lock.acquire() locks.append(lock) for i in nloops: # 派生线程 # 传递循环号，时间、锁对象 _thread.start_new_thread(loop, (i, loops[i], locks[i])) for i in nloops: # 等待所有线程的锁都释放完了才执行主线程 while locks[i].locked(): pass print('all DONE at:', ctime())if __name__ == '__main__': main() 执行结果 未再设置时间等待所有线程执行结束，而是在线程全部结束后马上运行主线程代码 123456starting at: Mon Feb 26 09:37:39 2018start loop 1 at: Mon Feb 26 09:37:39 2018start loop 0 at: Mon Feb 26 09:37:39 2018loop 1 done at: Mon Feb 26 09:37:41 2018loop 0 done at: Mon Feb 26 09:37:43 2018all DONE at: Mon Feb 26 09:37:43 2018 threading模块threading模块提供了更高级别、功能更全面的线程管理,还包括许多非常好用的同步机制 threading模块的对象 对象 描述 Thread 表示一个执行线程的对象 Lock 锁原语对象(和thread模块中的锁一样) RLock 可重入锁对象，使单一线程可以（再次）获得已持有的锁（锁递归） Condition 条件变量对象，使得一个线程等待另一个线程满足特定的“条件”，比如改变状态或某个数据值 Event 条件变量的通用版本，任何数量的线程等待某个事件的发生，在改事件发生后所有线程将被激活 Semaphone 为线程间共享的有限资源提供一个“计数器”，如果没有可用资源时会被阻塞 BoundSemaphone 与Semaphone相似，不过它不允许超过初始值 Timer 与Thread相似，不过它要在运行前等待一段时间 Barrier 创建一个“障碍”,必须达到指定数量的线程后才可以继续 Thread类 Thread对象的属性和方法 属性 描述 name 线程名 ident 线程的标识符 daemon 布尔标志，表示这个线程是否是守护线程 Thread对象方法 _init_(group=None, target=None, name=None, args=(), kwargs={}, verbose=None, daemon=就返回None) 实例化一个线程对象，需要一个可调用的target，以及参数args或kargs。还可以传递name或group参数。daemon的值将会设定thread.daemon属性/标志 start() 开始执行该线程 run() 定义线程功能的方法(通常在子类中被应用开发者重写) join(timeout=None) 直至启动的线程终止之前一直挂起；除非给出了timeout(秒)，否则会一直阻塞 使用Thread类，可以有很多方法创建线程。其中比较相似的三种方法是： 创建Thread的实例，传给它一个函数 创建Thread的实例，传给它一个可调用的类实例 派生Thread的子类，并创建子类的实例 创建Thread的实例，传给它一个函数join() 方法可以让主线程等待所有线程执行完毕，或者在提供了超时时间的情况下达到超时时间。join()方法只有在需要等待线程完成的时候才是有用的。 代码 1234567891011121314151617181920212223242526272829303132333435#!/usr/bin/python# -*- coding:UTF-8 -*-import threadingfrom time import ctime, sleeploops = [4, 2]def loop(nloop, sec): print('start loop', nloop, 'at:', ctime()) sleep(sec) print('loop', nloop, 'done at:', ctime())def main(): print('starting at:', ctime()) threads = [] nloops = range(len(loops)) for i in nloops: t = threading.Thread(target=loop, args=(i, loops[i])) threads.append(t) for i in nloops: # 启动线程 threads[i].start() for i in nloops: # 等待所有线程结束 threads[i].join() print('all DONE at:', ctime())if __name__ == '__main__': main() 结果 123456starting at: Mon Feb 26 14:29:36 2018start loop 0 at: Mon Feb 26 14:29:36 2018start loop 1 at: Mon Feb 26 14:29:36 2018loop 1 done at: Mon Feb 26 14:29:38 2018loop 0 done at: Mon Feb 26 14:29:40 2018all DONE at: Mon Feb 26 14:29:40 2018 创建Thread的实例，传给它一个可调用的类实例将传递进去一个可调用类(实例)而不仅仅是一个函数 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#!/usr/bin/python3# -*- coding:UTF-8 -*-import threadingfrom time import ctime, sleeploops = [4, 2]class ThreadFunc(object): def __init__(self, func, args, name=''): self.name = name self.func = func self.args = args def __call__(self): # Thread类的代码将调用ThreadFunc对象，此时会调用这个方法 # 因为init方法已经设定相关值，所以不需要再将其传递给Thread()的构造函数 self.func(*self.args)def loop(nloop, sec): print('start loop', nloop, 'at:', ctime()) sleep(sec) print('loop ', nloop, 'done at:', ctime())def main(): print('starting at:', ctime()) threads = [] nloops = range(len(loops)) for i in nloops: # 创建所有线程 t = threading.Thread(target=ThreadFunc(loop, (i, loops[i]))) threads.append(t) for i in nloops: threads[i].start() for i in nloops: # 等待所有线程 threads[i].join() print('all DONE at:', ctime())if __name__ == '__main__': main() 结果 123456starting at: Mon Feb 26 14:47:28 2018start loop 0 at: Mon Feb 26 14:47:28 2018start loop 1 at: Mon Feb 26 14:47:28 2018loop 1 done at: Mon Feb 26 14:47:30 2018loop 0 done at: Mon Feb 26 14:47:32 2018all DONE at: Mon Feb 26 14:47:32 2018 派生Thread的子类，并创建子类的实例(推荐)将Thread子类化，而不是直接对其实例化。这将在定制线程对象的时候拥有更多的灵活性，也能简化线程创建的调用过程 代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#!/usr/bin/python3# -*- coding:UTF-8 -*-import threadingfrom time import ctime, sleeploops = [4, 2]class MyThread(threading.Thread): def __init__(self, func, args, name=''): # 必须先调用基类的构造函数 threading.Thread.__init__(self) self.name = name self.func = func self.args = args def run(self): # 必须重写run()方法 self.func(*self.args)def loop(nloop, sec): print('start loop', nloop, 'at:', ctime()) sleep(sec) print('loop ', nloop, 'done at:', ctime())def main(): print('starting at:', ctime()) threads = [] nloops = range(len(loops)) for i in nloops: # 创建所有线程 t = MyThread(loop, (i, loops[i]), loop.__name__) threads.append(t) for i in nloops: threads[i].start() for i in nloops: # 等待所有线程 threads[i].join() print('all DONE at:', ctime())if __name__ == '__main__': main() 结果 123456starting at: Mon Feb 26 15:08:33 2018start loop 0 at: Mon Feb 26 15:08:33 2018start loop 1 at: Mon Feb 26 15:08:33 2018loop 1 done at: Mon Feb 26 15:08:35 2018loop 0 done at: Mon Feb 26 15:08:37 2018all DONE at: Mon Feb 26 15:08:37 2018 单线程和多线程执行的对比先后使用单线程和多线程执行三个独立的递归函数，代码中加入sleep()是为了减慢执行速度，能够更好的看到效果。 myThread.py 1234567891011121314151617181920212223#!/usr/bin/python3# -*- coding:UTF-8 -*-import threadingfrom time import ctime, sleepclass MyThread(threading.Thread): def __init__(self, func, args, name=''): threading.Thread.__init__(self) self.name = name self.func = func self.args = args def get_result(self): # 返回每一次的执行结果 return self.res def run(self): print('starting at:', ctime()) self.res = self.func(*self.args) print('done at:', ctime()) compare.py 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#!/usr/bin/python3# -*- coding:UTF-8 -*-from myThread import MyThreadfrom time import ctime, sleepdef fib(x): # 斐波拉契 sleep(0.005) if x &lt; 2: return 1 return fib(x-2)+fib(x-1)def fac(x): # 阶乘 sleep(0.1) if x &lt; 2: return 1 return x*fac(x-1)def sum(x): # 累加 sleep(0.1) if x &lt; 2: return 1 return x + sum(x-1)funcs = [fib, fac, sum]n = 12def main(): nfuncs = range(len(funcs)) print('***SINGLE THREAD***') for i in nfuncs: # 单线程顺序执行 print('starting', funcs[i].__name__, 'at:', ctime()) print(funcs[i](n)) print(funcs[i].__name__, 'finished at:', ctime(), '\n') print('\n ***MULTIPLE THREADS***') threads = [] for i in nfuncs: # 多线程执行 t = MyThread(funcs[i], (n,),funcs[i].__name__) threads.append(t) for i in nfuncs: threads[i].start() for i in nfuncs: threads[i].join() print(threads[i].get_result()) print('all DONE')if __name__ == '__main__': main() 结果 12345678910111213141516171819202122232425***SINGLE THREAD***starting fib at: Mon Feb 26 15:36:22 2018233fib finished at: Mon Feb 26 15:36:24 2018starting fac at: Mon Feb 26 15:36:24 2018479001600fac finished at: Mon Feb 26 15:36:25 2018starting sum at: Mon Feb 26 15:36:25 201878sum finished at: Mon Feb 26 15:36:26 2018 ***MULTIPLE THREADS***starting at: Mon Feb 26 15:36:26 2018starting at: Mon Feb 26 15:36:26 2018starting at: Mon Feb 26 15:36:26 2018done at: Mon Feb 26 15:36:28 2018done at: Mon Feb 26 15:36:28 2018done at: Mon Feb 26 15:36:29 201823347900160078all DONE]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python网络编程(二)]]></title>
    <url>%2F2018%2F02%2F24%2FPython%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[上篇对Python中的socket模块的简单应用做了描述和记录，下面便是对SocketServer模块和Twisted框架做一个简要的记录 socketserver模块socketserver是标准库的一个高级模块，它的目标是简化很多样板代码，它们是创建网络客户端和服务器所必需的代码。 socketserver模块类 类 描述 BaseServer 包含核心服务器功能和mix-in类的钩子；仅用于推导，这样不会创建这个类的实例；可以用TCPServer或UDPServer创建类的实例 TCPServer/UDPServer 基础的网络同步TCP/UDP服务器 UnixStreamServer/UnixDatagramServer 基于文件的基础同步TCP/UDP服务器 ForkingMixIn/ThreadingMixIn 核心派出或线程功能；只用作mix-in类与一个服务器类配合实现一些异步性；不能直接实例化这个类 ForkingTCPServer/ForkingUDPServer ForkingMaxIn和TCPServer/UDPServer的组合 ThreadingTCPServer/ThreadingUDPServer ThreadingMixIn和TCPServer/UDPServer的组合 BaseRequestHandler 包含处理服务请求的核心功能；仅用于推导，无法创建这个类的实例；可以使用StreamRequestHandler或DatagramRequestHandler创建类的实例 StreamRequestHandler/DatagramRequestHandler 实现TCP/UDP服务器的服务处理器 socketserver TCP服务器/客户端在原始服务器循环中，我们阻塞等待请求，当接收到请求时就对其提供服务，然后继续等待。在此处的服务器循环中，并非在服务器中创建代码，而是定义一个处理程序，当服务器接收到一个传入的请求时，服务器就可以调用 TCP服务器 1234567891011121314151617181920212223242526272829#!usr/bin/python3# -*- coding:UTF-8 -*-# 导入socketserver相关的类和time.ctime()的全部属性from socketserver import (TCPServer as TCP, StreamRequestHandler as SRH)from time import ctimeHOST = ''PORT = 12345ADDR = (HOST, PORT)class MyRequestHandler(SRH): # MyRequestHandler继承自StreamRequestHandler def handle(self): # 重写handle方法，当接收到一个客户端消息是，会调用handle()方法 print('...connected from:', self.client_address) # StreamRequestHandler将输入和输出套接字看做类似文件的对象 # 所以使用write()将字符串返回客户端，用readline()来获取客户端信息 self.wfile.write(bytes('[%s] %s' % ( ctime(), self.rfile.readline().decode('utf-8')), 'utf-8'))# 利用给定的主机信息和请求处理类创建了TCP服务器# 然后无限循环地等待并服务于客户端请求tcpServ = TCP(ADDR, MyRequestHandler)print('waiting for connection...')tcpServ.serve_forever() TCP客户端 12345678910111213141516171819202122232425#!usr/bin/python3# -*- coding:UTF-8 -*-from socket import *HOST = '127.0.0.1'PORT = 12345BUFSIZE = 1024ADDR = (HOST, PORT)while True: tcpSocket = socket(AF_INET, SOCK_STREAM) tcpSocket.connect(ADDR) data = input('&gt; ') if not data: break # 因为处理程序类对待套接字通信像文件一样，所以必须发送行终止符。 # 而服务器只是保留并重用这里发送的终止符 tcpSocket.send(bytes('%s\r\n' % data, 'utf-8')) data = tcpSocket.recv(BUFSIZE) if not data: break # 得到服务器返回的消息时，用strip()函数对其进行处理并使用print()自动提供的换行符 print(data.decode('utf-8').strip()) tcpSocket.close() socketserver TCP服务器和客户端运行结果 在客户端启动的时候连接了一次服务器，而每一次发送一个请求连接一次，所以发送了三个请求连接了四次服务器 TCP服务器运行结果 12345waiting for connection......connected from: (&apos;127.0.0.1&apos;, 51835)...connected from: (&apos;127.0.0.1&apos;, 51877)...connected from: (&apos;127.0.0.1&apos;, 51893)...connected from: (&apos;127.0.0.1&apos;, 51901) TCP客户端运行结果 1234567&gt; hello[Sat Feb 24 10:29:28 2018] hello&gt; hello[Sat Feb 24 10:29:44 2018] hello&gt; hi[Sat Feb 24 10:29:50 2018] hi&gt; Twisted框架的简单使用 Twisted是一个完整的事件驱动的网络框架，利用它既能使用也能开发完整的异步网络应用程序和协议。它不是Python标准库的一部分，所以需要单独下载和安装它1。 1pip3 install Twisted-17.9.0-cp36-cp36m-win_amd64.whl 安装成功显示 1234567891011Processing e:\迅雷下载\twisted-17.9.0-cp36-cp36m-win_amd64.whlRequirement already satisfied: Automat&gt;=0.3.0 in e:\python\python36\lib\site-packages (from Twisted==17.9.0)Requirement already satisfied: zope.interface&gt;=4.0.2 in e:\python\python36\lib\site-packages (from Twisted==17.9.0)Requirement already satisfied: incremental&gt;=16.10.1 in e:\python\python36\lib\site-packages (from Twisted==17.9.0)Requirement already satisfied: hyperlink&gt;=17.1.1 in e:\python\python36\lib\site-packages (from Twisted==17.9.0)Requirement already satisfied: constantly&gt;=15.1 in e:\python\python36\lib\site-packages (from Twisted==17.9.0)Requirement already satisfied: attrs in e:\python\python36\lib\site-packages (from Automat&gt;=0.3.0-&gt;Twisted==17.9.0)Requirement already satisfied: six in e:\python\python36\lib\site-packages (from Automat&gt;=0.3.0-&gt;Twisted==17.9.0)Requirement already satisfied: setuptools in e:\python\python36\lib\site-packages (from zope.interface&gt;=4.0.2-&gt;Twisted==17.9.0)Installing collected packages: TwistedSuccessfully installed Twisted-17.9.0 Twisted Reactor TCP 服务器/客户端TCP服务器 123456789101112131415161718192021222324252627282930313233#!usr/bin/python3# -*- coding:UTF-8 -*-# 常用模块导入，特别是twisted.internet的protocol和reactorfrom twisted.internet import protocol, reactorfrom time import ctime# 设置端口号PORT = 12345class TWServProtocol(protocol.Protocol): # 继承Protocol类 def connectionMade(self): # 重写connectionMade()方法 # 当一个客户端连接到服务器是会执行这个方法 client = self.client = self.transport.getPeer().host print('...connected from:', client) def dataReceived(self, data): # 重写dataReceived()方法 # 当服务器接收到客户端通过网络发送的一些数据的时候会调用此方法 self.transport.write(bytes('[%s] %s' % ( ctime(), data.decode('utf-8')), 'utf-8'))# 创建一个协议工厂，每次得到一个接入连接是，制造协议的一个实例# 在reactor中安装一个TCP监听器，以此检查服务请求# 当接收到一个请求时，就是创建一个就是创建一个TWServProtocol实例来处理客户端事务factory = protocol.Factory()factory.protocol = TWServProtocolprint('waiting for connection...')reactor.listenTCP(PORT, factory)reactor.run() TCP客户端 12345678910111213141516171819202122232425262728293031323334353637383940#!usr/bin/python# -*- coding:UTF-8 -*-from twisted.internet import protocol, reactorHOST = '127.0.0.1'PORT = 12345class TWClientProtocol(protocol.Protocol): def sendData(self): # 需要发送数据时调用 # 会在一个循环中继续，直到不输入任何内容来关闭连接 data = input('&gt; ') if data: print('...send %s...' % data) self.transport.write(bytes(data, 'utf-8')) else: self.transport.loseConnection() def connectionMade(self): # self.sendData() def dataReceived(self, data): print(data.decode('utf-8')) self.sendData()class TWClientFactory(protocol.ClientFactory): # 创建了一个客户端工厂 protocol = TWClientProtocol clientConnectionLost = clientConnectionFailed = \ lambda self, connector, reason: reactor.stop()# 创建了一个到服务器的连接并运行reactor，实例化了客户端工厂# 因为这里不是服务器，需要等待客户端与我们通信# 并且这个工厂为每一次连接都创建一个新的协议对象。# 客户端创建单个连接到服务器的协议对象，而服务器的工厂则创建一个来与客户端通信reactor.connectTCP(HOST, PORT, TWClientFactory())reactor.run() TCP服务器和客户端运行结果 服务器结果 12waiting for connection......connected from: 127.0.0.1 客户端结果 1234567&gt; hello...send hello...[Sat Feb 24 11:19:49 2018] hello&gt; hi...send hi...[Sat Feb 24 11:20:02 2018] hi&gt; 1.需要安装python对应的版本和位数 ↩]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python网络编程(一)]]></title>
    <url>%2F2018%2F02%2F22%2FPython%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[使用Python的一些模块来创建网络应用程序 socket()函数模块要创建套接字，必须使用socket.socket()函数socket(socket_family, socket_type, protocol = 0),其中socket_family是 AF_UNIX或 AF_INET,socket_type是 SOCK_STREAM 或 SOCK_DGRAM。1protocol通常省略，默认为0。 创建TCP/IP套接字 1tcpSock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) 创建UDP/IP套接字 1udpSock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) 套接字对象内接方法 名称 描述 服务器套接字方法 s.bind() 将地址(主机名、端口号对)绑定到套接字上 s.listen() 设置并启动TCP监听器 s.accept() 被动接受TCP客户端连接，一直等待知道连接到达(阻塞) 客户端套接字方法 s.connect() 主动发起TCP服务器连接 s.connect_ex() connect()的扩展版本，此时会以错误码的形式返回问题，而不是抛出一个异常 普通的套接字方法 s.recv() 接受TCP消息 s.recv_into() 接受TCP消息到指定的缓冲区 s.send() 发送TCP消息 s.sendall() 完整地发送TCP消息 s.recvfrom() 接受UDP消息 s.recvfrom_into() 接受UDP消息到指定的缓冲区 s.sendto() 发送UDP消息 s.getpeername() 连接到套接字(TCP)的远程地址 s.getsockname() 当前套接字的地址 s.getsockopt() 返回给定套接字选项的值 s.setsockopt() 设置给定套接字选项的值 s.shutdown() 关闭连接 s.close() 关闭套接字 s.detach() 在未关闭文件描述符的情况下关闭套接字，返回文件描述符 s.ioctl() 控制套接字的模式(仅支持Windows) 面向阻塞的套接字方法 s.setblocking() 设置套接字的阻塞或非阻塞模式 s.settimeout() 设置阻塞套接字操作的超时时间 s.gettimeout() 获取阻塞套接字操作的超时时间 面向文件的套接字方法 s.fileno() 套接字的文件描述符 s.makefile() 创建与套接字关联的文件对象 数据属性 s.family 套接字家族 s.type 套接字类型 s.proto 套接字协议 socket模块属性 属性名称 描述 数据属性 AF_UNIX、AF_INET、AF_INET6、AF_NETLINK、AF_TIPC Python中支持的套接字地址家族 SO_STREAM、SO_DGRAM 套接字类型(TCP=流，UDP=数据报) has_ipv6 指示是否支持IPv6的布尔标记 异常 error 套接字相关错误 herror 主机和地址相关错误 gaierror 地址相关错误 timeout 超时时间 函数 socket() 以给定的地址家族、套接字类型和协议类型(可选) 创建一个套接字对象 socketpair() 以给定的地址家族、套接字类型和协议类型(可选) 创建一个套接字对象 create_connection() 常规函数，它接收一个地址(主机号，端口号)对，返回套接字对象 fromfd() 以一个打开的文件描述符创建一个套接字对象 ssl() 通过套接字启动一个安全套接字层连接；不执行证书验证 getaddrinfo() 获取一个五元组序列形式的地址信息 getnameinfo() 给定一个套接字地址，返回(主机名，端口号)二元组 getfqdn() 返回完整的域名 gethostname() 返回当前主机名 gethostbyname() 将一个主机名映射到它的IP地址 gethostbyname_ex() gethostbyname()的扩展版本，它返回主机名、别名主机集合和IP地址列表 gethostbyaddr() 讲一个IP地址映射到DNS信息；返回与gethostbyname_ex()相同的三元组 getprotobyname() 将一个协议名(如‘TCP’)映射到一个数字 getservbyname()/getservbyport() 将一个服务名映射到一个端口号，或者反过来；对于任何一个函数来说，协议名都是可选的 ntohl()/ntohs() 将来自网络的整数装换为主机字节序 htonl()/htons() 将来自主机的整数转换为网络字节序 inet_aton()/inet_ntoa() 将IP地址八进制字符串转换成32位的包格式，或者反过来(仅用于IPv4地址) inet_pton()/inet_ntop() 将IP地址字符串转换成打包的二进制格式，或者反过来(同时适用于IPv4和IPv6) getdefaulttimeout()/setdefaulttimeout() 以秒(浮点数)为单位返回默认套接字超时时间；以秒(浮点数)为单位设置默认套接字超时时间 详情参阅socket模块文档 创建TCP服务器/客户端TCP服务器 下面是TCP服务器端的通用伪码，这是设计服务器的一种方式，可根据需求修改来操作服务器 123456789ss = socket() #创建服务器套接字ss.bind() #套接字与地址绑定ss.listen() #监听连接inf_loop: #服务器无线循环 cs = ss.accept() #接受客户端连接 comm_loop: #通信循环 cs.recv()/cs.send() #对话(接收/发送) cs.close() #关闭客户端套接字ss.close() #关闭服务器套接字 TCP时间戳服务器 12345678910111213141516171819202122232425262728293031323334353637383940#!usr/bin/python3# -*- coding:UTF-8 -*-# 导入socket模块和time.ctime()的所有属性from socket import *from time import ctime# HOST变量是空白，这是对bind()方法的标识，标识它可以使用任何可用的地址# 选择一个随机的端口号# 缓冲区大小为1KBHOST = ''PORT = 12345BUFSIZE = 1024ADDR = (HOST, PORT)# 分配了TCP服务套接字# 将套接字绑定到服务器地址# 开启TCP的监听调用# listen()方法的参数是在连接被转接或拒绝之前，传入连接请求的最大数tcpSerSock = socket(AF_INET, SOCK_STREAM)tcpSerSock.bind(ADDR)tcpSerSock.listen(5)while True: # 服务器循环，等待客户端的连接的连接 print('waiting for connection...') tcpCliSock, addr = tcpSerSock.accept() print('...connected from:', addr) while True: # 当一个连接请求出现时，进入对话循环，接收消息 data = tcpCliSock.recv(BUFSIZE) if not data: # 当消息为空时，退出对话循环 # 关闭客户端连接，等待下一个连接请求 break tcpCliSock.send(bytes('[%s] %s' % ( ctime(), data.decode('utf-8')), 'utf-8')) tcpCliSock.close() TCP客户端 下面是TCP客户端的通用伪码 12345cs = socket() #创建客户端套接字cs.connect() #尝试连接服务器comm_loop: #通信循环 cs.send()/cs.recv #对话(发送/接收)cs.close() #关闭客户端套接字 TCP时间戳客户端 1234567891011121314151617181920212223242526272829303132333435#!usr/bin/python3# -*- coding: UTF-8 -*-# 导入socket模块所有属性from socket import *# 服务器的主机名# 服务器的端口号,应与服务器设置的完全相同# 缓冲区大小为1KBHOST = '127.0.0.1'PORT = 12345BUFSIZE = 1024ADDR = (HOST, PORT)# 分配了TCP客户端套接字# 主动调用并连接到服务器tcpCliSock = socket(AF_INET, SOCK_STREAM)tcpCliSock.connect(ADDR)while True: # 无限循环，输入消息 data = bytes(input('&gt; '), 'utf-8') if not data: # 消息为空则退出循环 break # 发送输入的信息 # 接收服务器返回的信息，最后打印 tcpCliSock.send(data) data = tcpCliSock.recv(BUFSIZE) if not data: # 消息为空则退出循环 break print(data.decode('utf-8'))# 关闭客户端tcpCliSock.close() TCP服务器和客户端运行结果 在运行程序时，必须 首先运行服务器 程序，然后再运行客户端程序。如果先运行客户端程序，将会报未连接到服务器的错误。 按正确的顺序启动程序后，在客户端输入信息，将会接收到加上时间戳处理后的信息，如果直接输入回车，将会关闭客户端，而服务器将会等待下一个连接请求 服务器运行结果 123waiting for connection......connected from: (&apos;127.0.0.1&apos;, 53220)waiting for connection... 客户端运行结果 12345678&gt; hello[Fri Feb 23 14:22:58 2018] hello&gt; hi[Fri Feb 23 14:23:02 2018] hi&gt; hello world[Fri Feb 23 14:23:09 2018] hello world&gt;Process finished with exit code 0 创建UDP服务器/客户端UDP服务器 下面是UDP服务器的伪码 12345ss = socket() #创建服务器套接字ss.bind() #绑定服务器套接字inf_loop: #服务器无线循环 cs = ss.recvfrom()/ss.sendto() #关闭(接收/发送)ss.close() #关闭服务器套接字 UDP时间戳服务器 1234567891011121314151617181920212223#!usr/bin/python3# -*- coding:UTF-8 -*-# 导入socket模块和time.ctime()的全部属性from socket import *from time import ctime# 与TCP相同，由于是无连接，所以没有调用监听传入连接HOST = ''PORT = 12345BUFSIZE = 1024ADDR = (HOST, PORT)udpSerSock = socket(AF_INET, SOCK_DGRAM)udpSerSock.bind(ADDR)while True: # 进入循环等待消息，一条消息到达时，处理并返回它，然后等待下一条消息 print('waiting for message...') data, addr = udpSerSock.recvfrom(BUFSIZE) udpSerSock.sendto(bytes('[%s] %s' % ( ctime(), data.decode('utf-8')), 'utf-8'), addr) print('...received from and returned to:', addr) UDP客户端 下面是客户端的伪码 1234cs = socket() #创建客户端套接字comm_loop: #通信循环 cs.sendto()/cs.recvfrom() #对话(发送/接收)cs.close() #关闭客户端套接字 UDP时间戳客户端 12345678910111213141516171819202122#!usr/bin/python3 # -*- coding:UTF-8 -*- from socket import * HOST = '127.0.0.1' PORT = 12345 BUFSIZE = 1024 ADDR = (HOST, PORT) udpClienSock = socket(AF_INET, SOCK_DGRAM) while True: data = bytes(input('&gt;'), 'utf-8') if not data: break udpClienSock.sendto(data, ADDR) data, ADDR = udpClienSock.recvfrom(BUFSIZE) if not data: break print(data.decode('utf-8')) udpClienSock.close() UDP服务器和客户端运行结果 因为UDP面向无连接的服务，所以程序的启动顺序没有要求。当服务器处理完一个数据报之后在等待下一个继续处理 服务器运行结果 12345waiting for message......received from and returned to: (&apos;127.0.0.1&apos;, 51434)waiting for message......received from and returned to: (&apos;127.0.0.1&apos;, 51434)waiting for message... 客户端运行结果 1234567&gt;hello[Fri Feb 23 15:23:57 2018] hello&gt;hi[Fri Feb 23 15:24:03 2018] hi&gt;Process finished with exit code 0 1.AF_UNIX 是基于文件的套接字，代表 地址家族(address family):UNIX，AF_INET 是基于网络的套接字，代表 地址家族：因特网， AF_INET6 用于底6版因特网协议(IPv6)寻址。 SOCK_STREAM 表示面向连接的TCP套接字， SOCK_DGRAM 代表无连接的UDP套接字。 ↩]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python正则表达式(三)]]></title>
    <url>%2F2018%2F02%2F12%2FPython%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F-%E4%B8%89%2F</url>
    <content type="text"><![CDATA[在之前的两篇博文中，已经对正则表达式基本及核心的知识点进行了罗列和总结。而对于正则表达式的使用却缺乏实践。本文将基于《Python核心编程(第三版)》的练习题进行一些练习。 正则表达式 识别后续的字符串：“bat”、“bit”、“but”、“hat”、“hit”或者“hut”。 1234567891011121314import remode = re.compile(r'bat|bit|but|hat|hit|hut')#mode = re.compile(r'[bh][iau]t')strs = ['bat', 'bit', 'but', 'hat', 'hit', 'hut']for s in strs: if mode.match(s) is not None:mode.match(s).group()#输出结果'bat''bit''but''hat''hit''hut' 匹配由单个空格分隔的任意单词对，也就是姓和名。 12345678import remode = re.compile(r'^[A-Za-z]+ [A-Za-z]+$')strs = ['david Bob', 'D.Jone Steven', 'Lucy D May']for s in strs: if mode.match(s) is not None:mode.match(s).group()#输出结果'david Bob' 匹配由单个逗号和单个空白符分隔的任何单词和单个字母，如姓氏的首字母。 123456789import remode = re.compile(r'[A-Za-z]+,\s[A-Za-z]+')strs = ['david, Bob', 'D.Jone, Steven', 'Lucy, D, May']for s in strs: if mode.match(s) is not None:mode.match(s).group()#输出结果'david, Bob''Lucy, D' 匹配所有有效Python 标识符1的集合。 123456789101112import remode = re.compile(r'[^0-9][\w_]+')#用in排除关键字strs = ['1var', 'v_ar', '_var', 'var', 'var_9', 'var_']for s in strs: if mode.match(s) is not None:mode.match(s).group()#输出结果'v_ar''_var''var''var_9''var_' 根据读者当地的格式，匹配街道地址（使你的正则表达式足够通用，来匹配任意数量的街道单词，包括类型名称）。例如，美国街道地址使用如下格式：1180 BordeauxDrive。使你的正则表达式足够灵活，以支持多单词的街道名称，如3120 De la CruzBoulevard。 123456789import remode = re.compile(r'^\d&#123;4&#125;( [A-Z][a-z]+)+$')strs = ['1221 Bordeaux Drive', '54565 Bordeaux Drive', 'Bordeaux Drive', '1221 Bordeaux Drive Drive']for s in strs: if mode.match(s) is not None:mode.match(s).group()#输出结果'1221 Bordeaux Drive''1221 Bordeaux Drive Drive' 匹配以“www”起始且以“.com”结尾的简单Web 域名；例如，www://www. yahoo.com/。选做题：你的正则表达式也可以支持其他高级域名，如.edu、.net 等（例如，http://www.foothill.edu）。 1234567891011import remode = re.compile(r'^(http[s]?://)?www\.(\w+\.)+(com|net|edu)$')strs=['https://www.baidu.com', 'http://www.bilibili.com', 'www.baidu.com', 'baidu.com', 'www.cqupt.edu']for s in strs: if mode.match(s) is not None:mode.match(s).group()#输出结果'https://www.baidu.com''http://www.bilibili.com''www.baidu.com''www.cqupt.edu' 匹配所有能够表示Python 整数的字符串集。 12345678910import remode = re.compile(r'^\d+[lL]?$')strs = ['123', '123l', '12312L']for s in strs: if mode.match(s) is not None:mode.match(s).group()#输出结果'123''123l''12312L' 匹配所有能够表示Python 长整数的字符串集。 123456789import remode = re.compile(r'^\d+[lL]$')strs = ['123', '123l', '12312L']for s in strs: if mode.match(s) is not None:mode.match(s).group()#输出结果'123l''12312L' 匹配所有能够表示Python 浮点数的字符串集。 12345678910import remode = re.compile(r'(0|[1-9]\d*)(\.\d+)?$')strs = ['00.10', '0.123', '12.23', '12', '12.36l']for s in strs: if mode.match(s) is not None:mode.match(s).group() #输出结果'0.123''12.23''12' 匹配所有能够表示Python 复数的字符串集。 12345678910import remode = re.compile(r'^((0|[1-9]\d*)(\.\d+)?\+)?((0|[1-9]\d*)(\.\d+)?j)?$')strs = ['12.3+1.2j', '1+2j', '4j']for s in strs: if mode.match(s) is not None:mode.match(s).group() #输出结果'12.3+1.2j''1+2j''4j' 匹配所有能够表示有效电子邮件地址的集合（从一个宽松的正则表达式开始，然后尝试使它尽可能严谨，不过要保持正确的功能）。 123456789101112import remode = re.compile(r'^\w+@(\w+\.)+(com|com\.cn|net)$')strs = ['12345@qq.com', 'sina@163.com', 'qq@sina.com.cn', 'net@21cn.com', 'new123@163.sina.com']for s in strs: if mode.match(s) is not None:mode.match(s).group() #输出结果'12345@qq.com''sina@163.com''qq@sina.com.cn''net@21cn.com''new123@163.sina.com' type()。内置函数type()返回一个类型对象，如下所示，该对象将表示为一个Pythonic类型的字符串。 12345678910import remode = re.compile(r'&lt;type \'(.*)\'&gt;')strs = ['&lt;type \'int\'&gt;', '&lt;type \'float\'&gt;', '&lt;type \'builtin_function_or_method\'&gt;']for s in strs: if mode.match(s) is not None:mode.match(s).group(1)#输出结果'int''float''builtin_function_or_method' 处理日期。1.2 节提供了来匹配单个或者两个数字字符串的正则表达式模式，来表示1～9 的月份(0?[1-9])。创建一个正则表达式来表示标准日历中剩余三个月的数字。 12345678910import remode = re.compile(r'1[0-2]')strs = ['10', '11', '12']for s in strs: if mode.match(s) is not None:mode.match(s).group()#输出结果'10''11''12' 创建一个允许使用连字符的正则表达式，但是仅能用于正确的位置。例如，15 位的信用卡号码使用4-6-5 的模式，表明4 个数字-连字符-6 个数字-连字符-5 个数字；16 位的信用卡号码使用4-4-4-4 的模式。 123456789import remode = re.compile(r'\d&#123;4&#125;-((\d&#123;6&#125;-\d&#123;5&#125;)|(\d&#123;4&#125;-\d&#123;4&#125;-\d&#123;4&#125;))')strs = ['1234-567890-12345', '1234-5678-8012-3456']for s in strs: if mode.match(s) is not None:mode.match(s).group()#输出结果'1234-567890-12345''1234-5678-8012-3456' 1.标识符有字母、数字、下划线组成，但不能由数字开头 ↩]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python正则表达式(二)]]></title>
    <url>%2F2018%2F02%2F10%2FPython%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F-%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[正则表达式的匹配规则基本已经在上一篇博文中全部罗列出来了，下面便是结合到具体语言进行学习和练习了。由于个人兴趣和想要专研的方向，在这里将会使用Python 1 语言进行描述。 正则表达式和Python语言re模块：核心函数和方法 函数方法 描述 仅仅是re函数模块 compile(pattern, flags=0) 使用任何可选的标记来编译正则表达式的模式，然后返回一个正则表达式对象 re模块函数和正则表达式对象的方法 match(pattern, string, flags=0) 尝试使用带有可选的标记的正则表达式的模式来匹配字符串，如果匹配成功，就返回匹配对象；如果失败，就返回None search(pattern, string, flags=0) 使用可选标记搜索字符串中第一次出现的正则表达式模式。如果匹配成功，则返回匹配对象；如果匹配失败，怎返回None findall(pattern, string [,flags]) 查找字符串中所有(非重复)出现的正则表达式模式，并返回一个匹配列表 finditer(pattern, string[,flags]) 与findall()函数相同，但返回的不是一个列表，而是一个迭代器。对于每一次匹配，迭代器都返回一个匹配对象 split(pattern, string, max=0) 根据正则表达式的模式分隔符，split函数将字符串分割为列表，然后返回成功的列表，分割最多操作max次(默认分割所有匹配成功的位置) sub(pattern, repl, string, count=0) 使用repl替换所有正则表达式的模式在字符串中出现的位置，除非定义count，否则就讲替换所有出现的位置（另见subn()函数，该函数返回替换操作的数目） purge() 清除隐式编译的正则表达式模式 常见的匹配对象方法 group(num=0) 返回整个匹配对象，或者编号为num的特定子组 groups(default=None) 返回一个包含所有匹配子组的元组(如果没有成功匹配，则返回一个空元组) groupdict(default=None) 返回一个包含所有匹配的命名子组的字典，所有的子组名称作为字典的键(如果没有成功匹配，则返回一个空字典) 常用的模块属性（用于大多数正则表达式函数的标记） re.I,re.IGNORECASE 不去分大小写的匹配 re.L,re.LOCALE 根据所使用的本地语言环境通过\w、\w、\b、\B、\s、\S实现匹配 re.M,re.MULTILINE ^和$分别匹配目标字符串中行的起始和结尾，而不是严格匹配整个字符串本身的起始和结尾 re.S,re.DOTALL “.”(点号)通常匹配除了\n(换行符)之外的所有单个字符：该标记表示”.”(点号)能匹配全部字符 re.X,re.VERBOSE 通过反斜线转移，否则所有空格加上#(以及在该行中后续文字)都被忽略，除非在一个字符类中或者允许注释并且提高可读性 部分方法总结 compile(pattern, flags=0)2 使用预编译使用推荐的方式，但不是必须的，可以通过设置标志位(上表已罗列出使用频繁的标记，详情可以查阅文档),标志位通过 （|）合并 group(num=0) 和 groups(default=None) 匹配对象3的两个主要方法。 group() 要么返回整个匹配对象，要么按要求返回特定子组。 groups() 仅返回一个包含唯一或全部子组的元组。如果没有子组的要求，group() 返回整个匹配，groups() 返回一个空元组。 match(pattern, string, flags=0) match() 方法试图从字符串的起始部分对模式进行匹配。如果匹配成功，返回一个匹配对象；如果失败就返回None 12345678910111213 #匹配成功 m = re.match('foo', 'foo') #模式匹配字符串 if m is not None: #如果匹配成功，就输出匹配内容 m.group()'foo' #输出结果#匹配失败m = re.match('foo', 'Bfoo') #模式匹配字符串if m is not None: #如果匹配成功，就输出匹配内容 m.group() #因为起始字符为'B',所以匹配不成功，无任何输出 search(pattern, string, flags=0) search() 的工作方式和 match() 相同，不同之处在于 search() 会用它的字符串参数在任意位置对给定正则表达式模式搜索第一次出现的匹配情况。如果搜索到成功的匹配，就返回一个匹配对象；否则，就返回None。 123456#将上面使用match()方法匹配的串改用search()匹配m = re.search('foo', 'Bfoo') #模式匹配字符串if m is not None: #如果匹配成功，就输出匹配内容 m.group()'foo' #可以看到就算起始位置未能匹配，也能匹配成功 findall(pattern, string[,flags]) 和 finditer(pattern, string[,flags]) findall() 总是返回一个列表，如果没有找到匹配对象，返回一个空列表 finditer() 是一个与 findall() 类似但更节省内存的变体，finditer() 在匹配对象中迭代4 1234567891011121314#findall()匹配re.findall('car', 'carry the barcardi to the car') #模式匹配字符串['car', 'car', 'car'] #返回结果#finditer()匹配iter = re.finditer('car', 'carry the barcardi to the car') #模式匹配字符串for i in iter: #遍历迭代器 print(i.group())#输出结果carcarcar sub(pattern, repl, string, count=0) 和 subn(pattern, repl, string, count=0) sub() 和 subn() 用于实现搜索和替换功能。两者都是将某字符串中所有匹配正则表达式的部分进行某种形式的替换。和 sub() 不同的是，subn() 返回一个表示替换的总数，替换后的字符串和表示替换总数的数字一起作为一个拥有两个元素的元组返回 12345678910#sub()re.sub('car', 'cat', 'My car is not only a car.') #模式匹配字符串'My cat is not only a cat.' #输出结果#subn()re.subn('car', 'cat', 'My car is not only a car.') #模式匹配字符串('My cat is not only a cat.', 2) #输出结果 split(pattern, string, max=0) 正则表达式对象的 split() 方法和字符串的工作方式类似，但它是基于正则表达式的模式分割字符串。 123456789101112131415161718192021222324re.split(':', 'str1:str2:str3') #模式匹配字符串['str1', 'str2', 'str3'] #输出结果，与'str1:str2:str3'.split(':')相同#split()复杂用法#使用split()基于逗号分割字符串，如果空格紧跟在5个数字或者两个大写字母之后，就用split()分割该空格#使用(?=)正向前视断言，不适用输入字符串 而是使用后面的空格作为分割字符串import reDATA = ( 'Mountain View, CA 94040', 'Sunnyvale, CA', 'Los Altos, 94023', 'Cupertino 95014', 'Palo Alto CA',)for datum in DATA: print(re.split(', |(?= (?:\d&#123;5&#125;|[A-Z]&#123;2&#125;)) ', datum))#输出结果['Mountain View', 'CA', '94040']['Sunnyvale', 'CA']['Los Altos', '94023']['Cupertino', '95014']['Palo Alto', 'CA'] 符号的使用| 与 . 和 [] 包括择一匹配符号|、点号.，点号不匹配非字符或换行付\n（即空字符） 字符集[]中的字符只取其一 重复、特殊字符5以及分组 ?操作符表示前面的模式出现零次或一次+操作符表示前面的模式出现至少一次*操作符表示前面的模式出现任意次(包括0次)分组从左起第一个括号开始算第一个分组 123456789101112131415m = re.match('(\w(\w\w))-(\d\d\d)','abc-123')m.group() #完整匹配'abc-123' #输出结果m.group(1) #第一组'abc' #输出结果 m.group(2) #第二组'bc' #输出结果m.group(3) #第三组'123' #输出结果m.groups() #全部子组('abc', 'bc', '123') #输出结果 1.这里Python指代的是Python3.6.4 ↩2.预编译可以提升执行效率，而 re.compile() 方法提供了这个功能。模块函数会对已编译的对象进行缓存，所以无论使用 match() 和 search() 在执行时编译的正则表达式,还是使用 compile() 编译的表达式,在再次使用时都会查询缓存。但使用 compile() 同样可以节省查询缓存的时间 ↩3.除了正则表达式对象之外，还有另外一个对象类型：匹配对象。这些是成功调用 match() 和 search() 返回的对象。 ↩4.如果遇到无法调用 next()方法，可以使用 \_\_next\_\_()方法代替。 ↩5.特殊字符的详情可以参考上一篇博文 ↩]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>正则表达式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python正则表达式(一)]]></title>
    <url>%2F2018%2F02%2F09%2FPython%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F-%E4%B8%80%2F</url>
    <content type="text"><![CDATA[在Python的学习过程中，正则表达式始终是一道绕不过去的坎.无论提取服务器日志文件还是爬取网页，正则表达式始终扮演着至关重要的角色。下面便对自己学习过程中的一些正则表达式基础做一个总结。 特殊符号和字符 表示法 描述 正则表达式示例 备注 符号 literal 匹配文本字符串的字面值literal foo 只是匹配foo，相当于取等 re1&#124; re2 匹配正则表达式re1或re2 foo&#124; bar 匹配foo或者bar，二选一 . 匹配任何字符(除了\n之外) a.a 匹配axa、aaa、aca等，两个a中间可以是除了\n的任意字符 ^ 匹配字符串起始部分 ^Foo 匹配Foo，不匹配aFoo $ 匹配字符串终止部分 Bar$ 只匹配Bar,不匹配aBar等 * 匹配0次或者多次前面出现的正则表达式 [A-Za-z0-9]* 匹配任意多个字母或数字 + 匹配1次或者多次前面出现的正则表达式 [A-Za-z0-9]+ 匹配一到多个个字母或数字 ? 匹配0次或者1次前面出现的正则表达式 [A-Za-z0-9] 要么有一个字母或数字要么没有 {N} 匹配N次前面出现的正则表达式 [0-9]{3} 匹配三个数字 {M,N} 匹配M~N次前面出现的正则表达式 [0-9]{5,9} 匹配5到9个数字(包括5个和9个) […] 匹配来自字符集的任意单一字符 [aeiou] 匹配一个元音字母 [..x~y..] 匹配x~y范围内的任意单一字符 [A-Za-z] 匹配任意一个英文字母 ... 不匹配此字符集中出现的任何一个字符，包括某一范围的字符(如果在此字符集中出现) [\^aeiou][\^A-Za-z0-9] 匹配一个非元音字母和一个非字母数字字符 (*&#124;+&#124;?&#124;{})? 用于匹配上面频繁出现/重复出现符号的非贪婪版本(*、+、?、{}) .*?[a-z] (非贪婪是指尽可能少的匹配) (…) 匹配封闭的正则表达式,然后另存为子组 f(oo&#124; u)bar 匹配foobar,fubar 特殊字符 \d 匹配任何十进制数字，与[0-9]一致(\D和\d相反，不匹配任何非数值型的数字) data\d+.txt 匹配data1.txt、data12.txt \w 匹配任何字母数字字符，与[A-Za-z0-9]相同(\W与之相反) [A-Za-z_]\w+ 匹配任意字母或_加一个或多个字母数字字符(asda,_asda) \s 匹配任何空格字符，与[\n\t\r\v\f]相同(\S与之相反) of\sthe 匹配of the (\n:换行符&#124;\t:水平制表符&#124;\r:回车&#124;\v:垂直制表符&#124;\f:换页符) \b 匹配任何单词边界(\B与之相反) \bThe\b 匹配of The a，不匹配ofThe a \N 匹配已保存的子组N(与上面(…)配合使用) price:\16 匹配price:和前面第16个子组的值 \c 逐字匹配任何特殊字符(即仅按字面意义匹配，不包含特殊含义，\为对特殊字符的转义表示) * 匹配* \A(\Z) 匹配字符串的起始(结束)(另见上面的^和$) \ADear 匹配以Dear开头的 扩展表示法 （?iLmsux） 在正则表达式中嵌入一个或多个特殊”标记”参数(或者通过函数/方法) （?x）,(?im) (?:…) 表示一个匹配不用保存的分组 (?:\w+.)* 匹配任意多个一个或多个字母数字字符与.的组合但不保存改分组 (?P…) 像一个仅由name标识而不是数字ID标识的正则分组匹配 (?P) 给匹配的分组命名为data (?P=name) 在同一个字符串中匹配由(?P)分组之前的文本 (?P=data) 匹配名字为data的串 (?#…) 表示注释，所有内容都被忽略 (?#comment) (?=…) 匹配条件是如果…出现在之后的位置，而不使用输入字符串；称作正向前视断言 (?=.com) 如果一个字符串后面跟着“.com”才做匹配操作，并不适用任何目标字符串 (?!…) 匹配条件是如果…不出现在之后的位置，而不使用输入字符串；称作负向前视断言 (?!.net) 如果一个字符串后面不是跟着“.net”，才做匹配操作 (?&lt;=…) 匹配条件是如果…出现在之前的位置，而不使用输入字符串；称作正向后视断言 (?&lt;=800-) 如果字符串之前为“800-”才做匹配，并不使用任何输入字符串 (?&lt;!…) 匹配条件是如果…不出现在之前的位置，而不使用输入字符串；称作负向后视断言 (?&lt;!192\.168\.) 如果一个字符串之前不是“192.168.”才做匹配，并不适用任何输入字符串 (?(id/name)Y&#124;N 如果分组所提供的id或name(名称)存在，就返回正则表达式的条件匹配Y，如果不存在，就返回N;N是可选项 (?(1)y&#124;x) 如果一个匹配组1(\1)存在就y匹配；否则，就与x匹配 以上为正则表达式的一些基本的符号定义与用法，熟练掌握这些符号是写出高效表达式的基础。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>正则表达式</tag>
      </tags>
  </entry>
</search>
