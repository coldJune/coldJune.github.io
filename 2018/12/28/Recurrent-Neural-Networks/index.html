<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />










  <meta name="baidu-site-verification" content="true" />







  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />




  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico#/images/favicon-16x16-next.png?v=5.1.4">






  <meta name="keywords" content="tensorflow,Hands-On Machine Learning with Scikit-Learn and TensorFlow,神经网络," />










<meta name="description" content="准备123456789101112import numpy as np%matplotlib inlineimport matplotlibimport matplotlib.pyplot as pltimport tensorflow as tfdef reset_graph(seed=42):    tf.reset_default_graph()    tf.set_random_seed(">
<meta name="keywords" content="tensorflow,Hands-On Machine Learning with Scikit-Learn and TensorFlow,神经网络">
<meta property="og:type" content="article">
<meta property="og:title" content="Recurrent Neural Networks">
<meta property="og:url" content="http://coldjune.com/2018/12/28/Recurrent-Neural-Networks/index.html">
<meta property="og:site_name" content="Stay Hungary">
<meta property="og:description" content="准备123456789101112import numpy as np%matplotlib inlineimport matplotlibimport matplotlib.pyplot as pltimport tensorflow as tfdef reset_graph(seed=42):    tf.reset_default_graph()    tf.set_random_seed(">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://coldjune.com/2018/12/28/Recurrent-Neural-Networks/Recurrent%20Neural%20Networks_49_0.png">
<meta property="og:image" content="http://coldjune.com/2018/12/28/Recurrent-Neural-Networks/Recurrent%20Neural%20Networks_61_0.png">
<meta property="og:image" content="http://coldjune.com/2018/12/28/Recurrent-Neural-Networks/Recurrent%20Neural%20Networks_70_0.png">
<meta property="og:image" content="http://coldjune.com/2018/12/28/Recurrent-Neural-Networks/Recurrent%20Neural%20Networks_73_0.png">
<meta property="og:image" content="http://coldjune.com/2018/12/28/Recurrent-Neural-Networks/Recurrent%20Neural%20Networks_75_0.png">
<meta property="og:image" content="http://coldjune.com/2018/12/28/Recurrent-Neural-Networks/Recurrent%20Neural%20Networks_91_0.png">
<meta property="og:image" content="http://coldjune.com/2018/12/28/Recurrent-Neural-Networks/Recurrent%20Neural%20Networks_128_0.png">
<meta property="og:updated_time" content="2019-05-17T14:32:54.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Recurrent Neural Networks">
<meta name="twitter:description" content="准备123456789101112import numpy as np%matplotlib inlineimport matplotlibimport matplotlib.pyplot as pltimport tensorflow as tfdef reset_graph(seed=42):    tf.reset_default_graph()    tf.set_random_seed(">
<meta name="twitter:image" content="http://coldjune.com/2018/12/28/Recurrent-Neural-Networks/Recurrent%20Neural%20Networks_49_0.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://coldjune.com/2018/12/28/Recurrent-Neural-Networks/"/>





  <title>Recurrent Neural Networks | Stay Hungary</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
		<a href="https://github.com/coldJune" class="github-corner" aria-label="View source on Github"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; left: 0; transform: scale(-1, 1);" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style></a>    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Stay Hungary</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Programming is an art form</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://coldjune.com/2018/12/28/Recurrent-Neural-Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="邓小俊">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Stay Hungary">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Recurrent Neural Networks</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-28T15:43:28+08:00">
                2018-12-28
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2018/12/28/Recurrent-Neural-Networks/" class="leancloud_visitors" data-flag-title="Recurrent Neural Networks">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  5,902
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  37
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reset_graph</span><span class="params">(seed=<span class="number">42</span>)</span>:</span></span><br><span class="line">    tf.reset_default_graph()</span><br><span class="line">    tf.set_random_seed(seed)</span><br><span class="line">    np.random.seed(seed)</span><br></pre></td></tr></table></figure>
<h1 id="RNN基础"><a href="#RNN基础" class="headerlink" title="RNN基础"></a>RNN基础</h1><h2 id="手动实现RNN"><a href="#手动实现RNN" class="headerlink" title="手动实现RNN"></a>手动实现RNN</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">reset_graph()</span><br><span class="line"></span><br><span class="line">n_inputs = <span class="number">3</span></span><br><span class="line">n_neurons = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">X0 = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_inputs])</span><br><span class="line">X1 = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_inputs])</span><br><span class="line"></span><br><span class="line">Wx = tf.Variable(tf.random_normal(shape=[n_inputs, n_neurons], dtype=tf.float32))</span><br><span class="line">Wy = tf.Variable(tf.random_normal(shape=[n_neurons, n_neurons], dtype=tf.float32))</span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">1</span>, n_neurons], dtype=tf.float32))</span><br><span class="line"></span><br><span class="line">Y0 = tf.tanh(tf.matmul(X0, Wx) + b)</span><br><span class="line">Y1 = tf.tanh(tf.matmul(Y0, Wy) + tf.matmul(X1, Wx) + b)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">X0_batch = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">    [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">    [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>],</span><br><span class="line">    [<span class="number">9</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">])<span class="comment"># t=0</span></span><br><span class="line">X1_batch = np.array([</span><br><span class="line">    [<span class="number">9</span>, <span class="number">8</span>, <span class="number">7</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>],</span><br><span class="line">    [<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">])<span class="comment"># t=1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init.run()</span><br><span class="line">    Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict=&#123;X0: X0_batch, X1: X1_batch&#125;)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(Y0_val)</span><br></pre></td></tr></table></figure>
<pre><code>[[-0.0664006   0.9625767   0.68105793  0.7091854  -0.898216  ]
 [ 0.9977755  -0.719789   -0.9965761   0.9673924  -0.9998972 ]
 [ 0.99999774 -0.99898803 -0.9999989   0.9967762  -0.9999999 ]
 [ 1.         -1.         -1.         -0.99818915  0.9995087 ]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(Y1_val)</span><br></pre></td></tr></table></figure>
<pre><code>[[ 1.         -1.         -1.          0.4020025  -0.9999998 ]
 [-0.12210419  0.62805265  0.9671843  -0.9937122  -0.2583937 ]
 [ 0.9999983  -0.9999994  -0.9999975  -0.85943305 -0.9999881 ]
 [ 0.99928284 -0.99999815 -0.9999058   0.9857963  -0.92205757]]
</code></pre><h2 id="使用static-rnn"><a href="#使用static-rnn" class="headerlink" title="使用static_rnn()"></a>使用static_rnn()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">n_inputs = <span class="number">3</span></span><br><span class="line">n_neurons = <span class="number">5</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">reset_graph()</span><br><span class="line"></span><br><span class="line">X0 = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_inputs])</span><br><span class="line">X1 = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_inputs])</span><br><span class="line"></span><br><span class="line">basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons)</span><br><span class="line">out_put_seqs, states = tf.nn.static_rnn(basic_cell, [X0, X1], dtype=tf.float32)</span><br><span class="line">Y0, Y1 = out_put_seqs</span><br></pre></td></tr></table></figure>
<pre><code>WARNING:tensorflow:From &lt;ipython-input-7-64acfd881dc3&gt;:6: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">X0_batch = np.array([</span><br><span class="line">    [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">    [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">    [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>],</span><br><span class="line">    [<span class="number">9</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">])<span class="comment"># t=0</span></span><br><span class="line">X1_batch = np.array([</span><br><span class="line">    [<span class="number">9</span>, <span class="number">8</span>, <span class="number">7</span>],</span><br><span class="line">    [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">    [<span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>],</span><br><span class="line">    [<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">])<span class="comment"># t=1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init.run()</span><br><span class="line">    Y0_val, Y1_val = sess.run([Y0, Y1], feed_dict=&#123;X0: X0_batch, X1: X1_batch&#125;)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Y0_val</span><br></pre></td></tr></table></figure>
<pre><code>array([[ 0.30741334, -0.32884315, -0.6542847 , -0.9385059 ,  0.52089024],
       [ 0.99122757, -0.9542541 , -0.7518079 , -0.9995208 ,  0.9820235 ],
       [ 0.9999268 , -0.99783254, -0.8247353 , -0.9999963 ,  0.99947774],
       [ 0.996771  , -0.68750614,  0.8419969 ,  0.9303911 ,  0.8120684 ]],
      dtype=float32)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Y1_val</span><br></pre></td></tr></table></figure>
<pre><code>array([[ 0.99998885, -0.99976057, -0.0667929 , -0.9999803 ,  0.99982214],
       [-0.6524943 , -0.51520866, -0.37968948, -0.5922594 , -0.08968379],
       [ 0.99862397, -0.99715203, -0.03308626, -0.9991566 ,  0.9932902 ],
       [ 0.99681675, -0.9598194 ,  0.39660627, -0.8307606 ,  0.79671973]],
      dtype=float32)
</code></pre><h2 id="打包序列"><a href="#打包序列" class="headerlink" title="打包序列"></a>打包序列</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">n_steps = <span class="number">2</span></span><br><span class="line">n_inputs = <span class="number">3</span></span><br><span class="line">n_neurons = <span class="number">5</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">reset_graph()</span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_steps, n_inputs])</span><br><span class="line">X_seqs = tf.unstack(tf.transpose(X, perm=[<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line">basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons)</span><br><span class="line">out_put_seqs, states = tf.nn.static_rnn(basic_cell, X_seqs, dtype=tf.float32)</span><br><span class="line">outputs = tf.transpose(tf.stack(out_put_seqs), perm=[<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_batch = np.array([</span><br><span class="line">    [[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], [<span class="number">9</span>, <span class="number">8</span>, <span class="number">7</span>]],</span><br><span class="line">    [[<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]],</span><br><span class="line">    [[<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>]],</span><br><span class="line">    [[<span class="number">9</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]],</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init.run()</span><br><span class="line">    outputs_val = outputs.eval(feed_dict=&#123;X: X_batch&#125;)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(outputs_val)</span><br></pre></td></tr></table></figure>
<pre><code>[[[-0.45652324 -0.68064123  0.40938237  0.63104504 -0.45732826]
  [-0.9428799  -0.9998869   0.94055814  0.9999985  -0.9999997 ]]

 [[-0.8001535  -0.9921827   0.7817797   0.9971032  -0.9964609 ]
  [-0.637116    0.11300927  0.5798437   0.4310559  -0.6371699 ]]

 [[-0.93605185 -0.9998379   0.9308867   0.9999815  -0.99998295]
  [-0.9165386  -0.9945604   0.896054    0.99987197 -0.9999751 ]]

 [[ 0.9927369  -0.9981933  -0.55543643  0.9989031  -0.9953323 ]
  [-0.02746338 -0.73191994  0.7827872   0.9525682  -0.9781773 ]]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(np.transpose(outputs_val, axes=[<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<pre><code>[[-0.9428799  -0.9998869   0.94055814  0.9999985  -0.9999997 ]
 [-0.637116    0.11300927  0.5798437   0.4310559  -0.6371699 ]
 [-0.9165386  -0.9945604   0.896054    0.99987197 -0.9999751 ]
 [-0.02746338 -0.73191994  0.7827872   0.9525682  -0.9781773 ]]
</code></pre><h2 id="使用dynamic-rnn"><a href="#使用dynamic-rnn" class="headerlink" title="使用dynamic_rnn()"></a>使用dynamic_rnn()</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">n_steps = <span class="number">2</span></span><br><span class="line">n_inputs = <span class="number">3</span></span><br><span class="line">n_neurons = <span class="number">5</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">reset_graph()</span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_steps, n_inputs])</span><br><span class="line"></span><br><span class="line">basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons)</span><br><span class="line">outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X_batch = np.array([</span><br><span class="line">    [[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], [<span class="number">9</span>, <span class="number">8</span>, <span class="number">7</span>]],</span><br><span class="line">    [[<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]],</span><br><span class="line">    [[<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>]],</span><br><span class="line">    [[<span class="number">9</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]],</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init.run()</span><br><span class="line">    outputs_val = outputs.eval(feed_dict=&#123;X: X_batch&#125;)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(outputs_val)</span><br></pre></td></tr></table></figure>
<pre><code>[[[-0.85115266  0.87358344  0.5802911   0.8954789  -0.0557505 ]
  [-0.999996    0.99999577  0.9981815   1.          0.37679607]]

 [[-0.9983293   0.9992038   0.98071456  0.999985    0.25192663]
  [-0.7081804  -0.0772338  -0.85227895  0.5845349  -0.78780943]]

 [[-0.9999827   0.99999535  0.9992863   1.          0.5159072 ]
  [-0.9993956   0.9984095   0.83422637  0.99999976 -0.47325212]]

 [[ 0.87888587  0.07356028  0.97216916  0.9998546  -0.7351168 ]
  [-0.9134514   0.3600957   0.7624866   0.99817705  0.80142   ]]]
</code></pre><h2 id="设置序列长度"><a href="#设置序列长度" class="headerlink" title="设置序列长度"></a>设置序列长度</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">n_steps = <span class="number">2</span></span><br><span class="line">n_inputs = <span class="number">3</span></span><br><span class="line">n_neurons = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">reset_graph()</span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_steps, n_inputs])</span><br><span class="line">basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">seq_length = tf.placeholder(tf.int32, [<span class="keyword">None</span>])</span><br><span class="line">outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32, sequence_length=seq_length)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X_batch = np.array([</span><br><span class="line">    [[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], [<span class="number">9</span>, <span class="number">8</span>, <span class="number">7</span>]],</span><br><span class="line">    [[<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]],</span><br><span class="line">    [[<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>], [<span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>]],</span><br><span class="line">    [[<span class="number">9</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]],</span><br><span class="line">])</span><br><span class="line">seq_length_batch = np.array([<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init.run()</span><br><span class="line">    outputs_val, states_val = sess.run(</span><br><span class="line">        [outputs, states], feed_dict=&#123;X: X_batch, seq_length: seq_length_batch&#125;</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(outputs_val)</span><br></pre></td></tr></table></figure>
<pre><code>[[[-0.9123188   0.16516446  0.5548655  -0.39159346  0.20846416]
  [-1.          0.956726    0.99831694  0.99970174  0.96518576]]

 [[-0.9998612   0.6702289   0.9723653   0.6631046   0.74457586]
  [ 0.          0.          0.          0.          0.        ]]

 [[-0.99999976  0.8967997   0.9986295   0.9647514   0.93662   ]
  [-0.9999526   0.9681953   0.96002865  0.98706263  0.85459226]]

 [[-0.96435434  0.99501586 -0.36150697  0.9983378   0.999497  ]
  [-0.9613586   0.9568762   0.7132288   0.97729224 -0.0958299 ]]]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(states_val)</span><br></pre></td></tr></table></figure>
<pre><code>[[-1.          0.956726    0.99831694  0.99970174  0.96518576]
 [-0.9998612   0.6702289   0.9723653   0.6631046   0.74457586]
 [-0.9999526   0.9681953   0.96002865  0.98706263  0.85459226]
 [-0.9613586   0.9568762   0.7132288   0.97729224 -0.0958299 ]]
</code></pre><h2 id="训练序列分类器"><a href="#训练序列分类器" class="headerlink" title="训练序列分类器"></a>训练序列分类器</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">reset_graph()</span><br><span class="line"></span><br><span class="line">n_steps = <span class="number">28</span></span><br><span class="line">n_inputs = <span class="number">28</span></span><br><span class="line">n_neurons = <span class="number">150</span></span><br><span class="line">n_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_steps, n_inputs])</span><br><span class="line">y = tf.placeholder(tf.int32, [<span class="keyword">None</span>])</span><br><span class="line"></span><br><span class="line">basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons)</span><br><span class="line">outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">logits = tf.layers.dense(states, n_outputs)</span><br><span class="line">xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)</span><br><span class="line"></span><br><span class="line">loss = tf.reduce_mean(xentropy)</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)</span><br><span class="line">training_op = optimizer.minimize(loss)</span><br><span class="line">correct = tf.nn.in_top_k(logits, y, <span class="number">1</span>)</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()</span><br><span class="line">X_train = X_train.astype(np.float32).reshape(<span class="number">-1</span>, <span class="number">28</span>*<span class="number">28</span>) / <span class="number">255.0</span></span><br><span class="line">X_test = X_test.astype(np.float32).reshape(<span class="number">-1</span>, <span class="number">28</span>*<span class="number">28</span>) / <span class="number">255.0</span></span><br><span class="line"></span><br><span class="line">y_train = y_train.astype(np.int32)</span><br><span class="line">y_test = y_test.astype(np.int32)</span><br><span class="line"></span><br><span class="line">X_valid, X_train = X_train[:<span class="number">5000</span>], X_train[<span class="number">5000</span>:]</span><br><span class="line">y_valid, y_train = y_train[:<span class="number">5000</span>], y_train[<span class="number">5000</span>:]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shuffle_batch</span><span class="params">(X, y, batch_size)</span>:</span></span><br><span class="line">    rnd_idx = np.random.permutation(len(X))</span><br><span class="line">    n_batches = len(X) // batch_size</span><br><span class="line">    <span class="keyword">for</span> batch_idx <span class="keyword">in</span> np.array_split(rnd_idx, n_batches):</span><br><span class="line">        X_batch, y_batch = X[batch_idx], y[batch_idx]</span><br><span class="line">        <span class="keyword">yield</span> X_batch, y_batch</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_test = X_test.reshape((<span class="number">-1</span>, n_steps, n_inputs))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">n_epochs = <span class="number">30</span></span><br><span class="line">batch_size = <span class="number">150</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init.run()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">        <span class="keyword">for</span> X_batch, y_batch <span class="keyword">in</span> shuffle_batch(X_train, y_train, batch_size):</span><br><span class="line">            X_batch = X_batch.reshape(<span class="number">-1</span>, n_steps, n_inputs)</span><br><span class="line">            sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;)</span><br><span class="line">        acc_batch = accuracy.eval(feed_dict=&#123;X: X_batch, y:y_batch&#125;)</span><br><span class="line">        acc_test = accuracy.eval(feed_dict=&#123;X: X_test, y: y_test&#125;)</span><br><span class="line">        print(epoch, <span class="string">"Last batch accuracy:"</span>, acc_batch, <span class="string">"Test accuracy:"</span>, acc_test)</span><br></pre></td></tr></table></figure>
<pre><code>0 Last batch accuracy: 0.9533333 Test accuracy: 0.9288
1 Last batch accuracy: 0.96 Test accuracy: 0.9471
2 Last batch accuracy: 0.96 Test accuracy: 0.9499
3 Last batch accuracy: 0.96 Test accuracy: 0.9563
4 Last batch accuracy: 0.98 Test accuracy: 0.9677
5 Last batch accuracy: 0.93333334 Test accuracy: 0.9651
6 Last batch accuracy: 0.98 Test accuracy: 0.9685
7 Last batch accuracy: 0.96666664 Test accuracy: 0.9678
8 Last batch accuracy: 0.97333336 Test accuracy: 0.9693
9 Last batch accuracy: 0.99333334 Test accuracy: 0.9714
10 Last batch accuracy: 0.98 Test accuracy: 0.9752
11 Last batch accuracy: 0.9866667 Test accuracy: 0.9743
12 Last batch accuracy: 0.94666666 Test accuracy: 0.9716
13 Last batch accuracy: 0.97333336 Test accuracy: 0.9658
14 Last batch accuracy: 1.0 Test accuracy: 0.9772
15 Last batch accuracy: 0.98 Test accuracy: 0.974
16 Last batch accuracy: 0.99333334 Test accuracy: 0.9779
17 Last batch accuracy: 0.9866667 Test accuracy: 0.9775
18 Last batch accuracy: 0.9866667 Test accuracy: 0.9713
19 Last batch accuracy: 0.98 Test accuracy: 0.9724
20 Last batch accuracy: 0.9866667 Test accuracy: 0.9702
21 Last batch accuracy: 0.98 Test accuracy: 0.9758
22 Last batch accuracy: 0.98 Test accuracy: 0.9782
23 Last batch accuracy: 0.99333334 Test accuracy: 0.9778
24 Last batch accuracy: 0.9866667 Test accuracy: 0.9745
25 Last batch accuracy: 0.9866667 Test accuracy: 0.9741
26 Last batch accuracy: 0.9866667 Test accuracy: 0.9784
27 Last batch accuracy: 1.0 Test accuracy: 0.9808
28 Last batch accuracy: 0.99333334 Test accuracy: 0.9787
29 Last batch accuracy: 0.98 Test accuracy: 0.9813
</code></pre><h2 id="多层RNN"><a href="#多层RNN" class="headerlink" title="多层RNN"></a>多层RNN</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">reset_graph()</span><br><span class="line">n_steps = <span class="number">28</span></span><br><span class="line">n_inputs = <span class="number">28</span></span><br><span class="line">n_outputs = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_steps, n_inputs])</span><br><span class="line">y = tf.placeholder(tf.int32, [<span class="keyword">None</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">n_neurons = <span class="number">100</span></span><br><span class="line">n_layers = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">layers = [tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons,</span><br><span class="line">                                     activation=tf.nn.relu)</span><br><span class="line">         <span class="keyword">for</span> layer <span class="keyword">in</span> range(n_layers)]</span><br><span class="line">multi_layer_cell = tf.nn.rnn_cell.MultiRNNCell(layers)</span><br><span class="line">outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">states_concat = tf.concat(axis=<span class="number">1</span>, values=states)</span><br><span class="line">logits = tf.layers.dense(states_concat, n_outputs)</span><br><span class="line">xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)</span><br><span class="line">loss = tf.reduce_mean(xentropy)</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)</span><br><span class="line">training_op = optimizer.minimize(loss)</span><br><span class="line">correct = tf.nn.in_top_k(logits, y, <span class="number">1</span>)</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">n_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">150</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init.run()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">        <span class="keyword">for</span> X_batch, y_batch <span class="keyword">in</span> shuffle_batch(X_train, y_train, batch_size):</span><br><span class="line">            X_batch = X_batch.reshape((<span class="number">-1</span>, n_steps, n_inputs))</span><br><span class="line">            sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;)</span><br><span class="line">        acc_batch = accuracy.eval(feed_dict=&#123;X: X_batch, y: y_batch&#125;)</span><br><span class="line">        acc_test = accuracy.eval(feed_dict=&#123;X: X_test, y: y_test&#125;)</span><br><span class="line">        print(epoch, <span class="string">"Last batch accuracy:"</span>, acc_batch, <span class="string">"Test accuracy:"</span>, acc_test)</span><br><span class="line">file_writer = tf.summary.FileWriter(<span class="string">'rnn/multiRNN'</span>, tf.get_default_graph())</span><br></pre></td></tr></table></figure>
<pre><code>0 Last batch accuracy: 0.94 Test accuracy: 0.9318
1 Last batch accuracy: 0.96 Test accuracy: 0.9563
2 Last batch accuracy: 0.96 Test accuracy: 0.9709
3 Last batch accuracy: 0.9866667 Test accuracy: 0.9701
4 Last batch accuracy: 0.9866667 Test accuracy: 0.9773
5 Last batch accuracy: 0.9533333 Test accuracy: 0.9747
6 Last batch accuracy: 0.99333334 Test accuracy: 0.9763
7 Last batch accuracy: 0.9866667 Test accuracy: 0.9804
8 Last batch accuracy: 0.98 Test accuracy: 0.9755
9 Last batch accuracy: 0.96666664 Test accuracy: 0.9804
</code></pre><h2 id="时间序列"><a href="#时间序列" class="headerlink" title="时间序列"></a>时间序列</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">t_min, t_max = <span class="number">0</span>, <span class="number">30</span></span><br><span class="line">resolution = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">time_series</span><span class="params">(t)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> t * np.sin(t) / <span class="number">3</span> + <span class="number">2</span> * np.sin(t*<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">next_batch</span><span class="params">(batch_size, n_steps)</span>:</span></span><br><span class="line">    t0 = np.random.rand(batch_size, <span class="number">1</span>) * (t_max - t_min - n_steps * resolution)</span><br><span class="line">    Ts = t0 + np.arange(<span class="number">0</span>, n_steps + <span class="number">1</span>) * resolution</span><br><span class="line">    ys = time_series(Ts)</span><br><span class="line">    <span class="keyword">return</span> ys[:, :<span class="number">-1</span>].reshape(<span class="number">-1</span>, n_steps, <span class="number">1</span>), ys[:, <span class="number">1</span>:].reshape(<span class="number">-1</span>, n_steps, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">t = np.linspace(t_min, t_max, int((t_max - t_min) / resolution))</span><br><span class="line"></span><br><span class="line">n_steps = <span class="number">20</span></span><br><span class="line">t_instance = np.linspace(<span class="number">12.2</span>, <span class="number">12.2</span> + resolution * (n_steps + <span class="number">1</span>), n_steps + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">11</span>, <span class="number">4</span>))</span><br><span class="line">plt.subplot(<span class="number">121</span>)</span><br><span class="line">plt.title(<span class="string">"A time series (generated)"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.plot(t, time_series(t), label=<span class="string">r"$ t. \sin(t)/3 + 2 . \sin(5t)$"</span>)</span><br><span class="line">plt.plot(t_instance[:<span class="number">-1</span>], time_series(t_instance[:<span class="number">-1</span>]),</span><br><span class="line">                                      <span class="string">"b-"</span>, linewidth=<span class="number">3</span>, label=<span class="string">"A training instance"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"lower left"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">30</span>, <span class="number">-17</span>, <span class="number">13</span>])</span><br><span class="line">plt.xlabel(<span class="string">"Time"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Value"</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">122</span>)</span><br><span class="line">plt.title(<span class="string">"A training instance"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.plot(t_instance[:<span class="number">-1</span>], time_series(t_instance[:<span class="number">-1</span>]), <span class="string">"bo"</span>, markersize=<span class="number">10</span>, label=<span class="string">"instance"</span>)</span><br><span class="line">plt.plot(t_instance[<span class="number">1</span>:], time_series(t_instance[<span class="number">1</span>:]), <span class="string">"w*"</span>, markersize=<span class="number">10</span>, label=<span class="string">"target"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"upper left"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Time"</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2018/12/28/Recurrent-Neural-Networks/Recurrent%20Neural%20Networks_49_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_batch, y_batch = next_batch(<span class="number">1</span>, n_steps)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.c_[X_batch[<span class="number">0</span>], y_batch[<span class="number">0</span>]]</span><br></pre></td></tr></table></figure>
<pre><code>array([[ 1.38452097,  2.05081182],
       [ 2.05081182,  2.29742291],
       [ 2.29742291,  2.0465599 ],
       [ 2.0465599 ,  1.34009916],
       [ 1.34009916,  0.32948704],
       [ 0.32948704, -0.76115235],
       [-0.76115235, -1.68967022],
       [-1.68967022, -2.25492776],
       [-2.25492776, -2.34576159],
       [-2.34576159, -1.96789418],
       [-1.96789418, -1.24220428],
       [-1.24220428, -0.37478448],
       [-0.37478448,  0.39387907],
       [ 0.39387907,  0.84815766],
       [ 0.84815766,  0.85045064],
       [ 0.85045064,  0.3752526 ],
       [ 0.3752526 , -0.48422846],
       [-0.48422846, -1.53852738],
       [-1.53852738, -2.54795941],
       [-2.54795941, -3.28097239]])
</code></pre><h3 id="使用OutputProjectionWrapper"><a href="#使用OutputProjectionWrapper" class="headerlink" title="使用OutputProjectionWrapper"></a>使用OutputProjectionWrapper</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">reset_graph()</span><br><span class="line"></span><br><span class="line">n_steps = <span class="number">20</span></span><br><span class="line">n_inputs = <span class="number">1</span></span><br><span class="line">n_neurons = <span class="number">100</span></span><br><span class="line">n_outputs = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_steps, n_inputs])</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_steps, n_outputs])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cell = tf.contrib.rnn.OutputProjectionWrapper(</span><br><span class="line">    tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu),</span><br><span class="line">    output_size=n_outputs</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line">loss = tf.reduce_mean(tf.square(outputs -y))</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)</span><br><span class="line">training_op = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">n_iterations = <span class="number">1500</span></span><br><span class="line">batch_size = <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init.run()</span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> range(n_iterations):</span><br><span class="line">        X_batch, y_batch = next_batch(batch_size, n_steps)</span><br><span class="line">        sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;)</span><br><span class="line">        <span class="keyword">if</span> iteration % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            mse = loss.eval(feed_dict=&#123;X: X_batch, y: y_batch&#125;)</span><br><span class="line">            print(iteration, <span class="string">"\tMSE:"</span>, mse)</span><br><span class="line"></span><br><span class="line">    saver.save(sess, <span class="string">"rnn/my_time_series_model"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>0     MSE: 11.967254
100     MSE: 0.525841
200     MSE: 0.1495599
300     MSE: 0.07279411
400     MSE: 0.06158535
500     MSE: 0.05938873
600     MSE: 0.05470166
700     MSE: 0.047849063
800     MSE: 0.05107608
900     MSE: 0.047209196
1000     MSE: 0.047058314
1100     MSE: 0.047831465
1200     MSE: 0.04083041
1300     MSE: 0.047086805
1400     MSE: 0.041784383
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">"rnn/my_time_series_model"</span>)</span><br><span class="line"></span><br><span class="line">    X_new = time_series(np.array(t_instance[:<span class="number">-1</span>].reshape(<span class="number">-1</span>, n_steps, n_inputs)))</span><br><span class="line">    y_pred = sess.run(outputs, feed_dict=&#123;X: X_new&#125;)</span><br></pre></td></tr></table></figure>
<pre><code>INFO:tensorflow:Restoring parameters from rnn/my_time_series_model
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pred</span><br></pre></td></tr></table></figure>
<pre><code>array([[[-3.407753 ],
        [-2.4575484],
        [-1.1029298],
        [ 0.7815629],
        [ 2.2002175],
        [ 3.126768 ],
        [ 3.4037762],
        [ 3.3489153],
        [ 2.8798013],
        [ 2.2659323],
        [ 1.6447463],
        [ 1.5210768],
        [ 1.8972012],
        [ 2.7159088],
        [ 3.8894904],
        [ 5.140914 ],
        [ 6.142068 ],
        [ 6.666671 ],
        [ 6.6410103],
        [ 6.0725527]]], dtype=float32)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">"Testing the model"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.plot(t_instance[:<span class="number">-1</span>], time_series(t_instance[:<span class="number">-1</span>]),</span><br><span class="line">         <span class="string">"bo"</span>, markersize=<span class="number">10</span>, label=<span class="string">"instance"</span>)</span><br><span class="line">plt.plot(t_instance[<span class="number">1</span>:], time_series(t_instance[<span class="number">1</span>:]),</span><br><span class="line">        <span class="string">"w*"</span>, markersize=<span class="number">10</span>, label=<span class="string">"target"</span>)</span><br><span class="line">plt.plot(t_instance[<span class="number">1</span>:], y_pred[<span class="number">0</span>, :, <span class="number">0</span>], <span class="string">"r."</span>,</span><br><span class="line">        markersize=<span class="number">10</span>, label=<span class="string">"prediction"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"upper left"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Time"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2018/12/28/Recurrent-Neural-Networks/Recurrent%20Neural%20Networks_61_0.png" alt="png"></p>
<h3 id="不使用OutputProjectionWrapper"><a href="#不使用OutputProjectionWrapper" class="headerlink" title="不使用OutputProjectionWrapper()"></a>不使用OutputProjectionWrapper()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">reset_graph()</span><br><span class="line"></span><br><span class="line">n_steps = <span class="number">20</span></span><br><span class="line">n_inputs = <span class="number">1</span></span><br><span class="line">n_neurons = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_steps, n_inputs])</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_steps, n_outputs])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu)</span><br><span class="line">rnn_outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">n_outputs = <span class="number">1</span></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stacked_rnn_outputs = tf.reshape(rnn_outputs, [<span class="number">-1</span>, n_neurons])</span><br><span class="line">stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)</span><br><span class="line">outputs = tf.reshape(stacked_outputs, [<span class="number">-1</span>, n_steps, n_outputs])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">loss = tf.reduce_mean(tf.square(outputs - y))</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)</span><br><span class="line">training_op = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">saver = tf.train.Saver()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">n_iterations = <span class="number">1500</span></span><br><span class="line">batch_size = <span class="number">50</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init.run()</span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> range(n_iterations):</span><br><span class="line">        X_batch, y_batch = next_batch(batch_size, n_steps)</span><br><span class="line">        sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;)</span><br><span class="line">        <span class="keyword">if</span> iteration % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            mse = loss.eval(feed_dict=&#123;X: X_batch, y: y_batch&#125;)</span><br><span class="line">            print(iteration, <span class="string">"\tMSE:"</span>, mse)</span><br><span class="line"></span><br><span class="line">    X_new  = time_series(np.array(t_instance[:<span class="number">-1</span>].reshape(<span class="number">-1</span>, n_steps, n_inputs)))</span><br><span class="line">    y_pred = sess.run(outputs, feed_dict=&#123;X: X_new&#125;)</span><br><span class="line">    saver.save(sess, <span class="string">"rnn/my_time_series_model"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>0     MSE: 13.907029
100     MSE: 0.5056698
200     MSE: 0.19735886
300     MSE: 0.101214476
400     MSE: 0.06850145
500     MSE: 0.06291986
600     MSE: 0.055129297
700     MSE: 0.049436502
800     MSE: 0.050434686
900     MSE: 0.0482007
1000     MSE: 0.04809868
1100     MSE: 0.04982501
1200     MSE: 0.041912545
1300     MSE: 0.049292978
1400     MSE: 0.043140374
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pred</span><br></pre></td></tr></table></figure>
<pre><code>array([[[-3.4332483],
        [-2.4594698],
        [-1.1081185],
        [ 0.6882153],
        [ 2.1105688],
        [ 3.0585155],
        [ 3.5144088],
        [ 3.3531117],
        [ 2.808016 ],
        [ 2.1606152],
        [ 1.662645 ],
        [ 1.5578941],
        [ 1.9173537],
        [ 2.7210245],
        [ 3.8667865],
        [ 5.100083 ],
        [ 6.099999 ],
        [ 6.6480975],
        [ 6.6147423],
        [ 6.022089 ]]], dtype=float32)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">"Testing the model"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.plot(t_instance[:<span class="number">-1</span>], time_series(t_instance[:<span class="number">-1</span>]),</span><br><span class="line">         <span class="string">"bo"</span>, markersize=<span class="number">10</span>, label=<span class="string">"instance"</span>)</span><br><span class="line">plt.plot(t_instance[<span class="number">1</span>:], time_series(t_instance[<span class="number">1</span>:]),</span><br><span class="line">        <span class="string">"w*"</span>, markersize=<span class="number">10</span>, label=<span class="string">"target"</span>)</span><br><span class="line">plt.plot(t_instance[<span class="number">1</span>:], y_pred[<span class="number">0</span>, :, <span class="number">0</span>], <span class="string">"r."</span>,</span><br><span class="line">        markersize=<span class="number">10</span>, label=<span class="string">"prediction"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"upper left"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Time"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2018/12/28/Recurrent-Neural-Networks/Recurrent%20Neural%20Networks_70_0.png" alt="png"></p>
<h2 id="生成一个创造性的序列"><a href="#生成一个创造性的序列" class="headerlink" title="生成一个创造性的序列"></a>生成一个创造性的序列</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">"rnn/my_time_series_model"</span>)</span><br><span class="line"></span><br><span class="line">    sequence = [<span class="number">0.</span>] * n_steps</span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> range(<span class="number">300</span>):</span><br><span class="line">        X_batch = np.array(sequence[-n_steps:]).reshape(<span class="number">1</span>, n_steps, <span class="number">1</span>)</span><br><span class="line">        y_pred = sess.run(outputs, feed_dict=&#123;X: X_batch&#125;)</span><br><span class="line">        sequence.append(y_pred[<span class="number">0</span>, <span class="number">-1</span>, <span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>INFO:tensorflow:Restoring parameters from rnn/my_time_series_model
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">4</span>))</span><br><span class="line">plt.plot(np.arange(len(sequence)), sequence, <span class="string">"b-"</span>)</span><br><span class="line">plt.plot(t[:n_steps], sequence[:n_steps], <span class="string">"b--"</span>, linewidth=<span class="number">3</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Time"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Value"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2018/12/28/Recurrent-Neural-Networks/Recurrent%20Neural%20Networks_73_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">"rnn/my_time_series_model"</span>)</span><br><span class="line"></span><br><span class="line">    sequence1 = [<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n_steps)]</span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> range(len(t) - n_steps):</span><br><span class="line">        X_batch = np.array(sequence1[-n_steps:]).reshape(<span class="number">1</span>, n_steps, <span class="number">1</span>)</span><br><span class="line">        y_pred = sess.run(outputs, feed_dict=&#123;X: X_batch&#125;)</span><br><span class="line">        sequence1.append(y_pred[<span class="number">0</span>, <span class="number">-1</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    sequence2 = [time_series(i * resolution + t_min + (t_max - t_min/<span class="number">3</span>))</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(n_steps)]</span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> range(len(t) - n_steps):</span><br><span class="line">        X_batch = np.array(sequence2[-n_steps:]).reshape(<span class="number">1</span>, n_steps, <span class="number">1</span>)</span><br><span class="line">        y_pred = sess.run(outputs, feed_dict=&#123;X: X_batch&#125;)</span><br><span class="line">        sequence2.append(y_pred[<span class="number">0</span>, <span class="number">-1</span>, <span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<pre><code>INFO:tensorflow:Restoring parameters from rnn/my_time_series_model
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">11</span>,<span class="number">4</span>))</span><br><span class="line">plt.subplot(<span class="number">121</span>)</span><br><span class="line">plt.plot(t, sequence1, <span class="string">"b-"</span>)</span><br><span class="line">plt.plot(t[:n_steps], sequence1[:n_steps], <span class="string">"b-"</span>, linewidth=<span class="number">3</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Time"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Value"</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">122</span>)</span><br><span class="line">plt.plot(t, sequence2, <span class="string">"b-"</span>)</span><br><span class="line">plt.plot(t[:n_steps], sequence2[:n_steps], <span class="string">"b-"</span>, linewidth=<span class="number">3</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Time"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2018/12/28/Recurrent-Neural-Networks/Recurrent%20Neural%20Networks_75_0.png" alt="png"></p>
<h1 id="Deep-RNN"><a href="#Deep-RNN" class="headerlink" title="Deep RNN"></a>Deep RNN</h1><h2 id="MultiRNNCell"><a href="#MultiRNNCell" class="headerlink" title="MultiRNNCell"></a>MultiRNNCell</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">reset_graph()</span><br><span class="line"></span><br><span class="line">n_inputs = <span class="number">2</span></span><br><span class="line">n_steps = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_steps, n_inputs])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">n_neurons = <span class="number">100</span></span><br><span class="line">n_layers = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">layers = [tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons)</span><br><span class="line">         <span class="keyword">for</span> layer <span class="keyword">in</span> range(n_layers)]</span><br><span class="line"></span><br><span class="line">multi_layer_cell = tf.nn.rnn_cell.MultiRNNCell(layers)</span><br><span class="line">outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_batch = np.random.rand(<span class="number">2</span>, n_steps, n_inputs)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init.run()</span><br><span class="line">    outputs_val, states_val = sess.run([outputs, states], feed_dict=&#123;X: X_batch&#125;)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">outputs_val.shape</span><br></pre></td></tr></table></figure>
<pre><code>(2, 5, 100)
</code></pre><h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">reset_graph()</span><br><span class="line"></span><br><span class="line">n_inputs = <span class="number">1</span></span><br><span class="line">n_neurons = <span class="number">100</span></span><br><span class="line">n_layers = <span class="number">3</span></span><br><span class="line">n_steps = <span class="number">20</span></span><br><span class="line">n_outputs = <span class="number">1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_steps, n_inputs])</span><br><span class="line">y = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_steps, n_outputs])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">keep_prob = tf.placeholder_with_default(<span class="number">1.0</span>, shape=())</span><br><span class="line">cells = [tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons)</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> range(n_layers)]</span><br><span class="line">cells_drop = [tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=keep_prob)</span><br><span class="line">             <span class="keyword">for</span> cell <span class="keyword">in</span> cells]</span><br><span class="line">multi_layer_cell = tf.nn.rnn_cell.MultiRNNCell(cells_drop)</span><br><span class="line">rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">stacked_rnn_outputs = tf.reshape(rnn_outputs, [<span class="number">-1</span>, n_neurons])</span><br><span class="line">stacked_outputs = tf.layers.dense(stacked_rnn_outputs, n_outputs)</span><br><span class="line">outputs = tf.reshape(stacked_outputs, [<span class="number">-1</span>, n_steps, n_outputs])</span><br><span class="line"></span><br><span class="line">loss = tf.reduce_mean(tf.square(outputs - y))</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)</span><br><span class="line">training_op = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">saver = tf.train.Saver()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">n_iterations = <span class="number">1500</span></span><br><span class="line">batch_size = <span class="number">50</span></span><br><span class="line">train_keep_prob = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init.run()</span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> range(n_iterations):</span><br><span class="line">        X_batch, y_batch = next_batch(batch_size, n_steps)</span><br><span class="line">        _, mse = sess.run([training_op, loss],</span><br><span class="line">                          feed_dict=&#123;</span><br><span class="line">                              X: X_batch,</span><br><span class="line">                              y: y_batch,</span><br><span class="line">                              keep_prob: train_keep_prob</span><br><span class="line">                          &#125;)</span><br><span class="line">        <span class="keyword">if</span> iteration % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(iteration, <span class="string">"Training MSE:"</span>, mse)</span><br><span class="line">    saver.save(sess,<span class="string">"rnn/my_dropout_time_series_model"</span>)</span><br></pre></td></tr></table></figure>
<pre><code>0 Training MSE: 16.10992
100 Training MSE: 4.2036242
200 Training MSE: 3.7243023
300 Training MSE: 3.8051453
400 Training MSE: 3.1154072
500 Training MSE: 3.4736195
600 Training MSE: 3.4444861
700 Training MSE: 3.3598778
800 Training MSE: 4.1624136
900 Training MSE: 4.263299
1000 Training MSE: 3.5078833
1100 Training MSE: 4.2051315
1200 Training MSE: 2.7443748
1300 Training MSE: 4.583499
1400 Training MSE: 5.121917
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver.restore(sess, <span class="string">"rnn/my_dropout_time_series_model"</span>)</span><br><span class="line"></span><br><span class="line">    X_new  = time_series(np.array(t_instance[:<span class="number">-1</span>].reshape(<span class="number">-1</span>, n_steps, n_inputs)))</span><br><span class="line">    y_pred = sess.run(outputs, feed_dict=&#123;X: X_new&#125;)</span><br></pre></td></tr></table></figure>
<pre><code>INFO:tensorflow:Restoring parameters from rnn/my_dropout_time_series_model
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">plt.title(<span class="string">"Testing the model"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.plot(t_instance[:<span class="number">-1</span>], time_series(t_instance[:<span class="number">-1</span>]),</span><br><span class="line">         <span class="string">"bo"</span>, markersize=<span class="number">10</span>, label=<span class="string">"instance"</span>)</span><br><span class="line">plt.plot(t_instance[<span class="number">1</span>:], time_series(t_instance[<span class="number">1</span>:]),</span><br><span class="line">        <span class="string">"w*"</span>, markersize=<span class="number">10</span>, label=<span class="string">"target"</span>)</span><br><span class="line">plt.plot(t_instance[<span class="number">1</span>:], y_pred[<span class="number">0</span>, :, <span class="number">0</span>], <span class="string">"r."</span>,</span><br><span class="line">        markersize=<span class="number">10</span>, label=<span class="string">"prediction"</span>)</span><br><span class="line">plt.legend(loc=<span class="string">"upper left"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Time"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2018/12/28/Recurrent-Neural-Networks/Recurrent%20Neural%20Networks_91_0.png" alt="png"></p>
<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">reset_graph()</span><br><span class="line"></span><br><span class="line">lstm_cell = tf.nn.rnn_cell.LSTMCell(name=<span class="string">"basic_lstm_cell"</span>,</span><br><span class="line">                                   num_units=n_neurons)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">n_steps = <span class="number">28</span></span><br><span class="line">n_inputs = <span class="number">28</span></span><br><span class="line">n_neurons = <span class="number">150</span></span><br><span class="line">n_outputs = <span class="number">10</span></span><br><span class="line">n_layers = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_steps, n_inputs])</span><br><span class="line">y = tf.placeholder(tf.int32, [<span class="keyword">None</span>])</span><br><span class="line"></span><br><span class="line">lstm_cells = [tf.nn.rnn_cell.LSTMCell(num_units=n_neurons, name=<span class="string">"basic_lstm_cell"</span>)</span><br><span class="line">             <span class="keyword">for</span> layer <span class="keyword">in</span> range(n_layers)]</span><br><span class="line">multi_cell = tf.nn.rnn_cell.MultiRNNCell(lstm_cells)</span><br><span class="line">outputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32)</span><br><span class="line">top_layer_h_state = states[<span class="number">-1</span>][<span class="number">1</span>]</span><br><span class="line">logits = tf.layers.dense(top_layer_h_state, n_outputs, name=<span class="string">"softmax"</span>)</span><br><span class="line">xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)</span><br><span class="line">loss = tf.reduce_mean(xentropy, name=<span class="string">"loss"</span>)</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)</span><br><span class="line">training_op = optimizer.minimize(loss)</span><br><span class="line">correct = tf.nn.in_top_k(logits, y, <span class="number">1</span>)</span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">states</span><br></pre></td></tr></table></figure>
<pre><code>(LSTMStateTuple(c=&lt;tf.Tensor &#39;rnn/while/Exit_3:0&#39; shape=(?, 150) dtype=float32&gt;, h=&lt;tf.Tensor &#39;rnn/while/Exit_4:0&#39; shape=(?, 150) dtype=float32&gt;),
 LSTMStateTuple(c=&lt;tf.Tensor &#39;rnn/while/Exit_5:0&#39; shape=(?, 150) dtype=float32&gt;, h=&lt;tf.Tensor &#39;rnn/while/Exit_6:0&#39; shape=(?, 150) dtype=float32&gt;),
 LSTMStateTuple(c=&lt;tf.Tensor &#39;rnn/while/Exit_7:0&#39; shape=(?, 150) dtype=float32&gt;, h=&lt;tf.Tensor &#39;rnn/while/Exit_8:0&#39; shape=(?, 150) dtype=float32&gt;))
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">top_layer_h_state</span><br></pre></td></tr></table></figure>
<pre><code>&lt;tf.Tensor &#39;rnn/while/Exit_8:0&#39; shape=(?, 150) dtype=float32&gt;
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">n_epochs = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">150</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init.run()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">        <span class="keyword">for</span> X_batch, y_batch <span class="keyword">in</span> shuffle_batch(X_train, y_train, batch_size):</span><br><span class="line">            X_batch = X_batch.reshape((<span class="number">-1</span>, n_steps, n_inputs))</span><br><span class="line">            sess.run(training_op, feed_dict=&#123;X: X_batch, y: y_batch&#125;)</span><br><span class="line">        acc_batch = accuracy.eval(feed_dict=&#123;X: X_batch, y: y_batch&#125;)</span><br><span class="line">        acc_test = accuracy.eval(feed_dict=&#123;X: X_test, y: y_test&#125;)</span><br><span class="line">        print(epoch, <span class="string">"Last batch accuracy:"</span>, acc_batch, <span class="string">"Test accuracy:"</span>, acc_test)</span><br></pre></td></tr></table></figure>
<pre><code>0 Last batch accuracy: 0.9533333 Test accuracy: 0.9481
1 Last batch accuracy: 0.96 Test accuracy: 0.9699
2 Last batch accuracy: 0.96 Test accuracy: 0.9639
3 Last batch accuracy: 1.0 Test accuracy: 0.9808
4 Last batch accuracy: 0.9866667 Test accuracy: 0.9826
5 Last batch accuracy: 1.0 Test accuracy: 0.986
6 Last batch accuracy: 1.0 Test accuracy: 0.9872
7 Last batch accuracy: 0.99333334 Test accuracy: 0.9882
8 Last batch accuracy: 1.0 Test accuracy: 0.9837
9 Last batch accuracy: 0.99333334 Test accuracy: 0.9883
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=n_neurons, use_peepholes=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gru_cell = tf.nn.rnn_cell.GRUCell(num_units=n_neurons)</span><br></pre></td></tr></table></figure>
<h1 id="嵌入向量"><a href="#嵌入向量" class="headerlink" title="嵌入向量"></a>嵌入向量</h1><h2 id="获取数据"><a href="#获取数据" class="headerlink" title="获取数据"></a>获取数据</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> urllib</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> errno</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"></span><br><span class="line">WORDS_PATH = <span class="string">"datasets/words"</span></span><br><span class="line">WORDS_URL = <span class="string">"http://mattmahoney.net/dc/text8.zip"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mkdir_p</span><span class="params">(path)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        os.makedirs(path=path)</span><br><span class="line">    <span class="keyword">except</span> OSError <span class="keyword">as</span> exc:</span><br><span class="line">        <span class="keyword">if</span> esc.errno == errno.EEXIST <span class="keyword">and</span> os.path.isdir(path):</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fetch_words_data</span><span class="params">(words_rul=WORDS_URL, words_path=WORDS_PATH)</span>:</span></span><br><span class="line">    os.makedirs(words_path, exist_ok=<span class="keyword">True</span>)</span><br><span class="line">    zip_path = os.path.join(words_path, <span class="string">"words.zip"</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(zip_path):</span><br><span class="line">        urllib.request.urlretrieve(words_rul, zip_path)</span><br><span class="line">    <span class="keyword">with</span> zipfile.ZipFile(zip_path) <span class="keyword">as</span> f:</span><br><span class="line">        data = f.read(f.namelist()[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> data.decode(<span class="string">"ascii"</span>).split()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">words = fetch_words_data()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">words[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;anarchism&#39;, &#39;originated&#39;, &#39;as&#39;, &#39;a&#39;, &#39;term&#39;]
</code></pre><h2 id="建立字典"><a href="#建立字典" class="headerlink" title="建立字典"></a>建立字典</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line">vocabulary_size = <span class="number">50000</span></span><br><span class="line"></span><br><span class="line">vocabulary = [(<span class="string">"UNK"</span>, <span class="keyword">None</span>)] + Counter(words).most_common(vocabulary_size - <span class="number">1</span>)</span><br><span class="line">vocabulary = np.array([word <span class="keyword">for</span> word, _ <span class="keyword">in</span> vocabulary])</span><br><span class="line">dictionary = &#123;word: code <span class="keyword">for</span> code, word <span class="keyword">in</span> enumerate(vocabulary)&#125;</span><br><span class="line">data = np.array([dictionary.get(word, <span class="number">0</span>) <span class="keyword">for</span> word <span class="keyword">in</span> words])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">" "</span>.join(words[:<span class="number">9</span>]), data[:<span class="number">9</span>]</span><br></pre></td></tr></table></figure>
<pre><code>(&#39;anarchism originated as a term of abuse first used&#39;,
 array([5234, 3081,   12,    6,  195,    2, 3134,   46,   59]))
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">" "</span>.join(vocabulary[word_index]</span><br><span class="line">         <span class="keyword">for</span> word_index <span class="keyword">in</span> [<span class="number">5234</span>, <span class="number">3081</span>,   <span class="number">12</span>,    <span class="number">6</span>,  <span class="number">195</span>,    <span class="number">2</span>, <span class="number">3134</span>,   <span class="number">46</span>,   <span class="number">59</span>])</span><br></pre></td></tr></table></figure>
<pre><code>&#39;anarchism originated as a term of abuse first used&#39;
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">words[<span class="number">24</span>], data[<span class="number">24</span>]</span><br></pre></td></tr></table></figure>
<pre><code>(&#39;culottes&#39;, 0)
</code></pre><h2 id="Generate-batches"><a href="#Generate-batches" class="headerlink" title="Generate batches"></a>Generate batches</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_batch</span><span class="params">(batch_size, num_skips, skip_window)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> data_index</span><br><span class="line">    <span class="keyword">assert</span> batch_size % num_skips == <span class="number">0</span></span><br><span class="line">    <span class="keyword">assert</span> num_skips &lt;= <span class="number">2</span> * skip_window</span><br><span class="line">    batch = np.ndarray(shape=[batch_size], dtype=np.int32)</span><br><span class="line">    labels = np.ndarray(shape=[batch_size, <span class="number">1</span>], dtype=np.int32)</span><br><span class="line">    span = <span class="number">2</span> * skip_window + <span class="number">1</span></span><br><span class="line">    buffer = deque(maxlen=span)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(span):</span><br><span class="line">        buffer.append(data[data_index])</span><br><span class="line">        data_index = (data_index + <span class="number">1</span>) % len(data)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(batch_size // num_skips):</span><br><span class="line">        target = skip_window</span><br><span class="line">        targets_to_avoid = [skip_window]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(num_skips):</span><br><span class="line">            <span class="keyword">while</span> target <span class="keyword">in</span> targets_to_avoid:</span><br><span class="line">                target = np.random.randint(<span class="number">0</span>, span)</span><br><span class="line">            targets_to_avoid.append(target)</span><br><span class="line">            batch[i * num_skips + j] = buffer[skip_window]</span><br><span class="line">            labels[i * num_skips + j] = buffer[target]</span><br><span class="line">        buffer.append(data[data_index])</span><br><span class="line">        data_index = (data_index + <span class="number">1</span>) % len(data)</span><br><span class="line">    <span class="keyword">return</span> batch, labels</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">42</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_index = <span class="number">0</span></span><br><span class="line">batch, labels = generate_batch(<span class="number">8</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">batch, [vocabulary[word] <span class="keyword">for</span> word <span class="keyword">in</span> batch]</span><br></pre></td></tr></table></figure>
<pre><code>(array([3081, 3081,   12,   12,    6,    6,  195,  195]),
 [&#39;originated&#39;, &#39;originated&#39;, &#39;as&#39;, &#39;as&#39;, &#39;a&#39;, &#39;a&#39;, &#39;term&#39;, &#39;term&#39;])
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">labels, [vocabulary[word] <span class="keyword">for</span> word <span class="keyword">in</span> labels[:, <span class="number">0</span>]]</span><br></pre></td></tr></table></figure>
<pre><code>(array([[  12],
        [5234],
        [   6],
        [3081],
        [  12],
        [ 195],
        [   2],
        [   6]]),
 [&#39;as&#39;, &#39;anarchism&#39;, &#39;a&#39;, &#39;originated&#39;, &#39;as&#39;, &#39;term&#39;, &#39;of&#39;, &#39;a&#39;])
</code></pre><h2 id="创建模型"><a href="#创建模型" class="headerlink" title="创建模型"></a>创建模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">embedding_size = <span class="number">128</span></span><br><span class="line">skip_window = <span class="number">1</span></span><br><span class="line">num_skips = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">valid_size = <span class="number">16</span></span><br><span class="line">valid_window = <span class="number">100</span></span><br><span class="line">valid_examples = np.random.choice(valid_window, valid_size, replace=<span class="keyword">False</span>)</span><br><span class="line">num_sampled = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">reset_graph()</span><br><span class="line"></span><br><span class="line">train_labels = tf.placeholder(tf.int32, shape=[batch_size, <span class="number">1</span>])</span><br><span class="line">valid_dataset = tf.constant(valid_examples, dtype=tf.int32)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vocabulary_size = <span class="number">50000</span></span><br><span class="line">embedding_size = <span class="number">150</span></span><br><span class="line"></span><br><span class="line">init_embeds = tf.random_uniform([vocabulary_size, embedding_size], <span class="number">-1.0</span>, <span class="number">1.0</span>)</span><br><span class="line">embeddings = tf.Variable(init_embeds)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_inputs = tf.placeholder(tf.int32, shape=[<span class="keyword">None</span>])</span><br><span class="line">embed = tf.nn.embedding_lookup(embeddings, train_inputs)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">nce_weights = tf.Variable(</span><br><span class="line">    tf.truncated_normal([vocabulary_size, embedding_size],</span><br><span class="line">                       stddev= <span class="number">1.0</span> / np.sqrt(embedding_size))</span><br><span class="line">)</span><br><span class="line">nce_biases = tf.Variable(tf.zeros([vocabulary_size]))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">loss = tf.reduce_mean(</span><br><span class="line">    tf.nn.nce_loss(nce_weights, nce_biases, train_labels, embed,</span><br><span class="line">                  num_sampled, vocabulary_size)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)</span><br><span class="line">training_op = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line">norm = tf.sqrt(tf.reduce_mean(tf.square(embeddings), axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>))</span><br><span class="line">normalized_embedding = embeddings / norm</span><br><span class="line">valid_embeddings = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)</span><br><span class="line">similarity = tf.matmul(valid_embeddings, normalized_embedding, transpose_b=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">num_steps = <span class="number">10001</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init.run()</span><br><span class="line"></span><br><span class="line">    average_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> range(num_steps):</span><br><span class="line">        print(<span class="string">"\rIteration:&#123;&#125;"</span>.format(step), end=<span class="string">""</span>)</span><br><span class="line">        batch_inputs, batch_labels = generate_batch(batch_size,num_skips, skip_window)</span><br><span class="line">        feed_dict = &#123;train_inputs: batch_inputs, train_labels: batch_labels&#125;</span><br><span class="line"></span><br><span class="line">        _, loss_val = sess.run([training_op, loss], feed_dict=feed_dict)</span><br><span class="line">        average_loss += loss_val</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">if</span> step &gt; <span class="number">0</span>:</span><br><span class="line">                average_loss /= <span class="number">2000</span></span><br><span class="line">            print(<span class="string">"Average loss at step"</span>, step, <span class="string">":"</span>, average_loss)</span><br><span class="line">            average_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            sim = similarity.eval()</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(valid_size):</span><br><span class="line">                valid_word = vocabulary[valid_examples[i]]</span><br><span class="line">                top_k = <span class="number">8</span></span><br><span class="line">                nearest = (-sim[i, :]).argsort()[<span class="number">1</span>: top_k+<span class="number">1</span>]</span><br><span class="line">                log_str = <span class="string">"Nearest to %s"</span> % valid_word</span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> range(top_k):</span><br><span class="line">                    close_word = vocabulary[nearest[k]]</span><br><span class="line">                    log_str = <span class="string">"%s %s,"</span> % (log_str, close_word)</span><br><span class="line">                print(log_str)</span><br><span class="line">        final_embeddings = normalized_embedding.eval()</span><br></pre></td></tr></table></figure>
<pre><code>Iteration:0Average loss at step 0 : 290.5275573730469
Nearest to over tt, tuned, manichaeans, fractional, cambridge, balaguer, fluoride, strenuously,
Nearest to one imagines, tijuana, hindrance, steadfastly, motorcyclist, lords, letting, adolfo,
Nearest to were bezier, antibodies, nicknamed, panthers, compiler, tao, smarter, busy,
Nearest to may failure, rna, efficacious, aspirin, lecompton, definitive, geese, amphibious,
Nearest to two annihilate, bettors, wir, cindy, epinephrine, team, voluntarily, crystallize,
Nearest to its knob, abeokuta, bracelet, bastards, ivens, objectivity, blanton, cold,
Nearest to than lame, watts, stones, sram, elves, zarqawi, applets, cloves,
Nearest to these pedro, condoned, neck, ssn, supervising, doug, thereto, melton,
Nearest to they lowly, deportation, shrewd, reznor, tojo, decadent, occured, risotto,
Nearest to is interests, golfers, dropouts, richards, egyptians, legionnaires, leonel, opener,
Nearest to up clair, drives, steadfast, missed, nashville, kilowatts, anal, vinland,
Nearest to he transitioned, winchell, resh, goldsmiths, standardised, markings, pursued, satirized,
Nearest to people blissymbolics, mike, buffers, untouchables, carolingian, posted, ville, hypertalk,
Nearest to more cactus, sta, reformation, poets, diligently, rsc, ravaged, nabokov,
Nearest to was russo, rammed, investiture, glucagon, heck, adventurer, sharada, homing,
Nearest to UNK reykjav, fi, rosalyn, mainline, archaeologist, armstrong, stevenage, ean,
Iteration:2000Average loss at step 2000 : 133.45819056224823
Iteration:4000Average loss at step 4000 : 62.97674214935303
Iteration:6000Average loss at step 6000 : 40.385357957839965
Iteration:8000Average loss at step 8000 : 31.5875605533123
Iteration:10000Average loss at step 10000 : 25.615500225067137
Nearest to over tikal, seal, scriptores, felony, bougainville, chapter, dubrovnik, valdemar,
Nearest to one eight, nine, six, two, seven, four, three, five,
Nearest to were was, logan, antlia, anaximenes, songs, by, aga, hood,
Nearest to may zero, to, theism, eight, can, packing, would, creativity,
Nearest to two zero, one, five, four, six, three, eight, nine,
Nearest to its the, mechanisms, antipope, alcmene, alemanni, alexandra, alder, topped,
Nearest to than lit, but, quantity, barbados, asmara, proxima, constructing, floors,
Nearest to these nur, antipopes, floors, nightclubs, mainly, and, other, aurelianus,
Nearest to they that, cain, angilbert, nine, autoerotic, alexandrovich, three, some,
Nearest to is yahya, are, tt, but, was, stamp, ttt, politican,
Nearest to up refrigerant, incompleteness, rensselaer, four, persistence, astor, lumped, assigned,
Nearest to he campylobacter, his, but, eight, later, antigens, UNK, in,
Nearest to people autoerotic, stained, trigonometry, satrap, rijndael, equality, fiesta, songs,
Nearest to more less, ahmad, ski, conjectures, zyklon, physically, rarely, ah,
Nearest to was became, calcite, is, were, had, asphyxiation, suffixes, nmt,
Nearest to UNK and, one, the, dmt, a, bromide, ananda, tile,
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.save(<span class="string">"rnn/my_final_embeddings.npy"</span>, final_embeddings)</span><br></pre></td></tr></table></figure>
<h2 id="plot-the-embeddings"><a href="#plot-the-embeddings" class="headerlink" title="plot the embeddings"></a>plot the embeddings</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_with_labels</span><span class="params">(low_dim_embs, labels)</span>:</span></span><br><span class="line">    <span class="keyword">assert</span> low_dim_embs.shape[<span class="number">0</span>] &gt;= len(labels) , <span class="string">"More labels than embeddings"</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">18</span>, <span class="number">18</span>))</span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> enumerate(labels):</span><br><span class="line">        x, y = low_dim_embs[i, :]</span><br><span class="line">        plt.scatter(x, y)</span><br><span class="line">        plt.annotate(label,</span><br><span class="line">                    xy=(x, y),</span><br><span class="line">                    xytext=(<span class="number">5</span>, <span class="number">2</span>),</span><br><span class="line">                    textcoords=<span class="string">'offset points'</span>,</span><br><span class="line">                    ha=<span class="string">'right'</span>,</span><br><span class="line">                    va=<span class="string">'bottom'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line"></span><br><span class="line">tsne = TSNE(perplexity=<span class="number">30</span>, n_components=<span class="number">2</span>, init=<span class="string">"pca"</span>, n_iter=<span class="number">5000</span>)</span><br><span class="line">plot_only = <span class="number">500</span></span><br><span class="line">low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])</span><br><span class="line">labels = [vocabulary[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(plot_only)]</span><br><span class="line">plot_with_labels(low_dim_embs, labels)</span><br></pre></td></tr></table></figure>
<p><img src="/2018/12/28/Recurrent-Neural-Networks/Recurrent%20Neural%20Networks_128_0.png" alt="png"></p>
<h2 id="Machine-Translation"><a href="#Machine-Translation" class="headerlink" title="Machine Translation"></a>Machine Translation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">reset_graph()</span><br><span class="line"></span><br><span class="line">n_steps = <span class="number">50</span></span><br><span class="line">n_neurons = <span class="number">200</span></span><br><span class="line">n_layers = <span class="number">3</span></span><br><span class="line">num_encoder_symbols = <span class="number">20000</span></span><br><span class="line">num_decoder_symbols = <span class="number">20000</span></span><br><span class="line">embedding_size = <span class="number">150</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.int32, [<span class="keyword">None</span>, n_steps])</span><br><span class="line">Y = tf.placeholder(tf.int32, [<span class="keyword">None</span>, n_steps])</span><br><span class="line">W = tf.placeholder(tf.float32, [<span class="keyword">None</span>, n_steps - <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">Y_input = Y[:, :<span class="number">-1</span>]</span><br><span class="line">Y_target = Y[:, <span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">encoder_inputs = tf.unstack(tf.transpose(X))</span><br><span class="line">decoder_inputs = tf.unstack(tf.transpose(Y_input))</span><br><span class="line"></span><br><span class="line">lstm_cells = [tf.nn.rnn_cell.LSTMCell(num_units=n_neurons, name=<span class="string">"basic_lstm_cell"</span>)</span><br><span class="line">            <span class="keyword">for</span> layer <span class="keyword">in</span> range(n_layers)]</span><br><span class="line">cell = tf.nn.rnn_cell.MultiRNNCell(lstm_cells)</span><br><span class="line"></span><br><span class="line">outputs_seqs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(</span><br><span class="line">    encoder_inputs,</span><br><span class="line">    decoder_inputs,</span><br><span class="line">    cell,</span><br><span class="line">    num_encoder_symbols,</span><br><span class="line">    num_decoder_symbols,</span><br><span class="line">    embedding_size</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">logits = tf.transpose(tf.unstack(outputs_seqs), perm=[<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">logits_flat = tf.reshape(logits, [<span class="number">-1</span>, num_decoder_symbols])</span><br><span class="line">Y_target_flat = tf.reshape(Y_target, [<span class="number">-1</span>])</span><br><span class="line">W_flat = tf.reshape(W, [<span class="number">-1</span>])</span><br><span class="line">xentropy = W_flat * tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y_target_flat,</span><br><span class="line">                                                                   logits=logits_flat)</span><br><span class="line">loss = tf.reduce_mean(xentropy)</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)</span><br><span class="line">traninig_op = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure>
<hr>
<p><a href="https://github.com/coldJune/machineLearning/blob/master/handson-ml/Recurrent%20Neural%20Networks.ipynb" target="_blank" rel="noopener">源文件</a></p>

      
    </div>
    
    
    
		<div>
		  
		    <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-apple"></i>感谢您的阅读-------------</div>
    
</div>
		  
		</div>
		<div>
      
        
<div class="my_post_copyright">
  <script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

  <!-- JS库 sweetalert 可修改路径 -->
  <script src="https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js"></script>
  <script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>
  <p><span>本文标题:</span><a href="/2018/12/28/Recurrent-Neural-Networks/">Recurrent Neural Networks</a></p>
  <p><span>文章作者:</span><a href="/" title="访问 邓小俊 的个人博客">邓小俊</a></p>
  <p><span>发布时间:</span>2018年12月28日 - 15:12</p>
  <p><span>最后更新:</span>2019年05月17日 - 22:05</p>
  <p><span>原始链接:</span><a href="/2018/12/28/Recurrent-Neural-Networks/" title="Recurrent Neural Networks">http://coldjune.com/2018/12/28/Recurrent-Neural-Networks/</a>
    <span class="copy-path"  title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="http://coldjune.com/2018/12/28/Recurrent-Neural-Networks/"  aria-label="复制成功！"></i></span>
  </p>
  <p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p>  
</div>
<script> 
    var clipboard = new Clipboard('.fa-clipboard');
      $(".fa-clipboard").click(function(){
      clipboard.on('success', function(){
        swal({   
          title: "",   
          text: '复制成功',
          icon: "success", 
          showConfirmButton: true
          });
        });
    });  
</script>

      
		</div>
    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/tensorflow/" rel="tag"><i class="fa fa-tag"></i> tensorflow</a>
          
            <a href="/tags/Hands-On-Machine-Learning-with-Scikit-Learn-and-TensorFlow/" rel="tag"><i class="fa fa-tag"></i> Hands-On Machine Learning with Scikit-Learn and TensorFlow</a>
          
            <a href="/tags/神经网络/" rel="tag"><i class="fa fa-tag"></i> 神经网络</a>
          
        </div>
      

      
      
        <div class="post-widgets">
        

        

        
          
          <div id="needsharebutton-postbottom">
            <span class="btn">
              <i class="fa fa-share-alt" aria-hidden="true"></i>
            </span>
          </div>
        
        </div>
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/12/24/Convolutional-Neural-Networks/" rel="next" title="Convolutional Neural Networks">
                <i class="fa fa-chevron-left"></i> Convolutional Neural Networks
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/01/03/Autoencoder/" rel="prev" title="Autoencoder">
                Autoencoder <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">邓小俊</p>
              <p class="site-description motion-element" itemprop="description">人所恐惧的不是孤独本身，而是害怕承认人本生而孤独的事实</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">70</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">57</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#准备"><span class="nav-number">1.</span> <span class="nav-text">准备</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RNN基础"><span class="nav-number">2.</span> <span class="nav-text">RNN基础</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#手动实现RNN"><span class="nav-number">2.1.</span> <span class="nav-text">手动实现RNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用static-rnn"><span class="nav-number">2.2.</span> <span class="nav-text">使用static_rnn()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#打包序列"><span class="nav-number">2.3.</span> <span class="nav-text">打包序列</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用dynamic-rnn"><span class="nav-number">2.4.</span> <span class="nav-text">使用dynamic_rnn()</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#设置序列长度"><span class="nav-number">2.5.</span> <span class="nav-text">设置序列长度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#训练序列分类器"><span class="nav-number">2.6.</span> <span class="nav-text">训练序列分类器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多层RNN"><span class="nav-number">2.7.</span> <span class="nav-text">多层RNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#时间序列"><span class="nav-number">2.8.</span> <span class="nav-text">时间序列</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#使用OutputProjectionWrapper"><span class="nav-number">2.8.1.</span> <span class="nav-text">使用OutputProjectionWrapper</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#不使用OutputProjectionWrapper"><span class="nav-number">2.8.2.</span> <span class="nav-text">不使用OutputProjectionWrapper()</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#生成一个创造性的序列"><span class="nav-number">2.9.</span> <span class="nav-text">生成一个创造性的序列</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Deep-RNN"><span class="nav-number">3.</span> <span class="nav-text">Deep RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#MultiRNNCell"><span class="nav-number">3.1.</span> <span class="nav-text">MultiRNNCell</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dropout"><span class="nav-number">3.2.</span> <span class="nav-text">Dropout</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#LSTM"><span class="nav-number">4.</span> <span class="nav-text">LSTM</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#嵌入向量"><span class="nav-number">5.</span> <span class="nav-text">嵌入向量</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#获取数据"><span class="nav-number">5.1.</span> <span class="nav-text">获取数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#建立字典"><span class="nav-number">5.2.</span> <span class="nav-text">建立字典</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Generate-batches"><span class="nav-number">5.3.</span> <span class="nav-text">Generate batches</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#创建模型"><span class="nav-number">5.4.</span> <span class="nav-text">创建模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#训练模型"><span class="nav-number">5.5.</span> <span class="nav-text">训练模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#plot-the-embeddings"><span class="nav-number">5.6.</span> <span class="nav-text">plot the embeddings</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Machine-Translation"><span class="nav-number">5.7.</span> <span class="nav-text">Machine Translation</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2018 &mdash; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">邓小俊</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">博文包含字数&#58;</span>
    
    <span title="博文包含字数">277.7k</span>
  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    
      <div id="needsharebutton-float">
        <span class="btn">
          <i class="fa fa-share-alt" aria-hidden="true"></i>
        </span>
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("EdodKu35EbSo4VoLhJYQMd09-gzGzoHsz", "qpErN7JcsIzusWGMNuw1QcxN");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  
  
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">

  
  
  <script src="/lib/needsharebutton/needsharebutton.js"></script>

  <script>
    
      pbOptions = {};
      
          pbOptions.iconStyle = "box";
      
          pbOptions.boxForm = "horizontal";
      
          pbOptions.position = "bottomCenter";
      
          pbOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-postbottom', pbOptions);
    
    
      flOptions = {};
      
          flOptions.iconStyle = "box";
      
          flOptions.boxForm = "horizontal";
      
          flOptions.position = "middleRight";
      
          flOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-float', flOptions);
    
  </script>

  

  
  


  

  

 
</body>
</html>
