<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />










  <meta name="baidu-site-verification" content="true" />







  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />




  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico#/images/favicon-16x16-next.png?v=5.1.4">






  <meta name="keywords" content="机器学习," />










<meta name="description" content="对自己接触和学习到的机器学习算法相关知识点进行一个梳理和总结">
<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习总结">
<meta property="og:url" content="http://coldjune.com/2019/08/12/机器学习总结/index.html">
<meta property="og:site_name" content="Stay Hungary">
<meta property="og:description" content="对自己接触和学习到的机器学习算法相关知识点进行一个梳理和总结">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://coldjune.com/2019/08/12/机器学习总结/混淆矩阵.png">
<meta property="og:image" content="http://coldjune.com/2019/08/12/机器学习总结/PR图.png">
<meta property="og:image" content="http://coldjune.com/2019/08/12/机器学习总结/ROC和AUC.png">
<meta property="og:image" content="http://coldjune.com/2019/08/12/机器学习总结/代价矩阵.png">
<meta property="og:image" content="http://coldjune.com/2019/08/12/机器学习总结/代价曲线和期望总体代价.png">
<meta property="og:image" content="http://coldjune.com/2019/08/12/机器学习总结/偏差与方差.png">
<meta property="og:image" content="http://coldjune.com/2019/08/12/机器学习总结/高偏差模型.png">
<meta property="og:image" content="http://coldjune.com/2019/08/12/机器学习总结/高方差模型.png">
<meta property="og:image" content="http://coldjune.com/2019/08/12/机器学习总结/箱线图.jpeg">
<meta property="og:image" content="http://coldjune.com/2019/08/12/机器学习总结/正态分布.png">
<meta property="og:image" content="http://coldjune.com/2019/08/12/机器学习总结/梯度下降.png">
<meta property="og:image" content="http://coldjune.com/2019/08/12/机器学习总结/点到平面距离.png">
<meta property="og:image" content="http://coldjune.com/2019/08/12/机器学习总结/支持向量与间隔.png">
<meta property="og:image" content="http://coldjune.com/2019/08/12/机器学习总结/软间隔.png">
<meta property="og:image" content="http://coldjune.com/2019/08/12/机器学习总结/替代损失函数.png">
<meta property="og:image" content="http://coldjune.com/2019/08/12/机器学习总结/异或问题.png">
<meta property="og:updated_time" content="2021-10-24T05:52:30.409Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习总结">
<meta name="twitter:description" content="对自己接触和学习到的机器学习算法相关知识点进行一个梳理和总结">
<meta name="twitter:image" content="http://coldjune.com/2019/08/12/机器学习总结/混淆矩阵.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://coldjune.com/2019/08/12/机器学习总结/"/>





  <title>机器学习总结 | Stay Hungary</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
		<a href="https://github.com/coldJune" class="github-corner" aria-label="View source on Github"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; left: 0; transform: scale(-1, 1);" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style></a>    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Stay Hungary</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Programming is an art form</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://coldjune.com/2019/08/12/机器学习总结/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="邓小俊">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Stay Hungary">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器学习总结</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-12T20:12:43+08:00">
                2019-08-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2019/08/12/机器学习总结/" class="leancloud_visitors" data-flag-title="机器学习总结">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  11,659
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  43
                </span>
              
            </div>
          

          
              <div class="post-description">
                  对自己接触和学习到的机器学习算法相关知识点进行一个梳理和总结
              </div>
          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设用<strong>P</strong>来评估计算机程序在某任务类<strong>T</strong>上的性能，若一个程序通过利用经验<strong>E</strong>在<strong>T</strong>中任务上获得了性能改善，则我们就说关于<strong>T</strong>和<strong>P</strong>，该程序对<strong>E</strong>进行了学习。<br><br>Mitcell,1997</p>
<h1 id="模型评估与选择"><a href="#模型评估与选择" class="headerlink" title="模型评估与选择"></a>模型评估与选择</h1><h2 id="误差"><a href="#误差" class="headerlink" title="误差"></a>误差</h2><ul>
<li><p>误差</p>
<blockquote>
<p><strong>误差</strong>是学习器的实际预测输出与样本真实输出之间的差异，其中在训练集上的误差称为<strong>训练误差</strong>或者<strong>经验误差</strong>，在新样本上的误差称为<strong>泛化误差</strong>。</p>
</blockquote>
</li>
<li><p>过拟合与欠拟合</p>
<blockquote>
<p><strong>过拟合</strong>是指学习器把训练样本学习得“太好”，可能将训练样本的一些特点当做所有潜在样本都具有的一般性质而导致泛化性能下降。<strong>欠拟合</strong>则相反，它表示对训练样本的一般性质都未曾学到。欠拟合可以克服而过拟合只能缓解。</p>
</blockquote>
</li>
</ul>
<h2 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h2><ul>
<li><p>留出法</p>
<blockquote>
<p><strong>留出法</strong>直接将训练集划分为两个不相交的子集，一个作为训练集，一个作为测试集。为了保证被划分后的数据拥有和原始数据同样的分布，避免因数据划分而引入额外的偏差影响最终的结果，可以采用<strong>分层采样</strong>(<em>stratified sampling</em>)来保留类别比例；因一个数据集可能存在多种划分方式，如果单次使用留出法可能导致结果不够稳定可靠，一般可采用随机划分、重复进行实验后取平均值作为留出法的评估结果；数据划分将导致最后的模型不是整个数据集的训练结果，而只是一部分数据训练出来的，这将降低评估结果的<strong>保真性</strong>(<em>fidelity</em>)，由于没有完美的解决方案，一般是将大约$\frac{2}{3}$~$\frac{4}{5}$的样本用作训练，剩余用于测试(测试集至少应含30个样例)</p>
</blockquote>
</li>
<li><p>交叉验证法</p>
<blockquote>
<p><strong>交叉验证法</strong>将数据分为$k$个大小相似的互斥子集，每个子集尽可能保持数据分布的一致性(分层采样)，每次用$k-1$个子集的并集作为训练集，剩下的那个作为测试集，如此进行$k$次则可通过$k$组训练/测试集得到$k$个测试结果，最后取均值。交叉验证法又称为<strong>k折交叉验证</strong>(<em>k-fold cross validation</em>)，它评估结果的稳定性和保真性很大程度上取决于$k$，这里$k$的取值常用10；与留出法类似，一个数据集可能存在多种划分，为减小划分引入的差别，k折验证法需要使用不同的划分进行$p$次，最终的结果是这p次k折交叉验证结果的均值。</p>
</blockquote>
</li>
<li><p>留一法</p>
<blockquote>
<p><strong>留一法</strong>(<em>Leave-One-Out, LOO</em>)是交叉验证法的特例，它将大小为$m$的数据集划分成$m$个子集，即每个子集只包含一个样本，这样就不会受随机划分的影响，同时也让用训练集训练的模型和期望评估的用整个数据集训练的模型相似(两个数据集样本数差一)，使结果更为准确。但是当数据量变大时，留一法需要训练$m$个模型，这个计算开销是巨大而不能忍受的。</p>
</blockquote>
</li>
<li><p>自助法</p>
<blockquote>
<p>所谓<strong>自助法</strong>(<em>booststrapping</em>)就是通过<strong>自助采样</strong>(<em>booststrap sampling</em>)<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup>从原始数据集$D$中有放回地随机选取$m$个样本组成训练用数据集$D’$，因为是有放回的采样，所以$D$中的一部分样本可能在$D’$中多次出现，而另一部分样本则不会出现，样本在$m$次采样中始终不出现的概率为$(1-\frac{1}{1})^m$，对$m$取极限可得$\lim_{m \rightarrow \inf}{(1- \frac{1}{m})^m} = \frac{1}{e} \approx 0.368$。这说明通过自助采样之后有$36.8\%$的数据未参与训练，因此我们可以使用这部分数据作为测试数据集，这样获得的测试结果叫做<strong>包外估计</strong>(<em>out-of-bag estimate</em>)。虽然自助法在数据集较小，难以有效划分有效训练集/测试集时很有效，并且由于它能参数多个不同的训练集，在集成学习中也能发挥巨大的作用，但由于它产生的数据集改变了原始数据集的分布，引入了估计偏差，因此在数据量足够多时还是使用留一法或交叉验证法会更好一些。</p>
</blockquote>
</li>
</ul>
<h2 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h2><h3 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;回归任务中最常用的性能度量是<strong>均方误差</strong>，即对各个样本预测值$f(\boldsymbol{x}_i)$与对应真实值$y_i$的差值的平方进行求和再取平均数：<script type="math/tex">E(x;D)=\frac{1}{m}\sum^1_m(f(\boldsymbol{x}_i-y_i))^2</script></p>
<h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><h4 id="错误率与精度"><a href="#错误率与精度" class="headerlink" title="错误率与精度"></a>错误率与精度</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>错误率</strong>是分类错误的样本数占样本总数的比例，<strong>精度</strong>是分类正确的样本数占样本总数的比例，两者相加为$1$。<br></p>
<ul>
<li>错误率<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup><script type="math/tex; mode=display">E(f;D)=\frac{1}{m}\sum_m^1\boldsymbol{I}(f(\boldsymbol{x}_i \ne y_i))</script></li>
<li>精度<script type="math/tex; mode=display">acc(f;D)=\frac{1}{m}\sum_m^1\boldsymbol{I}(f(\boldsymbol{x}_i = y_i))</script><h4 id="查准率-准确率-、查全率-召回率-和F1"><a href="#查准率-准确率-、查全率-召回率-和F1" class="headerlink" title="查准率(准确率)、查全率(召回率)和F1"></a>查准率(准确率)、查全率(召回率)和F1</h4>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>查准率</strong>表示分类结果中真正为正的样本(<em>真正例</em>)在分类为正的样本中所占的比例，<strong>查全率</strong>表示分类结果中真正为正的样本在总样本中所占的比例。对于这两个度量标准，可以通过混淆矩阵进行直观的展现，<br><img src="/2019/08/12/机器学习总结/混淆矩阵.png" alt="混淆矩阵"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;其中<strong>真正例</strong>(<em>true positive</em>)表示预测为真实际也为真，<strong>假反例</strong>(<em>false negative</em>)表示实际为真预测为加，<strong>假正例</strong>(<em>false negative</em>)表示预测为真实际为假，<strong>真反例</strong>(<em>true negative</em>)表示实际为假预测也为假，这四种情形对应的样例数之和为总的样本数。而查准率和查全率用可以用这几种情形进行表示</li>
<li>查准率(准确率)<script type="math/tex; mode=display">P = \frac{TP}{TP+FP}</script></li>
<li>查全率(召回率)<script type="math/tex; mode=display">F = \frac{TP}{TP+FN}</script></li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;查准率和查全率是一对相互矛盾的度量，查准率高则查全率低，反之亦然。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>P-R曲线</strong>又名<strong>PR图</strong>，其横轴为查全率，纵轴为查准率，P-R曲线往往是非平滑非单调的。如果一个学习器的P-R曲线被另一个学习器的曲线完全包住，则说明后者的性能优于前者；如果两者有交叉，则只能在具体的查准率和查全率下进行比较。<br><img src="/2019/08/12/机器学习总结/PR图.png" alt="P-R图"><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>平衡点</strong>(<em>Break-Event Point,BEP</em>)是一个综合考虑查准率和查全率的度量，它的取值为“查准率=查全率”时的值，一般而言，BEP越大学习器越优。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>F1度量</strong>同样时综合考虑查准率和查全率的度量，它比BEP复杂一些。F1是查准率和查全率的调和平均$\frac{1}{F1}=\frac{1}{2}\cdot (\frac{1}{P}+\frac{1}{R})$，它比算数平均$\frac{P+R}{2}$和几何平均$\sqrt{P\times R}$更重视较小值。</p>
<ul>
<li>F1<script type="math/tex; mode=display">F1=\frac{2\times P\times R}{P+R}=\frac{2\times TP}{样例总数+TP-TN}</script></li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当应用对查准率和查全率的重视程度不同时，就要是F1的一般形式$F_\beta$来表达出对查准率/查全率的偏好，$\beta&gt;0$度量了查全率和查准率的相对重要性，$\beta=1$将退化成F1度量；$\beta&gt;1$时查全率的影响更大；$\beta&lt;1$时查准率的影响更大。</p>
<ul>
<li>$F_\beta$<script type="math/tex; mode=display">F_\beta=\frac{(1+\beta)\times P\times R}{(\beta^2\times P)+R}</script></li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在多分类任务中，需要考虑的混淆矩阵将不止一个，可能需要在$n$个混淆矩阵上综合考虑查全率和查准率，这时候有两种不同的度量。一种是先在各个混淆矩阵上计算出各自的查准率和查全率$(P_1,R_1),(P_2,R_2),\cdots,(P_n,R_n)$，然后求平均值，这样得到的是<strong>宏查准率</strong>(<em>macro-P</em>)，<strong>宏查全率</strong>(<em>macro-R</em>)，<strong>宏F1</strong>(<em>macro-F1</em>);另一种是先计算混淆矩阵对应元素的平均值,$\overline{TP}(TP),\overline{FP}(FP),\overline{TN}(TN),\overline{FN}(FN)$，在基于这些平均值计算出<strong>微查准率</strong>(<em>micro-P</em>)，<strong>微查全率</strong>(<em>micro-R</em>)，<strong>微F1</strong>(<em>micro-F1</em>)。</p>
<ul>
<li>宏查准率，宏查全率，宏F1<script type="math/tex; mode=display">P_{macro}=\frac{1}{n}\sum_{i=1}^nP_i</script><script type="math/tex; mode=display">R_{macro}=\frac{1}{n}\sum_{i=1}^nR_i</script><script type="math/tex; mode=display">F1_{macro}=\frac{2\times P_{macro}\times R_{macro}}{P_{macro}+R_{macro}}</script></li>
<li>微查准率，微查全率，微F1<script type="math/tex; mode=display">P_{micro}=\frac{\overline{TP}}{\overline{TP}+\overline{FP}}</script><script type="math/tex; mode=display">R_{micro}=\frac{\overline{TP}}{\overline{TP}+\overline{FN}}</script><script type="math/tex; mode=display">F1_{micro}=\frac{2\times P_{micro} \times R_{micro}}{P_{micro}+R_{micro}}</script></li>
</ul>
<h4 id="ROC与AUC"><a href="#ROC与AUC" class="headerlink" title="ROC与AUC"></a>ROC与AUC</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>阈值</strong>是在分类过程中确定样本属于哪一类的标准值，大于阈值则划分为正类，否则为反类。如果将预测结果进行排序，最可能为正例的排在前面(概率大的)，最不可能的排在后面(概率小的)，则阈值便成为了区分正反例的<strong>截断点</strong>。如果更看重查准率，则可选取排序中靠前的位置进行截断(阈值大)，如果更看中查全率，则可选取排序中靠后的位置进行截断(阈值小)。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>受试者工作特征</strong>(<em>Receiver Operating Characteristic, ROC</em>)是通过考察排序本身的好坏和截断点的不同来研究学习器的泛化性能。ROC曲线的纵轴是<strong>真正例率</strong>(<em>True Positive Rate, TPR</em>)，横轴为<strong>假正例率</strong>(<em>False Positive Rate, FPR</em>)，对预测结果进行排序，然后按顺序逐个把样本当成正例进行预测，每次计算出这两个重要的值，然后以它们作为坐标作图。给定$m^+$个正例和$m^-$个反例，具体步骤如下：</p>
<ol>
<li>根据学习器预测结果对样例进行排序</li>
<li>将分类阈值设为最大(所有样例均为反例，真正例率和假正例率均为0)，在坐标$(0,0)$处标记一个点</li>
<li>将分类阈值依次设置为每个样例的预测值(即依次将每个样例标记为正例)。假设前一个标记点为$(x,y)$,则如果当前样例为正例，则标记为$(x,y+\frac{1}{m^+})$，若为反例则标记为$(x+\frac{1}{m^-},y)$</li>
<li>用线段将相邻的点连接</li>
</ol>
<ul>
<li>真正例率<script type="math/tex; mode=display">TPR = \frac{TP}{TP+FN}</script></li>
<li>假正例率<script type="math/tex; mode=display">FPR = \frac{FP}{FP+TN}</script></li>
<li>ROC<br><img src="/2019/08/12/机器学习总结/ROC和AUC.png" alt="ROC和AUC"></li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;与PR图类似，如果一个学习器的ROC曲线完全被另一个包住，则后者的性能优于前者；如果两个学习器的ROC曲线发生交叉则难以判定。AUC适用于这种交叉情况下的性能比较。<strong>AUC</strong>(<em>Area Under ROC Curve</em>)是ROC曲线下的面积，假定ROC曲线是由坐标${(x_1,y_1),(x_2,y_2),\cdots,(x_m,y_m)}$的点连接形成，则AUC的计算公式为：</p>
<script type="math/tex; mode=display">AUC=\frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_i)\cdot(y_i+y_{i+1})</script><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;AUC考虑的是样本预测的排序质量，因此它与排序误差有紧密的关系。$\boldsymbol{l_{rank}}$对应的是ROC曲线之上的面积，是对排序损失的的定义，若一个正例在ROC曲线上对应标记点的坐标为$(x,y)$，则$x$恰是排序在其前的反例所占的比例，即假正例率，因此<script type="math/tex">AUC=1-l_{rank}</script></p>
<ul>
<li>$l_{rank}$<script type="math/tex; mode=display">l_{rank}=\frac{1}{m^+m^-}\sum_{x^+ \in D+}\sum_{x^-\in D^-}(\boldsymbol{I}(f(x^+)<f(x^-))+\frac{1}{2}\boldsymbol{I}(f(x^+)=f(x^-)))</script>其中$m^+$表示正例个数，$m^-$表示反例个数，$D^+$表示正例集合，$D^-$表示反例集合。公式的含义为：考虑每一对正反例，若正例的预测值小于反例，则记一分，若想等则记0.5分，将所有分值相加并除以总共的对数则得到排序的损失(loss)。</li>
</ul>
<h4 id="代价敏感错误率和代价曲线"><a href="#代价敏感错误率和代价曲线" class="headerlink" title="代价敏感错误率和代价曲线"></a>代价敏感错误率和代价曲线</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>代价敏感错误率</strong>是为权衡不同类型错误造成的不同损失，通过给错误赋予非均等代价(<em>unequal cost</em>)以达到最小化总体代价(<em>total cost</em>)<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup>的目的一种度量方式。</p>
<ul>
<li>代价矩阵<br><img src="/2019/08/12/机器学习总结/代价矩阵.png" alt="代价矩阵"><br>$cost_{ij}$表示将第$i$类样本预测为第$j$样本的代价，$cost_{ii}$一般为0，如果第0类判别为第1类的损失更大，则$cost_{01}&gt;cost_{10}$，损失相差越大，则$cost_{01}$和$cost_{10}$的值差别越大，同时两则的差值重要的是代价比值而非绝对差值。</li>
<li>代价敏感错误率<script type="math/tex; mode=display">E(f;D:cost)=\frac{1}{m}(\sum_{x_i\in D^+}\boldsymbol{I}(f(\boldsymbol{x}_i)\ne y_i)\times cost_{01}+ \sum_{x_i\in D^-}\boldsymbol{I}(f(\boldsymbol{x}_i)\ne y_i)\times cost_{10})</script>$D^+$为正例子集，$D^-$为反例子集，$cost_{01}$表示第0类预测为第1类的代价，$cost_{10}$表示第1类预测为第0类的代价；若$cost_{ij}$不限于0、1，则可以定义出多分类的代价敏感错误率。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>代价曲线</strong>(<em>cost curve</em>)能在非均等代价下反应出学习器的期望总体代价。代价曲线的横轴是取值为$[0,1]$的正例概率代价，纵轴是取值为$[0,1]$的归一化<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup>代价。</li>
<li>正例概率代价<script type="math/tex; mode=display">P(+)cost=\frac{p\times cost_{01}}{p\times cost_{01}+(1-p)\times cost_{10}}</script>p是样例为正例的概率</li>
<li>归一化代价<script type="math/tex; mode=display">cost_{norm}=\frac{FNR\times p\times cost_{01}+FPR\times (1-p)\times cost_{10}}{p\times cost_{01}+(1-p)\times cost_{10}}</script>FPR是假正例率，$FNR=1-TPR$是假反例率</li>
<li>代价曲线的绘制<ol>
<li>设ROC曲线上的点为$(TPR,FPR)$，据此计算出FNR</li>
<li>在代价平面上绘制一条$(0,FPR)$到$(1,FNR)$的线段，线段下的面积即表示该条件下的期望总代价</li>
<li>如此将ROC曲线上的每个点转化为代价平面上的线段(ROC曲线上的每一个点对应代价平面上的一条线段)</li>
<li>取所有线段的下界，围成的面积即为在所有条件下学习器的期望总代价</li>
</ol>
</li>
</ul>
<p><img src="/2019/08/12/机器学习总结/代价曲线和期望总体代价.png" alt="代价曲线和期望总体代价"></p>
<h3 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;聚类的性能指标分为两类，一类是将聚类结果与参考模型(<em>reference model</em>)进行比较，这称为<strong>外部指标</strong>(<em>external index</em>)；另一类是直接考察聚类结果而不参考任何模型，这称为<strong>内部指标</strong>(<em>internal index</em>)。</p>
<h4 id="外部指标"><a href="#外部指标" class="headerlink" title="外部指标"></a>外部指标</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对数据集$D=\{x_1,x_2,\cdots,x_m\}$，聚类结果的簇划分为$C=\{C_1,C_2,\cdots,C_k\}$，参考模型的簇划分为<script type="math/tex">C^*=\{C_1^*,C_2^*,\cdots,C_s^*\}</script>，令<script type="math/tex">\boldsymbol{\lambda}</script>和<script type="math/tex">\boldsymbol{\lambda^*}</script>分别表示<script type="math/tex">C</script>和<script type="math/tex">C^*</script>对应的簇的标记向量，将样本两两配对定义：</p>
<script type="math/tex; mode=display">a=|SS|,\ SS=\{(\boldsymbol{x}_i\boldsymbol{x}_j)|\lambda_i=\lambda_j,\lambda_i^*=\lambda_j^*,i<j\}</script><script type="math/tex; mode=display">b=|SD|,\ SD=\{(\boldsymbol{x}_i\boldsymbol{x}_j)|\lambda_i=\lambda_j,\lambda_i^* \ne \lambda_j^*,i<j\}</script><script type="math/tex; mode=display">c=|DS|,\ DS=\{(\boldsymbol{x}_i\boldsymbol{x}_j)|\lambda_i\ne \lambda_j,\lambda_i^*=\lambda_j^*,i<j\}</script><script type="math/tex; mode=display">d=|DD|,\ DD=\{(\boldsymbol{x}_i\boldsymbol{x}_j)|\lambda_i\ne \lambda_j,\lambda_i^*\ne \lambda_j^*,i<j\}</script><p>$SS$包含了在$C$中隶属于相同簇且在<script type="math/tex">C^*</script>也隶属于相同簇的样本对,$SD$包含了在$C$中隶属于相同簇但在<script type="math/tex">C^*</script>中隶属于不同簇的样本对,$DS$包含了在$C$中隶属于不同的簇但在<script type="math/tex">C^*</script>中隶属于相同簇的样本对,$DD$包含了在$C$中隶属于不同的簇在<script type="math/tex">C^*</script>中也隶属于不同的簇的样本对。每个样本对$(\boldsymbol{x}_i,\boldsymbol{x}_j)(i&lt;j)$仅能出现在一个集合中，因此$a+b+c+d=\frac{m(m-1)}{2}$</p>
<ul>
<li>Jaccard系数(<em>Jaccard Coefficient, JC</em>)<script type="math/tex; mode=display">JC=\frac{a}{a+b+c}</script></li>
<li>FM指数(<em>Fowlkes and Mallows Index, FMI</em>)<script type="math/tex; mode=display">FMI=\sqrt{\frac{a}{a+b}\cdot \frac{a}{a+c}}</script></li>
<li>Rand指数(<em>Rand Index, RI</em>)<script type="math/tex; mode=display">RI=\frac{2(a+d)}{m(m-1)}</script>这些值的结果均在$[0,1]$，值越大越好<h4 id="内部指标"><a href="#内部指标" class="headerlink" title="内部指标"></a>内部指标</h4>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设聚类结果的簇划分为$C=\{C_1,C_2,\cdots,C_k\}$，则可定义<script type="math/tex; mode=display">avg(C) = \frac{2}{|C|(|C|-1)}\sum_{1\le i<j\le |C|}dist(\boldsymbol{x}_i,\boldsymbol{x}_j)</script><script type="math/tex; mode=display">diam(C)=max_{1\le i<j\le |C|}dist(\boldsymbol{x}_i,\boldsymbol{x}_j)</script><script type="math/tex; mode=display">d_{min}(C_i,C_j)=min_{\boldsymbol{x}_i \in C_i,\boldsymbol{x}_j \in C_j}dist(\boldsymbol{x}_i,\boldsymbol{x}_j)</script><script type="math/tex; mode=display">d_{cen}(C_i,C_j)=dist(\boldsymbol{\mu}_i,\boldsymbol{\mu}_j)</script></li>
</ul>
<ol>
<li>$dist(\cdot,\cdot)$用于计算两个样本之间的距离；</li>
<li>$\boldsymbol{\mu}$代表簇$C$的中心点$\boldsymbol{\mu}=\frac{1}{|C|}\sum_{1\le i\le |C|}\boldsymbol{x}_i$;</li>
<li>$avg(C)$表示簇内样本的平均距离</li>
<li>$diam(C)$表示簇内样本的最远距离</li>
<li>$d_{min}(C_i,C_j)$表示簇$C_i$和簇$C_j$两个簇之间最近样本间的距离</li>
<li>$d_{cen}(C_i,C_j)$表示簇$C_i$和簇$C_j$两个簇中心点间的距离</li>
</ol>
<ul>
<li>DB指数(<em>, Davies-Bouldin Index,DBI</em>)<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>DBI指数</strong>(戴维森堡丁指数/分类适确性指标)计算任意两类别的类内平均距离之和除以两聚类中心距离求最大值，DBI越小意味着类内距离越小同时类间距离越大。<script type="math/tex; mode=display">DBI=\frac{1}{k}\sum_{i=1}^k\max_{j\ne i }(\frac{avg(C_i)+avg(C_j)}{d_{cen}(C_i,C_j)})</script></li>
<li>Dunn指数(<em>Dunn Index,DI</em>)<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>Dunn指数</strong>(邓恩指数)计算两个簇之间最近样本间的距离(类间)除以任意簇的簇内样本的最远距离，DI越大意味着簇间距离越大同时类内距离越小。<script type="math/tex; mode=display">DI=\min_{1\le i\le k}\{\min_{j\ne i}(\frac{d_{min}(C_i,C_j)}{max_{1\le l\le k}diam(C_l)})\}</script>DBI越小越好，DI则越大越好。由于距离使用的是欧式聚集，所以两者对环状分布的聚类度量都不好。<h4 id="距离计算"><a href="#距离计算" class="headerlink" title="距离计算"></a>距离计算</h4>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>连续属性</strong>在定义域上有无穷多个可能的取值，<strong>有序属性</strong>表示属性之间有明显的远近关系(1与2的距离在数值上显然比1与3的距离近，所以表示数值的$\{1,2,3\}$就是有序属性的集合)。</li>
<li>闵可夫斯基距离(<em>Minkowski distance</em>)<script type="math/tex; mode=display">dist_{mk}(\boldsymbol{x}_i,\boldsymbol{x}_j)=(\sum_{u=1}^n|x_{iu}-x_{ju}|^p)^\frac{1}{p}</script></li>
<li>欧式距离(<em>Euclidean distance</em>)<script type="math/tex; mode=display">dist_{ed}(\boldsymbol{x}_i,\boldsymbol{x}_j)=\|\boldsymbol{x}_i-\boldsymbol{x}_j\|_2=\sqrt{(\sum_{u=1}^n|x_{iu}-x_{ju}|^2)} \tag{p=2}</script></li>
<li>曼哈顿距离(<em>Manhattan distance</em>)<script type="math/tex; mode=display">dist_{md}(\boldsymbol{x}_i,\boldsymbol{x}_j)=\|\boldsymbol{x}_i-\boldsymbol{x}_j\|=(\sum_{u=1}^n|x_{iu}-x_{ju}|) \tag{p=1}</script></li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>离散属性</strong>在定义域上有有限多个取值，<strong>无序属性</strong>表示属性之间没有远近大小关系(如实物电脑、书、杯子之间就没法确定一定的顺序，所以{电脑、书、杯子}就是无序属性的集合)。</p>
<ul>
<li>VDM(<em>Value Difference Metric</em>)<script type="math/tex; mode=display">VDM_p(a,b)=\sum_{k=1}^p|\frac{m_{u,a,i}}{m_{u,a}}-\frac{m_{u,b,i}}{m_{u,b}}|^p</script>$m_{u,a}$表示属性$u$上取值为$a$的样本数；$m_{u,a,i}$表示在第$i$个样本簇中在属性$u$上取值为$a$的样本数；$k$表示样本簇数。此为属性$u$上两个离散值$a$和$b$之间的$VDM$距离。</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假定有$n_c$个有序属性，$n-n_c$个无序属性，则混合属性的距离计算如下：</p>
<script type="math/tex; mode=display">MinkovDM_p(\boldsymbol{x}_i,\boldsymbol{x}_j)=(\sum_{u=1}^{n_c}|x_{iu}-x_{ju}|^p+\sum_{u=n_c+1}^nVDM_p(x_{iu},x_{ju}))^\frac{1}{p}</script><h2 id="比较检验"><a href="#比较检验" class="headerlink" title="比较检验"></a>比较检验</h2><p>// TODO 因无法完全理解各个比较检验公式的含义，暂时不在此总结</p>
<h2 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>偏差</strong>度量了学习器的期望预测结果与真实结果的偏离程度，刻画了学习算法本身的拟合能力；<strong>方差</strong>度量了同样大小的训练集的变动所导致的学习性能的改变，刻画了数据扰动所带来的影响；<strong>噪声</strong>表达了当前任务上任何学习算法所能达到的期望泛化误差的下界，刻画了学习问题本身的难度；<strong>泛化误差</strong>是偏差、方差和噪声之和<sup id="fnref:6"><a href="#fn:6" rel="footnote">6</a></sup>。</p>
<ul>
<li>期望预测<script type="math/tex; mode=display">\bar{f}(\boldsymbol{x})=E_D(f(\boldsymbol{x};D))</script></li>
<li>偏差<script type="math/tex; mode=display">bias^2(\boldsymbol{x})=(f(\bar{\boldsymbol{x}})-y)^2</script></li>
<li>方差<script type="math/tex; mode=display">var(\boldsymbol{x})=E_D[(f(\boldsymbol{x};D)-\bar{f}(\boldsymbol{x}))^2]</script></li>
<li>噪声<script type="math/tex; mode=display">\varepsilon^2= E_D[(y_D-y)^2]</script></li>
<li>泛化误差(令噪声期望$E_D[y_d-y]=0$)<script type="math/tex; mode=display">
\begin{aligned}
E(f;D) &= E_D[(f(\boldsymbol{x};D)-y_D)^2] \\
      &= E_D[(f(\boldsymbol{x};D)-\bar{f}(\boldsymbol{x})+\bar{f}(\boldsymbol{x})-y_D)^2]\\ 
     & = E_D[(f(\boldsymbol{x};D)-\bar{f}(\boldsymbol{x}))^2] +E_D[(\bar{f}(\boldsymbol{x})-y_D)^2] + E_D[2(f(\boldsymbol{x};D)-\bar{f}(\boldsymbol{x}))(\bar{f}(\boldsymbol{x})-y_D)] \\
     &=E_D[(f(\boldsymbol{x};D)-\bar{f}(\boldsymbol{x}))^2] +E_D[(\bar{f}(\boldsymbol{x})-y_D)^2]\\
     &=var(\boldsymbol{x}) + E_D[(\bar{f}(\boldsymbol{x})-y+y-y_D)^2]\\
     &=var(\boldsymbol{x}) +  E_D[(\bar{f}(\boldsymbol{x})-y)^2]+E_D[(y-y_D)^2]+E_D[2(\bar{f}(\boldsymbol{x})-y)(y-y_D)]\\
     &=var(\boldsymbol{x})+E_D[(\bar{f}(\boldsymbol{x})-y)^2]+E_D[(y_D-y)^2] \\
     &=var(\boldsymbol{x})+bias^2(\boldsymbol{x}) + \varepsilon^2
\end{aligned}</script>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当算法训练不足时，数据的扰动不足以使学习器受到明显的影响，这时算法处于欠拟合状态，主要受偏差的影响；当算法训练充足之后，算法有着很强的拟合能力，当数据出现轻微变动的时候都足以使学习器发生显著变化，这时是方差起着决定性作用，如果学习器学习能力过强，将自身独有的特性作为全局特性(非全局特性被学习)，这时学习器出现过拟合。<br><img src="/2019/08/12/机器学习总结/偏差与方差.png" alt="误差和偏差"></li>
</ul>
<h3 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>学习曲线</strong>是一种可视化的性能指标，它根据训练数据和测试数据比较模型的性能。通过绘制随着数据增加的训练误差和测试误差曲线来度量模型的偏差和方差。高偏差是欠拟合的表现，其模型的学习曲线在趋于平稳之后会很靠近但是整体误差远高于期望误差；高方差是过拟合的表现，其模型的学习曲线趋于平稳后两条曲线会有明显缝隙，且测试误差高于期望误差，训练误差低于期望误差。<strong>欠拟合</strong>可通过提升模型复杂度解决、减少正则化参数、增加新的特征，而<strong>过拟合</strong>只能通过降低模型复杂度、增加数据、使用正则化方法、采用dropout等方式缓解。</p>
<ul>
<li>根据一系列训练实例中的训练和测试数据比较模型的指标性能。<ul>
<li>高偏差模型<br><img src="/2019/08/12/机器学习总结/高偏差模型.png" alt="高偏差模型"></li>
<li>高方差模型<br><img src="/2019/08/12/机器学习总结/高方差模型.png" alt="高方差模型"></li>
</ul>
</li>
</ul>
<h1 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h1><h2 id="数据标准化"><a href="#数据标准化" class="headerlink" title="数据标准化"></a>数据标准化</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>标准化</strong>是指将数据按比例缩放，让它的分布落入一个固定的区间。</p>
<h3 id="Min-Max标准化"><a href="#Min-Max标准化" class="headerlink" title="Min-Max标准化"></a>Min-Max标准化</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>Min-Max标准化</strong>也叫离差标准化，它通过对数据进行线性变换使结果落到$[0,1]$区间，其缺点为当有新数据加入导致最大最小值变化时，所有数据均需重新计算。</p>
<script type="math/tex; mode=display">x_i^*=\frac{x_i-min(\boldsymbol{x})}{max(\boldsymbol{x})-min(\boldsymbol{x})}</script><h3 id="Z-score标准化"><a href="#Z-score标准化" class="headerlink" title="Z-score标准化"></a>Z-score标准化</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>Z-score标准化</strong>也叫零-均值规范化，亦称标准差标准化，经过处理的数据均值为$0$，标准差为$1$。</p>
<script type="math/tex; mode=display">x_i^*=\frac{x_i-\bar{\boldsymbol{x}}}{\sigma}</script><p>其中$\bar{\boldsymbol{x}}$为$\boldsymbol{x}$的均值，$\sigma$为$\boldsymbol{x}$的标准差，$\sigma=\sqrt{\frac{1}{n}\sum_{i=1}^n(x_i -\bar{\boldsymbol{x}})^2}$</p>
<h3 id="小数定标规范法"><a href="#小数定标规范法" class="headerlink" title="小数定标规范法"></a>小数定标规范法</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>小数定标规范法</strong>通过移动小数点的位置将数据映射到区间$[-1,1]$，移动的位数由数据中绝对值的最大值确定。</p>
<script type="math/tex; mode=display">x_i^*=\frac{x_i}{10^k}</script><p>$k$为移动的位数</p>
<h3 id="离散属性处理"><a href="#离散属性处理" class="headerlink" title="离散属性处理"></a>离散属性处理</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对于离散属性如果存在有序关系，则通过连续化将其转换为连续值，如高度的取值为高,中,低可以转换为${1.0,0.5,0}$；如果离散属性不存在有序关系，假设有$k$个属性值，则可以转换为$k$纬向量，如出行方式的取值为汽车,自行车,公交车可以转换为$(1,0,0)(0,1,0)(0,0,1)$</p>
<h2 id="缺失值"><a href="#缺失值" class="headerlink" title="缺失值"></a>缺失值</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>缺失值</strong>是指在数据集中为空的那部分数据。</p>
<h3 id="缺失值分类"><a href="#缺失值分类" class="headerlink" title="缺失值分类"></a>缺失值分类</h3><ul>
<li>不存在型空值<br><strong>不存在型空值</strong>指的是无法填入的值，即该对象在该属性上无法取值，如未婚人士的配偶姓名。</li>
<li>存在型空值<br><strong>存在型空值</strong>指的是对象在该属性上的值是存在的，但由于某种原因在数据集中缺失。一般而言空值是指代的存在型空值。</li>
<li>占位型空值<br><strong>占位型空值</strong>无法确定是不存在型空值和存在型空值，这种空值除填充空位外不代表任何其它信息。</li>
</ul>
<h3 id="处理方式"><a href="#处理方式" class="headerlink" title="处理方式"></a>处理方式</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;在对缺失值进行处理之前，需要先确定这部分值缺失的原因，如果其包含有特定的意义，则定义一个默认值表示这部分数据，如果只是统计缺失或则数据不全，再分类进行讨论。</p>
<h4 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;删除是指的直接删除这一条存在缺失值的数据，这种方式会造成大量的资源浪费，当数据量较小时，删除数据会导致数据的信息丢失从而影响数据的客观性，当缺失值较多且呈现出非随机分布时，删除这部分数据可能导致数据发生偏离从而得出错误的结论，以下是一些使用场景</p>
<ul>
<li>类标记缺失(分类任务)</li>
<li>一条数据存在大量缺失值</li>
<li>删除的数据占数据集的比例非常小</li>
</ul>
<h4 id="补齐"><a href="#补齐" class="headerlink" title="补齐"></a>补齐</h4><ol>
<li>人工填写<br>当数据量不大时可以使用这种方式，这种方式产生的数据偏离较小</li>
<li>特殊值填充<br>使用特定的值填充空值如字符串<code>None</code>、<code>NULL</code>或者<code>0</code>，但这样可能会引入新的属性值，从而导致数据产生巨大的偏差，一般不采用</li>
<li>平均值填充<br>如果数据是数值型的有序数据，可以使用平均值填充；如果数据是离散数据或者数值型的无序数据则可以使用众数(即改特征出现次数最多的属性值)填充</li>
<li>k近邻法<br>在数据集中找到<em>k</em>条与该数据最近(一般是欧式距离)的数据，然后将<em>k</em>个值加权平均来得到缺失值的数据</li>
<li>就近补齐<br>在数据集中找到一条和该数据集最相似(一般而言是距离最近)的数据进行填充，这可以算是k近邻法的特例</li>
<li>使用所有的值<br>使用所有可能的取值进行填充，在数据量很大取值很多或缺失值很多的情况下计算代价会很大</li>
<li>回归<br>使用完整的数据集建立回归模型，对于包含空值的数据将已知属性代入模型中求解缺失值</li>
<li>期望最大化方法(EM)<br>在缺失类型为随机缺失的条件下，假设模型对于完整的样本是正确的，通过观测数据的边际分布可以对未知参数进行<a href="https://baike.baidu.com/item/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/3350286?fr=aladdin" target="_blank" rel="noopener">极大似然估计</a>。这种方式适用于有效样本的数量足够以保证ML估计值是渐近无偏的并服从正态分布。但是这种方法可能会陷入局部极值，收敛速度不快且计算复杂。</li>
</ol>
<h4 id="不处理"><a href="#不处理" class="headerlink" title="不处理"></a>不处理</h4><p>贝叶斯网络和人工神经网络可以在包含空值的数据集上进行数据挖掘</p>
<h2 id="异常值"><a href="#异常值" class="headerlink" title="异常值"></a>异常值</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>异常值</strong>(<em>outlier</em>)是指数据集中偏离大部分数据的数据点，也称<strong>离群点</strong>。</p>
<h3 id="异常值检测方法"><a href="#异常值检测方法" class="headerlink" title="异常值检测方法"></a>异常值检测方法</h3><h4 id="简单统计"><a href="#简单统计" class="headerlink" title="简单统计"></a>简单统计</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对属性值进行一个描述性的统计，并规定范围，从而查看哪些值超过了这个范围即为异常值</p>
<h5 id="箱线图"><a href="#箱线图" class="headerlink" title="箱线图"></a>箱线图</h5><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>箱线图</strong>根据<strong>IQR(四分位距)</strong>来进行绘制。箱线图中的矩形表示的就是<strong>IQR</strong>，矩形的上下边界分别就是指的$Q_3$和$Q_1$，而矩形中间的一条线是<strong>中位数</strong>，上下的线段分别叫做<strong>上极限</strong>和<strong>下极限</strong>，而超出这个界限的那些点就被视为异常点。</p>
<ul>
<li>IQR<script type="math/tex; mode=display">IQR = Q_3 -Q_1</script>$Q_3$表示上四分位数即$\frac{3}{4}$分位数；$Q_1$表示下四分位数即$\frac{1}{4}$分位数</li>
<li>上极限<script type="math/tex; mode=display">upper = Q_3+ 1.5IQR</script></li>
<li>下极限<script type="math/tex; mode=display">down = Q_1 - 1.5IQR</script></li>
<li>箱线图<br><img src="/2019/08/12/机器学习总结/箱线图.jpeg" alt="箱线图"></li>
</ul>
<h4 id="3-sigma-原则"><a href="#3-sigma-原则" class="headerlink" title="$3\sigma$原则"></a>$3\sigma$原则</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;若数据服从<a href="https://baike.baidu.com/item/%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83/829892?fr=aladdin" target="_blank" rel="noopener">正态分布</a>，则根据正态分布的定义可知数据距离平均值$3\sigma$之外的概率为$P(|x-\mu|&gt;3\sigma)\le 0.003$，这种极小概率事件被认为是不可能的，所以如果出现这种类型的样本则被认为是异常值。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设$n$维数据集合为$\vec{x_i}=(x_{i,1},x_{i,2},\cdots,x_{i,n}),i\in\{1,2,\cdots,m\}$，通过公式计算每个纬度的均值$\mu_{j}$和方差$\sigma_j,j\in\{1,2,\cdots,n\}$，然后在正太分布的假设下计算数据$\vec{x_i}$的概率$p(\vec{x})$可知其是否是异常点。</p>
<ul>
<li>均值<script type="math/tex; mode=display">\mu_j=\frac{\sum_{i=1}^mx_{i,j}}{m}</script></li>
<li>方差<script type="math/tex; mode=display">\sigma_j^2=\frac{\sum_{i=1}^m(x_{i,j}-\mu_{j})^2}{m}</script></li>
<li>$p(\vec{x})$<script type="math/tex; mode=display">p(\vec{x})=\prod_{j=1}^np(x_j;\mu_j,\sigma_j^2)=\prod_{j=1}^n\frac{1}{\sqrt{2\pi}\sigma_j}\exp(-\frac{(x_j-\mu_j)^2}{2\sigma_j^2})</script></li>
<li>正态分布<br><img src="/2019/08/12/机器学习总结/正态分布.png" alt="正态分布"></li>
</ul>
<h4 id="使用距离检测多元离群点"><a href="#使用距离检测多元离群点" class="headerlink" title="使用距离检测多元离群点"></a>使用距离检测多元离群点</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当数据不服从正态分布时，通过远离平均距离多少倍的标准差来进行判定，倍数的取值由经验决定。</p>
<h3 id="异常值的处理"><a href="#异常值的处理" class="headerlink" title="异常值的处理"></a>异常值的处理</h3><ol>
<li>删除异常值</li>
<li>用平均数或中位数修正</li>
<li>采用处理缺失值的方法</li>
<li>取对数减少极值影响</li>
<li>压缩极值到上下极限</li>
<li>不处理</li>
</ol>
<h1 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>监督学习</strong>是指通过给定一个已知正确输出的数据集，使用该数据集训练出一个表示输入和输出之间关系的模型。监督学习的典型代表是<strong>分类</strong>(<em>classification</em>)和<strong>回归</strong>(<em>regression</em>)。</p>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>线性模型</strong>试图学习一个通过属性的线性组合来进行预测的函数，而许多非线性模型也可在线性模型的基础上通过引入层级结构和高维映射获得。线性模型的一般表达式如下:</p>
<script type="math/tex; mode=display">f(x)=w_1x_1+w_2x_2+w_3x_3+...+w_dx_d+b</script><p>向量形式为:</p>
<script type="math/tex; mode=display">f(x)=\boldsymbol{w^Tx}+b</script><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;线性模型因为其中的$\boldsymbol{w}$直观地表达了各属性在预测中的重要性，所以具有很好的<strong>可解释性</strong>。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>线性回归</strong>试图学得一个线性模型来尽可能准确地预测。</p>
<h3 id="性能度量-1"><a href="#性能度量-1" class="headerlink" title="性能度量"></a>性能度量</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;回归一般使用<strong>均方误差</strong>作为性能度量，其优化目标则是使均方误差最小化。</p>
<ul>
<li>均方误差最小化<script type="math/tex; mode=display">
\begin{aligned}
  (w^*,b^*)&=argmin_{(w,b)}\sum_{i=1}^m\Big(f(x_i)-y_i\Big)^2\\
  &=argmin_{(w,b)}\sum_{i=1}^m(wx_i+b-y_i)^2\\
  &=argmin_{(w,b)}\sum_{i=1}^m(y_i-wx_i-b)^2\\
\end{aligned}</script><script type="math/tex">w^*</script>,<script type="math/tex">b^*</script>表示$w$和$b$的解</li>
</ul>
<h3 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于均方误差最小化的模型求解方法叫做<strong>最小二乘法</strong>。线性回归中最小二乘法试图找到一条直线使所有样本到直线上的欧式距离之和最小。求解$w$和$b$使均方误差($E_{(w,b)}=\sum_{i=1}^m(y_i-wx_i-b)^2$)最小化的过程称为线性回归模型的最小二乘<strong>参数估计</strong>(<em>parameter estimation</em>)。其求解闭式解(<em>closed-form</em>)如下：</p>
<ol>
<li>对$w$和$b$分别求导</li>
<li>令求导后的两式分别等于0</li>
</ol>
<h4 id="单元线性回归"><a href="#单元线性回归" class="headerlink" title="单元线性回归"></a>单元线性回归</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;单属性值的线性回归试图学习$f(x_i)=wx_i+b$,使得$f(x_i)\approx y_i$</p>
<ul>
<li>对$w$求导<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial E_{(w,b)}}{\partial w} &= -\sum_{i=1}^m2x_i(y_i-wx_i-b) \\
&=2\sum_{i=1}^mx_i\Big(wx_i-(y_i-b)\Big)\\  
&=2\Big(w\sum_{i=1}^mx_i^2 - \sum_{i=1}^m(y_i-b)x_i\Big)\\
&=0\\
\implies\\ 
w\sum_{i=1}^mx_i^2 &= \sum_{i=1}^m(y_i-b)x_i\\
\implies \\
w &= \frac{\sum_{i=1}^m(y_i-b)x_i}{\sum_{i=1}^mx_i^2}
\end{aligned}</script></li>
<li>对$b$求导<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial E_{(w,b)}}{\partial b} &=-\sum_{i=1}^m2(y_i-wx_i-b)\\
&=2\sum_{i=1}^m\Big(b-(y_i-wx_i)\Big)  \\
&=2\Big(mb-\sum_{i=1}^m(y_i-wx_i)\Big) \\
&=0\\
\implies\\
mb &= \sum_{i=1}^m(y_i-wx_i)\\
\implies\\
b &= \frac{\sum_{i=1}^m(y_i-wx_i)}{m}
\end{aligned}</script></li>
<li>联立求解<script type="math/tex; mode=display">
\begin{cases}
  w = \frac{\sum_{i=1}^m(y_i-b)x_i}{\sum_{i=1}^mx_i^2}\\
      \\
  b = \frac{\sum_{i=1}^m(y_i-wx_i)}{m}
\end{cases}</script>令$m\bar{y}=\sum_{i=1}^my_i$、$m\bar{x}=\sum_{i=1}^mx_i$，则可得出<script type="math/tex; mode=display">
\begin{cases}
  w = \frac{\sum_{i=1}^mx_iy_i-b\sum_{i=1}^mx_i}{\sum_{i=1}^mx_i^2} = \frac{\sum_{i=1}^mx_iy_i-bm\bar{x}}{\sum_{i=1}^mx_i^2} \\
      \\
  b = \frac{m\bar{y}-wm\bar{x}}{m} = \bar{y}-w\bar{x} 
\end{cases}</script>将$b$代入$w$中<script type="math/tex; mode=display">
\begin{aligned}
  w\sum_{i=1}^mx_i^2 &=\sum_{i=1}^mx_iy_i-(\bar{y}-w\bar{x})m\bar{x}\\
  &= \sum_{i=1}^mx_iy_i-m\bar{x}\bar{y}+mw\bar{x}^2 \\
  w(\sum_{i=1}^mx_i^2 -m\bar{x}^2) &= \sum_{i=1}^mx_iy_i-m\bar{x}\bar{y} \\
  w\Big(\sum_{i=1}^mx_i^2-\frac{1}{m}(\sum_{i=1}^mx_i)^2\Big) &= \sum_{i=1}^mx_iy_i-\bar{x}\sum_{i=1}^my_i \\
  w &= \frac{\sum_{i=1}^my_i(x_i-\bar{x})}{\sum_{i=1}^mx_i^2-\frac{1}{m}\Big(\sum_{i=1}^mx_i\Big)^2}
\end{aligned}</script>最后得出的闭式解为：<script type="math/tex; mode=display">
\begin{cases}
  w = \frac{\sum_{i=1}^my_i(x_i-\bar{x})}{\sum_{i=1}^mx_i^2-\frac{1}{m}\Big(\sum_{i=1}^mx_i\Big)^2}\\
  b = \bar{y}-w\bar{x} 
\end{cases}</script></li>
</ul>
<h4 id="多元线性回归"><a href="#多元线性回归" class="headerlink" title="多元线性回归"></a>多元线性回归</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当数据集$D$中的样本不只由一个属性描述，而是由$d$个属性时，试图学得$f(\boldsymbol{x_i})=\boldsymbol{w^Tx_i}+b$,使得$f(\boldsymbol{x_i})\approx y_i$。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;将$w$和$b$吸收入向量形式$\hat{w}=(w;b)$，数据集$D$表示为一个$m\times(d+1)$大小的矩阵$\boldsymbol{X}$，其中每一行对应一个样本，该行前$d$个元素对应样本的$d$个属性值，最后一个元素恒为1:</p>
<script type="math/tex; mode=display">
\boldsymbol{X} = \left( 
    \begin{matrix}
        x_{11} & x_{12} & \cdots & x_{1d} & 1\\
        x_{21} & x_{22} & \cdots & x_{2d} & 1\\
        \vdots & \vdots & \ddots & \vdots & \vdots\\
        x_{m1} & x_{m2} & \cdots & x_{md} & 1
    \end{matrix}
    \right) = \left(
    \begin{matrix}
        \boldsymbol{x_1^T} & 1 \\
        \boldsymbol{x_2^T} & 1 \\
        \vdots & \vdots \\
        \boldsymbol{x_m^T} & 1
    \end{matrix}
    \right)</script><p>然后将标记表示为向量形式$\boldsymbol{y} = (y_1;y_2;\cdots;y_m)$，则得到多元线性回归的均方误差最小化公式为:</p>
<script type="math/tex; mode=display">\boldsymbol{\hat{w}^*}=argmin_{\hat{\boldsymbol{w}}}(\boldsymbol{y}- \boldsymbol{X\hat{w}})^T(\boldsymbol{y}-\boldsymbol{X\hat{w}})</script><p>令$\boldsymbol{E_{\hat{w}}}=(\boldsymbol{y}- \boldsymbol{X\hat{w}})^T(\boldsymbol{y}-\boldsymbol{X\hat{w}})$并对$\hat{\boldsymbol{w}}$求导可得：</p>
<script type="math/tex; mode=display">
\frac{\partial\boldsymbol{E_{\hat{w}}}}{\partial\boldsymbol{\hat{w}}}=-2\boldsymbol{X}^T(\boldsymbol{y}-\boldsymbol{X\hat{w}})=2\boldsymbol{X}^T(\boldsymbol{X\hat{w}}-\boldsymbol{y})</script><p>当$\boldsymbol{X}^T\boldsymbol{X}$为<a href="https://baike.baidu.com/item/%E6%BB%A1%E7%A7%A9%E7%9F%A9%E9%98%B5/10017113" target="_blank" rel="noopener">满秩矩阵(<em>full-rank matrix</em>)</a>或<a href="https://baike.baidu.com/item/%E6%AD%A3%E5%AE%9A%E7%9F%A9%E9%98%B5" target="_blank" rel="noopener">正定矩阵(<em>positive definite matrix</em>)</a>使导数为0可得:</p>
<script type="math/tex; mode=display">\begin{aligned}
    2\boldsymbol{X}^T(\boldsymbol{X\hat{w}}-\boldsymbol{y}) = 0 \\
    \implies 
    \boldsymbol{X}^T\boldsymbol{X\hat{w}}-\boldsymbol{X}^T\boldsymbol{y}=0 \\
    \implies
    \boldsymbol{X}^T\boldsymbol{X\hat{w}}=\boldsymbol{X}^T\boldsymbol{y}\\
    \implies
    \boldsymbol{\hat{w}}=(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}
\end{aligned}</script><p>令$\boldsymbol{\hat{x}_i}=(\boldsymbol{x_i},1)$则多元线性回归的模型为</p>
<script type="math/tex; mode=display">
f(\boldsymbol{\hat{x}_i})=\boldsymbol{\hat{x}}_i^T(\boldsymbol{X}^T\boldsymbol{X})^{-1}\boldsymbol{X}^T\boldsymbol{y}</script><p>当属性值超过样本数量时将导致$\boldsymbol{X}$的列数多于行数，此时$\boldsymbol{X}^T\boldsymbol{X}$将不再是满秩矩阵，这将得出多个$\boldsymbol{\hat{w}}$都能使得均方误差最小化，而选择哪一个作为最终的解将由学习算法的归纳偏好<sup id="fnref:7"><a href="#fn:7" rel="footnote">7</a></sup>决定，常见的做法使引入<strong>正则化</strong>(<em>regularization</em>)，同时还可以通过减少特征属性(特征属性之间有强的相关关系)来使得矩阵满秩。</p>
<h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>梯度下降法</strong>通过迭代调整参数来达到最小化损失函数(这里讨论的为均方误差)的目的。其中<strong>梯度</strong>指的是利用梯度方向来最小化损失函数，而梯度方向指的是在此点上升最快的方向，也就是切线方向，只需要沿着这个方向的反方向则能迅速减小损失函数的值而在最后收敛直至稳定。切线的方向可以通过对整个损失函数求导得出，最后的梯度下降的过程如下：</p>
<script type="math/tex; mode=display">
\text{repeat until converage}\{ 
\theta_j := \theta_j - \alpha\frac{\partial J(\theta_j)}{\partial\theta_j}
\}</script><p>其中$J(\theta) = \frac{1}{2}\sum_{i=1}^m(h_\theta\big(x^{(i)}\big)-y^{(i)})^2$，$\alpha$为学习率，决定了每一次下降的步长，$h(x)$是假设函数为</p>
<script type="math/tex; mode=display">
h_\theta(x)=\theta_o+\theta_1x_1+\theta_2x_2+\cdots+\theta_nx_n=\sum_{i=0}^n\theta_ix_i=\theta^Tx</script><p>梯度方向的计算：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial J(\theta_j)}{\partial\theta_j} &=   \frac{\partial}{\partial\theta_j}\frac{1}{2}(h_\theta(x)-y)^2\\
&= 2\cdot\frac{1}{2}(h_\theta(x)-y)\cdot\frac{\partial}{\partial\theta_j}(h_\theta(x)-y)\\
&=(h_\theta(x)-y)\cdot\frac{\partial}{\partial\theta_j}\Big(\sum_{i=0}^n\theta_ix_i-y\Big)\\
&=(h_\theta(x)-y)x_j
\end{aligned}</script><ul>
<li>梯度下降<br><img src="/2019/08/12/机器学习总结/梯度下降.png" alt="梯度下降"><h4 id="梯度下降算法的运行过程"><a href="#梯度下降算法的运行过程" class="headerlink" title="梯度下降算法的运行过程"></a>梯度下降算法的运行过程</h4></li>
</ul>
<ol>
<li>$x_k=a$，沿着负梯度方向移动到$x_{k+1}=b$有<script type="math/tex; mode=display">b=a-\nabla F(a)\implies f(a)\geq f(b)</script></li>
<li>从$x_0$为出发点，每次沿着当前函数梯度反方向移动一定距离$\alpha k$，得到：<script type="math/tex; mode=display">x_0,x_1,x_2,\cdots,x_n</script></li>
<li>对应各点函数值的关系为:<script type="math/tex; mode=display">f(x_0)\geq f(x_1)\geq f(x_2)\geq \cdots\geq f(x_n)</script></li>
<li>当$n$足够大时，$f(x)$将收敛到局部最小值</li>
</ol>
<h3 id="最小二乘法和梯度下降对比"><a href="#最小二乘法和梯度下降对比" class="headerlink" title="最小二乘法和梯度下降对比"></a>最小二乘法和梯度下降对比</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">最小二乘法</th>
<th style="text-align:center">梯度下降</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">不需要选择$\alpha$</td>
<td style="text-align:center">需要选择$\alpha$</td>
</tr>
<tr>
<td style="text-align:center">不需要迭代</td>
<td style="text-align:center">需要多次迭代</td>
</tr>
<tr>
<td style="text-align:center">$O(n^3)$，需要计算$X^TX$的转置</td>
<td style="text-align:center">$O(kn^2)$</td>
</tr>
<tr>
<td style="text-align:center">当特征属性很多时计算很慢</td>
<td style="text-align:center">当特征属性很多时表现不错</td>
</tr>
</tbody>
</table>
</div>
<h2 id="对数几率回归"><a href="#对数几率回归" class="headerlink" title="对数几率回归"></a>对数几率回归</h2><h3 id="对数几率函数"><a href="#对数几率函数" class="headerlink" title="对数几率函数"></a>对数几率函数</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>对数几率函数</strong>是一种Sigmoid函数<sup id="fnref:8"><a href="#fn:8" rel="footnote">8</a></sup>，它将$z$值转化为一个接近0或1的$y$值，并且其输出值在$z=0$附近变化很陡。将线性回归的假设函数$h_\theta(x)=\theta^Tx$代入代入$z$使得$z=\theta^Tx$后即可得到对数几率回归的假设函数。</p>
<ul>
<li>对数几率函数<script type="math/tex; mode=display">g(z)=\frac{1}{1+e^{-z}}</script></li>
<li>对数几率回归假设函数<script type="math/tex; mode=display">h_\theta(x)=\frac{1}{1+e^{(-\theta^Tx)}}</script>将$h_\theta(x)$视为$x$为正例的可能性，$1-h_\theta(x)$视为为反例的可能性，两者的比值$\frac{h_\theta(x)}{1-h_\theta(x)}$则被称为<strong>几率</strong>，这反映了$x$作为正例的相对可能性，对几率取对数可以得到<strong>对数几率</strong>(<em>log odds, logit</em>) $\log\frac{h_\theta(x)}{1-h_\theta(x)}$，即对数几率回归假设函数可一变化为:<script type="math/tex; mode=display">
\begin{aligned}
  \log\frac{h_\theta(x)}{1-h_\theta(x)} &= \log\Bigg(\frac{\frac{1}{1+e^{(-\theta^Tx)}}}{1-\frac{1}{1+e^{(-\theta^Tx)}}}\Bigg)\\
  &=\log\Bigg(\frac{\frac{1}{1+e^{(-\theta^Tx)}}}{\frac{1+e^{(-\theta^Tx)}-1}{1+e^{(-\theta^Tx)}}}\Bigg) \\
  &= \log\Big(\frac{1}{1+e^{(-\theta^Tx)}}\cdot \frac{1+e^{(-\theta^Tx)}}{e^{(-\theta^Tx)}}\Big) \\
  & =\log \big(\frac{1}{e^{(-\theta^Tx)}}\big) \\
  &= \log e^{\theta^Tx} \\
  &= \theta^Tx
\end{aligned}</script></li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;用线性模型的预测结果去逼近真实标记的对数几率对应的模型就称为<strong>对数几率模型</strong>(<em>Llogitstic regression, logit regression</em>)，也称逻辑回归。这是一种分类的学习方法，有以下优点:</p>
<ol>
<li>可以直接对分类进行建模而无需对数据分布进行假设，避免了假设分布不准确造成的问题</li>
<li>除了能预测类别之外还可以的到近似概率预测，可以用于利用概率进行辅助决策的任务</li>
<li>对数函数是任意阶可导的凸函数，可以通过许多数值优化算法直接进行最优解求解。</li>
</ol>
<h3 id="对数似然函数"><a href="#对数似然函数" class="headerlink" title="对数似然函数"></a>对数似然函数</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;逻辑回归使用<strong>极大似然法</strong>进行参数估计。<a href="https://baike.baidu.com/item/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0/6011241?fr=aladdin" target="_blank" rel="noopener"><strong>似然函数</strong></a>指的是在给定输出$x$时，关于参数$\theta$的似然函数$L(\theta|x)$(在数值上)等于给定参数$\theta$后变量$x$的概率$L(\theta|x)=p(X=x|\theta)$。在已经得到实验结果的情况下使用使该结果出现的可能性最大的$X$作为真$X$就是极大似然法。对似然函数取对数之后就是对数似然函数，使用对数可以在不改变函数的单调性的情况下降低指数函数的计算复杂度，提高计算效率。</p>
<ul>
<li>参数估计<br>假定：<script type="math/tex; mode=display">p(y=1|x;\theta)=h_\theta(x)</script><script type="math/tex; mode=display">p(y=0|x;\theta)=1-h_\theta(x)</script>合并后可得:<script type="math/tex; mode=display">
p(y|x;\theta) = \big(h_\theta(x)\big)^y + \big(1-h_\theta(x))\big)^{(1-y)}</script>令$L(\theta)=p(\vec{y}|X;\theta)$，则：<script type="math/tex; mode=display">
\begin{aligned}
  L(\theta) &= \prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta)\\
  &=\prod_{i=1}^m\big(h_\theta(x^{(i)}\big)^{y^{(i)}}\big(1-h_\theta(x^{(i)})\big)^{(1-y^{(i)})}
\end{aligned}</script>取对数得到对数似然函数:<script type="math/tex; mode=display">
\begin{aligned}
  l(\theta) &= \log L(\theta) \\
  &= \sum_{i=1}^m\Big(y^{(i)}\log h(x^{(i)}) + (1-y^{(i)})\log \big(1-h(x^{(i)})\big)\Big)
\end{aligned}</script></li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;式$l(\theta)$是关于$\theta$的高阶可导连续凸函数，其同样可以使用梯度下降法求解最优解。</p>
<ul>
<li>求解<sup id="fnref:9"><a href="#fn:9" rel="footnote">9</a></sup><br>先对$l(\theta)$求导，$h_\theta(x)=g(\theta^Tx)$，$g(z) = \frac{1}{1+e^{-z}}$:<script type="math/tex; mode=display">
\begin{aligned}
 \frac{\partial l(\theta)}{\partial\theta_j} &= \Big(y\frac{1}{g(\theta^Tx)} - (1-y)\frac{1}{1-g(\theta^Tx)}\Big)\frac{\partial}{\partial\theta_j}g(\theta^Tx) \\
 &= \Big(y\frac{1}{g(\theta^Tx)} - (1-y)\frac{1}{1-g(\theta^Tx)}\Big)g(\theta^Tx)(1-g(\theta^Tx))\frac{\partial}{\partial\theta_j}\theta^Tx \\
 &=\big(y(1-g(\theta^Tx))-(1-y)g(\theta^Tx)\big)x_j \\
 &=(y-h_\theta(x))x_j
\end{aligned}</script>最后的到参数优化过程为:<script type="math/tex; mode=display">
repeat\{
  \theta_j := \theta_j -\alpha\frac{\partial}{\partial \theta_j}l(\theta)
  \}</script><script type="math/tex; mode=display">
repeat\{
  \theta_j := \theta_j - \alpha\sum_{i=1}^m\Big(y^{(i)}-h_\theta(x^{(i)})\Big)x_j^{(i)}
  \}</script></li>
</ul>
<h2 id="K近邻算法-KNN"><a href="#K近邻算法-KNN" class="headerlink" title="K近邻算法(KNN)"></a>K近邻算法(KNN)</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>K近邻算法</strong>(<em>KNN</em>)采用测量不同特征值之间的距离<sup id="fnref:10"><a href="#fn:10" rel="footnote">10</a></sup>来进行分类，它通过将新数据的每个特征和样本集中数据对应的特征进行比较，然后通过算法提取样本集中特征最相似的<em>N</em>个数据的实际标签，通过计算(类别标签一般使用投票法，数值型的一般使用平均值)得出该数据的标签值。K近邻算法具有精确度高、对异常值不敏感、无数据输入假定的优点，但是其每次都要计算所有样本与新样本的距离且由于它无法得出模型，所以需要保存所有样本数据，这导致该算法具有极高的时间复杂度和空间复杂度。</p>
<h2 id="支持向量机-SVM"><a href="#支持向量机-SVM" class="headerlink" title="支持向量机(SVM)"></a>支持向量机(SVM)</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>支持向量机</strong>(<em>SVM</em>)是一种基于最大间隔来分隔数据的算法，具有泛化错误率低、计算开销小和结果易解释的优点，但是它对参数调节和核函数的选择敏感。</p>
<h3 id="硬间隔"><a href="#硬间隔" class="headerlink" title="硬间隔"></a>硬间隔</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;分类学习是基于训练集在样本空间中找到一个划分超平面来将不同类别的样本分开，而SVM是找到离正负样本的距离都很大的超平面，即最大间隔。这个划分超平民啊可以被法向量$\boldsymbol{w}$和位移$b$决定，其中法向量$\boldsymbol{w}=(w_1;w_2;\cdots;w_d)$决定了超平面的方向，$b$决定了超平面与原点之间的距离。则划分超平面的公式为:</p>
<script type="math/tex; mode=display">\boldsymbol{w}^T\boldsymbol{x}+b=0</script><p>该公式来源如下:<br><img src="/2019/08/12/机器学习总结/点到平面距离.png" alt="点到平面距离"></p>
<ul>
<li>在$R^3$的空间里，一个平面可以由平面上一个点$P_0$以及一个垂直平面的法向量$w$确定</li>
<li>任意取平面上一个点$P$从原点到$P$，$P_0$做两个向量$x$,$x_0$<br>因为法向量垂直于平面，所以可得:<script type="math/tex; mode=display">
\begin{aligned}
  \vec{w}\cdot(\vec{x}-\vec{x_0})&=0 \\
  \vec{w}\cdot\vec{x}-\vec{w}\cdot\vec{x_0}&=0
\end{aligned}</script>令$b=-\vec{w}\cdot\vec{x_0}$可得:<script type="math/tex; mode=display">\vec{w}\cdot\vec{x}+b=0</script>推广到$R^n$即可得到上式。</li>
</ul>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;假设超平面能将样本正确分类，即对于样本$(x_i,y_i)\in D$，若$y_i=+1$,则有$\boldsymbol{w}^T\boldsymbol{x}_i+b&gt;0$；若$y_i=-11$,则有$\boldsymbol{w}^T\boldsymbol{x}_i+b&lt;0$。令:</p>
<script type="math/tex; mode=display">
\begin{cases}
    \boldsymbol{w}^T\boldsymbol{x}_i + b \ge +1, y_i=+1\\
     \boldsymbol{w}^T\boldsymbol{x}_i + b \le -1, y_i=-1
\end{cases}</script><p>距离超平面最近的几个训练样本点使得上式的等号成立，这几个点被称为<strong>支持向量</strong>(<em>support vector</em>)。两个异类支持向量到超平面的距离之和称为<strong>间隔</strong>(<em>margin</em>)，可由样本空间中任意点$x$到超平面的距离公式推导出来:</p>
<ul>
<li><p>任意点到超平面的距离</p>
<script type="math/tex; mode=display">
r=\frac{|\boldsymbol{w}^T\boldsymbol{x}+b|}{\|\boldsymbol{w}\|}</script><p>此公式可通过点到平面的距离推广</p>
</li>
<li><p>异类支持向量到超平面的距离之和</p>
<script type="math/tex; mode=display">
\gamma=\frac{2}{\|\boldsymbol{w}\|}</script><p>2是因为支持向量取等号所以距离为1，两个之和即为2</p>
</li>
<li><p>支持向量与间隔<br><img src="/2019/08/12/机器学习总结/支持向量与间隔.png" alt="支持向量与间隔"></p>
</li>
</ul>
<h4 id="最优化问题"><a href="#最优化问题" class="headerlink" title="最优化问题"></a>最优化问题</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;寻找划分超平面的最大间隔(<em>maximum margin</em>)即是求解$\boldsymbol{w}$和$b$在满足约束条件的情况下$\gamma$的最大值。</p>
<script type="math/tex; mode=display">
\begin{cases}
    \max_{\boldsymbol{w},b}\frac{2}{\|\boldsymbol{w}\|} \\
    \\
    s.t. \ y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)\ge1 \  i=1,2,\cdots,m
\end{cases}</script><p>其中最大化$\frac{2}{|\boldsymbol{w}|}$等价于最小化$\frac{1}{2}|\boldsymbol{w}|^2$，则上式等价于:</p>
<script type="math/tex; mode=display">
\begin{cases}
    \min_{\boldsymbol{w},b}\frac{1}{2}\|\boldsymbol{w}\|^2 \\
    \\
    s.t. \ y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)\ge1 \  i=1,2,\cdots,m
\end{cases}</script><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;上述问题是一个凸二次优化(<em>convex quadratic programming</em>)问题，可以通过使用拉格朗日乘子法<sup id="fnref:11"><a href="#fn:11" rel="footnote">11</a></sup>来求解其对偶问题(<em>dual problem</em>)。具体做法是对每条约束添加拉格朗日乘子$\alpha_i\ge0$得$\alpha_i(1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b))$，则该问题的拉格朗日函数为:</p>
<script type="math/tex; mode=display">
l(\boldsymbol{w},b, \boldsymbol{\alpha}) = \frac{1}{2}\|\boldsymbol{w}\|^2 + \sum_{i=1}^m\alpha_i(1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b))</script><p>其中$\boldsymbol{\alpha}=(\alpha_1;\alpha_2;\cdots;\alpha_m)$<br>因为原问题是极小极大$\min_{\boldsymbol{w},b}\max_\boldsymbol{\alpha}l(\boldsymbol{w},b, \boldsymbol{\alpha})$，转换成对偶问题则是求极大极小$\max_\boldsymbol{\alpha}\min_{\boldsymbol{w},b}l(\boldsymbol{w},b, \boldsymbol{\alpha})$，所以求极小值先令$l(\boldsymbol{w},b, \boldsymbol{\alpha})$对$\boldsymbol{w}$和$b$的偏导分别为0，然后代入$l(\boldsymbol{w},b, \boldsymbol{\alpha})$后即可:</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \frac{\partial}{\partial\boldsymbol{w}}l(\boldsymbol{w},b, \boldsymbol{\alpha}) &= \boldsymbol{w} - \sum_{i=1}^m\alpha_iy_i\boldsymbol{x}_i =0 \\
    \implies \boldsymbol{w} &= \sum_{i=1}^m\alpha_iy_i\boldsymbol{x}_i
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
    \frac{\partial}{\partial b}l(\boldsymbol{w},b, \boldsymbol{\alpha}) &= \sum_{i=1}^m\alpha_iy_i=0  \\
    \implies 0 &= \sum_{i=1}^m\alpha_iy_i
\end{aligned}</script><p>将得到的结果代入$l(\boldsymbol{w},b, \boldsymbol{\alpha})$进行化解:</p>
<script type="math/tex; mode=display">
\begin{aligned}
    l(\boldsymbol{w},b, \boldsymbol{\alpha}) &= \frac{1}{2}\|\boldsymbol{w}\|^2 + \sum_{i=1}^m\alpha_i(1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)) \\
    &=\frac{1}{2}\boldsymbol{w}^T\boldsymbol{w}+\sum_{i=1}\alpha_i-\boldsymbol{w}^T\sum_1^m\alpha_iy_i\boldsymbol{x}_i-b\sum_{i=1}^m\alpha_iy_i \\
    &=\frac{1}{2}\boldsymbol{w}^T\sum_{i=1}^m\alpha_iy_i\boldsymbol{x}_i+\sum_{i=1}\alpha_i-\boldsymbol{w}^T\sum_1^m\alpha_iy_i\boldsymbol{x}_i-b\cdot0\\
    &= \sum_{i=1}\alpha_i - \frac{1}{2}\Big(\sum_{i=1}^m\alpha_iy_i\boldsymbol{x}_i\Big)^T\sum_{i=1}^m\alpha_iy_i\boldsymbol{x}_i \\
    &=\sum_{i=1}\alpha_i - \frac{1}{2}\sum_{i,j=1}^m\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j
\end{aligned}</script><p>最后得到对偶问题的解为:</p>
<script type="math/tex; mode=display">
\begin{cases}
    \max_{\boldsymbol{\alpha}}\sum_{i=1}^m\alpha_i - \frac{1}{2}\sum_{i,j=1}^m\alpha_i\alpha_jy_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j \\
    \\
    s.t. \sum_{i=1}^m\alpha_iy_i=0 \\
    \\
    \alpha_i \ge0, \ i=1,2,\cdots,m
\end{cases}</script><p>解出$\boldsymbol{\alpha}$后求出$\boldsymbol{w}$和$b$即可得到模型:</p>
<script type="math/tex; mode=display">
f(x) = \sum_{i=1}^m\alpha_iy_i\boldsymbol{x}_i^T\boldsymbol{x}+b</script><p>通过求解对偶问题，将原问题对$\boldsymbol{w}$的求解转化成对$\boldsymbol{\alpha}$的求解，将纬度从特征数量(和$\boldsymbol{x}$的属性一致)转变为样本数量，因为原问题存在不等式约束。所以求解后同样需要满足KKT(<em>Karush-Kuhn-Tucker</em>)条件:</p>
<script type="math/tex; mode=display">
\begin{cases}
    \alpha_i\ge0 \\
    y_if(\boldsymbol{x}_i)-1\ge0\\
    \alpha_i(y_if(\boldsymbol{x}_i)-1)=0
\end{cases}</script><p>由约束条件可知对于任意训练样本$\boldsymbol{x}_i,y_i$总有$\alpha_i=0$或$y_if(\boldsymbol{x}_i)=1$;若$\alpha_i=0$则不会对$f(\boldsymbol{x})$有影响；若$\alpha_i&gt;0$，则一定有$y_if(\boldsymbol{x}_i)=1$，所对应的样本点位于最大间隔边界上是一个支持向量。所以在支持向量训练完之后大部分样本都不需要保留。</p>
<h3 id="软间隔"><a href="#软间隔" class="headerlink" title="软间隔"></a>软间隔</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;硬间隔假定训练样本在样本空间是线性可分的，存在一个超平面将不同类的样本完全划分开。<strong>软间隔</strong>(<em>soft margin</em>)则允许部分样本被错误分类，以此缓解强制线性可分造成的过拟合和解决线性不可分问题。软件隔允许某些样本不满足约束$y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)\ge1$，最后的优化目标将不同于硬间隔:</p>
<script type="math/tex; mode=display">
\min_{\boldsymbol{w},b}\frac{1}{2}\|\boldsymbol{w}\|^2 + C\sum_{i=1}^ml_{0/1}(y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)-1)</script><p>其中$C&gt;0$为常数，当$C$无穷大时上式等同于$\min_{\boldsymbol{w},b}\frac{1}{2}|\boldsymbol{w}|^2$；当$C$取有限值时则表示允许一些样本不满足约束。$l_{0/1}$是0/1损失函数:</p>
<script type="math/tex; mode=display">
l_{0/1}(z) = \left\{
  \begin{array}{lr}
    1 & ,\ z < 0\\
    0 & ,\  其它
  \end{array}
\right.</script><p>$l_{0/1}$具有非凸、非连续性等不太好的数学性质使得软间隔的优化不易求解，所以使用一些<strong>替代损失</strong>(<em>surrogate loss</em>)函数<sup id="fnref:12"><a href="#fn:12" rel="footnote">12</a></sup>来代替$l_{0/1}$，这些函数是凸的连续函数且是$l_{0/1}$的上界:</p>
<ul>
<li>hinge损失<script type="math/tex; mode=display">
l_{hinge}(z)=max(0, 1-z)</script></li>
<li>指数损失(<em>exponential loss</em>)<script type="math/tex; mode=display">
l_{exp}(z)=exp(-z)</script></li>
<li>对率损失(<em>logistic loss</em>)<script type="math/tex; mode=display">
l_{log}(z)=log(1+exp(-z))</script>最后引入<strong>松弛变量</strong>(<em>slack variables</em>)$\varepsilon_i\ge0$即可得到<strong>软间隔支持向量机</strong>，将优化目标重写为:<script type="math/tex; mode=display">
\begin{cases}
  \min_{\boldsymbol{w},b,\varepsilon}\frac{1}{2}\|\boldsymbol{w}\|^2+C\sum_{i=1}^m\varepsilon_i\\
  \\
  s.t. \ y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)\ge 1-\varepsilon_i \\
  \\
  \varepsilon_i\ge0,\ i=1,2,\cdots,m
\end{cases}</script></li>
<li>软间隔<br><img src="/2019/08/12/机器学习总结/软间隔.png" alt="软间隔"></li>
<li>替代损失函数<br><img src="/2019/08/12/机器学习总结/替代损失函数.png" alt="替代损失函数"></li>
</ul>
<h4 id="软间隔的最优化"><a href="#软间隔的最优化" class="headerlink" title="软间隔的最优化"></a>软间隔的最优化</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;对每条约束$y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)\ge 1-\varepsilon_i $添加拉格朗日乘子$\alpha_i\ge0$得$\alpha_i(1-\varepsilon_iy_i(\boldsymbol{w}^T\boldsymbol{x}_i+b))$;对每条约束$\varepsilon_i\ge0$添加拉格朗日乘子$\multimap_i$得$-\mu_i\varepsilon_i$，最后得到拉格朗日函数为:</p>
<script type="math/tex; mode=display">
L(\boldsymbol{w},b,\boldsymbol{\alpha},\boldsymbol{\varepsilon},\boldsymbol{\mu})=\frac{1}{2}\|\boldsymbol{w}\|^2+C\sum_{i=1}^m\varepsilon_i+\sum_{i=1}^m\alpha_i(1-\varepsilon_iy_i(\boldsymbol{w}^T\boldsymbol{x}_i+b))-\sum_{i=1}^m\mu_i\varepsilon_i</script><p>令$L(\boldsymbol{w},b,\boldsymbol{\alpha},\boldsymbol{\varepsilon},\boldsymbol{\mu})$分别对$\boldsymbol{w}$、$b$、$\varepsilon_i$的偏导为零可得:</p>
<script type="math/tex; mode=display">
\begin{aligned}
    \boldsymbol{w}&=\sum_{i=1}^m\alpha_iy_i\boldsymbol{x}_i \\
    0&=\sum_{i=1}^m\alpha_iy_i \\
    C&=\alpha_i+\mu_i
\end{aligned}</script><p>将其代入$L(\boldsymbol{w},b,\boldsymbol{\alpha},\boldsymbol{\varepsilon},\boldsymbol{\mu})$中可得对偶问题:</p>
<script type="math/tex; mode=display">
\begin{cases}
    \max_\alpha \sum_{i=1}^m\alpha_i -\frac{1}{2}\sum_{i,j=1}^m\alpha_i\alpha_jy_iy_i\boldsymbol{x}_i^T\boldsymbol{x}_j \\
    \\
    s.t. \ \sum_{i=1}^m\alpha_iy_i=0 \\
    \\
    0\le\alpha_i\le C, \ i=1,2,\cdots,m
\end{cases}</script><h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;硬间隔解决了严格线性可分的问题，而软间隔解决了非严格线性可分的问题，这些都是在当前样本空间能够找到一个划分超平面能将训练样本正确分类，而<strong>核函数</strong>则是为了解决线性不可分问题，即在当前样本空间找不到这样一个划分超平面来划分训练样（“异或”问题），它将样本从原始空间映射到一个更高维的特征空间使得样本在这个特征空间里线性可分<sup id="fnref:13"><a href="#fn:13" rel="footnote">13</a></sup>。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;令$\phi(\boldsymbol{x})$表示将$\boldsymbol{x}$映射后的特征向量，则在特征空间中划分超平面对应的模型为:</p>
<script type="math/tex; mode=display">
f(x)=\boldsymbol{w}^T\phi(\boldsymbol{x})+b</script><p>同理，优化目标可以写为:</p>
<script type="math/tex; mode=display">
\begin{cases}
    \min_{\boldsymbol{w},b}\frac{1}{2}\|\boldsymbol{w}\|^2 \\
    \\
    s.t. \ y_i\big(\boldsymbol{w}^T\phi(\boldsymbol{x}_i)+b\big)\ge1, \ i=1,2,\cdots,m
\end{cases}</script><p>对偶问题为:</p>
<script type="math/tex; mode=display">
\begin{cases}
    \max_{\boldsymbol{\alpha}}\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i,j=1}^m\alpha_i\alpha_jy_iy_j\phi(\boldsymbol{x}_i)^T\phi(\boldsymbol{x}_j) \\
    \\
    s.t.\ \sum_{i=1}^m\alpha_iy_i=0, \\
    \\
    \alpha_i \ge0, \ i=1,2,\cdots,m
\end{cases}</script><p>式中计算$\phi(\boldsymbol{x}_i)^T\phi(\boldsymbol{x}_j)$是指的样本$\boldsymbol{x}_i$和$\boldsymbol{x}_j$映射到高维特征空间之后的内积，因为映射后的空间维数可能很高（可能无穷维）而导致$\phi(\boldsymbol{x}_i)^T\phi(\boldsymbol{x}_j)$计算困难,所以设计核函数<sup id="fnref:14"><a href="#fn:14" rel="footnote">14</a></sup>如下:</p>
<script type="math/tex; mode=display">
\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j)=\langle\phi(\boldsymbol{x}_i),\phi(\boldsymbol{x}_j)\rangle=\phi(\boldsymbol{x}_i)^T\phi(\boldsymbol{x}_j)</script><p>即$\boldsymbol{x}_i$和$\boldsymbol{x}_j$的内积等于它们在原始样本空间中通过函数$\kappa(\cdot,\cdot)$计算的结果，这样就不用去计算高维特征中的内积。上面的对偶问题则可以改写为如下形式:</p>
<script type="math/tex; mode=display">
\begin{cases}
   \max_{\boldsymbol{\alpha}}\sum_{i=1}^m\alpha_i-\frac{1}{2}\sum_{i,j=1}^m\alpha_i\alpha_jy_iy_j\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j)\\
   \\
   s.t.\ \sum_{i=1}^m\alpha_iy_i=0\\
   \\
   \alpha_i \ge0,\ i=1,2,\cdots,m
\end{cases}</script><p>最后求解得到模型为:</p>
<script type="math/tex; mode=display">
\begin{aligned}
    f(\boldsymbol{x})&=\boldsymbol{w}^T\phi(\boldsymbol{x})+b\\
    &=\sum_{i=1}^m\alpha_iy_i\phi(\boldsymbol{x})^T\phi(\boldsymbol{x})+b\\
    &=\sum_{i=1}^m\alpha_iy_i\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j)+b
\end{aligned}</script><ul>
<li>常见核函数</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">名称</th>
<th style="text-align:center">表达式</th>
<th style="text-align:center">参数</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">线性核</td>
<td style="text-align:center">$\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j)=\boldsymbol{x}_i^T\boldsymbol{x}_j$</td>
</tr>
<tr>
<td style="text-align:center">多项式核</td>
<td style="text-align:center">$\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j)=(\boldsymbol{x}_i^T\boldsymbol{x}_j)^d$</td>
<td style="text-align:center">$d\ge1为多项式的次数，d=1退化为线性核$</td>
</tr>
<tr>
<td style="text-align:center">高斯核(RBF核)</td>
<td style="text-align:center">$\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j)=\exp(-\frac{&#124;\boldsymbol{x}_i-\boldsymbol{x}_j&#124;^2}{2\sigma^2})$</td>
<td style="text-align:center">$\sigma&gt;0$为高斯核的带宽</td>
</tr>
<tr>
<td style="text-align:center">拉普拉斯核</td>
<td style="text-align:center">$\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j)=\exp(-\frac{&#124;\boldsymbol{x}_i-\boldsymbol{x}_j&#124;}{\sigma})$</td>
<td style="text-align:center">$\sigma&gt;0$</td>
</tr>
<tr>
<td style="text-align:center">Sigmoid核</td>
<td style="text-align:center">$\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j)=\tanh(\beta\boldsymbol{x}_i^T\boldsymbol{x}_j+\theta)$</td>
<td style="text-align:center">$\tanh$为双曲正切函数，$\beta&gt;0$,$\theta&lt;0$</td>
</tr>
</tbody>
</table>
</div>
<p>核函数可以相互组合形成新的核函数。</p>
<ul>
<li>异或问题<br><img src="/2019/08/12/机器学习总结/异或问题.png" alt="异或问题"></li>
</ul>
<h3 id="SMO"><a href="#SMO" class="headerlink" title="SMO"></a>SMO</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<strong>SMO</strong>(<em>Sequential Minimal Optimization</em>)表示最小序列优化<sup id="fnref:15"><a href="#fn:15" rel="footnote">15</a></sup>，它将大优化问题分解为多个小优化问题来求解。SMO的目标是求解出一系列的$\alpha$和$b$然后计算权重向量$w$最后得到划分超平面。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;因为在对偶问题中存在约束$\sum_{i=1}^m\alpha_iy_i=0$，若固定$\alpha_i$之外的其它变量则$\alpha_i$可由其它变量导出，所以SMO的基本思路为先固定$\alpha_i$之外的所有参数然后求$\alpha_i$的极值。因为改变一个$\alpha$会使得约束失效，所以SMO选择同时改变两个$\alpha$。则SMO的一般步骤为:</p>
<ol>
<li>选取一对需要更新的变量$\alpha_i$和$\alpha_j$</li>
<li>固定$\alpha_i$和$\alpha_j$以外的参数，求解对偶问题然后获得更新后的$\alpha_i$和$\alpha_j$</li>
<li>重复上面的步骤直至收敛<br>只需要选取$\alpha_i$和$\alpha_j$中有一个不满足KKT条件，目标函数就会在迭代后减小。KKT条件违背的程度越大，则变量更新后可能导致的目标函数值减幅越大。所以SMO先选取违背KKT条件程度最大的变量，第二个变量应选择一个使目标函数值减小最快的变量。SMO采用一种启发式的算法：选取的两变量所对应样本之间的间隔最大。<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;当仅考虑$\alpha_i$和$\alpha_j$时，约束<script type="math/tex; mode=display">
\begin{cases}
 s.t. \sum_{i=1}^m\alpha_iy_i=0 \\
 \\
 \alpha_i \ge0, \ i=1,2,\cdots,m
\end{cases}</script>可重写为:<script type="math/tex; mode=display">
\alpha_iy_i+\alpha_jy_j=c,\ \alpha_i\ge0,\ \alpha_j\ge=0</script>其中<script type="math/tex; mode=display">
c = -\sum_{k\ne i,j}\alpha_ky_k</script>是使$\sum_{i=1}^m\alpha_iy_i=0$的常数，将重写后的约束$\alpha_iy_i+\alpha_jy_j=c$代入原对偶问题可以消去变量$\alpha_j$，最后变成一个关于$\alpha_i$的单变量二次规划问题，并且只剩下一个$\alpha_i\ge0$的约束。这样的问题是具有闭式解的。</li>
</ol>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;偏移量$b$可以通过支持向量求解。对任意的支持向量都有$(\boldsymbol{x}_s,y_s)$都有$y_sf(\boldsymbol{x}_s)=1$，即为:</p>
<script type="math/tex; mode=display">
y_s\Big(\sum_{i\in S}\alpha_iy_i\boldsymbol{x}_i^T\boldsymbol{x}_s+b\Big)=1</script><p>其中$S=\{i|\alpha_i&gt;0,i=1,2,\cdots,m\}$为所有支持向量的下标集。可以选取任意支持向量直接求的$b$，但现实任务中往往使用所有支持向量求解的平均值这一更具有鲁棒性的方法:</p>
<script type="math/tex; mode=display">
b=\frac{1}{|S|}\sum_{s\in S}\Big(y_s-\sum_{i\in S}\alpha_iy_i\boldsymbol{x}_i^T\boldsymbol{x}_s\Big)</script><h1 id="不搞了"><a href="#不搞了" class="headerlink" title="不搞了"></a>不搞了</h1><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style:none; padding-left: 0;"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">1.</span><span style="display: inline-block; vertical-align: top;">即有放回采样</span><a href="#fnref:1" rev="footnote"> ↩</a></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">2.</span><span style="display: inline-block; vertical-align: top;">$\boldsymbol{I}$为指示函数，成立时取值为$1$</span><a href="#fnref:2" rev="footnote"> ↩</a></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">3.</span><span style="display: inline-block; vertical-align: top;">均等代价的度量方式如一般的错误率都是直接计算错误次数</span><a href="#fnref:3" rev="footnote"> ↩</a></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">4.</span><span style="display: inline-block; vertical-align: top;">归一化是指将不同范围的值映射到相同的固定范围中，常见为$[0,1]$</span><a href="#fnref:4" rev="footnote"> ↩</a></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">5.</span><span style="display: inline-block; vertical-align: top;">原文中为$d_{cen}(\boldsymbol{\mu}_i,\boldsymbol{\mu}_j)$，这里应该计算的是两个聚类中心之间的距离，我个人觉得依照上面定义的定义会更好理解，具体可参考<a href="https://en.wikipedia.org/wiki/Davies%E2%80%93Bouldin_index" target="_blank" rel="noopener">维基百科</a></span><a href="#fnref:5" rev="footnote"> ↩</a></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">6.</span><span style="display: inline-block; vertical-align: top;">这里计算的是回归算法中的偏差和方差</span><a href="#fnref:6" rev="footnote"> ↩</a></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">7.</span><span style="display: inline-block; vertical-align: top;">机器学习算法在学习过程中对某种类型假设的偏好称为归纳偏好(<em>inductive bias</em>)</span><a href="#fnref:7" rev="footnote"> ↩</a></li><li id="fn:8"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">8.</span><span style="display: inline-block; vertical-align: top;">Sigmoid函数指的是形似$S$的函数</span><a href="#fnref:8" rev="footnote"> ↩</a></li><li id="fn:9"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">9.</span><span style="display: inline-block; vertical-align: top;">$g'(z) = (\frac{1}{1+e^(-z)})'= \frac{e^{-z}}{(1+e^{-z})^2}$，将$g(z) = \frac{1}{1+e^{-z}}$代入简化后得$g'(z)=g(z)\big(1-g(z)\big)$</span><a href="#fnref:9" rev="footnote"> ↩</a></li><li id="fn:10"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">10.</span><span style="display: inline-block; vertical-align: top;">距离计算见<a href="http://coldjune.com/2019/08/12/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/#%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97">模型评估与选择-性能度量-聚类距离计算</a>,这里一般用的是欧式距离</span><a href="#fnref:10" rev="footnote"> ↩</a></li><li id="fn:11"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">11.</span><span style="display: inline-block; vertical-align: top;"><a href="https://baike.baidu.com/item/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0%E6%B3%95/8550443?fr=aladdin" target="_blank" rel="noopener">拉格朗日乘子法</a>将一个有n个变量与k个约束条件的最优化问题转换为一个有n + k个变量的方程组的极值问题，其变量不受任何约束</span><a href="#fnref:11" rev="footnote"> ↩</a></li><li id="fn:12"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">12.</span><span style="display: inline-block; vertical-align: top;">采用hinge损失则软间隔优化目标变为$\min_{\boldsymbol{w},b}\frac{1}{2}|\boldsymbol{w}|^2+C\sum_{i=1}^m\max(0,1-y_i(\boldsymbol{w}^Tx_i+b))$</span><a href="#fnref:12" rev="footnote"> ↩</a></li><li id="fn:13"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">13.</span><span style="display: inline-block; vertical-align: top;">如果原始空间是有限维则一定存在一个高维特征空间使样本可分</span><a href="#fnref:13" rev="footnote"> ↩</a></li><li id="fn:14"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">14.</span><span style="display: inline-block; vertical-align: top;">令$\chi$为输入空间，$\kappa(\cdot,\cdot)$是定义在$\chi\times\chi$上的对称函数，则$\kappa$是核函数当且仅当对于任意数据$D={\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_m}$，核矩阵(<em>kernel matrix</em>)$K$是半正定的。即只要一个对称函数所对应的核矩阵是半正定的，它就能作为<a href="https://baike.baidu.com/item/%E6%A0%B8%E5%87%BD%E6%95%B0/4693132?fr=aladdin" target="_blank" rel="noopener">核函数</a>使用。</span><a href="#fnref:14" rev="footnote"> ↩</a></li><li id="fn:15"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">15.</span><span style="display: inline-block; vertical-align: top;">SMO的实现可以参考<a href="http://coldjune.com/2018/05/22/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-SVM/">支持向量机(SVM)</a>，这是根据机器学习实战写的博文。</span><a href="#fnref:15" rev="footnote"> ↩</a></li></ol></div></div>
      
    </div>
    
    
    
		<div>
		  
		    <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-apple"></i>感谢您的阅读-------------</div>
    
</div>
		  
		</div>
		<div>
      
        
<div class="my_post_copyright">
  <script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

  <!-- JS库 sweetalert 可修改路径 -->
  <script src="https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js"></script>
  <script src="https://unpkg.com/sweetalert/dist/sweetalert.min.js"></script>
  <p><span>本文标题:</span><a href="/2019/08/12/机器学习总结/">机器学习总结</a></p>
  <p><span>文章作者:</span><a href="/" title="访问 邓小俊 的个人博客">邓小俊</a></p>
  <p><span>发布时间:</span>2019年08月12日 - 20:08</p>
  <p><span>最后更新:</span>2021年10月24日 - 13:10</p>
  <p><span>原始链接:</span><a href="/2019/08/12/机器学习总结/" title="机器学习总结">http://coldjune.com/2019/08/12/机器学习总结/</a>
    <span class="copy-path"  title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="http://coldjune.com/2019/08/12/机器学习总结/"  aria-label="复制成功！"></i></span>
  </p>
  <p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p>  
</div>
<script> 
    var clipboard = new Clipboard('.fa-clipboard');
      $(".fa-clipboard").click(function(){
      clipboard.on('success', function(){
        swal({   
          title: "",   
          text: '复制成功',
          icon: "success", 
          showConfirmButton: true
          });
        });
    });  
</script>

      
		</div>
    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a>
          
        </div>
      

      
      
        <div class="post-widgets">
        

        

        
          
          <div id="needsharebutton-postbottom">
            <span class="btn">
              <i class="fa fa-share-alt" aria-hidden="true"></i>
            </span>
          </div>
        
        </div>
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/06/27/House-Prices-Advanced-Regression-Techniques-2/" rel="next" title="House Prices: Advanced Regression Techniques(2)">
                <i class="fa fa-chevron-left"></i> House Prices: Advanced Regression Techniques(2)
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/02/08/Spring脑图/" rel="prev" title="Spring脑图">
                Spring脑图 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">邓小俊</p>
              <p class="site-description motion-element" itemprop="description">人所恐惧的不是孤独本身，而是害怕承认人本生而孤独的事实</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">70</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">57</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#模型评估与选择"><span class="nav-number">1.</span> <span class="nav-text">模型评估与选择</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#误差"><span class="nav-number">1.1.</span> <span class="nav-text">误差</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模型评估"><span class="nav-number">1.2.</span> <span class="nav-text">模型评估</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#性能度量"><span class="nav-number">1.3.</span> <span class="nav-text">性能度量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#回归"><span class="nav-number">1.3.1.</span> <span class="nav-text">回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分类"><span class="nav-number">1.3.2.</span> <span class="nav-text">分类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#错误率与精度"><span class="nav-number">1.3.2.1.</span> <span class="nav-text">错误率与精度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#查准率-准确率-、查全率-召回率-和F1"><span class="nav-number">1.3.2.2.</span> <span class="nav-text">查准率(准确率)、查全率(召回率)和F1</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ROC与AUC"><span class="nav-number">1.3.2.3.</span> <span class="nav-text">ROC与AUC</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#代价敏感错误率和代价曲线"><span class="nav-number">1.3.2.4.</span> <span class="nav-text">代价敏感错误率和代价曲线</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#聚类"><span class="nav-number">1.3.3.</span> <span class="nav-text">聚类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#外部指标"><span class="nav-number">1.3.3.1.</span> <span class="nav-text">外部指标</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#内部指标"><span class="nav-number">1.3.3.2.</span> <span class="nav-text">内部指标</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#距离计算"><span class="nav-number">1.3.3.3.</span> <span class="nav-text">距离计算</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#比较检验"><span class="nav-number">1.4.</span> <span class="nav-text">比较检验</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#偏差和方差"><span class="nav-number">1.5.</span> <span class="nav-text">偏差和方差</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#学习曲线"><span class="nav-number">1.5.1.</span> <span class="nav-text">学习曲线</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#特征工程"><span class="nav-number">2.</span> <span class="nav-text">特征工程</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据标准化"><span class="nav-number">2.1.</span> <span class="nav-text">数据标准化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Min-Max标准化"><span class="nav-number">2.1.1.</span> <span class="nav-text">Min-Max标准化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Z-score标准化"><span class="nav-number">2.1.2.</span> <span class="nav-text">Z-score标准化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#小数定标规范法"><span class="nav-number">2.1.3.</span> <span class="nav-text">小数定标规范法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#离散属性处理"><span class="nav-number">2.1.4.</span> <span class="nav-text">离散属性处理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#缺失值"><span class="nav-number">2.2.</span> <span class="nav-text">缺失值</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#缺失值分类"><span class="nav-number">2.2.1.</span> <span class="nav-text">缺失值分类</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#处理方式"><span class="nav-number">2.2.2.</span> <span class="nav-text">处理方式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#删除"><span class="nav-number">2.2.2.1.</span> <span class="nav-text">删除</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#补齐"><span class="nav-number">2.2.2.2.</span> <span class="nav-text">补齐</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#不处理"><span class="nav-number">2.2.2.3.</span> <span class="nav-text">不处理</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#异常值"><span class="nav-number">2.3.</span> <span class="nav-text">异常值</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#异常值检测方法"><span class="nav-number">2.3.1.</span> <span class="nav-text">异常值检测方法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#简单统计"><span class="nav-number">2.3.1.1.</span> <span class="nav-text">简单统计</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#箱线图"><span class="nav-number">2.3.1.1.1.</span> <span class="nav-text">箱线图</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-sigma-原则"><span class="nav-number">2.3.1.2.</span> <span class="nav-text">$3\sigma$原则</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用距离检测多元离群点"><span class="nav-number">2.3.1.3.</span> <span class="nav-text">使用距离检测多元离群点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#异常值的处理"><span class="nav-number">2.3.2.</span> <span class="nav-text">异常值的处理</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#监督学习"><span class="nav-number">3.</span> <span class="nav-text">监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#线性回归"><span class="nav-number">3.1.</span> <span class="nav-text">线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#性能度量-1"><span class="nav-number">3.1.1.</span> <span class="nav-text">性能度量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#最小二乘法"><span class="nav-number">3.1.2.</span> <span class="nav-text">最小二乘法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#单元线性回归"><span class="nav-number">3.1.2.1.</span> <span class="nav-text">单元线性回归</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#多元线性回归"><span class="nav-number">3.1.2.2.</span> <span class="nav-text">多元线性回归</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降法"><span class="nav-number">3.1.3.</span> <span class="nav-text">梯度下降法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度下降算法的运行过程"><span class="nav-number">3.1.3.1.</span> <span class="nav-text">梯度下降算法的运行过程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#最小二乘法和梯度下降对比"><span class="nav-number">3.1.4.</span> <span class="nav-text">最小二乘法和梯度下降对比</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#对数几率回归"><span class="nav-number">3.2.</span> <span class="nav-text">对数几率回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#对数几率函数"><span class="nav-number">3.2.1.</span> <span class="nav-text">对数几率函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对数似然函数"><span class="nav-number">3.2.2.</span> <span class="nav-text">对数似然函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#K近邻算法-KNN"><span class="nav-number">3.3.</span> <span class="nav-text">K近邻算法(KNN)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#支持向量机-SVM"><span class="nav-number">3.4.</span> <span class="nav-text">支持向量机(SVM)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#硬间隔"><span class="nav-number">3.4.1.</span> <span class="nav-text">硬间隔</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#最优化问题"><span class="nav-number">3.4.1.1.</span> <span class="nav-text">最优化问题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#软间隔"><span class="nav-number">3.4.2.</span> <span class="nav-text">软间隔</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#软间隔的最优化"><span class="nav-number">3.4.2.1.</span> <span class="nav-text">软间隔的最优化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#核函数"><span class="nav-number">3.4.3.</span> <span class="nav-text">核函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SMO"><span class="nav-number">3.4.4.</span> <span class="nav-text">SMO</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#不搞了"><span class="nav-number">4.</span> <span class="nav-text">不搞了</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2018 &mdash; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">邓小俊</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">博文包含字数&#58;</span>
    
    <span title="博文包含字数">277.7k</span>
  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    
      <div id="needsharebutton-float">
        <span class="btn">
          <i class="fa fa-share-alt" aria-hidden="true"></i>
        </span>
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("EdodKu35EbSo4VoLhJYQMd09-gzGzoHsz", "qpErN7JcsIzusWGMNuw1QcxN");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  
  
  <link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css">

  
  
  <script src="/lib/needsharebutton/needsharebutton.js"></script>

  <script>
    
      pbOptions = {};
      
          pbOptions.iconStyle = "box";
      
          pbOptions.boxForm = "horizontal";
      
          pbOptions.position = "bottomCenter";
      
          pbOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-postbottom', pbOptions);
    
    
      flOptions = {};
      
          flOptions.iconStyle = "box";
      
          flOptions.boxForm = "horizontal";
      
          flOptions.position = "middleRight";
      
          flOptions.networks = "Weibo,Wechat,Douban,QQZone,Twitter,Facebook";
      
      new needShareButton('#needsharebutton-float', flOptions);
    
  </script>

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

 
</body>
</html>
